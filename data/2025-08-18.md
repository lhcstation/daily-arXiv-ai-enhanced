<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 94]
- [cs.CL](#cs.CL) [Total: 44]
- [cs.RO](#cs.RO) [Total: 30]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Privacy Enhancement for Gaze Data Using a Noise-Infused Autoencoder](https://arxiv.org/abs/2508.10918)
*Samantha Aziz,Oleg Komogortsev*

Main category: cs.CV

TL;DR: 提出一种基于潜在噪声自编码器的隐私增强机制，保护用户视线数据隐私，同时保持数据可用性。


<details>
  <summary>Details</summary>
Motivation: 解决视线数据在跨会话使用中的隐私问题，防止未经同意的用户再识别。

Method: 使用潜在噪声自编码器，平衡隐私保护和数据实用性。

Result: 显著降低生物识别可能性，同时最小化实用性损失。

Conclusion: 该机制在保护隐私的同时，保留了视线数据的可用性，为视线系统隐私保护提供了有效方案。

Abstract: We present a privacy-enhancing mechanism for gaze signals using a
latent-noise autoencoder that prevents users from being re-identified across
play sessions without their consent, while retaining the usability of the data
for benign tasks. We evaluate privacy-utility trade-offs across biometric
identification and gaze prediction tasks, showing that our approach
significantly reduces biometric identifiability with minimal utility
degradation. Unlike prior methods in this direction, our framework retains
physiologically plausible gaze patterns suitable for downstream use, which
produces favorable privacy-utility trade-off. This work advances privacy in
gaze-based systems by providing a usable and effective mechanism for protecting
sensitive gaze data.

</details>


### [2] [A Survey on Video Temporal Grounding with Multimodal Large Language Model](https://arxiv.org/abs/2508.10922)
*Jianlong Wu,Wei Liu,Ye Liu,Meng Liu,Liqiang Nie,Zhouchen Lin,Chang Wen Chen*

Main category: cs.CV

TL;DR: 本文综述了基于多模态大语言模型（MLLMs）的视频时间定位（VTG）研究，通过三维分类法系统分析了当前研究，并讨论了数据集、评估方法和未来方向。


<details>
  <summary>Details</summary>
Motivation: 填补关于VTG-MLLMs的全面综述的空白，推动视频时间定位领域的发展。

Method: 采用三维分类法：1) MLLMs的功能角色；2) 训练范式；3) 视频特征处理技术。

Result: 总结了VTG-MLLMs的现状、性能优势和泛化能力，并提出了未来研究方向。

Conclusion: VTG-MLLMs在视频时间定位中表现优异，但仍存在局限性，未来需进一步探索。

Abstract: The recent advancement in video temporal grounding (VTG) has significantly
enhanced fine-grained video understanding, primarily driven by multimodal large
language models (MLLMs). With superior multimodal comprehension and reasoning
abilities, VTG approaches based on MLLMs (VTG-MLLMs) are gradually surpassing
traditional fine-tuned methods. They not only achieve competitive performance
but also excel in generalization across zero-shot, multi-task, and multi-domain
settings. Despite extensive surveys on general video-language understanding,
comprehensive reviews specifically addressing VTG-MLLMs remain scarce. To fill
this gap, this survey systematically examines current research on VTG-MLLMs
through a three-dimensional taxonomy: 1) the functional roles of MLLMs,
highlighting their architectural significance; 2) training paradigms, analyzing
strategies for temporal reasoning and task adaptation; and 3) video feature
processing techniques, which determine spatiotemporal representation
effectiveness. We further discuss benchmark datasets, evaluation protocols, and
summarize empirical findings. Finally, we identify existing limitations and
propose promising research directions. For additional resources and details,
readers are encouraged to visit our repository at
https://github.com/ki-lw/Awesome-MLLMs-for-Video-Temporal-Grounding.

</details>


### [3] [VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By \underline{V}alue \underline{S}ign \underline{F}lip](https://arxiv.org/abs/2508.10931)
*Wenqi Guo,Shan Du*

Main category: cs.CV

TL;DR: VSF是一种通过翻转负提示的注意力值符号来动态抑制不需要内容的方法，适用于少步扩散和流匹配图像生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如CFG、NASA、NAG）在负提示引导方面存在不足，VSF旨在以低计算开销实现更高效的负提示引导。

Method: VSF通过翻转负提示的注意力值符号动态抑制不需要的内容，适用于MMDiT架构和基于交叉注意力的模型。

Result: 实验表明，VSF在少步模型中显著优于现有方法，在非少步模型中甚至优于CFG，同时保持图像质量。

Conclusion: VSF是一种高效且通用的负提示引导方法，适用于多种图像生成任务。

Abstract: We introduce Value Sign Flip (VSF), a simple and efficient method for
incorporating negative prompt guidance in few-step diffusion and flow-matching
image generation models. Unlike existing approaches such as classifier-free
guidance (CFG), NASA, and NAG, VSF dynamically suppresses undesired content by
flipping the sign of attention values from negative prompts. Our method
requires only small computational overhead and integrates effectively with
MMDiT-style architectures such as Stable Diffusion 3.5 Turbo, as well as
cross-attention-based models like Wan. We validate VSF on challenging datasets
with complex prompt pairs and demonstrate superior performance in both static
image and video generation tasks. Experimental results show that VSF
significantly improves negative prompt adherence compared to prior methods in
few-step models, and even CFG in non-few-step models, while maintaining
competitive image quality. Code and ComfyUI node are available in
https://github.com/weathon/VSF/tree/main.

</details>


### [4] [Relative Pose Regression with Pose Auto-Encoders: Enhancing Accuracy and Data Efficiency for Retail Applications](https://arxiv.org/abs/2508.10933)
*Yoli Shavit,Yosi Keller*

Main category: cs.CV

TL;DR: 论文提出了一种基于相机姿态自动编码器（PAE）的相对姿态回归（RPR）方法，用于提升单图像绝对姿态回归（APR）的定位精度，无需额外存储图像或姿态数据。


<details>
  <summary>Details</summary>
Motivation: 现代零售环境中，精确的相机定位对提升客户体验、优化库存管理和实现自主操作至关重要。现有APR方法虽有效，但结合视觉和空间场景先验的方法通常更准确。

Method: 扩展PAE至RPR任务，提出一种新的重定位方案，利用PAE-based RPR优化APR预测。首先验证PAE-based RPR的有效性，再通过实验展示其提升APR定位精度的能力。

Result: 在室内基准测试中，该方法显著提升了APR的定位精度，且仅需30%的训练数据即可达到竞争性性能，大幅减少数据收集负担。

Conclusion: PAE-based RPR是一种高效且数据友好的方法，适用于零售环境中的相机定位任务。

Abstract: Accurate camera localization is crucial for modern retail environments,
enabling enhanced customer experiences, streamlined inventory management, and
autonomous operations. While Absolute Pose Regression (APR) from a single image
offers a promising solution, approaches that incorporate visual and spatial
scene priors tend to achieve higher accuracy. Camera Pose Auto-Encoders (PAEs)
have recently been introduced to embed such priors into APR. In this work, we
extend PAEs to the task of Relative Pose Regression (RPR) and propose a novel
re-localization scheme that refines APR predictions using PAE-based RPR,
without requiring additional storage of images or pose data. We first introduce
PAE-based RPR and establish its effectiveness by comparing it with image-based
RPR models of equivalent architectures. We then demonstrate that our refinement
strategy, driven by a PAE-based RPR, enhances APR localization accuracy on
indoor benchmarks. Notably, our method is shown to achieve competitive
performance even when trained with only 30% of the data, substantially reducing
the data collection burden for retail deployment. Our code and pre-trained
models are available at: https://github.com/yolish/camera-pose-auto-encoders

</details>


### [5] [ViPE: Video Pose Engine for 3D Geometric Perception](https://arxiv.org/abs/2508.10934)
*Jiahui Huang,Qunjie Zhou,Hesam Rabeti,Aleksandr Korovko,Huan Ling,Xuanchi Ren,Tianchang Shen,Jun Gao,Dmitry Slepichev,Chen-Hsuan Lin,Jiawei Ren,Kevin Xie,Joydeep Biswas,Laura Leal-Taixe,Sanja Fidler*

Main category: cs.CV

TL;DR: ViPE是一种高效视频处理引擎，用于从无约束原始视频中估计相机内参、运动和密集深度图，支持多种场景和相机模型，性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决从野外视频中获取一致且精确的3D注释的挑战，以支持空间AI系统的发展。

Method: ViPE通过高效算法估计相机内参、运动和密集深度图，适用于动态自拍视频、电影镜头等多种场景。

Result: ViPE在TUM/KITTI序列上表现优于现有基线18%/50%，并在单GPU上以3-5FPS运行。

Conclusion: ViPE及其标注的大规模数据集开源，旨在加速空间AI系统的开发。

Abstract: Accurate 3D geometric perception is an important prerequisite for a wide
range of spatial AI systems. While state-of-the-art methods depend on
large-scale training data, acquiring consistent and precise 3D annotations from
in-the-wild videos remains a key challenge. In this work, we introduce ViPE, a
handy and versatile video processing engine designed to bridge this gap. ViPE
efficiently estimates camera intrinsics, camera motion, and dense, near-metric
depth maps from unconstrained raw videos. It is robust to diverse scenarios,
including dynamic selfie videos, cinematic shots, or dashcams, and supports
various camera models such as pinhole, wide-angle, and 360{\deg} panoramas. We
have benchmarked ViPE on multiple benchmarks. Notably, it outperforms existing
uncalibrated pose estimation baselines by 18%/50% on TUM/KITTI sequences, and
runs at 3-5FPS on a single GPU for standard input resolutions. We use ViPE to
annotate a large-scale collection of videos. This collection includes around
100K real-world internet videos, 1M high-quality AI-generated videos, and 2K
panoramic videos, totaling approximately 96M frames -- all annotated with
accurate camera poses and dense depth maps. We open-source ViPE and the
annotated dataset with the hope of accelerating the development of spatial AI
systems.

</details>


### [6] [HQ-OV3D: A High Box Quality Open-World 3D Detection Framework based on Diffision Model](https://arxiv.org/abs/2508.10935)
*Qi Liu,Yabei Li,Hongsong Wang,Lei He*

Main category: cs.CV

TL;DR: HQ-OV3D框架通过高质量伪标签生成和改进，提升开放词汇3D检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统封闭集3D检测无法满足开放世界应用需求，现有方法在几何质量上存在不足。

Method: 提出IMCV提案生成器和ACA去噪器，利用跨模态几何一致性和标注类几何先验改进伪标签质量。

Result: 在新型类上mAP提升7.37%，伪标签质量显著优于现有方法。

Conclusion: HQ-OV3D可作为独立检测器或插件，提升开放词汇检测流程的伪标签质量。

Abstract: Traditional closed-set 3D detection frameworks fail to meet the demands of
open-world applications like autonomous driving. Existing open-vocabulary 3D
detection methods typically adopt a two-stage pipeline consisting of
pseudo-label generation followed by semantic alignment. While vision-language
models (VLMs) recently have dramatically improved the semantic accuracy of
pseudo-labels, their geometric quality, particularly bounding box precision,
remains commonly neglected.To address this issue, we propose a High Box Quality
Open-Vocabulary 3D Detection (HQ-OV3D) framework, dedicated to generate and
refine high-quality pseudo-labels for open-vocabulary classes. The framework
comprises two key components: an Intra-Modality Cross-Validated (IMCV) Proposal
Generator that utilizes cross-modality geometric consistency to generate
high-quality initial 3D proposals, and an Annotated-Class Assisted (ACA)
Denoiser that progressively refines 3D proposals by leveraging geometric priors
from annotated categories through a DDIM-based denoising mechanism.Compared to
the state-of-the-art method, training with pseudo-labels generated by our
approach achieves a 7.37% improvement in mAP on novel classes, demonstrating
the superior quality of the pseudo-labels produced by our framework. HQ-OV3D
can serve not only as a strong standalone open-vocabulary 3D detector but also
as a plug-in high-quality pseudo-label generator for existing open-vocabulary
detection or annotation pipelines.

</details>


### [7] [Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy Prediction](https://arxiv.org/abs/2508.10936)
*Cheng Chen,Hao Huang,Saurabh Bagchi*

Main category: cs.CV

TL;DR: 提出了一种基于稀疏3D语义高斯泼溅的协作3D语义占用预测方法，解决了现有方法通信成本高或依赖深度估计的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉方法在3D语义占用预测中依赖密集3D体素或2D平面特征，导致高通信成本或额外监督需求，限制了协作场景的适用性。

Method: 通过共享和融合中间高斯基元，实现跨代理融合、几何与语义联合编码，以及稀疏对象中心消息传递。

Result: 在mIoU和IoU上分别比单代理和基线协作方法提升8.42/3.28和5.11/22.41点，通信量减少至34.6%时仍保持性能优势。

Conclusion: 该方法在有限通信预算下表现出鲁棒性能，为协作感知提供了高效解决方案。

Abstract: Collaborative perception enables connected vehicles to share information,
overcoming occlusions and extending the limited sensing range inherent in
single-agent (non-collaborative) systems. Existing vision-only methods for 3D
semantic occupancy prediction commonly rely on dense 3D voxels, which incur
high communication costs, or 2D planar features, which require accurate depth
estimation or additional supervision, limiting their applicability to
collaborative scenarios. To address these challenges, we propose the first
approach leveraging sparse 3D semantic Gaussian splatting for collaborative 3D
semantic occupancy prediction. By sharing and fusing intermediate Gaussian
primitives, our method provides three benefits: a neighborhood-based
cross-agent fusion that removes duplicates and suppresses noisy or inconsistent
Gaussians; a joint encoding of geometry and semantics in each primitive, which
reduces reliance on depth supervision and allows simple rigid alignment; and
sparse, object-centric messages that preserve structural information while
reducing communication volume. Extensive experiments demonstrate that our
approach outperforms single-agent perception and baseline collaborative methods
by +8.42 and +3.28 points in mIoU, and +5.11 and +22.41 points in IoU,
respectively. When further reducing the number of transmitted Gaussians, our
method still achieves a +1.9 improvement in mIoU, using only 34.6%
communication volume, highlighting robust performance under limited
communication budgets.

</details>


### [8] [Personalized Face Super-Resolution with Identity Decoupling and Fitting](https://arxiv.org/abs/2508.10937)
*Jiarui Yang,Hang Guo,Wen Huang,Tao Dai,Shutao Xia*

Main category: cs.CV

TL;DR: 论文提出了一种名为IDFSR的新方法，用于在极端退化场景下提升人脸超分辨率（FSR）的身份（ID）恢复能力，同时减少幻觉效应。


<details>
  <summary>Details</summary>
Motivation: 现有FSR方法在极端退化（如放大倍数>8倍）时，常因输入图像中关键属性和ID信息丢失而生成缺乏真实ID约束的幻觉人脸。

Method: IDFSR通过掩码、变形参考图像和利用GT图像的ID嵌入，设计了一种身份解耦与拟合方法，包括预训练扩散模型和轻量级微调。

Result: 实验表明，IDFSR在极端退化条件下显著优于现有方法，尤其在ID一致性上表现优异。

Conclusion: IDFSR通过身份解耦与拟合，有效提升了极端退化场景下的ID恢复能力和图像感知质量。

Abstract: In recent years, face super-resolution (FSR) methods have achieved remarkable
progress, generally maintaining high image fidelity and identity (ID)
consistency under standard settings. However, in extreme degradation scenarios
(e.g., scale $> 8\times$), critical attributes and ID information are often
severely lost in the input image, making it difficult for conventional models
to reconstruct realistic and ID-consistent faces. Existing methods tend to
generate hallucinated faces under such conditions, producing restored images
lacking authentic ID constraints. To address this challenge, we propose a novel
FSR method with Identity Decoupling and Fitting (IDFSR), designed to enhance ID
restoration under large scaling factors while mitigating hallucination effects.
Our approach involves three key designs: 1) \textbf{Masking} the facial region
in the low-resolution (LR) image to eliminate unreliable ID cues; 2)
\textbf{Warping} a reference image to align with the LR input, providing style
guidance; 3) Leveraging \textbf{ID embeddings} extracted from ground truth (GT)
images for fine-grained ID modeling and personalized adaptation. We first
pretrain a diffusion-based model to explicitly decouple style and ID by forcing
it to reconstruct masked LR face regions using both style and identity
embeddings. Subsequently, we freeze most network parameters and perform
lightweight fine-tuning of the ID embedding using a small set of target ID
images. This embedding encodes fine-grained facial attributes and precise ID
information, significantly improving both ID consistency and perceptual
quality. Extensive quantitative evaluations and visual comparisons demonstrate
that the proposed IDFSR substantially outperforms existing approaches under
extreme degradation, particularly achieving superior performance on ID
consistency.

</details>


### [9] [Deep Learning for Automated Identification of Vietnamese Timber Species: A Tool for Ecological Monitoring and Conservation](https://arxiv.org/abs/2508.10938)
*Tianyu Song,Van-Doan Duong,Thi-Phuong Le,Ton Viet Ta*

Main category: cs.CV

TL;DR: 该研究利用深度学习技术自动分类越南常见的十种木材，通过评估五种卷积神经网络架构，发现ShuffleNetV2在性能和计算效率上表现最佳，准确率达99.29%。


<details>
  <summary>Details</summary>
Motivation: 传统木材分类方法依赖人工且耗时，需要专业知识，因此研究探索深度学习以实现高效、自动化的木材分类。

Method: 构建了自定义图像数据集，并评估了五种卷积神经网络架构（ResNet50、EfficientNet、MobileViT、MobileNetV3和ShuffleNetV2）。

Result: ShuffleNetV2表现最佳，平均准确率为99.29%，F1分数为99.35%。

Conclusion: 轻量级深度学习模型在资源有限环境中具有实时、高精度木材分类的潜力，为生态信息学提供了可扩展的图像解决方案。

Abstract: Accurate identification of wood species plays a critical role in ecological
monitoring, biodiversity conservation, and sustainable forest management.
Traditional classification approaches relying on macroscopic and microscopic
inspection are labor-intensive and require expert knowledge. In this study, we
explore the application of deep learning to automate the classification of ten
wood species commonly found in Vietnam. A custom image dataset was constructed
from field-collected wood samples, and five state-of-the-art convolutional
neural network architectures--ResNet50, EfficientNet, MobileViT, MobileNetV3,
and ShuffleNetV2--were evaluated. Among these, ShuffleNetV2 achieved the best
balance between classification performance and computational efficiency, with
an average accuracy of 99.29\% and F1-score of 99.35\% over 20 independent
runs. These results demonstrate the potential of lightweight deep learning
models for real-time, high-accuracy species identification in
resource-constrained environments. Our work contributes to the growing field of
ecological informatics by providing scalable, image-based solutions for
automated wood classification and forest biodiversity assessment.

</details>


### [10] [NIRMAL Pooling: An Adaptive Max Pooling Approach with Non-linear Activation for Enhanced Image Classification](https://arxiv.org/abs/2508.10940)
*Nirmal Gaud,Krishna Kumar Jha,Jhimli Adhikari,Adhini Nasarin P S,Joydeep Das,Samarth S Deshpande,Nitasha Barara,Vaduguru Venkata Ramya,Santu Saha,Mehmet Tarik Baran,Sarangi Venkateshwarlu,Anusha M D,Surej Mouli,Preeti Katiyar,Vipin Kumar Chaudhary*

Main category: cs.CV

TL;DR: NIRMAL Pooling是一种新型的CNN池化层，结合自适应最大池化和非线性激活函数，提升图像分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统池化方法在复杂数据集上表现有限，需要更灵活和鲁棒的池化技术。

Method: NIRMAL Pooling动态调整池化参数，结合ReLU激活函数，提升特征表达能力。

Result: 在MNIST Digits、MNIST Fashion和CIFAR-10数据集上，NIRMAL Pooling均优于标准Max Pooling。

Conclusion: NIRMAL Pooling为图像识别任务提供了更灵活可靠的池化方法。

Abstract: This paper presents NIRMAL Pooling, a novel pooling layer for Convolutional
Neural Networks (CNNs) that integrates adaptive max pooling with non-linear
activation function for image classification tasks. The acronym NIRMAL stands
for Non-linear Activation, Intermediate Aggregation, Reduction, Maximum,
Adaptive, and Localized. By dynamically adjusting pooling parameters based on
desired output dimensions and applying a Rectified Linear Unit (ReLU)
activation post-pooling, NIRMAL Pooling improves robustness and feature
expressiveness. We evaluated its performance against standard Max Pooling on
three benchmark datasets: MNIST Digits, MNIST Fashion, and CIFAR-10. NIRMAL
Pooling achieves test accuracies of 99.25% (vs. 99.12% for Max Pooling) on
MNIST Digits, 91.59% (vs. 91.44%) on MNIST Fashion, and 70.49% (vs. 68.87%) on
CIFAR-10, demonstrating consistent improvements, particularly on complex
datasets. This work highlights the potential of NIRMAL Pooling to enhance CNN
performance in diverse image recognition tasks, offering a flexible and
reliable alternative to traditional pooling methods.

</details>


### [11] [Topological Structure Description for Artcode Detection Using the Shape of Orientation Histogram](https://arxiv.org/abs/2508.10942)
*Liming Xu,Dave Towey,Andrew P. French,Steve Benford*

Main category: cs.CV

TL;DR: 论文研究了Artcodes的检测问题，提出了一种新的特征描述符（形状方向直方图）来识别拓扑结构相似的装饰性标记，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着智能手机和VR/AR技术的普及，环境中虚拟与现实结合的物体增多，识别这些物体（如Artcodes）是触发后续交互的第一步。

Method: 提出了一种新的特征描述符（形状方向直方图）来描述Artcodes的拓扑结构，并构建了检测系统进行实验验证。

Result: 实验结果表明，该特征描述符能有效表示拓扑结构，检测系统在Artcode提案检测中表现良好。

Conclusion: 本研究为基于特征的拓扑物体检测系统提供了初步尝试，为未来交互和应用开辟了新可能性。

Abstract: The increasing ubiquity of smartphones and resurgence of VR/AR techniques, it
is expected that our everyday environment may soon be decorating with objects
connecting with virtual elements. Alerting to the presence of these objects is
therefore the first step for motivating follow-up further inspection and
triggering digital material attached to the objects. This work studies a
special kind of these objects -- Artcodes -- a human-meaningful and
machine-readable decorative markers that camouflage themselves with freeform
appearance by encoding information into their topology. We formulate this
problem of recongising the presence of Artcodes as Artcode proposal detection,
a distinct computer vision task that classifies topologically similar but
geometrically and semantically different objects as a same class. To deal with
this problem, we propose a new feature descriptor, called the shape of
orientation histogram, to describe the generic topological structure of an
Artcode. We collect datasets and conduct comprehensive experiments to evaluate
the performance of the Artcode detection proposer built upon this new feature
vector. Our experimental results show the feasibility of the proposed feature
vector for representing topological structures and the effectiveness of the
system for detecting Artcode proposals. Although this work is an initial
attempt to develop a feature-based system for detecting topological objects
like Artcodes, it would open up new interaction opportunities and spark
potential applications of topological object detection.

</details>


### [12] [Analysis of the Compaction Behavior of Textile Reinforcements in Low-Resolution In-Situ CT Scans via Machine-Learning and Descriptor-Based Methods](https://arxiv.org/abs/2508.10943)
*Christian Düreth,Jan Condé-Wolter,Marek Danczak,Karsten Tittmann,Jörn Jaschinski,Andreas Hornig,Maik Gude*

Main category: cs.CV

TL;DR: 该研究提出了一种利用低分辨率CT量化纺织复合材料中嵌套行为的框架，通过3D-UNet分割和两点相关函数分析，提取了关键几何特征。


<details>
  <summary>Details</summary>
Motivation: 理解纺织复合材料的多尺度结构对预测其力学性能至关重要，尤其是嵌套行为对刚度、渗透性和损伤容限的影响。

Method: 采用低分辨率CT进行原位压实实验，利用3D-UNet进行语义分割，并通过两点相关函数分析空间结构。

Result: 模型分割性能优异（IoU=0.822，F1=0.902），提取的几何特征与显微图像验证结果一致。

Conclusion: 该方法为从工业CT数据中提取几何特征提供了可靠途径，并为复合材料预成型体的逆向建模和结构分析奠定了基础。

Abstract: A detailed understanding of material structure across multiple scales is
essential for predictive modeling of textile-reinforced composites. Nesting --
characterized by the interlocking of adjacent fabric layers through local
interpenetration and misalignment of yarns -- plays a critical role in defining
mechanical properties such as stiffness, permeability, and damage tolerance.
This study presents a framework to quantify nesting behavior in dry textile
reinforcements under compaction using low-resolution computed tomography (CT).
In-situ compaction experiments were conducted on various stacking
configurations, with CT scans acquired at 20.22 $\mu$m per voxel resolution. A
tailored 3D{-}UNet enabled semantic segmentation of matrix, weft, and fill
phases across compaction stages corresponding to fiber volume contents of
50--60 %. The model achieved a minimum mean Intersection-over-Union of 0.822
and an $F1$ score of 0.902. Spatial structure was subsequently analyzed using
the two-point correlation function $S_2$, allowing for probabilistic extraction
of average layer thickness and nesting degree. The results show strong
agreement with micrograph-based validation. This methodology provides a robust
approach for extracting key geometrical features from industrially relevant CT
data and establishes a foundation for reverse modeling and descriptor-based
structural analysis of composite preforms.

</details>


### [13] [iWatchRoad: Scalable Detection and Geospatial Visualization of Potholes for Smart Cities](https://arxiv.org/abs/2508.10945)
*Rishi Raj Sahoo,Surbhi Saswati Mohanty,Subhankar Mishra*

Main category: cs.CV

TL;DR: iWatchRoad是一个端到端系统，用于自动检测道路坑洼、GPS标记和实时地图绘制，适用于印度多样化的道路环境。


<details>
  <summary>Details</summary>
Motivation: 道路坑洼对安全和车辆寿命构成威胁，尤其在印度道路维护不足的情况下。

Method: 利用自标注数据集和YOLO模型进行实时检测，结合OCR模块提取时间戳并与GPS同步，数据通过OpenStreetMap可视化。

Result: 系统在复杂条件下提高了检测准确性，并生成政府可用的道路评估数据。

Conclusion: iWatchRoad是一个成本低、硬件高效且可扩展的解决方案，适用于发展中国家的道路管理。

Abstract: Potholes on the roads are a serious hazard and maintenance burden. This poses
a significant threat to road safety and vehicle longevity, especially on the
diverse and under-maintained roads of India. In this paper, we present a
complete end-to-end system called iWatchRoad for automated pothole detection,
Global Positioning System (GPS) tagging, and real time mapping using
OpenStreetMap (OSM). We curated a large, self-annotated dataset of over 7,000
frames captured across various road types, lighting conditions, and weather
scenarios unique to Indian environments, leveraging dashcam footage. This
dataset is used to fine-tune, Ultralytics You Only Look Once (YOLO) model to
perform real time pothole detection, while a custom Optical Character
Recognition (OCR) module was employed to extract timestamps directly from video
frames. The timestamps are synchronized with GPS logs to geotag each detected
potholes accurately. The processed data includes the potholes' details and
frames as metadata is stored in a database and visualized via a user friendly
web interface using OSM. iWatchRoad not only improves detection accuracy under
challenging conditions but also provides government compatible outputs for road
assessment and maintenance planning through the metadata visible on the
website. Our solution is cost effective, hardware efficient, and scalable,
offering a practical tool for urban and rural road management in developing
regions, making the system automated. iWatchRoad is available at
https://smlab.niser.ac.in/project/iwatchroad

</details>


### [14] [IPG: Incremental Patch Generation for Generalized Adversarial Patch Training](https://arxiv.org/abs/2508.10946)
*Wonho Lee,Hyunsik Na,Jisu Lee,Daeseon Choi*

Main category: cs.CV

TL;DR: 本文提出了一种名为增量补丁生成（IPG）的方法，能高效生成对抗性补丁，比现有方法快11.1倍，同时保持攻击性能。实验表明IPG能生成泛化性强的补丁，覆盖更多模型漏洞，并为构建鲁棒模型提供数据基础。


<details>
  <summary>Details</summary>
Motivation: 对抗性补丁对AI模型的鲁棒性构成挑战，尤其在计算机视觉任务中。传统对抗性示例无法有效应对此类攻击，因此需要更高效的方法生成对抗性补丁。

Method: 提出增量补丁生成（IPG），通过实验和消融研究验证其效率，包括YOLO特征分布可视化和对抗训练结果。

Result: IPG生成的补丁泛化性强，能覆盖更广的模型漏洞，且生成效率比现有方法高11.1倍。

Conclusion: IPG在对抗性补丁防御和实际应用（如自动驾驶、安防系统和医学影像）中具有潜力，能提升AI模型在动态高风险环境中的鲁棒性。

Abstract: The advent of adversarial patches poses a significant challenge to the
robustness of AI models, particularly in the domain of computer vision tasks
such as object detection. In contradistinction to traditional adversarial
examples, these patches target specific regions of an image, resulting in the
malfunction of AI models. This paper proposes Incremental Patch Generation
(IPG), a method that generates adversarial patches up to 11.1 times more
efficiently than existing approaches while maintaining comparable attack
performance. The efficacy of IPG is demonstrated by experiments and ablation
studies including YOLO's feature distribution visualization and adversarial
training results, which show that it produces well-generalized patches that
effectively cover a broader range of model vulnerabilities. Furthermore,
IPG-generated datasets can serve as a robust knowledge foundation for
constructing a robust model, enabling structured representation, advanced
reasoning, and proactive defenses in AI security ecosystems. The findings of
this study suggest that IPG has considerable potential for future utilization
not only in adversarial patch defense but also in real-world applications such
as autonomous vehicles, security systems, and medical imaging, where AI models
must remain resilient to adversarial attacks in dynamic and high-stakes
environments.

</details>


### [15] [MedAtlas: Evaluating LLMs for Multi-Round, Multi-Task Medical Reasoning Across Diverse Imaging Modalities and Clinical Text](https://arxiv.org/abs/2508.10947)
*Ronghao Xu,Zhen Huang,Yangbo Wei,Xiaoqian Zhou,Zikang Xu,Ting Liu,Zihang Jiang,S. Kevin Zhou*

Main category: cs.CV

TL;DR: MedAtlas是一个新的医疗AI基准框架，旨在通过多模态、多任务和多轮对话评估模型在真实临床场景中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有医疗多模态基准局限于单图像、单任务，无法反映临床实践的多模态交互性和纵向性。

Method: MedAtlas框架支持多轮对话、多模态图像交互和多任务集成，任务包括开放式和封闭式问答、多图像联合推理及综合疾病诊断。

Result: 基准测试显示现有模型在多阶段临床推理中存在显著性能差距。

Conclusion: MedAtlas为开发稳健可信的医疗AI提供了具有挑战性的评估平台。

Abstract: Artificial intelligence has demonstrated significant potential in clinical
decision-making; however, developing models capable of adapting to diverse
real-world scenarios and performing complex diagnostic reasoning remains a
major challenge. Existing medical multi-modal benchmarks are typically limited
to single-image, single-turn tasks, lacking multi-modal medical image
integration and failing to capture the longitudinal and multi-modal interactive
nature inherent to clinical practice. To address this gap, we introduce
MedAtlas, a novel benchmark framework designed to evaluate large language
models on realistic medical reasoning tasks. MedAtlas is characterized by four
key features: multi-turn dialogue, multi-modal medical image interaction,
multi-task integration, and high clinical fidelity. It supports four core
tasks: open-ended multi-turn question answering, closed-ended multi-turn
question answering, multi-image joint reasoning, and comprehensive disease
diagnosis. Each case is derived from real diagnostic workflows and incorporates
temporal interactions between textual medical histories and multiple imaging
modalities, including CT, MRI, PET, ultrasound, and X-ray, requiring models to
perform deep integrative reasoning across images and clinical texts. MedAtlas
provides expert-annotated gold standards for all tasks. Furthermore, we propose
two novel evaluation metrics: Round Chain Accuracy and Error Propagation
Resistance. Benchmark results with existing multi-modal models reveal
substantial performance gaps in multi-stage clinical reasoning. MedAtlas
establishes a challenging evaluation platform to advance the development of
robust and trustworthy medical AI.

</details>


### [16] [From Promise to Practical Reality: Transforming Diffusion MRI Analysis with Fast Deep Learning Enhancement](https://arxiv.org/abs/2508.10950)
*Xinyi Wang,Michael Barnett,Frederique Boonstra,Yael Barnett,Mariano Cabezas,Arkiev D'Souza,Matthew C. Kiernan,Kain Kyle,Meng Law,Lynette Masters,Zihao Tang,Stephen Tisch,Sicong Tu,Anneke Van Der Walt,Dongang Wang,Fernando Calamante,Weidong Cai,Chenyu Wang*

Main category: cs.CV

TL;DR: FastFOD-Net是一种基于深度学习的FOD增强框架，显著提升了临床扩散MRI数据的分析效率和准确性，适用于多种神经系统疾病研究。


<details>
  <summary>Details</summary>
Motivation: 解决临床扩散MRI数据（单壳低角度分辨率）生成可靠FOD的挑战，并验证深度学习增强技术在健康与疾病数据中的适用性。

Method: 采用优化的端到端深度学习框架FastFOD-Net，显著提升训练和推理效率（比前代快60倍）。

Result: FastFOD-Net在健康对照和六种神经系统疾病中表现优异，支持疾病鉴别、连接组分析，并降低样本量需求。

Conclusion: FastFOD-Net有望加速临床神经科学研究，增强深度学习在扩散MRI中的可信度和应用范围。

Abstract: Fiber orientation distribution (FOD) is an advanced diffusion MRI modeling
technique that represents complex white matter fiber configurations, and a key
step for subsequent brain tractography and connectome analysis. Its reliability
and accuracy, however, heavily rely on the quality of the MRI acquisition and
the subsequent estimation of the FODs at each voxel. Generating reliable FODs
from widely available clinical protocols with single-shell and
low-angular-resolution acquisitions remains challenging but could potentially
be addressed with recent advances in deep learning-based enhancement
techniques. Despite advancements, existing methods have predominantly been
assessed on healthy subjects, which have proved to be a major hurdle for their
clinical adoption. In this work, we validate a newly optimized enhancement
framework, FastFOD-Net, across healthy controls and six neurological disorders.
This accelerated end-to-end deep learning framework enhancing FODs with
superior performance and delivering training/inference efficiency for clinical
use ($60\times$ faster comparing to its predecessor). With the most
comprehensive clinical evaluation to date, our work demonstrates the potential
of FastFOD-Net in accelerating clinical neuroscience research, empowering
diffusion MRI analysis for disease differentiation, improving interpretability
in connectome applications, and reducing measurement errors to lower sample
size requirements. Critically, this work will facilitate the more widespread
adoption of, and build clinical trust in, deep learning based methods for
diffusion MRI enhancement. Specifically, FastFOD-Net enables robust analysis of
real-world, clinical diffusion MRI data, comparable to that achievable with
high-quality research acquisitions.

</details>


### [17] [Empowering Multimodal LLMs with External Tools: A Comprehensive Survey](https://arxiv.org/abs/2508.10955)
*Wenbin An,Jiahao Nie,Yaqiang Wu,Feng Tian,Shijian Lu,Qinghua Zheng*

Main category: cs.CV

TL;DR: 综述探讨了如何通过外部工具增强多模态大语言模型（MLLMs）的性能，包括数据获取、任务改进、评估优化及未来方向。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在多模态任务中表现优异，但数据质量、复杂任务表现和评估协议不足限制了其可靠性和广泛应用。受人类利用外部工具启发，研究探索如何通过工具增强MLLMs。

Method: 通过四个维度系统分析外部工具的作用：高质量数据获取与标注、复杂任务性能提升、全面评估方法及当前局限与未来方向。

Result: 外部工具在提升MLLMs性能方面具有巨大潜力，尤其是在数据、任务和评估三个关键领域。

Conclusion: 外部工具是推动MLLMs发展的关键，未来需进一步探索其潜力与应用。

Abstract: By integrating the perception capabilities of multimodal encoders with the
generative power of Large Language Models (LLMs), Multimodal Large Language
Models (MLLMs), exemplified by GPT-4V, have achieved great success in various
multimodal tasks, pointing toward a promising pathway to artificial general
intelligence. Despite this progress, the limited quality of multimodal data,
poor performance on many complex downstream tasks, and inadequate evaluation
protocols continue to hinder the reliability and broader applicability of MLLMs
across diverse domains. Inspired by the human ability to leverage external
tools for enhanced reasoning and problem-solving, augmenting MLLMs with
external tools (e.g., APIs, expert models, and knowledge bases) offers a
promising strategy to overcome these challenges. In this paper, we present a
comprehensive survey on leveraging external tools to enhance MLLM performance.
Our discussion is structured along four key dimensions about external tools:
(1) how they can facilitate the acquisition and annotation of high-quality
multimodal data; (2) how they can assist in improving MLLM performance on
challenging downstream tasks; (3) how they enable comprehensive and accurate
evaluation of MLLMs; (4) the current limitations and future directions of
tool-augmented MLLMs. Through this survey, we aim to underscore the
transformative potential of external tools in advancing MLLM capabilities,
offering a forward-looking perspective on their development and applications.
The project page of this paper is publicly available
athttps://github.com/Lackel/Awesome-Tools-for-MLLMs.

</details>


### [18] [ORBIT: An Object Property Reasoning Benchmark for Visual Inference Tasks](https://arxiv.org/abs/2508.10956)
*Abhishek Kolari,Mohammadhossein Khojasteh,Yifan Jiang,Floris den Hengst,Filip Ilievski*

Main category: cs.CV

TL;DR: 论文提出了ORBIT基准测试，用于评估视觉语言模型在对象属性推理上的能力，发现现有模型在复杂推理和真实图像上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有视觉问答基准测试在对象属性推理上存在局限，缺乏代表性和复杂性，因此需要更系统的评估框架。

Method: 设计了包含三种图像类型、三种推理复杂度和四种对象属性的基准测试ORBIT，包含360张图像和1,080个计数问题。

Result: 12个先进视觉语言模型在零样本设置下表现不佳，最佳模型准确率仅40%，尤其在真实图像和复杂推理上表现差。

Conclusion: ORBIT揭示了现有模型的局限性，呼吁开发更可扩展的基准测试方法和更强的推理模型。

Abstract: While vision-language models (VLMs) have made remarkable progress on many
popular visual question answering (VQA) benchmarks, it remains unclear whether
they abstract and reason over depicted objects. Inspired by human object
categorisation, object property reasoning involves identifying and recognising
low-level details and higher-level abstractions. While current VQA benchmarks
consider a limited set of object property attributes like size, they typically
blend perception and reasoning, and lack representativeness in terms of
reasoning and image categories. To this end, we introduce a systematic
evaluation framework with images of three representative types, three reasoning
levels of increasing complexity, and four object property dimensions driven by
prior work on commonsense reasoning. We develop a procedure to instantiate this
benchmark into ORBIT, a multi-level reasoning VQA benchmark for object
properties comprising 360 images paired with a total of 1,080 count-based
questions. Experiments with 12 state-of-the-art VLMs in zero-shot settings
reveal significant limitations compared to humans, with the best-performing
model only reaching 40\% accuracy. VLMs struggle particularly with realistic
(photographic) images, counterfactual reasoning about physical and functional
properties, and higher counts. ORBIT points to the need to develop methods for
scalable benchmarking, generalize annotation guidelines, and explore additional
reasoning VLMs. We make the ORBIT benchmark and the experimental code available
to support such endeavors.

</details>


### [19] [CSNR and JMIM Based Spectral Band Selection for Reducing Metamerism in Urban Driving](https://arxiv.org/abs/2508.10962)
*Jiarong Li,Imad Ali Shah,Diarmaid Geever,Fiachra Collins,Enda Ward,Martin Glavin,Edward Jones,Brian Deegan*

Main category: cs.CV

TL;DR: 论文提出了一种基于高光谱成像（HSI）的方法，通过选择信息量最大的波段来增强对易受伤害道路使用者（VRU）的感知能力，减少视觉模糊问题。


<details>
  <summary>Details</summary>
Motivation: 解决RGB图像中因同色异谱现象导致的视觉模糊问题，提升自动驾驶系统中对VRU的感知能力。

Method: 采用信息论技术和图像质量指标（对比信噪比）选择最具信息量的HSI波段，并在H-City数据集上验证。

Result: 选定的HSI波段显著提升了VRU与背景的区分度，各项指标（欧氏距离、SAM、T²、CIE ΔE）均有显著改善。

Conclusion: 该方法为ADAS和自动驾驶系统提供了更优的输入数据，有助于提升道路安全性。

Abstract: Protecting Vulnerable Road Users (VRU) is a critical safety challenge for
automotive perception systems, particularly under visual ambiguity caused by
metamerism, a phenomenon where distinct materials appear similar in RGB
imagery. This work investigates hyperspectral imaging (HSI) to overcome this
limitation by capturing unique material signatures beyond the visible spectrum,
especially in the Near-Infrared (NIR). To manage the inherent
high-dimensionality of HSI data, we propose a band selection strategy that
integrates information theory techniques (joint mutual information
maximization, correlation analysis) with a novel application of an image
quality metric (contrast signal-to-noise ratio) to identify the most spectrally
informative bands. Using the Hyperspectral City V2 (H-City) dataset, we
identify three informative bands (497 nm, 607 nm, and 895 nm, $\pm$27 nm) and
reconstruct pseudo-color images for comparison with co-registered RGB.
Quantitative results demonstrate increased dissimilarity and perceptual
separability of VRU from the background. The selected HSI bands yield
improvements of 70.24%, 528.46%, 1206.83%, and 246.62% for dissimilarity
(Euclidean, SAM, $T^2$) and perception (CIE $\Delta E$) metrics, consistently
outperforming RGB and confirming a marked reduction in metameric confusion. By
providing a spectrally optimized input, our method enhances VRU separability,
establishing a robust foundation for downstream perception tasks in Advanced
Driver Assistance Systems (ADAS) and Autonomous Driving (AD), ultimately
contributing to improved road safety.

</details>


### [20] [EVCtrl: Efficient Control Adapter for Visual Generation](https://arxiv.org/abs/2508.10963)
*Zixiang Yang,Yue Ma,Yinhan Zhang,Shanhui Mo,Dongrui Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: 论文提出EVCtrl，一种轻量级控制适配器，通过时空双缓存策略减少冗余计算，提升图像和视频生成效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法如ControlNet虽能精确控制生成内容，但计算冗余和延迟问题严重，尤其在视频生成中。

Method: 提出时空双缓存策略：空间上分区处理控制信号，时间上选择性跳过冗余去噪步骤。

Result: 实验表明，EVCtrl在CogVideo-Controlnet和Wan2.1-Controlnet上分别实现2.16和2.05倍加速，且生成质量几乎无损。

Conclusion: EVCtrl无需重新训练模型即可高效实现可控生成，显著提升性能。

Abstract: Visual generation includes both image and video generation, training
probabilistic models to create coherent, diverse, and semantically faithful
content from scratch. While early research focused on unconditional sampling,
practitioners now demand controllable generation that allows precise
specification of layout, pose, motion, or style. While ControlNet grants
precise spatial-temporal control, its auxiliary branch markedly increases
latency and introduces redundant computation in both uncontrolled regions and
denoising steps, especially for video. To address this problem, we introduce
EVCtrl, a lightweight, plug-and-play control adapter that slashes overhead
without retraining the model. Specifically, we propose a spatio-temporal dual
caching strategy for sparse control information. For spatial redundancy, we
first profile how each layer of DiT-ControlNet responds to fine-grained
control, then partition the network into global and local functional zones. A
locality-aware cache focuses computation on the local zones that truly need the
control signal, skipping the bulk of redundant computation in global regions.
For temporal redundancy, we selectively omit unnecessary denoising steps to
improve efficiency. Extensive experiments on CogVideo-Controlnet,
Wan2.1-Controlnet, and Flux demonstrate that our method is effective in image
and video control generation without the need for training. For example, it
achieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and
Wan2.1-Controlnet, respectively, with almost no degradation in generation
quality.Codes are available in the supplementary materials.

</details>


### [21] [Not There Yet: Evaluating Vision Language Models in Simulating the Visual Perception of People with Low Vision](https://arxiv.org/abs/2508.10972)
*Rosiana Natalie,Wenqian Xu,Ruei-Che Chang,Rada Mihalcea,Anhong Guo*

Main category: cs.CV

TL;DR: 论文评估了视觉语言模型（VLMs）在模拟低视力人群视觉感知方面的能力，发现结合视力信息和图像响应示例能显著提高模型与参与者回答的一致性。


<details>
  <summary>Details</summary>
Motivation: 探索VLMs在无障碍领域的模拟能力，尤其是低视力人群的视觉感知。

Method: 通过调查40名低视力参与者构建数据集，生成VLM模拟代理，并评估其回答与原始回答的一致性。

Result: VLMs在仅提供少量信息时一致性较低（0.59），但结合视力信息和图像响应示例后显著提高（0.70）。

Conclusion: 结合多种信息类型能有效提升VLM模拟低视力感知的准确性，但额外示例效果有限。

Abstract: Advances in vision language models (VLMs) have enabled the simulation of
general human behavior through their reasoning and problem solving
capabilities. However, prior research has not investigated such simulation
capabilities in the accessibility domain. In this paper, we evaluate the extent
to which VLMs can simulate the vision perception of low vision individuals when
interpreting images. We first compile a benchmark dataset through a survey
study with 40 low vision participants, collecting their brief and detailed
vision information and both open-ended and multiple-choice image perception and
recognition responses to up to 25 images. Using these responses, we construct
prompts for VLMs (GPT-4o) to create simulated agents of each participant,
varying the included information on vision information and example image
responses. We evaluate the agreement between VLM-generated responses and
participants' original answers. Our results indicate that VLMs tend to infer
beyond the specified vision ability when given minimal prompts, resulting in
low agreement (0.59). The agreement between the agent' and participants'
responses remains low when only either the vision information (0.59) or example
image responses (0.59) are provided, whereas a combination of both
significantly increase the agreement (0.70, p < 0.0001). Notably, a single
example combining both open-ended and multiple-choice responses, offers
significant performance improvements over either alone (p < 0.0001), while
additional examples provided minimal benefits (p > 0.05).

</details>


### [22] [Are Large Pre-trained Vision Language Models Effective Construction Safety Inspectors?](https://arxiv.org/abs/2508.11011)
*Xuezheng Chen,Zhengbo Zou*

Main category: cs.CV

TL;DR: 该论文提出了ConstructionSite 10k数据集，包含10,000张建筑工地图像，用于评估和微调视觉语言模型（VLMs）在建筑安全检查中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏开放数据集来全面评估和优化VLMs在建筑安全检查中的应用，限制了其泛化能力。

Method: 构建包含10,000张图像的数据集，支持图像描述、安全违规视觉问答和建筑元素视觉定位任务。

Result: 评估显示现有VLMs在零样本和小样本设置下具有泛化能力，但需进一步训练以适应实际工地。

Conclusion: ConstructionSite 10k为研究人员提供了训练和评估VLMs的基准，推动了建筑安全检查技术的发展。

Abstract: Construction safety inspections typically involve a human inspector
identifying safety concerns on-site. With the rise of powerful Vision Language
Models (VLMs), researchers are exploring their use for tasks such as detecting
safety rule violations from on-site images. However, there is a lack of open
datasets to comprehensively evaluate and further fine-tune VLMs in construction
safety inspection. Current applications of VLMs use small, supervised datasets,
limiting their applicability in tasks they are not directly trained for. In
this paper, we propose the ConstructionSite 10k, featuring 10,000 construction
site images with annotations for three inter-connected tasks, including image
captioning, safety rule violation visual question answering (VQA), and
construction element visual grounding. Our subsequent evaluation of current
state-of-the-art large pre-trained VLMs shows notable generalization abilities
in zero-shot and few-shot settings, while additional training is needed to make
them applicable to actual construction sites. This dataset allows researchers
to train and evaluate their own VLMs with new architectures and techniques,
providing a valuable benchmark for construction safety inspection.

</details>


### [23] [Can Multi-modal (reasoning) LLMs detect document manipulation?](https://arxiv.org/abs/2508.11021)
*Zisheng Liang,Kidus Zewde,Rudra Pratap Singh,Disha Patil,Zexi Chen,Jiayu Xue,Yao Yao,Yifei Chen,Qinzhe Liu,Simiao Ren*

Main category: cs.CV

TL;DR: 研究评估了多模态大语言模型在检测伪造文档中的表现，发现部分模型在零样本泛化上优于传统方法，但模型大小与准确性关联有限。


<details>
  <summary>Details</summary>
Motivation: 文档伪造对依赖安全文档的行业构成威胁，需要有效的检测机制。

Method: 通过提示优化和模型推理分析，评估多模态LLMs在检测伪造文档中的能力。

Result: 部分多模态LLMs在零样本泛化上表现优异，但模型大小与准确性关联不大。

Conclusion: 多模态LLMs在文档伪造检测中具有潜力，需任务特定优化。

Abstract: Document fraud poses a significant threat to industries reliant on secure and
verifiable documentation, necessitating robust detection mechanisms. This study
investigates the efficacy of state-of-the-art multi-modal large language models
(LLMs)-including OpenAI O1, OpenAI 4o, Gemini Flash (thinking), Deepseek Janus,
Grok, Llama 3.2 and 4, Qwen 2 and 2.5 VL, Mistral Pixtral, and Claude 3.5 and
3.7 Sonnet-in detecting fraudulent documents. We benchmark these models against
each other and prior work on document fraud detection techniques using a
standard dataset with real transactional documents. Through prompt optimization
and detailed analysis of the models' reasoning processes, we evaluate their
ability to identify subtle indicators of fraud, such as tampered text,
misaligned formatting, and inconsistent transactional sums. Our results reveal
that top-performing multi-modal LLMs demonstrate superior zero-shot
generalization, outperforming conventional methods on out-of-distribution
datasets, while several vision LLMs exhibit inconsistent or subpar performance.
Notably, model size and advanced reasoning capabilities show limited
correlation with detection accuracy, suggesting task-specific fine-tuning is
critical. This study underscores the potential of multi-modal LLMs in enhancing
document fraud detection systems and provides a foundation for future research
into interpretable and scalable fraud mitigation strategies.

</details>


### [24] [MedSAMix: A Training-Free Model Merging Approach for Medical Image Segmentation](https://arxiv.org/abs/2508.11032)
*Yanwu Yang,Guinan Su,Jiesi Hu,Francesco Sammarco,Jonas Geiping,Thomas Wolfers*

Main category: cs.CV

TL;DR: MedSAMix是一种无需训练的方法，通过整合通用模型（如SAM）和专用模型（如MedSAM）的优势，提升医学图像分割的性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割模型（如MedSAM）因数据有限、异质性和分布偏移等问题，泛化能力受限。

Method: 提出一种零阶优化方法，自动寻找最优层合并方案，并通过单任务和多目标优化满足不同临床需求。

Result: 在25个医学分割任务中，MedSAMix显著提升性能，专用任务和多任务评估分别提高6.67%和4.37%。

Conclusion: MedSAMix有效解决了模型偏差问题，在领域专一性和泛化性上均表现优异。

Abstract: Universal medical image segmentation models have emerged as a promising
paradigm due to their strong generalizability across diverse tasks, showing
great potential for a wide range of clinical applications. This potential has
been partly driven by the success of general-purpose vision models such as the
Segment Anything Model (SAM), which has inspired the development of various
fine-tuned variants for medical segmentation tasks. However, fine-tuned
variants like MedSAM are trained on comparatively limited medical imaging data
that often suffers from heterogeneity, scarce annotations, and distributional
shifts. These challenges limit their ability to generalize across a wide range
of medical segmentation tasks. In this regard, we propose MedSAMix, a
training-free model merging method that integrates the strengths of both
generalist models (e.g., SAM) and specialist models (e.g., MedSAM) for medical
image segmentation. In contrast to traditional model merging approaches that
rely on manual configuration and often result in suboptimal outcomes, we
propose a zero-order optimization method to automatically discover optimal
layer-wise merging solutions. Furthermore, for clinical applications, we
develop two regimes to meet the demand of domain-specificity and
generalizability in different scenarios by single-task optimization and
multi-objective optimization respectively. Extensive evaluations on 25 medical
segmentation tasks demonstrate that MedSAMix effectively mitigates model bias
and consistently improves performance in both domain-specific accuracy and
generalization, achieving improvements of 6.67% on specialized tasks and 4.37%
on multi-task evaluations.

</details>


### [25] [Advancing 3D Scene Understanding with MV-ScanQA Multi-View Reasoning Evaluation and TripAlign Pre-training Dataset](https://arxiv.org/abs/2508.11058)
*Wentao Mo,Qingchao Chen,Yuxin Peng,Siyuan Huang,Yang Liu*

Main category: cs.CV

TL;DR: 论文提出了MV-ScanQA数据集和TripAlign预训练语料库，以解决现有3D视觉语言数据集的局限性，并开发了LEGO方法用于多视图推理。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉语言数据集在远距离多视图场景理解和多对象上下文对齐方面存在不足，限制了模型的发展。

Method: 提出MV-ScanQA数据集（68%问题需多视图推理）和TripAlign预训练语料库（1M三元组），并开发LEGO方法，将2D LVLMs知识迁移至3D领域。

Result: LEGO在MV-ScanQA及现有3D密集描述和问答基准上达到最先进性能。

Conclusion: MV-ScanQA和TripAlign为3D视觉语言学习提供了更丰富的多视图和多对象对齐信号，推动了该领域的发展。

Abstract: The advancement of 3D vision-language (3D VL) learning is hindered by several
limitations in existing 3D VL datasets: they rarely necessitate reasoning
beyond a close range of objects in single viewpoint, and annotations often link
instructions to single objects, missing richer contextual alignments between
multiple objects. This significantly curtails the development of models capable
of deep, multi-view 3D scene understanding over distant objects. To address
these challenges, we introduce MV-ScanQA, a novel 3D question answering dataset
where 68% of questions explicitly require integrating information from multiple
views (compared to less than 7% in existing datasets), thereby rigorously
testing multi-view compositional reasoning. To facilitate the training of
models for such demanding scenarios, we present TripAlign dataset, a
large-scale and low-cost 2D-3D-language pre-training corpus containing 1M <2D
view, set of 3D objects, text> triplets that explicitly aligns groups of
contextually related objects with text, providing richer, view-grounded
multi-object multimodal alignment signals than previous single-object
annotations. We further develop LEGO, a baseline method for the multi-view
reasoning challenge in MV-ScanQA, transferring knowledge from pre-trained 2D
LVLMs to 3D domain with TripAlign. Empirically, LEGO pre-trained on TripAlign
achieves state-of-the-art performance not only on the proposed MV-ScanQA, but
also on existing benchmarks for 3D dense captioning and question answering.
Datasets and code are available at
https://matthewdm0816.github.io/tripalign-mvscanqa.

</details>


### [26] [Data-Driven Abdominal Phenotypes of Type 2 Diabetes in Lean, Overweight, and Obese Cohorts](https://arxiv.org/abs/2508.11063)
*Lucas W. Remedios,Chloe Choe,Trent M. Schwartz,Dingjie Su,Gaurav Rudravaram,Chenyu Gao,Aravind R. Krishnan,Adam M. Saunders,Michael E. Kim,Shunxing Bao,Alvin C. Powers,Bennett A. Landman,John Virostko*

Main category: cs.CV

TL;DR: 研究利用AI分析腹部3D影像，揭示不同BMI人群中2型糖尿病的共同腹部特征。


<details>
  <summary>Details</summary>
Motivation: 尽管BMI是2型糖尿病的已知风险因素，但瘦人和肥胖人群中疾病表现不一致，提示详细体成分分析可能揭示腹部表型。

Method: 通过分割腹部CT扫描生成可解释的测量数据，使用随机森林分类模型和SHAP分析识别风险特征，并在不同BMI亚组中验证。

Result: 随机森林模型的AUC为0.72-0.74，发现脂肪肌肉、年龄、内脏脂肪等是共同风险因素。

Conclusion: 腹部特征对2型糖尿病的影响在不同体重人群中可能一致。

Abstract: Purpose: Although elevated BMI is a well-known risk factor for type 2
diabetes, the disease's presence in some lean adults and absence in others with
obesity suggests that detailed body composition may uncover abdominal
phenotypes of type 2 diabetes. With AI, we can now extract detailed
measurements of size, shape, and fat content from abdominal structures in 3D
clinical imaging at scale. This creates an opportunity to empirically define
body composition signatures linked to type 2 diabetes risk and protection using
large-scale clinical data. Approach: To uncover BMI-specific diabetic abdominal
patterns from clinical CT, we applied our design four times: once on the full
cohort (n = 1,728) and once on lean (n = 497), overweight (n = 611), and obese
(n = 620) subgroups separately. Briefly, our experimental design transforms
abdominal scans into collections of explainable measurements through
segmentation, classifies type 2 diabetes through a cross-validated random
forest, measures how features contribute to model-estimated risk or protection
through SHAP analysis, groups scans by shared model decision patterns
(clustering from SHAP) and links back to anatomical differences
(classification). Results: The random-forests achieved mean AUCs of 0.72-0.74.
There were shared type 2 diabetes signatures in each group; fatty skeletal
muscle, older age, greater visceral and subcutaneous fat, and a smaller or
fat-laden pancreas. Univariate logistic regression confirmed the direction of
14-18 of the top 20 predictors within each subgroup (p < 0.05). Conclusions:
Our findings suggest that abdominal drivers of type 2 diabetes may be
consistent across weight classes.

</details>


### [27] [HierOctFusion: Multi-scale Octree-based 3D Shape Generation via Part-Whole-Hierarchy Message Passing](https://arxiv.org/abs/2508.11106)
*Xinjie Gao,Bi'an Du,Wei Hu*

Main category: cs.CV

TL;DR: HierOctFusion是一种基于八叉树的多尺度扩散模型，通过分层特征交互和语义部分信息注入，提升了3D内容生成的精细度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法将3D对象视为整体，忽略了语义部分层次结构，且高分辨率建模计算成本高。HierOctFusion通过分层生成和部分感知设计解决了这些问题。

Method: 提出HierOctFusion模型，结合多尺度八叉树扩散和跨注意力机制，注入部分级信息，并构建带部分标注的3D数据集。

Result: 实验表明，HierOctFusion在形状质量和效率上优于现有方法。

Conclusion: HierOctFusion通过分层和部分感知设计，显著提升了3D内容生成的性能。

Abstract: 3D content generation remains a fundamental yet challenging task due to the
inherent structural complexity of 3D data. While recent octree-based diffusion
models offer a promising balance between efficiency and quality through
hierarchical generation, they often overlook two key insights: 1) existing
methods typically model 3D objects as holistic entities, ignoring their
semantic part hierarchies and limiting generalization; and 2) holistic
high-resolution modeling is computationally expensive, whereas real-world
objects are inherently sparse and hierarchical, making them well-suited for
layered generation. Motivated by these observations, we propose HierOctFusion,
a part-aware multi-scale octree diffusion model that enhances hierarchical
feature interaction for generating fine-grained and sparse object structures.
Furthermore, we introduce a cross-attention conditioning mechanism that injects
part-level information into the generation process, enabling semantic features
to propagate effectively across hierarchical levels from parts to the whole.
Additionally, we construct a 3D dataset with part category annotations using a
pre-trained segmentation model to facilitate training and evaluation.
Experiments demonstrate that HierOctFusion achieves superior shape quality and
efficiency compared to prior methods.

</details>


### [28] [UWB-PostureGuard: A Privacy-Preserving RF Sensing System for Continuous Ergonomic Sitting Posture Monitoring](https://arxiv.org/abs/2508.11115)
*Haotang Li,Zhenyu Qi,Sen He,Kebin Peng,Sheng Tan,Yili Ren,Tomas Cerny,Jiyue Zhao,Zi Wang*

Main category: cs.CV

TL;DR: UWB-PostureGuard是一种基于超宽带技术的隐私保护坐姿监测系统，通过非接触式监测提升预防性健康管理。


<details>
  <summary>Details</summary>
Motivation: 长时间使用电脑时的不良坐姿已成为公共健康问题，传统监测方法存在隐私和舒适性缺陷。

Method: 利用商用UWB设备提取坐姿特征，开发PoseGBDT模型捕捉时间依赖性。

Result: 在10名参与者和19种姿势的测试中，系统达到99.11%的准确率，且对环境变量具有鲁棒性。

Conclusion: 该系统为低成本、可扩展的隐私保护移动健康解决方案，改善生活质量。

Abstract: Improper sitting posture during prolonged computer use has become a
significant public health concern. Traditional posture monitoring solutions
face substantial barriers, including privacy concerns with camera-based systems
and user discomfort with wearable sensors. This paper presents
UWB-PostureGuard, a privacy-preserving ultra-wideband (UWB) sensing system that
advances mobile technologies for preventive health management through
continuous, contactless monitoring of ergonomic sitting posture. Our system
leverages commercial UWB devices, utilizing comprehensive feature engineering
to extract multiple ergonomic sitting posture features. We develop PoseGBDT to
effectively capture temporal dependencies in posture patterns, addressing
limitations of traditional frame-wise classification approaches. Extensive
real-world evaluation across 10 participants and 19 distinct postures
demonstrates exceptional performance, achieving 99.11% accuracy while
maintaining robustness against environmental variables such as clothing
thickness, additional devices, and furniture configurations. Our system
provides a scalable, privacy-preserving mobile health solution on existing
platforms for proactive ergonomic management, improving quality of life at low
costs.

</details>


### [29] [Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation](https://arxiv.org/abs/2508.11134)
*Bing Liu,Le Wang,Hao Liu,Mingming Liu*

Main category: cs.CV

TL;DR: 提出了一种基于残差的高效双向扩散模型（RBDM），能够同时实现去雾和生成雾的双向转换，通过双马尔可夫链和统一评分函数优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度去雾方法仅关注去雾，缺乏雾与无雾图像之间的双向转换能力，RBDM旨在解决这一问题。

Method: 设计双马尔可夫链实现残差平滑转换；通过扰动图像和预测噪声学习条件分布；引入统一评分函数降低计算成本。

Result: RBDM在仅15步采样下实现尺寸无关的双向转换，在合成和真实数据集上表现优于或媲美现有方法。

Conclusion: RBDM成功解决了双向转换问题，性能优越且计算高效。

Abstract: Current deep dehazing methods only focus on removing haze from hazy images,
lacking the capability to translate between hazy and haze-free images. To
address this issue, we propose a residual-based efficient bidirectional
diffusion model (RBDM) that can model the conditional distributions for both
dehazing and haze generation. Firstly, we devise dual Markov chains that can
effectively shift the residuals and facilitate bidirectional smooth transitions
between them. Secondly, the RBDM perturbs the hazy and haze-free images at
individual timesteps and predicts the noise in the perturbed data to
simultaneously learn the conditional distributions. Finally, to enhance
performance on relatively small datasets and reduce computational costs, our
method introduces a unified score function learned on image patches instead of
entire images. Our RBDM successfully implements size-agnostic bidirectional
transitions between haze-free and hazy images with only 15 sampling steps.
Extensive experiments demonstrate that the proposed method achieves superior or
at least comparable performance to state-of-the-art methods on both synthetic
and real-world datasets.

</details>


### [30] [A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations](https://arxiv.org/abs/2508.11141)
*Bin Ma,Yifei Zhang,Yongjin Xian,Qi Li,Linna Zhou,Gongxun Miao*

Main category: cs.CV

TL;DR: 本文提出了一种基于对比学习的跨模态谣言检测方案MICC，通过多尺度图像与上下文相关性探索算法，显著提升了谣言检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有谣言检测方法常忽略图像内容及跨视觉尺度的上下文与图像关系，导致关键信息丢失。

Method: 设计了SCLIP编码器生成统一语义嵌入，引入跨模态多尺度对齐模块，并通过尺度感知融合网络整合特征。

Result: 在两个真实数据集上验证，性能显著优于现有方法。

Conclusion: MICC方法有效且具有实际应用潜力。

Abstract: Existing rumor detection methods often neglect the content within images as
well as the inherent relationships between contexts and images across different
visual scales, thereby resulting in the loss of critical information pertinent
to rumor identification. To address these issues, this paper presents a novel
cross-modal rumor detection scheme based on contrastive learning, namely the
Multi-scale Image and Context Correlation exploration algorithm (MICC).
Specifically, we design an SCLIP encoder to generate unified semantic
embeddings for text and multi-scale image patches through contrastive
pretraining, enabling their relevance to be measured via dot-product
similarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is
introduced to identify image regions most relevant to the textual semantics,
guided by mutual information maximization and the information bottleneck
principle, through a Top-K selection strategy based on a cross-modal relevance
matrix constructed between the text and multi-scale image patches. Moreover, a
scale-aware fusion network is designed to integrate the highly correlated
multi-scale image features with global text features by assigning adaptive
weights to image regions based on their semantic importance and cross-modal
relevance. The proposed methodology has been extensively evaluated on two
real-world datasets. The experimental results demonstrate that it achieves a
substantial performance improvement over existing state-of-the-art approaches
in rumor detection, highlighting its effectiveness and potential for practical
applications.

</details>


### [31] [LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction](https://arxiv.org/abs/2508.11153)
*Maoquan Zhang,Bisser Raytchev,Xiujuan Sun*

Main category: cs.CV

TL;DR: LEARN是一个布局感知的扩散框架，用于生成STEM教育中教学对齐的插图。


<details>
  <summary>Details</summary>
Motivation: 解决STEM教育中抽象科学概念的可视化问题，同时减少认知负荷。

Method: 利用BookCover数据集，结合布局条件生成、对比视觉语义训练和提示调制。

Result: 生成连贯的视觉序列，支持中高层次的推理，并减少无关认知负荷。

Conclusion: LEARN为生成式AI在教育中的应用提供了新方向，具有多模态整合潜力。

Abstract: LEARN is a layout-aware diffusion framework designed to generate
pedagogically aligned illustrations for STEM education. It leverages a curated
BookCover dataset that provides narrative layouts and structured visual cues,
enabling the model to depict abstract and sequential scientific concepts with
strong semantic alignment. Through layout-conditioned generation, contrastive
visual-semantic training, and prompt modulation, LEARN produces coherent visual
sequences that support mid-to-high-level reasoning in line with Bloom's
taxonomy while reducing extraneous cognitive load as emphasized by Cognitive
Load Theory. By fostering spatially organized and story-driven narratives, the
framework counters fragmented attention often induced by short-form media and
promotes sustained conceptual focus. Beyond static diagrams, LEARN demonstrates
potential for integration with multimodal systems and curriculum-linked
knowledge graphs to create adaptive, exploratory educational content. As the
first generative approach to unify layout-based storytelling, semantic
structure learning, and cognitive scaffolding, LEARN represents a novel
direction for generative AI in education. The code and dataset will be released
to facilitate future research and practical deployment.

</details>


### [32] [Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models](https://arxiv.org/abs/2508.11165)
*Bing Liu,Le Wang,Mingming Liu,Hao Liu,Rui Yao,Yong Zhou,Peng Liu,Tongqiang Xia*

Main category: cs.CV

TL;DR: 提出了一种基于EM算法和双向布朗桥扩散模型（EM-B3DM）的半监督图像去雾方法，通过两阶段学习方案解决了真实世界雾霾图像去雾的难题。


<details>
  <summary>Details</summary>
Motivation: 现有去雾方法难以处理真实世界中的厚雾场景，主要原因是缺乏真实配对数据和鲁棒先验。

Method: 采用两阶段学习方案：第一阶段使用EM算法解耦配对图像的联合分布，并用布朗桥扩散模型建模；第二阶段利用预训练模型和大规模未配对数据进一步提升性能。此外，引入RDC块增强细节。

Result: 在合成和真实数据集上，EM-B3DM表现优于或至少与现有最佳方法相当。

Conclusion: EM-B3DM是一种高效的去雾方法，尤其适用于真实世界厚雾场景。

Abstract: Existing dehazing methods deal with real-world haze images with difficulty,
especially scenes with thick haze. One of the main reasons is the lack of
real-world paired data and robust priors. To avoid the costly collection of
paired hazy and clear images, we propose an efficient semi-supervised image
dehazing method via Expectation-Maximization and Bidirectional Brownian Bridge
Diffusion Models (EM-B3DM) with a two-stage learning scheme. In the first
stage, we employ the EM algorithm to decouple the joint distribution of paired
hazy and clear images into two conditional distributions, which are then
modeled using a unified Brownian Bridge diffusion model to directly capture the
structural and content-related correlations between hazy and clear images. In
the second stage, we leverage the pre-trained model and large-scale unpaired
hazy and clear images to further improve the performance of image dehazing.
Additionally, we introduce a detail-enhanced Residual Difference Convolution
block (RDC) to capture gradient-level information, significantly enhancing the
model's representation capability. Extensive experiments demonstrate that our
EM-B3DM achieves superior or at least comparable performance to
state-of-the-art methods on both synthetic and real-world datasets.

</details>


### [33] [VFM-Guided Semi-Supervised Detection Transformer for Source-Free Object Detection in Remote Sensing Images](https://arxiv.org/abs/2508.11167)
*Jianhong Han,Yupei Wang,Liang Chen*

Main category: cs.CV

TL;DR: VG-DETR是一种基于视觉基础模型的半监督框架，用于解决源自由目标检测（SFOD）中的伪标签噪声问题，通过双级对齐和伪标签挖掘策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 在遥感场景中，源域数据不可用导致传统域适应方法受限，SFOD虽为替代方案，但常因伪标签噪声而训练失败。

Method: VG-DETR结合视觉基础模型（VFM），通过伪标签挖掘策略和双级特征对齐（实例级和图像级）减少噪声并增强特征提取。

Result: 实验表明VG-DETR在源自由遥感检测任务中表现优异。

Conclusion: VG-DETR通过VFM的语义先验和双级对齐，有效解决了SFOD中的伪标签噪声问题，提升了检测性能。

Abstract: Unsupervised domain adaptation methods have been widely explored to bridge
domain gaps. However, in real-world remote-sensing scenarios, privacy and
transmission constraints often preclude access to source domain data, which
limits their practical applicability. Recently, Source-Free Object Detection
(SFOD) has emerged as a promising alternative, aiming at cross-domain
adaptation without relying on source data, primarily through a self-training
paradigm. Despite its potential, SFOD frequently suffers from training collapse
caused by noisy pseudo-labels, especially in remote sensing imagery with dense
objects and complex backgrounds. Considering that limited target domain
annotations are often feasible in practice, we propose a Vision
foundation-Guided DEtection TRansformer (VG-DETR), built upon a semi-supervised
framework for SFOD in remote sensing images. VG-DETR integrates a Vision
Foundation Model (VFM) into the training pipeline in a "free lunch" manner,
leveraging a small amount of labeled target data to mitigate pseudo-label noise
while improving the detector's feature-extraction capability. Specifically, we
introduce a VFM-guided pseudo-label mining strategy that leverages the VFM's
semantic priors to further assess the reliability of the generated
pseudo-labels. By recovering potentially correct predictions from
low-confidence outputs, our strategy improves pseudo-label quality and
quantity. In addition, a dual-level VFM-guided alignment method is proposed,
which aligns detector features with VFM embeddings at both the instance and
image levels. Through contrastive learning among fine-grained prototypes and
similarity matching between feature maps, this dual-level alignment further
enhances the robustness of feature representations against domain gaps.
Extensive experiments demonstrate that VG-DETR achieves superior performance in
source-free remote sensing detection tasks.

</details>


### [34] [Better Supervised Fine-tuning for VQA: Integer-Only Loss](https://arxiv.org/abs/2508.11170)
*Baihong Qian,Haotian Fan,Wenjie Liao,Yunqiu Wang,Tao Li,Junhui Cui*

Main category: cs.CV

TL;DR: 论文提出了一种名为IOVQA的微调方法，通过整数标签和目标掩码策略优化视觉语言模型在视频质量评估任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉内容评估中存在结果不精确和损失计算低效的问题，限制了模型对关键评估指标的关注。

Method: 采用整数标签（范围[10,50]）和目标掩码策略（仅暴露标签的前两位数）进行微调。

Result: 实验表明，该方法显著提升了模型在VQA任务中的准确性和一致性，在VQualA 2025挑战赛中排名第三。

Conclusion: 整数标签微调在定量评估场景中优化视觉语言模型的有效性得到了验证。

Abstract: With the rapid advancement of vision language models(VLM), their ability to
assess visual content based on specific criteria and dimensions has become
increasingly critical for applications such as video-theme consistency
assessment and visual quality scoring. However, existing methods often suffer
from imprecise results and inefficient loss calculation, which limit the focus
of the model on key evaluation indicators. To address this, we propose
IOVQA(Integer-only VQA), a novel fine-tuning approach tailored for VLMs to
enhance their performance in video quality assessment tasks. The key innovation
of IOVQA lies in its label construction and its targeted loss calculation
mechanism. Specifically, during dataset curation, we constrain the model's
output to integers within the range of [10,50], ensuring numerical stability,
and convert decimal Overall_MOS to integer before using them as labels. We also
introduce a target-mask strategy: when computing the loss, only the first
two-digit-integer of the label is unmasked, forcing the model to learn the
critical components of the numerical evaluation. After fine-tuning the
Qwen2.5-VL model using the constructed dataset, experimental results
demonstrate that the proposed method significantly improves the model's
accuracy and consistency in the VQA task, ranking 3rd in VQualA 2025
GenAI-Bench AIGC Video Quality Assessment Challenge -- Track I. Our work
highlights the effectiveness of merely leaving integer labels during
fine-tuning, providing an effective idea for optimizing VLMs in quantitative
evaluation scenarios.

</details>


### [35] [Exploring the Tradeoff Between Diversity and Discrimination for Continuous Category Discovery](https://arxiv.org/abs/2508.11173)
*Ruobing Jiang,Yang Liu,Haobing Liu,Yanwei Yu,Chunyang Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为IDOD的方法，用于连续类别发现（CCD），通过独立多样性模块、联合发现模块和正交性增量模块解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的CCD方法在发现新类别和分类之间存在矛盾，且容易累积错误，同时占用较多存储空间防止遗忘。

Method: IDOD方法包括独立多样性模块（使用对比损失训练）、联合发现模块（单阶段新类别发现）和正交性增量模块（生成正交原型）。

Result: 实验表明，IDOD在细粒度数据集上优于现有方法。

Conclusion: IDOD通过减少错误累积和降低存储开销，有效解决了CCD问题。

Abstract: Continuous category discovery (CCD) aims to automatically discover novel
categories in continuously arriving unlabeled data. This is a challenging
problem considering that there is no number of categories and labels in the
newly arrived data, while also needing to mitigate catastrophic forgetting.
Most CCD methods cannot handle the contradiction between novel class discovery
and classification well. They are also prone to accumulate errors in the
process of gradually discovering novel classes. Moreover, most of them use
knowledge distillation and data replay to prevent forgetting, occupying more
storage space. To address these limitations, we propose Independence-based
Diversity and Orthogonality-based Discrimination (IDOD). IDOD mainly includes
independent enrichment of diversity module, joint discovery of novelty module,
and continuous increment by orthogonality module. In independent enrichment,
the backbone is trained separately using contrastive loss to avoid it focusing
only on features for classification. Joint discovery transforms multi-stage
novel class discovery into single-stage, reducing error accumulation impact.
Continuous increment by orthogonality module generates mutually orthogonal
prototypes for classification and prevents forgetting with lower space overhead
via representative representation replay. Experimental results show that on
challenging fine-grained datasets, our method outperforms the state-of-the-art
methods.

</details>


### [36] [Fine-Grained VLM Fine-tuning via Latent Hierarchical Adapter Learning](https://arxiv.org/abs/2508.11176)
*Yumiao Zhao,Bo Jiang,Yuhe Ding,Xiao Wang,Jin Tang,Bin Luo*

Main category: cs.CV

TL;DR: 论文提出了一种名为LatHAdapter的新型适配器方法，用于改进视觉-语言模型在少样本分类任务中的微调性能，通过利用潜在语义层次结构和双曲空间学习来优化类别与图像的关联。


<details>
  <summary>Details</summary>
Motivation: 现有适配器方法在少样本分类任务中未能充分捕捉类别与图像样本之间的一对多关系，且难以处理未知类别的关联。

Method: LatHAdapter通过引入可学习的属性提示作为桥梁，并在双曲空间中投影类别、属性和图像，利用层次正则化学习潜在语义层次结构。

Result: 在四个少样本任务上的实验表明，LatHAdapter在已知类别的适应和未知类别的泛化上均优于其他微调方法。

Conclusion: LatHAdapter通过潜在语义层次结构和双曲空间学习，显著提升了少样本分类任务的性能。

Abstract: Adapter-based approaches have garnered attention for fine-tuning pre-trained
Vision-Language Models (VLMs) on few-shot classification tasks. These methods
strive to develop a lightweight module that better aligns visual and (category)
textual representations, thereby enhancing performance on downstream few-shot
learning tasks. However, existing adapters generally learn/align (category)
textual-visual modalities via explicit spatial proximity in the underlying
embedding space, which i) fails to capture the inherent one-to-many
associations between categories and image samples and ii) struggles to
establish accurate associations between the unknown categories and images. To
address these issues, inspired by recent works on hyperbolic learning, we
develop a novel Latent Hierarchical Adapter (LatHAdapter) for fine-tuning VLMs
on downstream few-shot classification tasks. The core of LatHAdapter is to
exploit the latent semantic hierarchy of downstream training data and employ it
to provide richer, fine-grained guidance for the adapter learning process.
Specifically, LatHAdapter first introduces some learnable `attribute' prompts
as the bridge to align categories and images. Then, it projects the categories,
attribute prompts, and images within each batch in a hyperbolic space, and
employs hierarchical regularization to learn the latent semantic hierarchy of
them, thereby fully modeling the inherent one-to-many associations among
categories, learnable attributes, and image samples. Extensive experiments on
four challenging few-shot tasks show that the proposed LatHAdapter consistently
outperforms many other fine-tuning approaches, particularly in adapting known
classes and generalizing to unknown classes.

</details>


### [37] [Versatile Video Tokenization with Generative 2D Gaussian Splatting](https://arxiv.org/abs/2508.11183)
*Zhenghao Chen,Zicong Chen,Lei Liu,Yiming Wu,Dong Xu*

Main category: cs.CV

TL;DR: GVT是一种基于2D高斯生成策略的视频标记化方法，通过空间自适应和时间分离提升视频处理性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频标记化方法在空间和时间维度上存在局限性，GVT旨在通过生成2D高斯分布和分离静态与动态内容来解决这些问题。

Method: GVT采用STGE机制生成2D高斯分布，并通过GSP策略分离静态与动态内容。

Result: GVT在视频重建、动作识别和压缩任务中表现优异，优于基线方法。

Conclusion: GVT通过空间自适应和时间分离实现了高效的视频标记化，为视频处理任务提供了新思路。

Abstract: Video tokenization procedure is critical for a wide range of video processing
tasks. Most existing approaches directly transform video into fixed-grid and
patch-wise tokens, which exhibit limited versatility. Spatially, uniformly
allocating a fixed number of tokens often leads to over-encoding in
low-information regions. Temporally, reducing redundancy remains challenging
without explicitly distinguishing between static and dynamic content. In this
work, we propose the Gaussian Video Transformer (GVT), a versatile video
tokenizer built upon a generative 2D Gaussian Splatting (2DGS) strategy. We
first extract latent rigid features from a video clip and represent them with a
set of 2D Gaussians generated by our proposed Spatio-Temporal Gaussian
Embedding (STGE) mechanism in a feed-forward manner. Such generative 2D
Gaussians not only enhance spatial adaptability by assigning higher (resp.,
lower) rendering weights to regions with higher (resp., lower) information
content during rasterization, but also improve generalization by avoiding
per-video optimization.To enhance the temporal versatility, we introduce a
Gaussian Set Partitioning (GSP) strategy that separates the 2D Gaussians into
static and dynamic sets, which explicitly model static content shared across
different time-steps and dynamic content specific to each time-step, enabling a
compact representation.We primarily evaluate GVT on the video reconstruction,
while also assessing its performance on action recognition and compression
using the UCF101, Kinetics, and DAVIS datasets. Extensive experiments
demonstrate that GVT achieves a state-of-the-art video reconstruction quality,
outperforms the baseline MAGVIT-v2 in action recognition, and delivers
comparable compression performance.

</details>


### [38] [CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector](https://arxiv.org/abs/2508.11185)
*Abhinav Kumar,Yuliang Guo,Zhihao Zhang,Xinyu Huang,Liu Ren,Xiaoming Liu*

Main category: cs.CV

TL;DR: 论文研究了单目3D目标检测器在不同相机高度下的性能问题，提出了一种新方法CHARM3R，通过结合两种深度估计提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D检测器在相机高度变化时性能下降，尤其对未见过的相机高度表现不佳。

Method: 通过系统分析相机高度变化对深度估计的影响，提出CHARM3R模型，结合回归和地面深度估计。

Result: CHARM3R在未见相机高度上的泛化能力提升超过45%，在CARLA数据集上达到SOTA性能。

Conclusion: 结合不同深度估计方法能显著提升单目3D检测器在相机高度变化下的鲁棒性。

Abstract: Monocular 3D object detectors, while effective on data from one ego camera
height, struggle with unseen or out-of-distribution camera heights. Existing
methods often rely on Plucker embeddings, image transformations or data
augmentation. This paper takes a step towards this understudied problem by
first investigating the impact of camera height variations on state-of-the-art
(SoTA) Mono3D models. With a systematic analysis on the extended CARLA dataset
with multiple camera heights, we observe that depth estimation is a primary
factor influencing performance under height variations. We mathematically prove
and also empirically observe consistent negative and positive trends in mean
depth error of regressed and ground-based depth models, respectively, under
camera height changes. To mitigate this, we propose Camera Height Robust
Monocular 3D Detector (CHARM3R), which averages both depth estimates within the
model. CHARM3R improves generalization to unseen camera heights by more than
$45\%$, achieving SoTA performance on the CARLA dataset. Codes and Models at
https://github.com/abhi1kumar/CHARM3R

</details>


### [39] [Generating Dialogues from Egocentric Instructional Videos for Task Assistance: Dataset, Method and Benchmark](https://arxiv.org/abs/2508.11192)
*Lavisha Aggarwal,Vikas Bahirwani,Lin Li,Andrea Colaco*

Main category: cs.CV

TL;DR: 论文提出了一种自动将单人教学视频转化为任务指导对话的方法，并构建了HowToDIV数据集，用于多步骤任务辅助对话研究。


<details>
  <summary>Details</summary>
Motivation: 现实世界中复杂任务需要专家知识，但缺乏基于对话和视频的任务辅助数据集。

Method: 利用大语言模型自动将单人教学视频转化为两人对话，并与视频片段对齐。

Result: 构建了包含507个对话、6636个问答对和24小时视频的HowToDIV数据集，并建立了基准性能。

Conclusion: 该方法为任务辅助对话研究提供了高效的数据集生成方案，并设定了未来研究的基准。

Abstract: Many everyday tasks ranging from fixing appliances, cooking recipes to car
maintenance require expert knowledge, especially when tasks are complex and
multi-step. Despite growing interest in AI agents, there is a scarcity of
dialogue-video datasets grounded for real world task assistance. In this paper,
we propose a simple yet effective approach that transforms single-person
instructional videos into task-guidance two-person dialogues, aligned with fine
grained steps and video-clips. Our fully automatic approach, powered by large
language models, offers an efficient alternative to the substantial cost and
effort required for human-assisted data collection. Using this technique, we
build HowToDIV, a large-scale dataset containing 507 conversations, 6636
question-answer pairs and 24 hours of videoclips across diverse tasks in
cooking, mechanics, and planting. Each session includes multi-turn conversation
where an expert teaches a novice user how to perform a task step by step, while
observing user's surrounding through a camera and microphone equipped wearable
device. We establish the baseline benchmark performance on HowToDIV dataset
through Gemma-3 model for future research on this new task of dialogues for
procedural-task assistance.

</details>


### [40] [UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning](https://arxiv.org/abs/2508.11196)
*Jiajin Guan,Haibo Mei,Bonan Zhang,Dan Liu,Yuanshuang Fu,Yue Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级视觉语言模型UAV-VL-R1，专为无人机航拍图像设计，通过混合训练方法提升性能，并在新数据集HRVQA-VL上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 通用视觉语言模型在无人机航拍图像上性能下降，因其高分辨率、复杂空间语义和实时性要求，限制了其在结构化航拍推理任务中的应用。

Method: 采用监督微调（SFT）和多阶段强化学习（RL）的混合方法，结合GRPO算法提升推理的结构化和可解释性。

Result: UAV-VL-R1在零样本准确率上比基线模型高48.17%，甚至优于36倍大的模型变体，且内存占用低，适合实时部署。

Conclusion: UAV-VL-R1通过混合训练方法有效解决了航拍图像的推理挑战，兼具高性能和轻量化，适用于资源受限的无人机平台。

Abstract: Recent advances in vision-language models (VLMs) have demonstrated strong
generalization in natural image tasks. However, their performance often
degrades on unmanned aerial vehicle (UAV)-based aerial imagery, which features
high resolution, complex spatial semantics, and strict real-time constraints.
These challenges limit the applicability of general-purpose VLMs to structured
aerial reasoning tasks. To address these challenges, we propose UAV-VL-R1, a
lightweight VLM explicitly designed for aerial visual reasoning. It is trained
using a hybrid method that combines supervised fine-tuning (SFT) and
multi-stage reinforcement learning (RL). We leverage the group relative policy
optimization (GRPO) algorithm to promote structured and interpretable reasoning
through rule-guided rewards and intra-group policy alignment. To support model
training and evaluation, we introduce a high-resolution visual question
answering dataset named HRVQA-VL, which consists of 50,019 annotated samples
covering eight UAV-relevant reasoning tasks, including object counting,
transportation recognition, and spatial scene inference. Experimental results
show that UAV-VL-R1 achieves a 48.17% higher zero-shot accuracy than the
Qwen2-VL-2B-Instruct baseline and even outperforms its 72B-scale variant, which
is 36x larger, on multiple tasks. Ablation studies reveal that while SFT
improves semantic alignment, it may reduce reasoning diversity in mathematical
tasks. GRPO-based RL compensates for this limitation by enhancing logical
flexibility and the robustness of inference. Additionally, UAV-VL-R1 requires
only 3.9GB of memory under FP16 inference and can be quantized to 2.5GB with
INT8, supporting real-time deployment on resource-constrained UAV platforms.

</details>


### [41] [A Coarse-to-Fine Human Pose Estimation Method based on Two-stage Distillation and Progressive Graph Neural Network](https://arxiv.org/abs/2508.11212)
*Zhangjian Ji,Wenjin Zhang,Shaotong Qiao,Kai Feng,Yuhua Qian*

Main category: cs.CV

TL;DR: 提出了一种新颖的粗到细两阶段知识蒸馏框架，用于轻量级人体姿态估计，通过结构损失和渐进图卷积网络提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有姿态估计方法计算资源消耗大，传统知识蒸馏未充分利用关节上下文信息。

Method: 两阶段蒸馏：第一阶段用结构损失传递语义知识；第二阶段用IGP-GCN渐进优化姿态。

Result: 在COCO和CrowdPose数据集上表现优异，尤其在复杂场景下性能提升显著。

Conclusion: 该方法实现了轻量且高精度的人体姿态估计，适用于复杂场景。

Abstract: Human pose estimation has been widely applied in the human-centric
understanding and generation, but most existing state-of-the-art human pose
estimation methods require heavy computational resources for accurate
predictions. In order to obtain an accurate, robust yet lightweight human pose
estimator, one feasible way is to transfer pose knowledge from a powerful
teacher model to a less-parameterized student model by knowledge distillation.
However, the traditional knowledge distillation framework does not fully
explore the contextual information among human joints. Thus, in this paper, we
propose a novel coarse-to-fine two-stage knowledge distillation framework for
human pose estimation. In the first-stage distillation, we introduce the human
joints structure loss to mine the structural information among human joints so
as to transfer high-level semantic knowledge from the teacher model to the
student model. In the second-stage distillation, we utilize an Image-Guided
Progressive Graph Convolutional Network (IGP-GCN) to refine the initial human
pose obtained from the first-stage distillation and supervise the training of
the IGP-GCN in the progressive way by the final output pose of teacher model.
The extensive experiments on the benchmark dataset: COCO keypoint and CrowdPose
datasets, show that our proposed method performs favorably against lots of the
existing state-of-the-art human pose estimation methods, especially for the
more complex CrowdPose dataset, the performance improvement of our model is
more significant.

</details>


### [42] [A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving](https://arxiv.org/abs/2508.11218)
*Jialin Li,Shuqi Wu,Ning Wang*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级不确定性模态建模（UMM）框架，用于解决多模态输入不确定或缺失时的行人重识别问题，结合了模态令牌映射、合成模态增强和跨模态交互学习，实验证明其高效且鲁棒。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中行人重识别（ReID）面临多模态输入不确定或缺失的挑战，传统方法难以应对，而预训练模型计算开销大，因此需要轻量级解决方案。

Method: 提出UMM框架，包括模态令牌映射器、合成模态增强策略和跨模态交互学习器，并利用CLIP的视觉-语言对齐能力高效融合多模态输入。

Result: 实验表明，UMM在不确定模态条件下具有强鲁棒性、泛化能力和计算效率。

Conclusion: UMM为自动驾驶场景中的行人重识别提供了可扩展且实用的解决方案。

Abstract: Re-Identification (ReID) is a critical technology in intelligent perception
systems, especially within autonomous driving, where onboard cameras must
identify pedestrians across views and time in real-time to support safe
navigation and trajectory prediction. However, the presence of uncertain or
missing input modalities--such as RGB, infrared, sketches, or textual
descriptions--poses significant challenges to conventional ReID approaches.
While large-scale pre-trained models offer strong multimodal semantic modeling
capabilities, their computational overhead limits practical deployment in
resource-constrained environments. To address these challenges, we propose a
lightweight Uncertainty Modal Modeling (UMM) framework, which integrates a
multimodal token mapper, synthetic modality augmentation strategy, and
cross-modal cue interactive learner. Together, these components enable unified
feature representation, mitigate the impact of missing modalities, and extract
complementary information across different data types. Additionally, UMM
leverages CLIP's vision-language alignment ability to fuse multimodal inputs
efficiently without extensive finetuning. Experimental results demonstrate that
UMM achieves strong robustness, generalization, and computational efficiency
under uncertain modality conditions, offering a scalable and practical solution
for pedestrian re-identification in autonomous driving scenarios.

</details>


### [43] [FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation](https://arxiv.org/abs/2508.11255)
*MengChao Wang,Qiang Wang,Fan Jiang,Mu Xu*

Main category: cs.CV

TL;DR: 论文提出了一种名为Talking-Critic的多模态奖励模型，用于量化生成视频在多维偏好上的表现，并构建了一个大规模偏好数据集Talking-NSQ。进一步提出了TLPO框架，通过分解偏好为专家模块，显著提升了肖像动画的质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在多维偏好（如动作自然性、唇同步准确性和视觉质量）上优化，且缺乏大规模标注数据集。

Method: 引入Talking-Critic奖励模型和Talking-NSQ数据集，提出TLPO框架，通过多专家模块融合优化偏好。

Result: Talking-Critic在人类偏好评分上优于现有方法，TLPO在唇同步、动作自然性和视觉质量上显著提升。

Conclusion: 论文通过奖励模型和优化框架，解决了多维偏好对齐问题，显著提升了肖像动画质量。

Abstract: Recent advances in audio-driven portrait animation have demonstrated
impressive capabilities. However, existing methods struggle to align with
fine-grained human preferences across multiple dimensions, such as motion
naturalness, lip-sync accuracy, and visual quality. This is due to the
difficulty of optimizing among competing preference objectives, which often
conflict with one another, and the scarcity of large-scale, high-quality
datasets with multidimensional preference annotations. To address these, we
first introduce Talking-Critic, a multimodal reward model that learns
human-aligned reward functions to quantify how well generated videos satisfy
multidimensional expectations. Leveraging this model, we curate Talking-NSQ, a
large-scale multidimensional human preference dataset containing 410K
preference pairs. Finally, we propose Timestep-Layer adaptive multi-expert
Preference Optimization (TLPO), a novel framework for aligning diffusion-based
portrait animation models with fine-grained, multidimensional preferences. TLPO
decouples preferences into specialized expert modules, which are then fused
across timesteps and network layers, enabling comprehensive, fine-grained
enhancement across all dimensions without mutual interference. Experiments
demonstrate that Talking-Critic significantly outperforms existing methods in
aligning with human preference ratings. Meanwhile, TLPO achieves substantial
improvements over baseline models in lip-sync accuracy, motion naturalness, and
visual quality, exhibiting superior performance in both qualitative and
quantitative evaluations. Ours project page:
https://fantasy-amap.github.io/fantasy-talking2/

</details>


### [44] [Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception](https://arxiv.org/abs/2508.11256)
*Junjie Wang,Keyu Chen,Yulin Li,Bin Chen,Hengshuang Zhao,Xiaojuan Qi,Zhuotao Tian*

Main category: cs.CV

TL;DR: DeCLIP通过解耦CLIP的自注意力模块，分别提取内容和上下文特征，结合视觉基础模型和扩散模型增强空间一致性和局部区分性，显著提升了开放词汇密集感知任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有密集视觉感知任务受限于预定义类别，而CLIP等视觉语言模型在开放词汇任务中表现不佳，主要因其局部特征表示不足。

Method: 提出DeCLIP框架，解耦自注意力模块，分别优化内容和上下文特征，结合视觉基础模型和扩散模型增强特征。

Result: DeCLIP在2D检测、分割、3D实例分割、视频实例分割和6D物体姿态估计等任务中均达到最先进性能。

Conclusion: DeCLIP为开放词汇密集感知任务提供了有效解决方案，显著提升了模型性能。

Abstract: Dense visual perception tasks have been constrained by their reliance on
predefined categories, limiting their applicability in real-world scenarios
where visual concepts are unbounded. While Vision-Language Models (VLMs) like
CLIP have shown promise in open-vocabulary tasks, their direct application to
dense perception often leads to suboptimal performance due to limitations in
local feature representation. In this work, we present our observation that
CLIP's image tokens struggle to effectively aggregate information from
spatially or semantically related regions, resulting in features that lack
local discriminability and spatial consistency. To address this issue, we
propose DeCLIP, a novel framework that enhances CLIP by decoupling the
self-attention module to obtain ``content'' and ``context'' features
respectively. \revise{The context features are enhanced by jointly distilling
semantic correlations from Vision Foundation Models (VFMs) and object integrity
cues from diffusion models, thereby enhancing spatial consistency. In parallel,
the content features are aligned with image crop representations and
constrained by region correlations from VFMs to improve local discriminability.
Extensive experiments demonstrate that DeCLIP establishes a solid foundation
for open-vocabulary dense perception, consistently achieving state-of-the-art
performance across a broad spectrum of tasks, including 2D detection and
segmentation, 3D instance segmentation, video instance segmentation, and 6D
object pose estimation.} Code is available at
https://github.com/xiaomoguhz/DeCLIP

</details>


### [45] [Vision-Language Models display a strong gender bias](https://arxiv.org/abs/2508.11262)
*Aiswarya Konavoor,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.CV

TL;DR: 研究探讨了视觉语言模型（VLM）在图像和文本对齐中可能隐含的性别偏见，通过分析人脸图像与职业描述短语的嵌入相似性，揭示了模型中的性别关联模式。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的对齐可能隐含社会刻板印象，但标准准确性指标难以察觉。研究旨在测试模型是否存在性别关联偏见。

Method: 使用220张人脸照片（按性别分组）和150条描述职业和活动的短语，计算图像和文本嵌入的余弦相似性，定义性别关联分数，并通过置信区间和标签交换模型验证。

Result: 研究提供了视觉语言空间中性别关联的详细映射，并开发了一个稳健的性别偏见评估框架。

Conclusion: 视觉语言模型可能隐含性别偏见，需要更细致的评估方法以减少社会刻板印象的放大。

Abstract: Vision-language models (VLM) align images and text in a shared representation
space that is useful for retrieval and zero-shot transfer. Yet, this alignment
can encode and amplify social stereotypes in subtle ways that are not obvious
from standard accuracy metrics. In this study, we test whether the contrastive
vision-language encoder exhibits gender-linked associations when it places
embeddings of face images near embeddings of short phrases that describe
occupations and activities. We assemble a dataset of 220 face photographs split
by perceived binary gender and a set of 150 unique statements distributed
across six categories covering emotional labor, cognitive labor, domestic
labor, technical labor, professional roles, and physical labor. We compute
unit-norm image embeddings for every face and unit-norm text embeddings for
every statement, then define a statement-level association score as the
difference between the mean cosine similarity to the male set and the mean
cosine similarity to the female set, where positive values indicate stronger
association with the male set and negative values indicate stronger association
with the female set. We attach bootstrap confidence intervals by resampling
images within each gender group, aggregate by category with a separate
bootstrap over statements, and run a label-swap null model that estimates the
level of mean absolute association we would expect if no gender structure were
present. The outcome is a statement-wise and category-wise map of gender
associations in a contrastive vision-language space, accompanied by
uncertainty, simple sanity checks, and a robust gender bias evaluation
framework.

</details>


### [46] [Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds](https://arxiv.org/abs/2508.11265)
*Pei He,Lingling Li,Licheng Jiao,Ronghua Shang,Fang Liu,Shuang Wang,Xu Liu,Wenping Ma*

Main category: cs.CV

TL;DR: 提出了一种类别级几何学习框架，通过感知点云特征的细粒度几何属性并耦合几何嵌入与语义学习，提升3D语义分割的领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决3D分割模型在未见环境中因忽略类别级分布和对齐而导致的领域泛化问题。

Method: 提出类别级几何嵌入（CGE）和几何一致学习（GCL），分别用于感知几何属性和对齐类别级几何嵌入。

Result: 实验验证了方法的有效性，分割精度优于现有领域泛化点云方法。

Conclusion: 通过类别级几何学习框架，显著提升了3D语义分割的领域泛化能力。

Abstract: Domain generalization in 3D segmentation is a critical challenge in deploying
models to unseen environments. Current methods mitigate the domain shift by
augmenting the data distribution of point clouds. However, the model learns
global geometric patterns in point clouds while ignoring the category-level
distribution and alignment. In this paper, a category-level geometry learning
framework is proposed to explore the domain-invariant geometric features for
domain generalized 3D semantic segmentation. Specifically, Category-level
Geometry Embedding (CGE) is proposed to perceive the fine-grained geometric
properties of point cloud features, which constructs the geometric properties
of each class and couples geometric embedding to semantic learning. Secondly,
Geometric Consistent Learning (GCL) is proposed to simulate the latent 3D
distribution and align the category-level geometric embeddings, allowing the
model to focus on the geometric invariant information to improve
generalization. Experimental results verify the effectiveness of the proposed
method, which has very competitive segmentation accuracy compared with the
state-of-the-art domain generalized point cloud methods.

</details>


### [47] [Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented Representation Engineering](https://arxiv.org/abs/2508.11272)
*Jun Li,Kai Li,Shaoguo Liu,Tingting Gao*

Main category: cs.CV

TL;DR: 提出了一种名为PMTFR的框架，通过金字塔匹配模型和无训练细化方法，解决了组合图像检索（CIR）中的挑战，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在组合图像检索中需要额外训练排名模型，且Chain-of-Thought（CoT）技术在CIR任务中应用有限。本文旨在通过无训练细化方法提升性能。

Method: 使用金字塔匹配模型（Pyramid Matching Model）和金字塔修补模块（Pyramid Patcher）增强视觉信息理解，结合CoT数据表示注入LVLMs，实现无训练细化。

Result: 在CIR基准测试中，PMTFR优于现有方法，尤其在监督CIR任务中表现突出。

Conclusion: PMTFR框架通过无训练细化方法显著提升了组合图像检索的性能，代码将公开。

Abstract: Composed Image Retrieval (CIR) presents a significant challenge as it
requires jointly understanding a reference image and a modified textual
instruction to find relevant target images. Some existing methods attempt to
use a two-stage approach to further refine retrieval results. However, this
often requires additional training of a ranking model. Despite the success of
Chain-of-Thought (CoT) techniques in reducing training costs for language
models, their application in CIR tasks remains limited -- compressing visual
information into text or relying on elaborate prompt designs. Besides, existing
works only utilize it for zero-shot CIR, as it is challenging to achieve
satisfactory results in supervised CIR with a well-trained model. In this work,
we proposed a framework that includes the Pyramid Matching Model with
Training-Free Refinement (PMTFR) to address these challenges. Through a simple
but effective module called Pyramid Patcher, we enhanced the Pyramid Matching
Model's understanding of visual information at different granularities.
Inspired by representation engineering, we extracted representations from COT
data and injected them into the LVLMs. This approach allowed us to obtain
refined retrieval scores in the Training-Free Refinement paradigm without
relying on explicit textual reasoning, further enhancing performance. Extensive
experiments on CIR benchmarks demonstrate that PMTFR surpasses state-of-the-art
methods in supervised CIR tasks. The code will be made public.

</details>


### [48] [Probing the Representational Power of Sparse Autoencoders in Vision Models](https://arxiv.org/abs/2508.11277)
*Matthew Lyle Olson,Musashi Hinck,Neale Ratzlaff,Changbai Li,Phillip Howard,Vasudev Lal,Shao-Yen Tseng*

Main category: cs.CV

TL;DR: 稀疏自编码器（SAEs）在视觉模型中的应用评估显示，其具有语义意义、提升泛化能力，并支持可控生成。


<details>
  <summary>Details</summary>
Motivation: 尽管SAEs在语言模型中广泛应用，但在视觉领域的研究较少，本文旨在填补这一空白。

Method: 通过多种视觉任务评估SAEs在视觉嵌入模型、多模态LMMs和扩散模型中的表现。

Result: SAEs特征具有语义意义，提升泛化能力，支持可控生成，并揭示跨模态共享表示。

Conclusion: SAEs在视觉模型中具有提升可解释性、泛化能力和可控性的潜力。

Abstract: Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting
the hidden states of large language models (LLMs). By learning to reconstruct
activations from a sparse bottleneck layer, SAEs discover interpretable
features from the high-dimensional internal representations of LLMs. Despite
their popularity with language models, SAEs remain understudied in the visual
domain. In this work, we provide an extensive evaluation the representational
power of SAEs for vision models using a broad range of image-based tasks. Our
experimental results demonstrate that SAE features are semantically meaningful,
improve out-of-distribution generalization, and enable controllable generation
across three vision model architectures: vision embedding models, multi-modal
LMMs and diffusion models. In vision embedding models, we find that learned SAE
features can be used for OOD detection and provide evidence that they recover
the ontological structure of the underlying model. For diffusion models, we
demonstrate that SAEs enable semantic steering through text encoder
manipulation and develop an automated pipeline for discovering
human-interpretable attributes. Finally, we conduct exploratory experiments on
multi-modal LLMs, finding evidence that SAE features reveal shared
representations across vision and language modalities. Our study provides a
foundation for SAE evaluation in vision models, highlighting their strong
potential improving interpretability, generalization, and steerability in the
visual domain.

</details>


### [49] [Unifying Scale-Aware Depth Prediction and Perceptual Priors for Monocular Endoscope Pose Estimation and Tissue Reconstruction](https://arxiv.org/abs/2508.11282)
*Muzammil Khan,Enzo Kerkhof,Matteo Fusaglia,Koert Kuhlmann,Theo Ruers,Françoise J. Siepel*

Main category: cs.CV

TL;DR: 提出了一种统一框架，结合尺度感知深度预测和时间约束感知细化，用于单目内窥镜组织重建，解决了深度模糊和组织变形等问题。


<details>
  <summary>Details</summary>
Motivation: 单目内窥镜姿态估计和组织重建面临深度模糊、组织变形、运动不一致等挑战，亟需改进以提高手术导航和空间感知能力。

Method: 框架包含MAPIS-Depth模块（结合Depth Pro和Depth Anything进行深度预测）和WEMA-RTDL模块（优化旋转和平移），并通过RAFT和LPIPS进行时间细化。

Result: 在HEVD和SCARED数据集上的评估表明，该方法优于现有技术，具有鲁棒性和优越性。

Conclusion: 该框架有效解决了单目内窥镜重建的挑战，为手术导航提供了更准确的3D表面重建。

Abstract: Accurate endoscope pose estimation and 3D tissue surface reconstruction
significantly enhances monocular minimally invasive surgical procedures by
enabling accurate navigation and improved spatial awareness. However, monocular
endoscope pose estimation and tissue reconstruction face persistent challenges,
including depth ambiguity, physiological tissue deformation, inconsistent
endoscope motion, limited texture fidelity, and a restricted field of view. To
overcome these limitations, a unified framework for monocular endoscopic tissue
reconstruction that integrates scale-aware depth prediction with
temporally-constrained perceptual refinement is presented. This framework
incorporates a novel MAPIS-Depth module, which leverages Depth Pro for robust
initialisation and Depth Anything for efficient per-frame depth prediction, in
conjunction with L-BFGS-B optimisation, to generate pseudo-metric depth
estimates. These estimates are temporally refined by computing pixel
correspondences using RAFT and adaptively blending flow-warped frames based on
LPIPS perceptual similarity, thereby reducing artefacts arising from
physiological tissue deformation and motion. To ensure accurate registration of
the synthesised pseudo-RGBD frames from MAPIS-Depth, a novel WEMA-RTDL module
is integrated, optimising both rotation and translation. Finally, truncated
signed distance function-based volumetric fusion and marching cubes are applied
to extract a comprehensive 3D surface mesh. Evaluations on HEVD and SCARED,
with ablation and comparative analyses, demonstrate the framework's robustness
and superiority over state-of-the-art methods.

</details>


### [50] [TimeMachine: Fine-Grained Facial Age Editing with Identity Preservation](https://arxiv.org/abs/2508.11284)
*Yilin Mi,Qixin Yan,Zheng-Peng Duan,Chunle Guo,Hubery Yin,Hao Liu,Chen Li,Chongyi Li*

Main category: cs.CV

TL;DR: TimeMachine是一种基于扩散模型的框架，通过多交叉注意力模块注入高精度年龄信息，实现细粒度年龄编辑并保持身份特征不变。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在面部图像编辑中取得进展，但细粒度年龄编辑同时保持身份特征仍具挑战性。

Method: 提出TimeMachine框架，结合多交叉注意力模块和Age Classifier Guidance模块，并构建HFFA数据集。

Result: 实验表明，TimeMachine在细粒度年龄编辑和身份一致性保持方面达到最优性能。

Conclusion: TimeMachine通过创新设计和高质量数据集，解决了细粒度年龄编辑的难题。

Abstract: With the advancement of generative models, facial image editing has made
significant progress. However, achieving fine-grained age editing while
preserving personal identity remains a challenging task.In this paper, we
propose TimeMachine, a novel diffusion-based framework that achieves accurate
age editing while keeping identity features unchanged. To enable fine-grained
age editing, we inject high-precision age information into the multi-cross
attention module, which explicitly separates age-related and identity-related
features. This design facilitates more accurate disentanglement of age
attributes, thereby allowing precise and controllable manipulation of facial
aging.Furthermore, we propose an Age Classifier Guidance (ACG) module that
predicts age directly in the latent space, instead of performing denoising
image reconstruction during training. By employing a lightweight module to
incorporate age constraints, this design enhances age editing accuracy by
modest increasing training cost. Additionally, to address the lack of
large-scale, high-quality facial age datasets, we construct a HFFA dataset
(High-quality Fine-grained Facial-Age dataset) which contains one million
high-resolution images labeled with identity and facial attributes.
Experimental results demonstrate that TimeMachine achieves state-of-the-art
performance in fine-grained age editing while preserving identity consistency.

</details>


### [51] [Hyperspectral vs. RGB for Pedestrian Segmentation in Urban Driving Scenes: A Comparative Study](https://arxiv.org/abs/2508.11301)
*Jiarong Li,Imad Ali Shah,Enda Ward,Martin Glavin,Edward Jones,Brian Deegan*

Main category: cs.CV

TL;DR: 研究探讨了高光谱成像（HSI）在行人分割中的潜力，通过对比RGB和两种降维方法（PCA和CSNR-JMIM），发现CSNR-JMIM在性能上优于RGB，显著提升了分割精度。


<details>
  <summary>Details</summary>
Motivation: RGB成像中行人与背景的视觉相似性（同色异谱现象）导致行人分割存在安全隐患，因此研究高光谱成像的潜力以提升分割效果。

Method: 使用H-City数据集，将128通道HSI数据降维为三通道（PCA和CSNR-JMIM），并评估了U-Net、DeepLabV3+和SegFormer三种分割模型。

Result: CSNR-JMIM在行人分割中平均IoU提升1.44%，F1-score提升2.18%；骑行者分割也有类似提升。

Conclusion: 通过优化HSI波段选择，显著提升了行人分割性能，展示了其在安全关键汽车应用中的潜力。

Abstract: Pedestrian segmentation in automotive perception systems faces critical
safety challenges due to metamerism in RGB imaging, where pedestrians and
backgrounds appear visually indistinguishable.. This study investigates the
potential of hyperspectral imaging (HSI) for enhanced pedestrian segmentation
in urban driving scenarios using the Hyperspectral City v2 (H-City) dataset. We
compared standard RGB against two dimensionality-reduction approaches by
converting 128-channel HSI data into three-channel representations: Principal
Component Analysis (PCA) and optimal band selection using Contrast
Signal-to-Noise Ratio with Joint Mutual Information Maximization (CSNR-JMIM).
Three semantic segmentation models were evaluated: U-Net, DeepLabV3+, and
SegFormer. CSNR-JMIM consistently outperformed RGB with an average improvements
of 1.44% in Intersection over Union (IoU) and 2.18% in F1-score for pedestrian
segmentation. Rider segmentation showed similar gains with 1.43% IoU and 2.25%
F1-score improvements. These improved performance results from enhanced
spectral discrimination of optimally selected HSI bands effectively reducing
false positives. This study demonstrates robust pedestrian segmentation through
optimal HSI band selection, showing significant potential for safety-critical
automotive applications.

</details>


### [52] [Controlling Multimodal LLMs via Reward-guided Decoding](https://arxiv.org/abs/2508.11616)
*Oscar Mañas,Pierluca D'Oro,Koustuv Sinha,Adriana Romero-Soriano,Michal Drozdzal,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: 该论文提出了一种通过奖励引导解码的方法，用于多模态大语言模型（MLLMs）的视觉定位改进，实现了对模型输出中对象精度和召回率的动态控制。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型的广泛应用，适应多样化用户需求变得尤为重要。论文旨在通过控制解码方法提升模型的视觉定位能力。

Method: 构建两个独立的奖励模型，分别控制输出中的对象精度和召回率，并通过动态调整奖励函数权重和解码搜索范围，实现对模型推理过程的实时控制。

Result: 在标准对象幻觉基准测试中，该方法显著提升了模型的可控性，并在幻觉缓解方面优于现有方法。

Conclusion: 该方法为MLLMs的推理过程提供了灵活的控制手段，同时有效提升了视觉定位性能。

Abstract: As Multimodal Large Language Models (MLLMs) gain widespread applicability, it
is becoming increasingly desirable to adapt them for diverse user needs. In
this paper, we study the adaptation of MLLMs through controlled decoding. To
achieve this, we introduce the first method for reward-guided decoding of MLLMs
and demonstrate its application in improving their visual grounding. Our method
involves building reward models for visual grounding and using them to guide
the MLLM's decoding process. Concretely, we build two separate reward models to
independently control the degree of object precision and recall in the model's
output. Our approach enables on-the-fly controllability of an MLLM's inference
process in two ways: first, by giving control over the relative importance of
each reward function during decoding, allowing a user to dynamically trade off
object precision for recall in image captioning tasks; second, by giving
control over the breadth of the search during decoding, allowing the user to
control the trade-off between the amount of test-time compute and the degree of
visual grounding. We evaluate our method on standard object hallucination
benchmarks, showing that it provides significant controllability over MLLM
inference, while consistently outperforming existing hallucination mitigation
methods.

</details>


### [53] [Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval](https://arxiv.org/abs/2508.11313)
*Weijia Liu,Jiuxin Cao,Bo Miao,Zhiheng Fu,Xuelin Zhu,Jiawei Ge,Bo Liu,Mehwish Nasim,Ajmal Mian*

Main category: cs.CV

TL;DR: 提出了一种去噪再检索的范式（DRNet），通过过滤无关视频片段提升视频时刻检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法编码所有视频片段（包括无关片段），破坏多模态对齐并阻碍优化。

Method: 采用文本条件去噪（TCD）和文本重建反馈（TRF）模块，动态识别噪声片段并净化多模态表示。

Result: 在Charades-STA和QVHighlights数据集上超越现有方法。

Conclusion: 去噪再检索范式可提升视频时刻检索性能，并适用于其他先进模型。

Abstract: Current text-driven Video Moment Retrieval (VMR) methods encode all video
clips, including irrelevant ones, disrupting multimodal alignment and hindering
optimization. To this end, we propose a denoise-then-retrieve paradigm that
explicitly filters text-irrelevant clips from videos and then retrieves the
target moment using purified multimodal representations. Following this
paradigm, we introduce the Denoise-then-Retrieve Network (DRNet), comprising
Text-Conditioned Denoising (TCD) and Text-Reconstruction Feedback (TRF)
modules. TCD integrates cross-attention and structured state space blocks to
dynamically identify noisy clips and produce a noise mask to purify multimodal
video representations. TRF further distills a single query embedding from
purified video representations and aligns it with the text embedding, serving
as auxiliary supervision for denoising during training. Finally, we perform
conditional retrieval using text embeddings on purified video representations
for accurate VMR. Experiments on Charades-STA and QVHighlights demonstrate that
our approach surpasses state-of-the-art methods on all metrics. Furthermore,
our denoise-then-retrieve paradigm is adaptable and can be seamlessly
integrated into advanced VMR models to boost performance.

</details>


### [54] [Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models](https://arxiv.org/abs/2508.11317)
*Yuchen Zhou,Jiayu Tang,Shuo Yang,Xiaoyan Xiao,Yuqin Dai,Wenhao Yang,Chao Gou,Xiaobo Xia,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 论文提出LogicBench基准和LogicCLIP框架，系统评估并提升视觉语言模型（VLM）的逻辑理解能力，填补现有模型在逻辑推理上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（如CLIP）在逻辑理解方面存在明显不足，限制了其在实际应用中的可靠性。

Method: 提出LogicBench基准（包含50,000+视觉语言对）和LogicCLIP训练框架，通过逻辑感知数据生成和多目标对比学习提升模型逻辑敏感性。

Result: LogicCLIP在LogicBench上显著优于基线模型，逻辑理解能力提升，同时保持通用视觉语言任务的竞争力。

Conclusion: LogicBench和LogicCLIP为提升VLM逻辑能力提供了重要资源，逻辑增强不影响通用性能。

Abstract: Vision-Language Models (VLMs), exemplified by CLIP, have emerged as
foundational for multimodal intelligence. However, their capacity for logical
understanding remains significantly underexplored, resulting in critical
''logical blindspots'' that limit their reliability in practical applications.
To systematically diagnose this, we introduce LogicBench, a comprehensive
benchmark with over 50,000 vision-language pairs across 9 logical categories
and 4 diverse scenarios: images, videos, anomaly detection, and medical
diagnostics. Our evaluation reveals that existing VLMs, even the
state-of-the-art ones, fall at over 40 accuracy points below human performance,
particularly in challenging tasks like Causality and Conditionality,
highlighting their reliance on surface semantics over critical logical
structures. To bridge this gap, we propose LogicCLIP, a novel training
framework designed to boost VLMs' logical sensitivity through advancements in
both data generation and optimization objectives. LogicCLIP utilizes
logic-aware data generation and a contrastive learning strategy that combines
coarse-grained alignment, a fine-grained multiple-choice objective, and a novel
logical structure-aware objective. Extensive experiments demonstrate
LogicCLIP's substantial improvements in logical comprehension across all
LogicBench domains, significantly outperforming baselines. Moreover, LogicCLIP
retains, and often surpasses, competitive performance on general
vision-language benchmarks, demonstrating that the enhanced logical
understanding does not come at the expense of general alignment. We believe
that LogicBench and LogicCLIP will be important resources for advancing VLM
logical capabilities.

</details>


### [55] [Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking](https://arxiv.org/abs/2508.11323)
*Haonan Zhang,Xinyao Wang,Boxi Wu,Tu Zheng,Wang Yunhua,Zheng Yang*

Main category: cs.CV

TL;DR: 论文提出了一种基于空间线索一致性的3D多目标跟踪方法DSC-Track，通过动态场景线索一致性解决拥挤环境和检测不准确的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在拥挤环境或检测不准确时表现不佳，忽视了物体间的几何关系，需利用空间线索但易受干扰。

Method: 设计了基于点对特征（PPF）的时空编码器，引入线索一致性变换模块，并采用动态更新机制。

Result: 在nuScenes和Waymo数据集上验证有效，nuScenes测试集AMOTA达70.3%。

Conclusion: DSC-Track通过空间线索一致性显著提升了跟踪性能，适用于复杂场景。

Abstract: 3D multi-object tracking is a critical and challenging task in the field of
autonomous driving. A common paradigm relies on modeling individual object
motion, e.g., Kalman filters, to predict trajectories. While effective in
simple scenarios, this approach often struggles in crowded environments or with
inaccurate detections, as it overlooks the rich geometric relationships between
objects. This highlights the need to leverage spatial cues. However, existing
geometry-aware methods can be susceptible to interference from irrelevant
objects, leading to ambiguous features and incorrect associations. To address
this, we propose focusing on cue-consistency: identifying and matching stable
spatial patterns over time. We introduce the Dynamic Scene Cue-Consistency
Tracker (DSC-Track) to implement this principle. Firstly, we design a unified
spatiotemporal encoder using Point Pair Features (PPF) to learn discriminative
trajectory embeddings while suppressing interference. Secondly, our
cue-consistency transformer module explicitly aligns consistent feature
representations between historical tracks and current detections. Finally, a
dynamic update mechanism preserves salient spatiotemporal information for
stable online tracking. Extensive experiments on the nuScenes and Waymo Open
Datasets validate the effectiveness and robustness of our approach. On the
nuScenes benchmark, for instance, our method achieves state-of-the-art
performance, reaching 73.2% and 70.3% AMOTA on the validation and test sets,
respectively.

</details>


### [56] [Noise Matters: Optimizing Matching Noise for Diffusion Classifiers](https://arxiv.org/abs/2508.11330)
*Yanghao Wang,Long Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为NoOp的噪声优化方法，用于解决扩散分类器（DC）中的噪声不稳定性问题，通过频率匹配和空间匹配原则优化噪声，提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散分类器（DC）存在噪声不稳定性问题，导致分类性能波动大且需要大量噪声采样来稳定结果，影响了分类速度。

Method: 提出NoOp方法，通过频率匹配优化数据集特定噪声，并通过空间匹配训练元网络生成图像特定噪声偏移，以替代随机噪声。

Result: 在多个数据集上的实验表明，NoOp能有效提升扩散分类器的稳定性和性能。

Conclusion: NoOp通过优化噪声解决了DC的噪声不稳定性问题，显著提升了分类速度和性能。

Abstract: Although today's pretrained discriminative vision-language models (e.g.,
CLIP) have demonstrated strong perception abilities, such as zero-shot image
classification, they also suffer from the bag-of-words problem and spurious
bias. To mitigate these problems, some pioneering studies leverage powerful
generative models (e.g., pretrained diffusion models) to realize generalizable
image classification, dubbed Diffusion Classifier (DC). Specifically, by
randomly sampling a Gaussian noise, DC utilizes the differences of denoising
effects with different category conditions to classify categories.
Unfortunately, an inherent and notorious weakness of existing DCs is noise
instability: different random sampled noises lead to significant performance
changes. To achieve stable classification performance, existing DCs always
ensemble the results of hundreds of sampled noises, which significantly reduces
the classification speed. To this end, we firstly explore the role of noise in
DC, and conclude that: there are some ``good noises'' that can relieve the
instability. Meanwhile, we argue that these good noises should meet two
principles: Frequency Matching and Spatial Matching. Regarding both principles,
we propose a novel Noise Optimization method to learn matching (i.e., good)
noise for DCs: NoOp. For frequency matching, NoOp first optimizes a
dataset-specific noise: Given a dataset and a timestep t, optimize one randomly
initialized parameterized noise. For Spatial Matching, NoOp trains a
Meta-Network that adopts an image as input and outputs image-specific noise
offset. The sum of optimized noise and noise offset will be used in DC to
replace random noise. Extensive ablations on various datasets demonstrated the
effectiveness of NoOp.

</details>


### [57] [GANDiff FR: Hybrid GAN Diffusion Synthesis for Causal Bias Attribution in Face Recognition](https://arxiv.org/abs/2508.11334)
*Md Asgor Hossain Reaj,Rajan Das Gupta,Md Yeasin Rahat,Nafiz Fahad,Md Jawadul Hasan,Tze Hui Liew*

Main category: cs.CV

TL;DR: GANDiff FR是一个合成框架，通过精确控制人口统计和环境因素来测量、解释和减少偏见，结合了StyleGAN3和扩散模型，实现了对姿态、光照和表情的细粒度控制。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一种可重复、精确的方法来量化和减少人脸识别系统中的偏见。

Method: 方法结合了StyleGAN3的身份保留生成和扩散模型的属性控制，生成了10,000张平衡的人脸图像，并通过自动检测和人工验证其真实性。

Result: 结果显示AdaFace减少了60%的组间差异，光照是剩余偏见的主要因素（42%）。合成数据在跨数据集评估中表现出色（r 0.85）。

Conclusion: GANDiff FR为公平性审计提供了一个可重复的标准，尽管计算开销较高，但生成了更多属性变体，并支持透明的偏见评估。

Abstract: We introduce GANDiff FR, the first synthetic framework that precisely
controls demographic and environmental factors to measure, explain, and reduce
bias with reproducible rigor. GANDiff FR unifies StyleGAN3-based
identity-preserving generation with diffusion-based attribute control, enabling
fine-grained manipulation of pose around 30 degrees, illumination (four
directions), and expression (five levels) under ceteris paribus conditions. We
synthesize 10,000 demographically balanced faces across five cohorts validated
for realism via automated detection (98.2%) and human review (89%) to isolate
and quantify bias drivers. Benchmarking ArcFace, CosFace, and AdaFace under
matched operating points shows AdaFace reduces inter-group TPR disparity by 60%
(2.5% vs. 6.3%), with illumination accounting for 42% of residual bias.
Cross-dataset evaluation on RFW, BUPT, and CASIA WebFace confirms strong
synthetic-to-real transfer (r 0.85). Despite around 20% computational overhead
relative to pure GANs, GANDiff FR yields three times more attribute-conditioned
variants, establishing a reproducible, regulation-aligned (EU AI Act) standard
for fairness auditing. Code and data are released to support transparent,
scalable bias evaluation.

</details>


### [58] [Index-Aligned Query Distillation for Transformer-based Incremental Object Detection](https://arxiv.org/abs/2508.11339)
*Mingxiao Ma,Shunyao Zhu,Guoliang Kang*

Main category: cs.CV

TL;DR: 论文提出了一种名为IAQD的新蒸馏方法，用于解决基于Transformer的增量目标检测中的知识遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 在增量目标检测任务中，传统的匈牙利匹配方法可能导致知识遗忘，因此需要一种更有效的方法来保持对旧类别的检测能力。

Method: 提出IAQD方法，通过索引对齐的查询蒸馏，仅对关键查询进行蒸馏，避免干扰新类别的学习。

Result: 实验表明，IAQD有效缓解了知识遗忘，达到了最新的最优性能。

Conclusion: IAQD是一种高效的蒸馏方法，适用于基于Transformer的增量目标检测任务。

Abstract: Incremental object detection (IOD) aims to continuously expand the capability
of a model to detect novel categories while preserving its performance on
previously learned ones. When adopting a transformer-based detection model to
perform IOD, catastrophic knowledge forgetting may inevitably occur, meaning
the detection performance on previously learned categories may severely
degenerate. Previous typical methods mainly rely on knowledge distillation (KD)
to mitigate the catastrophic knowledge forgetting of transformer-based
detection models. Specifically, they utilize Hungarian Matching to build a
correspondence between the queries of the last-phase and current-phase
detection models and align the classifier and regressor outputs between matched
queries to avoid knowledge forgetting. However, we observe that in IOD task,
Hungarian Matching is not a good choice. With Hungarian Matching, the query of
the current-phase model may match different queries of the last-phase model at
different iterations during KD. As a result, the knowledge encoded in each
query may be reshaped towards new categories, leading to the forgetting of
previously encoded knowledge of old categories. Based on our observations, we
propose a new distillation approach named Index-Aligned Query Distillation
(IAQD) for transformer-based IOD. Beyond using Hungarian Matching, IAQD
establishes a correspondence between queries of the previous and current phase
models that have the same index. Moreover, we perform index-aligned
distillation only on partial queries which are critical for the detection of
previous categories. In this way, IAQD largely preserves the previous semantic
and spatial encoding capabilities without interfering with the learning of new
categories. Extensive experiments on representative benchmarks demonstrate that
IAQD effectively mitigates knowledge forgetting, achieving new state-of-the-art
performance.

</details>


### [59] [Cost-Effective Active Labeling for Data-Efficient Cervical Cell Classification](https://arxiv.org/abs/2508.11340)
*Yuanlin Liu,Zhihan Zhou,Mingqiang Wei,Youyi Song*

Main category: cs.CV

TL;DR: 提出了一种名为“主动标注”的方法，通过减少人工标注成本，高效构建具有代表性的宫颈细胞分类训练数据集。


<details>
  <summary>Details</summary>
Motivation: 现有宫颈细胞分类方法需要大量代表性训练数据，人工成本高昂甚至难以承担。

Method: 利用分类器对未标注宫颈细胞图像的不确定性，选择最有价值的图像进行标注。

Result: 新算法显著提升了训练数据集的代表性，并有效降低了人工成本。

Conclusion: 该方法为高效宫颈细胞分类开辟了新途径，具有实际应用价值。

Abstract: Information on the number and category of cervical cells is crucial for the
diagnosis of cervical cancer. However, existing classification methods capable
of automatically measuring this information require the training dataset to be
representative, which consumes an expensive or even unaffordable human cost. We
herein propose active labeling that enables us to construct a representative
training dataset using a much smaller human cost for data-efficient cervical
cell classification. This cost-effective method efficiently leverages the
classifier's uncertainty on the unlabeled cervical cell images to accurately
select images that are most beneficial to label. With a fast estimation of the
uncertainty, this new algorithm exhibits its validity and effectiveness in
enhancing the representative ability of the constructed training dataset. The
extensive empirical results confirm its efficacy again in navigating the usage
of human cost, opening the avenue for data-efficient cervical cell
classification.

</details>


### [60] [Semantically Guided Adversarial Testing of Vision Models Using Language Models](https://arxiv.org/abs/2508.11341)
*Katarzyna Filus,Jorge M. Cruz-Duarte*

Main category: cs.CV

TL;DR: 本文提出了一种基于语义引导的对抗目标选择框架，利用预训练语言和视觉语言模型的跨模态知识转移，显著提升了对抗攻击的成功率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击的目标标签选择方法依赖随机性、模型预测或静态语义资源，缺乏可解释性、可重复性和灵活性。

Method: 使用BERT、TinyLLAMA和CLIP等预训练模型作为相似性来源，选择与真实标签最相关和最不相关的标签，构建最佳和最差对抗场景。

Result: 实验表明，该方法在三种视觉模型和五种攻击方法中表现优于静态语义数据库（如WordNet），尤其在远距离类别关系上。

Conclusion: 预训练模型适合构建可解释、标准化和可扩展的对抗基准，适用于不同架构和数据集。

Abstract: In targeted adversarial attacks on vision models, the selection of the target
label is a critical yet often overlooked determinant of attack success. This
target label corresponds to the class that the attacker aims to force the model
to predict. Now, existing strategies typically rely on randomness, model
predictions, or static semantic resources, limiting interpretability,
reproducibility, or flexibility. This paper then proposes a semantics-guided
framework for adversarial target selection using the cross-modal knowledge
transfer from pretrained language and vision-language models. We evaluate
several state-of-the-art models (BERT, TinyLLAMA, and CLIP) as similarity
sources to select the most and least semantically related labels with respect
to the ground truth, forming best- and worst-case adversarial scenarios. Our
experiments on three vision models and five attack methods reveal that these
models consistently render practical adversarial targets and surpass static
lexical databases, such as WordNet, particularly for distant class
relationships. We also observe that static testing of target labels offers a
preliminary assessment of the effectiveness of similarity sources, \textit{a
priori} testing. Our results corroborate the suitability of pretrained models
for constructing interpretable, standardized, and scalable adversarial
benchmarks across architectures and datasets.

</details>


### [61] [HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model](https://arxiv.org/abs/2508.11350)
*Zhenhao Zhang,Hanqing Wang,Xiangyu Zeng,Ziyu Cheng,Jiaxin Liu,Haoyu Yan,Zhirui Liu,Kaiyang Ji,Tianxiang Gui,Ke Hu,Kangyi Chen,Yahao Fan,Mokai Pan*

Main category: cs.CV

TL;DR: HOID-R1是一种新的人-物交互检测框架，结合了链式思维引导的监督微调和强化学习中的组相对策略优化，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大型语言模型但忽略了其3D空间理解能力，HOID-R1旨在弥补这一不足。

Method: 通过监督微调赋予模型推理能力，再结合组相对策略优化和多奖励信号优化策略，并引入MLLM监督机制减少幻觉。

Result: HOID-R1在HOI检测基准上达到最优性能，并在开放世界泛化中优于现有方法。

Conclusion: HOID-R1通过结合推理和强化学习，显著提升了人-物交互检测的性能和泛化能力。

Abstract: Understanding and recognizing human-object interaction (HOI) is a pivotal
application in AR/VR and robotics. Recent open-vocabulary HOI detection
approaches depend exclusively on large language models for richer textual
prompts, neglecting their inherent 3D spatial understanding capabilities. To
address this shortcoming, we introduce HOID-R1, the first HOI detection
framework that integrates chain-of-thought (CoT) guided supervised fine-tuning
(SFT) with group relative policy optimization (GRPO) within a reinforcement
learning (RL) paradigm. Specifically, we initially apply SFT to imbue the model
with essential reasoning capabilities, forcing the model to articulate its
thought process in the output. Subsequently, we integrate GRPO to leverage
multi-reward signals for policy optimization, thereby enhancing alignment
across diverse modalities. To mitigate hallucinations in the CoT reasoning, we
introduce an "MLLM-as-a-judge" mechanism that supervises the CoT outputs,
further improving generalization. Extensive experiments show that HOID-R1
achieves state-of-the-art performance on HOI detection benchmarks and
outperforms existing methods in open-world generalization to novel scenarios.

</details>


### [62] [Leveraging the RETFound foundation model for optic disc segmentation in retinal images](https://arxiv.org/abs/2508.11354)
*Zhenyi Zhao,Muthu Rama Krishnan Mookiah,Emanuele Trucco*

Main category: cs.CV

TL;DR: RETFound首次被用于视盘分割任务，表现优异，仅需少量任务特定数据即可超越现有最佳分割网络。


<details>
  <summary>Details</summary>
Motivation: 探索RETFound在视盘分割任务中的潜力，验证基础模型在非原始任务中的适用性。

Method: 通过微调RETFound头部网络，利用少量任务特定数据训练，并在多个公开和私有数据集上测试。

Result: 在五个数据集上平均Dice达到96%，表现优于现有最佳分割网络，并展示了良好的泛化和适应能力。

Conclusion: RETFound在视盘分割任务中表现出色，为基础模型替代任务特定架构提供了有力证据。

Abstract: RETFound is a well-known foundation model (FM) developed for fundus camera
and optical coherence tomography images. It has shown promising performance
across multiple datasets in diagnosing diseases, both eye-specific and
systemic, from retinal images. However, to our best knowledge, it has not been
used for other tasks. We present the first adaptation of RETFound for optic
disc segmentation, a ubiquitous and foundational task in retinal image
analysis. The resulting segmentation system outperforms state-of-the-art,
segmentation-specific baseline networks after training a head with only a very
modest number of task-specific examples. We report and discuss results with
four public datasets, IDRID, Drishti-GS, RIM-ONE-r3, and REFUGE, and a private
dataset, GoDARTS, achieving about 96% Dice consistently across all datasets.
Overall, our method obtains excellent performance in internal verification,
domain generalization and domain adaptation, and exceeds most of the
state-of-the-art baseline results. We discuss the results in the framework of
the debate about FMs as alternatives to task-specific architectures. The code
is available at: [link to be added after the paper is accepted]

</details>


### [63] [Does the Skeleton-Recall Loss Really Work?](https://arxiv.org/abs/2508.11374)
*Devansh Arora,Nitin Kumar,Sukrit Gupta*

Main category: cs.CV

TL;DR: 本文对Skeleton Recall Loss（SRL）进行了理论分析，发现其在管状结构分割任务中并未超越传统基线模型，揭示了拓扑保持损失函数的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究SRL损失函数在管状结构分割中的实际效果，验证其是否如原论文所述优于传统方法。

Method: 通过理论分析SRL的梯度，并在多个管状数据集上进行实验，对比SRL与传统基线模型的性能。

Result: 实验表明，SRL并未显著优于传统基线模型，理论分析也支持这一结论。

Conclusion: 拓扑保持损失函数（如SRL）在管状结构分割中存在局限性，为未来研究提供了重要参考。

Abstract: Image segmentation is an important and widely performed task in computer
vision. Accomplishing effective image segmentation in diverse settings often
requires custom model architectures and loss functions. A set of models that
specialize in segmenting thin tubular structures are topology
preservation-based loss functions. These models often utilize a pixel
skeletonization process claimed to generate more precise segmentation masks of
thin tubes and better capture the structures that other models often miss. One
such model, Skeleton Recall Loss (SRL) proposed by Kirchhoff et al.~\cite
{kirchhoff2024srl}, was stated to produce state-of-the-art results on benchmark
tubular datasets. In this work, we performed a theoretical analysis of the
gradients for the SRL loss. Upon comparing the performance of the proposed
method on some of the tubular datasets (used in the original work, along with
some additional datasets), we found that the performance of SRL-based
segmentation models did not exceed traditional baseline models. By providing
both a theoretical explanation and empirical evidence, this work critically
evaluates the limitations of topology-based loss functions, offering valuable
insights for researchers aiming to develop more effective segmentation models
for complex tubular structures.

</details>


### [64] [Unified Knowledge Distillation Framework: Fine-Grained Alignment and Geometric Relationship Preservation for Deep Face Recognition](https://arxiv.org/abs/2508.11376)
*Durgesh Mishra,Rishabh Uikey*

Main category: cs.CV

TL;DR: 提出了一种结合实例级嵌入蒸馏和关系相似性蒸馏的统一知识蒸馏方法，显著提升了人脸识别模型在边缘设备上的性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法在捕捉细粒度实例细节和复杂关系结构方面表现不佳，导致性能不理想。

Method: 提出两种新损失函数：实例级嵌入蒸馏（动态硬挖掘策略）和关系相似性蒸馏（记忆库机制和样本挖掘策略）。

Result: 在多个基准数据集上优于现有方法，甚至在某些情况下学生模型能超越教师模型的准确率。

Conclusion: 统一框架实现了更全面的知识蒸馏，显著提升了模型性能。

Abstract: Knowledge Distillation is crucial for optimizing face recognition models for
deployment in computationally limited settings, such as edge devices.
Traditional KD methods, such as Raw L2 Feature Distillation or Feature
Consistency loss, often fail to capture both fine-grained instance-level
details and complex relational structures, leading to suboptimal performance.
We propose a unified approach that integrates two novel loss functions,
Instance-Level Embedding Distillation and Relation-Based Pairwise Similarity
Distillation. Instance-Level Embedding Distillation focuses on aligning
individual feature embeddings by leveraging a dynamic hard mining strategy,
thereby enhancing learning from challenging examples. Relation-Based Pairwise
Similarity Distillation captures relational information through pairwise
similarity relationships, employing a memory bank mechanism and a sample mining
strategy. This unified framework ensures both effective instance-level
alignment and preservation of geometric relationships between samples, leading
to a more comprehensive distillation process. Our unified framework outperforms
state-of-the-art distillation methods across multiple benchmark face
recognition datasets, as demonstrated by extensive experimental evaluations.
Interestingly, when using strong teacher networks compared to the student, our
unified KD enables the student to even surpass the teacher's accuracy.

</details>


### [65] [G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration](https://arxiv.org/abs/2508.11379)
*Ramil Khafizov,Artem Komarichev,Ruslan Rakhimov,Peter Wonka,Evgeny Burnaev*

Main category: cs.CV

TL;DR: G-CUT3R是一种改进的3D场景重建方法，通过整合先验信息提升CUT3R模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖输入图像，而G-CUT3R利用深度、相机标定等辅助数据，提升重建效果。

Method: 在CUT3R基础上轻量修改，为每种模态设计专用编码器，通过零卷积融合特征。

Result: 在多个基准测试中表现优异，显著提升性能，兼容不同输入模态。

Conclusion: G-CUT3R能有效利用先验信息，灵活适应多种输入组合。

Abstract: We introduce G-CUT3R, a novel feed-forward approach for guided 3D scene
reconstruction that enhances the CUT3R model by integrating prior information.
Unlike existing feed-forward methods that rely solely on input images, our
method leverages auxiliary data, such as depth, camera calibrations, or camera
positions, commonly available in real-world scenarios. We propose a lightweight
modification to CUT3R, incorporating a dedicated encoder for each modality to
extract features, which are fused with RGB image tokens via zero convolution.
This flexible design enables seamless integration of any combination of prior
information during inference. Evaluated across multiple benchmarks, including
3D reconstruction and other multi-view tasks, our approach demonstrates
significant performance improvements, showing its ability to effectively
utilize available priors while maintaining compatibility with varying input
modalities.

</details>


### [66] [RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator](https://arxiv.org/abs/2508.11409)
*Zhiming Liu,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: RMFAT是一种轻量级循环框架，用于高效恢复受大气湍流影响的视频，显著降低计算负担并提升实时性能。


<details>
  <summary>Details</summary>
Motivation: 大气湍流导致视频质量下降，现有方法计算成本高，难以实时部署。

Method: 采用轻量级循环框架，仅需两帧输入，结合多尺度特征编码和解码模块增强时空一致性。

Result: 在清晰度恢复（SSIM提升9%）和推理速度（运行时间减少四倍）上优于现有方法。

Conclusion: RMFAT适用于实时大气湍流抑制任务，性能显著提升。

Abstract: Atmospheric turbulence severely degrades video quality by introducing
distortions such as geometric warping, blur, and temporal flickering, posing
significant challenges to both visual clarity and temporal consistency. Current
state-of-the-art methods are based on transformer and 3D architectures and
require multi-frame input, but their large computational cost and memory usage
limit real-time deployment, especially in resource-constrained scenarios. In
this work, we propose RMFAT: Recurrent Multi-scale Feature Atmospheric
Turbulence Mitigator, designed for efficient and temporally consistent video
restoration under AT conditions. RMFAT adopts a lightweight recurrent framework
that restores each frame using only two inputs at a time, significantly
reducing temporal window size and computational burden. It further integrates
multi-scale feature encoding and decoding with temporal warping modules at both
encoder and decoder stages to enhance spatial detail and temporal coherence.
Extensive experiments on synthetic and real-world atmospheric turbulence
datasets demonstrate that RMFAT not only outperforms existing methods in terms
of clarity restoration (with nearly a 9\% improvement in SSIM) but also
achieves significantly improved inference speed (more than a fourfold reduction
in runtime), making it particularly suitable for real-time atmospheric
turbulence suppression tasks.

</details>


### [67] [SelfAdapt: Unsupervised Domain Adaptation of Cell Segmentation Models](https://arxiv.org/abs/2508.11411)
*Fabian H. Reith,Jannik Franzen,Dinesh R. Palli,J. Lorenz Rumberger,Dagmar Kainmueller*

Main category: cs.CV

TL;DR: SelfAdapt是一种无需标签的自适应方法，用于改进预训练的细胞分割模型，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有通用模型（如Cellpose）在训练数据以外的领域表现不佳，而监督微调需要标注数据，但标注数据可能难以获取。

Method: 基于师生增强一致性训练，引入L2-SP正则化和无标签停止准则。

Result: 在LiveCell和TissueNet数据集上，AP0.5相对基线Cellpose提升高达29.64%。

Conclusion: SelfAdapt是一种高效的无监督自适应方法，可进一步提升模型性能，并已集成到Cellpose框架中。

Abstract: Deep neural networks have become the go-to method for biomedical instance
segmentation. Generalist models like Cellpose demonstrate state-of-the-art
performance across diverse cellular data, though their effectiveness often
degrades on domains that differ from their training data. While supervised
fine-tuning can address this limitation, it requires annotated data that may
not be readily available. We propose SelfAdapt, a method that enables the
adaptation of pre-trained cell segmentation models without the need for labels.
Our approach builds upon student-teacher augmentation consistency training,
introducing L2-SP regularization and label-free stopping criteria. We evaluate
our method on the LiveCell and TissueNet datasets, demonstrating relative
improvements in AP0.5 of up to 29.64% over baseline Cellpose. Additionally, we
show that our unsupervised adaptation can further improve models that were
previously fine-tuned with supervision. We release SelfAdapt as an easy-to-use
extension of the Cellpose framework. The code for our method is publicly
available at https: //github.com/Kainmueller-Lab/self_adapt.

</details>


### [68] [Training-free Dimensionality Reduction via Feature Truncation: Enhancing Efficiency in Privacy-preserving Multi-Biometric Systems](https://arxiv.org/abs/2508.11419)
*Florian Bayer,Maximilian Russo,Christian Rathgeb*

Main category: cs.CV

TL;DR: 该论文研究了多模态生物特征模板尺寸缩减对性能的影响，通过融合特征向量在保持准确性的同时显著减少计算负担。


<details>
  <summary>Details</summary>
Motivation: 生物特征识别的广泛应用使得模板隐私和安全性成为关键问题，而多模态融合和同态加密的计算挑战需要解决。

Method: 利用深度神经网络提取特征，并在同态加密下实现多模态特征融合与降维。

Result: 实验表明，多模态融合可将模板尺寸减少67%，同时保持或超越单模态的识别性能。

Conclusion: 多模态特征融合与降维是提升生物特征识别效率与安全性的有效方法。

Abstract: Biometric recognition is widely used, making the privacy and security of
extracted templates a critical concern. Biometric Template Protection schemes,
especially those utilizing Homomorphic Encryption, introduce significant
computational challenges due to increased workload. Recent advances in deep
neural networks have enabled state-of-the-art feature extraction for face,
fingerprint, and iris modalities. The ubiquity and affordability of biometric
sensors further facilitate multi-modal fusion, which can enhance security by
combining features from different modalities. This work investigates the
biometric performance of reduced multi-biometric template sizes. Experiments
are conducted on an in-house virtual multi-biometric database, derived from
DNN-extracted features for face, fingerprint, and iris, using the FRGC, MCYT,
and CASIA databases. The evaluated approaches are (i) explainable and
straightforward to implement under encryption, (ii) training-free, and (iii)
capable of generalization. Dimensionality reduction of feature vectors leads to
fewer operations in the Homomorphic Encryption (HE) domain, enabling more
efficient encrypted processing while maintaining biometric accuracy and
security at a level equivalent to or exceeding single-biometric recognition.
Our results demonstrate that, by fusing feature vectors from multiple
modalities, template size can be reduced by 67 % with no loss in Equal Error
Rate (EER) compared to the best-performing single modality.

</details>


### [69] [ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving](https://arxiv.org/abs/2508.11428)
*Jingyu Li,Bozhou Zhang,Xin Jin,Jiankang Deng,Xiatian Zhu,Li Zhang*

Main category: cs.CV

TL;DR: ImagiDrive整合视觉语言模型（VLM）和驾驶世界模型（DWM），通过统一的想象-规划循环提升自动驾驶的预测和场景生成能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需结合多模态理解和场景预测能力，但现有方法（VLM和DWM）独立发展，未充分发挥互补优势。

Method: 提出ImagiDrive框架，结合VLM的行为预测和DWM的场景生成，通过迭代优化和效率策略（如早停机制）实现高效整合。

Result: 在nuScenes和NAVSIM数据集上验证，ImagiDrive在开环和闭环条件下均优于现有方法。

Conclusion: ImagiDrive通过整合VLM和DWM，显著提升了自动驾驶的预测准确性和场景生成质量。

Abstract: Autonomous driving requires rich contextual comprehension and precise
predictive reasoning to navigate dynamic and complex environments safely.
Vision-Language Models (VLMs) and Driving World Models (DWMs) have
independently emerged as powerful recipes addressing different aspects of this
challenge. VLMs provide interpretability and robust action prediction through
their ability to understand multi-modal context, while DWMs excel in generating
detailed and plausible future driving scenarios essential for proactive
planning. Integrating VLMs with DWMs is an intuitive, promising, yet
understudied strategy to exploit the complementary strengths of accurate
behavioral prediction and realistic scene generation. Nevertheless, this
integration presents notable challenges, particularly in effectively connecting
action-level decisions with high-fidelity pixel-level predictions and
maintaining computational efficiency. In this paper, we propose ImagiDrive, a
novel end-to-end autonomous driving framework that integrates a VLM-based
driving agent with a DWM-based scene imaginer to form a unified
imagination-and-planning loop. The driving agent predicts initial driving
trajectories based on multi-modal inputs, guiding the scene imaginer to
generate corresponding future scenarios. These imagined scenarios are
subsequently utilized to iteratively refine the driving agent's planning
decisions. To address efficiency and predictive accuracy challenges inherent in
this integration, we introduce an early stopping mechanism and a trajectory
selection strategy. Extensive experimental validation on the nuScenes and
NAVSIM datasets demonstrates the robustness and superiority of ImagiDrive over
previous alternatives under both open-loop and closed-loop conditions.

</details>


### [70] [Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian Splatting](https://arxiv.org/abs/2508.11431)
*Simona Kocour,Assia Benbihi,Torsten Sattler*

Main category: cs.CV

TL;DR: 论文提出了一个评估框架和数据集Remove360，用于衡量3D高斯泼溅中物体移除后的语义残留问题，揭示了当前技术的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究物体移除后残留的语义信息对隐私保护和可编辑场景表示的重要性。

Method: 引入新基准和评估框架，使用Remove360数据集进行实验，分析语义残留。

Result: 实验表明当前方法在视觉几何缺失时仍能保留语义信息，但存在局限性。

Conclusion: 需要更鲁棒的解决方案来处理真实世界的复杂性。

Abstract: Understanding what semantic information persists after object removal is
critical for privacy-preserving 3D reconstruction and editable scene
representations. In this work, we introduce a novel benchmark and evaluation
framework to measure semantic residuals, the unintended semantic traces left
behind, after object removal in 3D Gaussian Splatting. We conduct experiments
across a diverse set of indoor and outdoor scenes, showing that current methods
can preserve semantic information despite the absence of visual geometry. We
also release Remove360, a dataset of pre/post-removal RGB images and
object-level masks captured in real-world environments. While prior datasets
have focused on isolated object instances, Remove360 covers a broader and more
complex range of indoor and outdoor scenes, enabling evaluation of object
removal in the context of full-scene representations. Given ground truth images
of a scene before and after object removal, we assess whether we can truly
eliminate semantic presence, and if downstream models can still infer what was
removed. Our findings reveal critical limitations in current 3D object removal
techniques and underscore the need for more robust solutions capable of
handling real-world complexity. The evaluation framework is available at
github.com/spatial-intelligence-ai/Remove360.git. Data are available at
huggingface.co/datasets/simkoc/Remove360.

</details>


### [71] [MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation](https://arxiv.org/abs/2508.11433)
*Qian Liang,Yujia Wu,Kuncheng Li,Jiwei Wei,Shiyuan He,Jinyu Guo,Ning Xie*

Main category: cs.CV

TL;DR: MM-R1框架通过跨模态思维链推理策略，解锁统一多模态大语言模型的个性化图像生成潜力，无需针对每个新主题进行数据密集型微调。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要针对每个新主题进行数据密集型微调，限制了可扩展性。MM-R1旨在解决这一问题。

Method: 采用跨模态思维链（X-CoT）推理策略，将个性化任务分解为视觉推理和生成过程，并结合GRPO优化生成对齐。

Result: 实验表明，MM-R1能以零样本方式生成高主题保真度和强文本对齐的个性化图像。

Conclusion: MM-R1为统一多模态大语言模型的个性化图像生成提供了高效且可扩展的解决方案。

Abstract: Multimodal Large Language Models (MLLMs) with unified architectures excel
across a wide range of vision-language tasks, yet aligning them with
personalized image generation remains a significant challenge. Existing methods
for MLLMs are frequently subject-specific, demanding a data-intensive
fine-tuning process for every new subject, which limits their scalability. In
this paper, we introduce MM-R1, a framework that integrates a cross-modal
Chain-of-Thought (X-CoT) reasoning strategy to unlock the inherent potential of
unified MLLMs for personalized image generation. Specifically, we structure
personalization as an integrated visual reasoning and generation process: (1)
grounding subject concepts by interpreting and understanding user-provided
images and contextual cues, and (2) generating personalized images conditioned
on both the extracted subject representations and user prompts. To further
enhance the reasoning capability, we adopt Grouped Reward Proximal Policy
Optimization (GRPO) to explicitly align the generation. Experiments demonstrate
that MM-R1 unleashes the personalization capability of unified MLLMs to
generate images with high subject fidelity and strong text alignment in a
zero-shot manner.

</details>


### [72] [Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation](https://arxiv.org/abs/2508.11446)
*Daniel Airinei,Elena Burceanu,Marius Leordeanu*

Main category: cs.CV

TL;DR: 提出了一种仅基于视觉输入的高效、实时且易于部署的深度学习室内导航方法，无需特殊传感器或地图知识。


<details>
  <summary>Details</summary>
Motivation: 室内导航因GPS信号差而困难，现有解决方案复杂且难以部署。

Method: 采用基于图路径生成的新方法，结合可解释的数据增强和课程学习，实现数据收集、标注和训练的自动化。

Result: 创建了一个大型购物中心视频数据集，并开发了Android应用，代码和数据公开。

Conclusion: 该方法仅依赖视觉输入，简化了部署，为室内导航提供了实用解决方案。

Abstract: Indoor navigation is a difficult task, as it generally comes with poor GPS
access, forcing solutions to rely on other sources of information. While
significant progress continues to be made in this area, deployment to
production applications is still lacking, given the complexity and additional
requirements of current solutions. Here, we introduce an efficient, real-time
and easily deployable deep learning approach, based on visual input only, that
can predict the direction towards a target from images captured by a mobile
device. Our technical approach, based on a novel graph-based path generation
method, combined with explainable data augmentation and curriculum learning,
includes contributions that make the process of data collection, annotation and
training, as automatic as possible, efficient and robust. On the practical
side, we introduce a novel largescale dataset, with video footage inside a
relatively large shopping mall, in which each frame is annotated with the
correct next direction towards different specific target destinations.
Different from current methods, ours relies solely on vision, avoiding the need
of special sensors, additional markers placed along the path, knowledge of the
scene map or internet access. We also created an easy to use application for
Android, which we plan to make publicly available. We make all our data and
code available along with visual demos on our project site

</details>


### [73] [Data-Driven Deepfake Image Detection Method -- The 2024 Global Deepfake Image Detection Challenge](https://arxiv.org/abs/2508.11464)
*Xiaoya Zhu,Yibing Nan,Shiguo Lian*

Main category: cs.CV

TL;DR: 论文提出了一种基于Swin Transformer V2-B分类网络的方法，结合在线数据增强和离线样本生成技术，用于检测Deepfake图像，并在竞赛中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的快速发展，Deepfake技术带来了大量AI生成内容，同时也对数字安全提出了前所未有的挑战，因此需要有效的检测方法。

Method: 采用Swin Transformer V2-B分类网络，结合在线数据增强和离线样本生成技术，提升训练样本多样性和模型泛化能力。

Result: 在Deepfake图像检测竞赛中获得了卓越奖。

Conclusion: 该方法通过结合先进网络架构和数据增强技术，有效提升了Deepfake图像的检测能力。

Abstract: With the rapid development of technology in the field of AI, deepfake
technology has emerged as a double-edged sword. It has not only created a large
amount of AI-generated content but also posed unprecedented challenges to
digital security. The task of the competition is to determine whether a face
image is a Deepfake image and output its probability score of being a Deepfake
image. In the image track competition, our approach is based on the Swin
Transformer V2-B classification network. And online data augmentation and
offline sample generation methods are employed to enrich the diversity of
training samples and increase the generalization ability of the model. Finally,
we got the award of excellence in Deepfake image detection.

</details>


### [74] [CoFi: A Fast Coarse-to-Fine Few-Shot Pipeline for Glomerular Basement Membrane Segmentation](https://arxiv.org/abs/2508.11469)
*Hongjin Fang,Daniel Reisenbüchler,Kenji Ikemura,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

TL;DR: CoFi是一种粗到细的少样本分割方法，用于电子显微镜图像中肾小球基底膜的精确分割，显著减少标注需求并提高效率。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习方法依赖大量标注，临床实用性低；少样本学习难以捕捉精细结构。CoFi旨在解决这些问题。

Method: CoFi先用三张标注图像训练轻量网络生成粗分割，再通过形态学修剪生成高质量点提示，引导SAM细化分割。

Result: Dice系数74.54%，推理速度1.9 FPS，标注和计算负担显著降低。

Conclusion: CoFi高效、准确，适用于研究和临床，潜力巨大。

Abstract: Accurate segmentation of the glomerular basement membrane (GBM) in electron
microscopy (EM) images is fundamental for quantifying membrane thickness and
supporting the diagnosis of various kidney diseases. While supervised deep
learning approaches achieve high segmentation accuracy, their reliance on
extensive pixel-level annotation renders them impractical for clinical
workflows. Few-shot learning can reduce this annotation burden but often
struggles to capture the fine structural details necessary for GBM analysis. In
this study, we introduce CoFi, a fast and efficient coarse-to-fine few-shot
segmentation pipeline designed for GBM delineation in EM images. CoFi first
trains a lightweight neural network using only three annotated images to
produce an initial coarse segmentation mask. This mask is then automatically
processed to generate high-quality point prompts with morphology-aware pruning,
which are subsequently used to guide SAM in refining the segmentation. The
proposed method achieved exceptional GBM segmentation performance, with a Dice
coefficient of 74.54% and an inference speed of 1.9 FPS. We demonstrate that
CoFi not only alleviates the annotation and computational burdens associated
with conventional methods, but also achieves accurate and reliable segmentation
results. The pipeline's speed and annotation efficiency make it well-suited for
research and hold strong potential for clinical applications in renal
pathology. The pipeline is publicly available at:
https://github.com/ddrrnn123/CoFi.

</details>


### [75] [TACR-YOLO: A Real-time Detection Framework for Abnormal Human Behaviors Enhanced with Coordinate and Task-Aware Representations](https://arxiv.org/abs/2508.11478)
*Xinyi Yin,Wenbo Yuan,Xuecheng Wu,Liangyu Fu,Danlei Huang*

Main category: cs.CV

TL;DR: TACR-YOLO是一种实时异常行为检测框架，通过改进小目标检测、任务冲突处理和多尺度融合，在PABD数据集上达到91.92%的mAP。


<details>
  <summary>Details</summary>
Motivation: 特殊场景下的异常行为检测需求日益增长，但现有YOLO方法在小目标、任务冲突和多尺度融合方面存在不足。

Method: 提出TACR-YOLO，引入坐标注意力模块、任务感知注意力模块和强化颈部网络，优化锚框尺寸并使用DIoU损失。

Result: 在PABD数据集上达到91.92% mAP，速度和鲁棒性表现优异。

Conclusion: TACR-YOLO为特殊场景下的异常行为检测提供了新思路，推动了该领域的进展。

Abstract: Abnormal Human Behavior Detection (AHBD) under special scenarios is becoming
increasingly crucial. While YOLO-based detection methods excel in real-time
tasks, they remain hindered by challenges including small objects, task
conflicts, and multi-scale fusion in AHBD. To tackle them, we propose
TACR-YOLO, a new real-time framework for AHBD. We introduce a Coordinate
Attention Module to enhance small object detection, a Task-Aware Attention
Module to deal with classification-regression conflicts, and a Strengthen Neck
Network for refined multi-scale fusion, respectively. In addition, we optimize
Anchor Box sizes using K-means clustering and deploy DIoU-Loss to improve
bounding box regression. The Personnel Anomalous Behavior Detection (PABD)
dataset, which includes 8,529 samples across four behavior categories, is also
presented. Extensive experimental results indicate that TACR-YOLO achieves
91.92% mAP on PABD, with competitive speed and robustness. Ablation studies
highlight the contribution of each improvement. This work provides new insights
for abnormal behavior detection under special scenarios, advancing its
progress.

</details>


### [76] [OpenConstruction: A Systematic Synthesis of Open Visual Datasets for Data-Centric Artificial Intelligence in Construction Monitoring](https://arxiv.org/abs/2508.11482)
*Ruoxin Xiong,Yanyu Wang,Jiannan Cai,Kaijian Liu,Yuansheng Zhu,Pingbo Tang,Nora El-Gohary*

Main category: cs.CV

TL;DR: 该研究系统回顾了2005-2024年间的51个公开建筑视觉数据集，提出了分类框架并创建了开源目录OpenConstruction，同时指出了现有数据集的不足并提出了基于FAIR原则的未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 建筑行业依赖视觉数据支持AI/ML应用，但现有数据集在规模、模态、标注质量和代表性上差异较大，缺乏系统性分析。

Method: 通过学术数据库和开放数据平台搜索51个数据集，采用结构化数据模式分类（数据基础、模态、标注框架和应用领域）。

Result: 创建了OpenConstruction开源目录，总结了数据集现状，并指出了关键局限性。

Conclusion: 研究为建筑行业数据驱动方法的发展提供了支持，并提出了基于FAIR原则的未来数据基础设施路线图。

Abstract: The construction industry increasingly relies on visual data to support
Artificial Intelligence (AI) and Machine Learning (ML) applications for site
monitoring. High-quality, domain-specific datasets, comprising images, videos,
and point clouds, capture site geometry and spatiotemporal dynamics, including
the location and interaction of objects, workers, and materials. However,
despite growing interest in leveraging visual datasets, existing resources vary
widely in sizes, data modalities, annotation quality, and representativeness of
real-world construction conditions. A systematic review to categorize their
data characteristics and application contexts is still lacking, limiting the
community's ability to fully understand the dataset landscape, identify
critical gaps, and guide future directions toward more effective, reliable, and
scalable AI applications in construction. To address this gap, this study
conducts an extensive search of academic databases and open-data platforms,
yielding 51 publicly available visual datasets that span the 2005-2024 period.
These datasets are categorized using a structured data schema covering (i) data
fundamentals (e.g., size and license), (ii) data modalities (e.g., RGB and
point cloud), (iii) annotation frameworks (e.g., bounding boxes), and (iv)
downstream application domains (e.g., progress tracking). This study
synthesizes these findings into an open-source catalog, OpenConstruction,
supporting data-driven method development. Furthermore, the study discusses
several critical limitations in the existing construction dataset landscape and
presents a roadmap for future data infrastructure anchored in the Findability,
Accessibility, Interoperability, and Reusability (FAIR) principles. By
reviewing the current landscape and outlining strategic priorities, this study
supports the advancement of data-centric solutions in the construction sector.

</details>


### [77] [CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models](https://arxiv.org/abs/2508.11484)
*Xiaoxue Wu,Bingjie Gao,Yu Qiao,Yaohui Wang,Xinyuan Chen*

Main category: cs.CV

TL;DR: CineTrans是一个新颖的框架，用于生成具有电影风格过渡的多镜头视频，通过掩码控制机制和专用数据集实现稳定且连贯的过渡。


<details>
  <summary>Details</summary>
Motivation: 尽管视频合成技术取得了显著进展，但多镜头视频生成的研究仍处于初级阶段，现有模型的镜头过渡能力有限且不稳定。

Method: 构建了Cine250K数据集，分析扩散模型的注意力图与镜头边界的关系，设计掩码控制机制，并在训练中应用该机制。

Result: CineTrans能够生成符合电影编辑风格的多镜头序列，避免了不稳定过渡或简单拼接，且在实验中的表现显著优于现有基线。

Conclusion: CineTrans通过创新的掩码控制机制和专用数据集，显著提升了多镜头视频生成的连贯性和质量。

Abstract: Despite significant advances in video synthesis, research into multi-shot
video generation remains in its infancy. Even with scaled-up models and massive
datasets, the shot transition capabilities remain rudimentary and unstable,
largely confining generated videos to single-shot sequences. In this work, we
introduce CineTrans, a novel framework for generating coherent multi-shot
videos with cinematic, film-style transitions. To facilitate insights into the
film editing style, we construct a multi-shot video-text dataset Cine250K with
detailed shot annotations. Furthermore, our analysis of existing video
diffusion models uncovers a correspondence between attention maps in the
diffusion model and shot boundaries, which we leverage to design a mask-based
control mechanism that enables transitions at arbitrary positions and transfers
effectively in a training-free setting. After fine-tuning on our dataset with
the mask mechanism, CineTrans produces cinematic multi-shot sequences while
adhering to the film editing style, avoiding unstable transitions or naive
concatenations. Finally, we propose specialized evaluation metrics for
transition control, temporal consistency and overall quality, and demonstrate
through extensive experiments that CineTrans significantly outperforms existing
baselines across all criteria.

</details>


### [78] [Automated Building Heritage Assessment Using Street-Level Imagery](https://arxiv.org/abs/2508.11486)
*Kristina Dabrock,Tim Johansson,Anna Donarelli,Mikael Mangold,Noah Pflugradt,Jann Michael Weinand,Jochen Linßen*

Main category: cs.CV

TL;DR: 利用GPT模型从建筑立面图像中识别文化遗产价值，结合建筑登记数据训练机器学习模型，验证结果显示混合数据效果优于仅用GPT数据。


<details>
  <summary>Details</summary>
Motivation: 传统建筑文化遗产评估方法耗时且成本高，需高效工具支持能源改造与文化遗产保护的平衡。

Method: 使用GPT模型分析建筑立面图像的文化遗产特征，结合建筑登记数据训练分类模型。

Result: 混合数据的F1得分为0.71，仅GPT数据的F1得分为0.60。

Conclusion: 该方法可提升数据库质量，支持能源改造中文化遗产价值的综合考虑。

Abstract: Detailed data is required to quantify energy conservation measures in
buildings, such as envelop retrofits, without compromising cultural heritage.
Novel artificial intelligence tools may improve efficiency in identifying
heritage values in buildings compared to costly and time-consuming traditional
inventories. In this study, the large language model GPT was used to detect
various aspects of cultural heritage value in fa\c{c}ade images. Using this
data and building register data as features, machine learning models were
trained to classify multi-family and non-residential buildings in Stockholm,
Sweden. Validation against an expert-created inventory shows a macro F1-score
of 0.71 using a combination of register data and features retrieved from GPT,
and a score of 0.60 using only GPT-derived data. The presented methodology can
contribute to a higher-quality database and thus support careful energy
efficiency measures and integrated consideration of heritage value in
large-scale energetic refurbishment scenarios.

</details>


### [79] [Perception in Plan: Coupled Perception and Planning for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.11488)
*Bozhou Zhang,Jingyu Li,Nan Song,Li Zhang*

Main category: cs.CV

TL;DR: VeteranAD提出了一种感知与规划耦合的端到端自动驾驶框架，通过将感知融入规划过程，实现目标导向的感知，提升规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法采用感知-规划分离的范式，限制了规划导向的优化潜力。本文旨在通过感知与规划的紧密耦合，提升自动驾驶的性能和可靠性。

Method: 提出VeteranAD框架，利用多模式锚定轨迹作为规划先验，设计感知模块沿轨迹收集交通元素，并采用自回归策略逐步预测未来轨迹。

Result: 在NAVSIM和Bench2Drive数据集上，VeteranAD实现了最先进的性能。

Conclusion: VeteranAD通过感知与规划的耦合设计，充分释放了规划导向端到端方法的潜力，提升了自动驾驶的准确性和可靠性。

Abstract: End-to-end autonomous driving has achieved remarkable advancements in recent
years. Existing methods primarily follow a perception-planning paradigm, where
perception and planning are executed sequentially within a fully differentiable
framework for planning-oriented optimization. We further advance this paradigm
through a perception-in-plan framework design, which integrates perception into
the planning process. This design facilitates targeted perception guided by
evolving planning objectives over time, ultimately enhancing planning
performance. Building on this insight, we introduce VeteranAD, a coupled
perception and planning framework for end-to-end autonomous driving. By
incorporating multi-mode anchored trajectories as planning priors, the
perception module is specifically designed to gather traffic elements along
these trajectories, enabling comprehensive and targeted perception. Planning
trajectories are then generated based on both the perception results and the
planning priors. To make perception fully serve planning, we adopt an
autoregressive strategy that progressively predicts future trajectories while
focusing on relevant regions for targeted perception at each step. With this
simple yet effective design, VeteranAD fully unleashes the potential of
planning-oriented end-to-end methods, leading to more accurate and reliable
driving behavior. Extensive experiments on the NAVSIM and Bench2Drive datasets
demonstrate that our VeteranAD achieves state-of-the-art performance.

</details>


### [80] [Hierarchical Graph Feature Enhancement with Adaptive Frequency Modulation for Visual Recognition](https://arxiv.org/abs/2508.11497)
*Feiyue Zhao,Zhichao Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为HGFE的框架，通过图推理增强CNN的结构感知和特征表示能力，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: CNN依赖规则网格结构，难以建模复杂拓扑关系和非局部语义，HGFE旨在解决这一问题。

Method: HGFE构建了局部和全局图结构，并引入自适应频率调制模块，动态平衡高低频信号传播。

Result: 在多个数据集（分类、检测、分割任务）上验证了HGFE能提升结构表示和整体性能。

Conclusion: HGFE是一种轻量级、端到端可训练的模块，可无缝集成到CNN中，显著提升模型表现。

Abstract: Convolutional neural networks (CNNs) have
  demonstrated strong performance in visual recognition tasks,
  but their inherent reliance on regular grid structures limits
  their capacity to model complex topological relationships and
  non-local semantics within images. To address this limita tion, we propose
the hierarchical graph feature enhancement
  (HGFE), a novel framework that integrates graph-based rea soning into CNNs to
enhance both structural awareness and
  feature representation. HGFE builds two complementary levels
  of graph structures: intra-window graph convolution to cap ture local spatial
dependencies and inter-window supernode
  interactions to model global semantic relationships. Moreover,
  we introduce an adaptive frequency modulation module that
  dynamically balances low-frequency and high-frequency signal
  propagation, preserving critical edge and texture information
  while mitigating over-smoothing. The proposed HGFE module
  is lightweight, end-to-end trainable, and can be seamlessly
  integrated into standard CNN backbone networks. Extensive
  experiments on CIFAR-100 (classification), PASCAL VOC,
  and VisDrone (detection), as well as CrackSeg and CarParts
  (segmentation), validated the effectiveness of the HGFE in
  improving structural representation and enhancing overall
  recognition performance.

</details>


### [81] [Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models](https://arxiv.org/abs/2508.11499)
*Erez Meoded*

Main category: cs.CV

TL;DR: 本文研究了如何使用TrOCR模型和针对历史手写特点的数据增强方法，显著提升了16世纪拉丁手稿的识别性能。


<details>
  <summary>Details</summary>
Motivation: 历史手写文本识别（HTR）对解锁档案文件的文化和学术价值至关重要，但稀缺的转录、语言变异和多样化的手写风格阻碍了数字化进程。

Method: 应用TrOCR模型，结合针对历史手写特点的图像预处理和数据增强技术，并评估集成学习方法。

Result: 最佳单模型增强方法（Elastic）的字符错误率（CER）为1.86，而前5投票集成的CER为1.60，相对提升了50%。

Conclusion: 领域特定的数据增强和集成策略显著提升了历史手稿的HTR性能。

Abstract: Historical handwritten text recognition (HTR) is essential for unlocking the
cultural and scholarly value of archival documents, yet digitization is often
hindered by scarce transcriptions, linguistic variation, and highly diverse
handwriting styles. In this study, we apply TrOCR, a state-of-the-art
transformer-based HTR model, to 16th-century Latin manuscripts authored by
Rudolf Gwalther. We investigate targeted image preprocessing and a broad suite
of data augmentation techniques, introducing four novel augmentation methods
designed specifically for historical handwriting characteristics. We also
evaluate ensemble learning approaches to leverage the complementary strengths
of augmentation-trained models. On the Gwalther dataset, our best single-model
augmentation (Elastic) achieves a Character Error Rate (CER) of 1.86, while a
top-5 voting ensemble achieves a CER of 1.60 - representing a 50% relative
improvement over the best reported TrOCR_BASE result and a 42% improvement over
the previous state of the art. These results highlight the impact of
domain-specific augmentations and ensemble strategies in advancing HTR
performance for historical manuscripts.

</details>


### [82] [AIM: Amending Inherent Interpretability via Self-Supervised Masking](https://arxiv.org/abs/2508.11502)
*Eyad Alshami,Shashank Agnihotri,Bernt Schiele,Margret Keuper*

Main category: cs.CV

TL;DR: AIM方法通过自监督掩码促进DNN使用真实特征而非虚假特征，无需额外标注，提升模型性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决DNN同时使用真实和虚假特征的问题，提升模型的可解释性和泛化能力。

Method: 利用多阶段编码特征指导自监督的样本特定特征掩码过程。

Result: 在多个数据集上验证，AIM显著提升可解释性（EPG评分）和准确性。

Conclusion: AIM有效促进真实特征的使用，提升泛化能力和人类对齐的可解释性。

Abstract: It has been observed that deep neural networks (DNNs) often use both genuine
as well as spurious features. In this work, we propose "Amending Inherent
Interpretability via Self-Supervised Masking" (AIM), a simple yet interestingly
effective method that promotes the network's utilization of genuine features
over spurious alternatives without requiring additional annotations. In
particular, AIM uses features at multiple encoding stages to guide a
self-supervised, sample-specific feature-masking process. As a result, AIM
enables the training of well-performing and inherently interpretable models
that faithfully summarize the decision process. We validate AIM across a
diverse range of challenging datasets that test both out-of-distribution
generalization and fine-grained visual understanding. These include
general-purpose classification benchmarks such as ImageNet100, HardImageNet,
and ImageWoof, as well as fine-grained classification datasets such as
Waterbirds, TravelingBirds, and CUB-200. AIM demonstrates significant dual
benefits: interpretability improvements, as measured by the Energy Pointing
Game (EPG) score, and accuracy gains over strong baselines. These consistent
gains across domains and architectures provide compelling evidence that AIM
promotes the use of genuine and meaningful features that directly contribute to
improved generalization and human-aligned interpretability.

</details>


### [83] [A Real-time Concrete Crack Detection and Segmentation Model Based on YOLOv11](https://arxiv.org/abs/2508.11517)
*Shaoze Huang,Qi Liu,Chao Chen,Yuhang Chen*

Main category: cs.CV

TL;DR: 论文提出YOLOv11-KW-TA-FP模型，用于高效检测混凝土裂缝，通过动态卷积、三重注意力机制和FP-IoU损失函数优化性能，实验显示显著提升。


<details>
  <summary>Details</summary>
Motivation: 长江三角洲地区基础设施老化迅速，传统人工检测效率低，现有深度学习模型对小目标裂缝检测效果不佳，需要更高效的解决方案。

Method: 基于YOLOv11n架构，结合动态KernelWarehouse卷积（KWConv）、三重注意力机制（TA）和FP-IoU损失函数，构建多任务检测与分割模型。

Result: 模型性能显著提升，达到91.3%的精确率、76.6%的召回率和86.4%的mAP@50，且在数据稀缺和噪声干扰下表现稳定。

Conclusion: 该研究为自动化基础设施检测提供了高效的计算机视觉解决方案，具有实际工程价值。

Abstract: Accelerated aging of transportation infrastructure in the rapidly developing
Yangtze River Delta region necessitates efficient concrete crack detection, as
crack deterioration critically compromises structural integrity and regional
economic growth. To overcome the limitations of inefficient manual inspection
and the suboptimal performance of existing deep learning models, particularly
for small-target crack detection within complex backgrounds, this paper
proposes YOLOv11-KW-TA-FP, a multi-task concrete crack detection and
segmentation model based on the YOLOv11n architecture. The proposed model
integrates a three-stage optimization framework: (1) Embedding dynamic
KernelWarehouse convolution (KWConv) within the backbone network to enhance
feature representation through a dynamic kernel sharing mechanism; (2)
Incorporating a triple attention mechanism (TA) into the feature pyramid to
strengthen channel-spatial interaction modeling; and (3) Designing an FP-IoU
loss function to facilitate adaptive bounding box regression penalization.
Experimental validation demonstrates that the enhanced model achieves
significant performance improvements over the baseline, attaining 91.3%
precision, 76.6% recall, and 86.4% mAP@50. Ablation studies confirm the
synergistic efficacy of the proposed modules. Furthermore, robustness tests
indicate stable performance under conditions of data scarcity and noise
interference. This research delivers an efficient computer vision solution for
automated infrastructure inspection, exhibiting substantial practical
engineering value.

</details>


### [84] [Multi-State Tracker: Enhancing Efficient Object Tracking via Multi-State Specialization and Interaction](https://arxiv.org/abs/2508.11531)
*Shilei Wang,Gong Cheng,Pujian Lai,Dong Gao,Junwei Han*

Main category: cs.CV

TL;DR: 提出了一种多状态跟踪器（MST），通过轻量级的状态特定增强（SSE）和跨状态交互（CSI）模块，显著提升了特征表示能力，同时保持低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有高效跟踪器因降低计算复杂度和模型参数而牺牲了特征表示能力，限制了其在复杂环境中的跟踪准确性。

Method: MST采用多状态生成（MSG）生成多阶段特征，SSE细化目标特征，CSI模块实现状态间信息交互。SSE和CSI基于轻量级HSA-SSD设计。

Result: MST在多个数据集上优于先前高效跟踪器，GOT-10K数据集上AO分数提升4.5%，计算开销仅0.1 GFLOPs和0.66 M参数。

Conclusion: MST通过轻量级设计显著提升了跟踪的准确性和鲁棒性，同时保持高效运行性能。

Abstract: Efficient trackers achieve faster runtime by reducing computational
complexity and model parameters. However, this efficiency often compromises the
expense of weakened feature representation capacity, thus limiting their
ability to accurately capture target states using single-layer features. To
overcome this limitation, we propose Multi-State Tracker (MST), which utilizes
highly lightweight state-specific enhancement (SSE) to perform specialized
enhancement on multi-state features produced by multi-state generation (MSG)
and aggregates them in an interactive and adaptive manner using cross-state
interaction (CSI). This design greatly enhances feature representation while
incurring minimal computational overhead, leading to improved tracking
robustness in complex environments. Specifically, the MSG generates multiple
state representations at multiple stages during feature extraction, while SSE
refines them to highlight target-specific features. The CSI module facilitates
information exchange between these states and ensures the integration of
complementary features. Notably, the introduced SSE and CSI modules adopt a
highly lightweight hidden state adaptation-based state space duality (HSA-SSD)
design, incurring only 0.1 GFLOPs in computation and 0.66 M in parameters.
Experimental results demonstrate that MST outperforms all previous efficient
trackers across multiple datasets, significantly improving tracking accuracy
and robustness. In particular, it shows excellent runtime performance, with an
AO score improvement of 4.5\% over the previous SOTA efficient tracker HCAT on
the GOT-10K dataset. The code is available at https://github.com/wsumel/MST.

</details>


### [85] [An Efficient Medical Image Classification Method Based on a Lightweight Improved ConvNeXt-Tiny Architecture](https://arxiv.org/abs/2508.11532)
*Jingsong Xia,Yue Yin,Xiuhan Li*

Main category: cs.CV

TL;DR: 该研究提出了一种基于改进ConvNeXt-Tiny架构的医学图像分类方法，通过结构优化和损失函数设计，在降低计算复杂度的同时提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的计算环境中实现高效高精度的医学图像分类仍具挑战性。

Method: 引入双全局池化特征融合策略和轻量级通道注意力模块SEVector，并加入特征平滑损失函数。

Result: 在CPU条件下，10个训练周期内测试集分类准确率达89.10%，损失值收敛稳定。

Conclusion: 该方法为资源受限环境下的医学图像分类提供了高效可行的解决方案。

Abstract: Intelligent analysis of medical imaging plays a crucial role in assisting
clinical diagnosis. However, achieving efficient and high-accuracy image
classification in resource-constrained computational environments remains
challenging. This study proposes a medical image classification method based on
an improved ConvNeXt-Tiny architecture. Through structural optimization and
loss function design, the proposed method enhances feature extraction
capability and classification performance while reducing computational
complexity. Specifically, the method introduces a dual global pooling (Global
Average Pooling and Global Max Pooling) feature fusion strategy into the
ConvNeXt-Tiny backbone to simultaneously preserve global statistical features
and salient response information. A lightweight channel attention module,
termed Squeeze-and-Excitation Vector (SEVector), is designed to improve the
adaptive allocation of channel weights while minimizing parameter overhead.
Additionally, a Feature Smoothing Loss is incorporated into the loss function
to enhance intra-class feature consistency and suppress intra-class variance.
Under CPU-only conditions (8 threads), the method achieves a maximum
classification accuracy of 89.10% on the test set within 10 training epochs,
exhibiting a stable convergence trend in loss values. Experimental results
demonstrate that the proposed method effectively improves medical image
classification performance in resource-limited settings, providing a feasible
and efficient solution for the deployment and promotion of medical imaging
analysis models.

</details>


### [86] [Reinforcing Video Reasoning Segmentation to Think Before It Segments](https://arxiv.org/abs/2508.11538)
*Sitong Gong,Lu Zhang,Yunzhi Zhuge,Xu Jia,Pingping Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: Veason-R1是一种专用于视频推理分割（VRS）的大型视觉语言模型，通过结构化推理和强化学习优化，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在推理时缺乏可解释性且性能不足，Veason-R1旨在通过结构化推理和强化学习解决这些问题。

Method: 采用Group Relative Policy Optimization（GRPO）和Chain-of-Thought（CoT）初始化训练模型，结合高质量CoT数据和奖励机制优化推理链。

Result: 在多个基准测试中表现优异，显著超越现有方法（如ReVOS和ReasonVOS），并增强了抗幻觉能力。

Conclusion: Veason-R1通过结构化推理和强化学习优化，实现了VRS任务的突破性进展。

Abstract: Video reasoning segmentation (VRS) endeavors to delineate referred objects in
videos guided by implicit instructions that encapsulate human intent and
temporal logic. Previous approaches leverage large vision language models
(LVLMs) to encode object semantics into <SEG> tokens for mask prediction.
However, this paradigm suffers from limited interpretability during inference
and suboptimal performance due to inadequate spatiotemporal reasoning. Drawing
inspiration from seminal breakthroughs in reinforcement learning, we introduce
Veason-R1, a specialized LVLM for VRS that emphasizes structured reasoning in
segmentation. Veason-R1 is trained through Group Relative Policy Optimization
(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, we
curate high-quality CoT training data to instill structured reasoning
trajectories, bridging video-level semantics and frame-level spatial grounding,
yielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPO
fine-tuning encourages efficient exploration of the reasoning space by
optimizing reasoning chains. To this end, we incorporate a holistic reward
mechanism that synergistically enhances spatial alignment and temporal
consistency, bolstering keyframe localization and fine-grained grounding.
Comprehensive empirical evaluations demonstrate that Veason-R1 achieves
state-of-the-art performance on multiple benchmarks, surpassing prior art by
significant margins (e.g., +1.3 J &F in ReVOS and +10.0 J &F in ReasonVOS),
while exhibiting robustness to hallucinations (+8.8 R). Our code and model
weights will be available at Veason-R1.

</details>


### [87] [Training-Free Anomaly Generation via Dual-Attention Enhancement in Diffusion Model](https://arxiv.org/abs/2508.11550)
*Zuo Zuo,Jiahao Dong,Yanyun Qu,Zongze Wu*

Main category: cs.CV

TL;DR: 提出了一种基于Stable Diffusion的无训练异常生成框架AAG，通过改进交叉注意力和自注意力机制，生成逼真的异常图像，提升下游异常检测任务性能。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测中数据稀缺问题突出，现有异常生成方法缺乏逼真性或需额外训练数据。

Method: AAG框架结合Stable Diffusion，提出交叉注意力增强（CAE）和自注意力增强（SAE）机制，通过文本提示和掩码生成特定区域的异常图像。

Result: 在MVTec AD和VisA数据集上验证了AAG的有效性，生成的异常图像能显著提升下游异常检测任务性能。

Conclusion: AAG为工业异常检测提供了一种高效、无需训练的异常生成方法，具有实际应用价值。

Abstract: Industrial anomaly detection (AD) plays a significant role in manufacturing
where a long-standing challenge is data scarcity. A growing body of works have
emerged to address insufficient anomaly data via anomaly generation. However,
these anomaly generation methods suffer from lack of fidelity or need to be
trained with extra data. To this end, we propose a training-free anomaly
generation framework dubbed AAG, which is based on Stable Diffusion (SD)'s
strong generation ability for effective anomaly image generation. Given a
normal image, mask and a simple text prompt, AAG can generate realistic and
natural anomalies in the specific regions and simultaneously keep contents in
other regions unchanged. In particular, we propose Cross-Attention Enhancement
(CAE) to re-engineer the cross-attention mechanism within Stable Diffusion
based on the given mask. CAE increases the similarity between visual tokens in
specific regions and text embeddings, which guides these generated visual
tokens in accordance with the text description. Besides, generated anomalies
need to be more natural and plausible with object in given image. We propose
Self-Attention Enhancement (SAE) which improves similarity between each normal
visual token and anomaly visual tokens. SAE ensures that generated anomalies
are coherent with original pattern. Extensive experiments on MVTec AD and VisA
datasets demonstrate effectiveness of AAG in anomaly generation and its
utility. Furthermore, anomaly images generated by AAG can bolster performance
of various downstream anomaly inspection tasks.

</details>


### [88] [TrajSV: A Trajectory-based Model for Sports Video Representations and Applications](https://arxiv.org/abs/2508.11569)
*Zheng Wang,Shihao Xu,Wei Shi*

Main category: cs.CV

TL;DR: TrajSV是一个基于轨迹的运动分析框架，解决了数据不可用、缺乏有效轨迹框架和标签不足的问题，通过三个模块（数据预处理、CRNet和VRNet）实现无监督学习，在运动视频检索、动作识别和视频字幕任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决运动分析中数据不可用、缺乏有效轨迹框架和标签不足的问题。

Method: 提出TrajSV框架，包括数据预处理、Clip Representation Network (CRNet)和Video Representation Network (VRNet)，使用轨迹增强的Transformer模块和无监督的三重对比损失优化表示。

Result: 在三种运动（足球、篮球、排球）和三种任务（视频检索、动作识别、视频字幕）中表现优异，视频检索提升近70%，动作识别在17类中9类领先，视频字幕提升近20%。

Conclusion: TrajSV在运动视频分析中表现卓越，解决了现有问题，并通过部署系统展示了实际应用潜力。

Abstract: Sports analytics has received significant attention from both academia and
industry in recent years. Despite the growing interest and efforts in this
field, several issues remain unresolved, including (1) data unavailability, (2)
lack of an effective trajectory-based framework, and (3) requirement for
sufficient supervision labels. In this paper, we present TrajSV, a
trajectory-based framework that addresses various issues in existing studies.
TrajSV comprises three components: data preprocessing, Clip Representation
Network (CRNet), and Video Representation Network (VRNet). The data
preprocessing module extracts player and ball trajectories from sports
broadcast videos. CRNet utilizes a trajectory-enhanced Transformer module to
learn clip representations based on these trajectories. Additionally, VRNet
learns video representations by aggregating clip representations and visual
features with an encoder-decoder architecture. Finally, a triple contrastive
loss is introduced to optimize both video and clip representations in an
unsupervised manner. The experiments are conducted on three broadcast video
datasets to verify the effectiveness of TrajSV for three types of sports (i.e.,
soccer, basketball, and volleyball) with three downstream applications (i.e.,
sports video retrieval, action spotting, and video captioning). The results
demonstrate that TrajSV achieves state-of-the-art performance in sports video
retrieval, showcasing a nearly 70% improvement. It outperforms baselines in
action spotting, achieving state-of-the-art results in 9 out of 17 action
categories, and demonstrates a nearly 20% improvement in video captioning.
Additionally, we introduce a deployed system along with the three applications
based on TrajSV.

</details>


### [89] [Causality Matters: How Temporal Information Emerges in Video Language Models](https://arxiv.org/abs/2508.11576)
*Yumeng Shi,Quanyu Long,Yin Wu,Wenya Wang*

Main category: cs.CV

TL;DR: 研究发现，视频语言模型中位置编码对时间理解影响有限，而帧顺序反转会导致性能显著下降。通过分析，揭示了时间信息通过帧间注意力逐步合成的机制，并提出了两种高效策略。


<details>
  <summary>Details</summary>
Motivation: 探索视频语言模型中时间理解的机制，特别是位置编码的作用，以提升模型性能。

Method: 通过移除或修改位置编码、反转帧顺序等实验，分析时间信息的整合机制，并提出跨模态注意力和时间退出机制。

Result: 发现时间信息通过帧间注意力逐步合成，位置编码影响较小，提出的策略在基准测试中有效。

Conclusion: 视频语言模型的时间理解主要依赖帧间注意力，而非位置编码，提出的高效策略为未来模型改进提供了方向。

Abstract: Video language models (VideoLMs) have made significant progress in multimodal
understanding. However, temporal understanding, which involves identifying
event order, duration, and relationships across time, still remains a core
challenge. Prior works emphasize positional encodings (PEs) as a key mechanism
for encoding temporal structure. Surprisingly, we find that removing or
modifying PEs in video inputs yields minimal degradation in the performance of
temporal understanding. In contrast, reversing the frame sequence while
preserving the original PEs causes a substantial drop. To explain this
behavior, we conduct substantial analysis experiments to trace how temporal
information is integrated within the model. We uncover a causal information
pathway: temporal cues are progressively synthesized through inter-frame
attention, aggregated in the final frame, and subsequently integrated into the
query tokens. This emergent mechanism shows that temporal reasoning emerges
from inter-visual token interactions under the constraints of causal attention,
which implicitly encodes temporal structure. Based on these insights, we
propose two efficiency-oriented strategies: staged cross-modal attention and a
temporal exit mechanism for early token truncation. Experiments on two
benchmarks validate the effectiveness of both approaches. To the best of our
knowledge, this is the first work to systematically investigate video temporal
understanding in VideoLMs, offering insights for future model improvement.

</details>


### [90] [DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring](https://arxiv.org/abs/2508.11591)
*Durga Joshi,Chandi Witharana,Robert Fahey,Thomas Worthley,Zhe Zhu,Diego Cerrai*

Main category: cs.CV

TL;DR: 提出了一种低成本、可重复的框架，利用车载摄像头视频数据实时评估和定位路边植被与基础设施。


<details>
  <summary>Details</summary>
Motivation: 传统遥感方法（如LiDAR）成本高且不适用于实时监测，本研究旨在提供一种快速、低成本的替代方案。

Method: 结合单目深度估计、深度误差校正和几何三角测量，从车载摄像头视频中生成空间和结构数据。

Result: 深度校正模型表现优异（R2=0.92），定位误差平均2.83米，高度估计误差树木2.09米、电线杆0.88米。

Conclusion: 该框架为城市植被和基础设施监测提供了实时、低成本解决方案，适用于公用事业公司和城市规划者。

Abstract: Our study introduces a novel, low-cost, and reproducible framework for
real-time, object-level structural assessment and geolocation of roadside
vegetation and infrastructure with commonly available but underutilized
dashboard camera (dashcam) video data. We developed an end-to-end pipeline that
combines monocular depth estimation, depth error correction, and geometric
triangulation to generate accurate spatial and structural data from
street-level video streams from vehicle-mounted dashcams. Depth maps were first
estimated using a state-of-the-art monocular depth model, then refined via a
gradient-boosted regression framework to correct underestimations, particularly
for distant objects. The depth correction model achieved strong predictive
performance (R2 = 0.92, MAE = 0.31 on transformed scale), significantly
reducing bias beyond 15 m. Further, object locations were estimated using
GPS-based triangulation, while object heights were calculated using pin hole
camera geometry. Our method was evaluated under varying conditions of camera
placement and vehicle speed. Low-speed vehicle with inside camera gave the
highest accuracy, with mean geolocation error of 2.83 m, and mean absolute
error (MAE) in height estimation of 2.09 m for trees and 0.88 m for poles. To
the best of our knowledge, it is the first framework to combine monocular depth
modeling, triangulated GPS-based geolocation, and real-time structural
assessment for urban vegetation and infrastructure using consumer-grade video
data. Our approach complements conventional RS methods, such as LiDAR and image
by offering a fast, real-time, and cost-effective solution for object-level
monitoring of vegetation risks and infrastructure exposure, making it
especially valuable for utility companies, and urban planners aiming for
scalable and frequent assessments in dynamic urban environments.

</details>


### [91] [CoreEditor: Consistent 3D Editing via Correspondence-constrained Diffusion](https://arxiv.org/abs/2508.11603)
*Zhe Zhu,Honghua Chen,Peng Li,Mingqiang Wei*

Main category: cs.CV

TL;DR: CoreEditor提出了一种基于文本驱动的3D编辑框架，通过引入对应约束注意力机制和语义相似性，解决了多视图一致性问题，显著提升了编辑质量和细节清晰度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在文本驱动的3D编辑中难以保持多视图一致性，导致编辑效果不足和细节模糊。

Method: 提出CoreEditor框架，结合对应约束注意力机制和语义相似性建模，实现多视图一致性编辑，并提供选择性编辑流程。

Result: 实验表明，CoreEditor能生成高质量、3D一致的编辑结果，细节更清晰，优于现有方法。

Conclusion: CoreEditor通过创新的对应约束和语义建模，显著提升了文本驱动的3D编辑效果。

Abstract: Text-driven 3D editing seeks to modify 3D scenes according to textual
descriptions, and most existing approaches tackle this by adapting pre-trained
2D image editors to multi-view inputs. However, without explicit control over
multi-view information exchange, they often fail to maintain cross-view
consistency, leading to insufficient edits and blurry details. We introduce
CoreEditor, a novel framework for consistent text-to-3D editing. The key
innovation is a correspondence-constrained attention mechanism that enforces
precise interactions between pixels expected to remain consistent throughout
the diffusion denoising process. Beyond relying solely on geometric alignment,
we further incorporate semantic similarity estimated during denoising, enabling
more reliable correspondence modeling and robust multi-view editing. In
addition, we design a selective editing pipeline that allows users to choose
preferred results from multiple candidates, offering greater flexibility and
user control. Extensive experiments show that CoreEditor produces high-quality,
3D-consistent edits with sharper details, significantly outperforming prior
methods.

</details>


### [92] [LoRAtorio: An intrinsic approach to LoRA Skill Composition](https://arxiv.org/abs/2508.11624)
*Niki Foteinopoulou,Ignas Budvytis,Stephan Liwicki*

Main category: cs.CV

TL;DR: LoRAtorio是一种无需训练的多LoRA组合框架，通过利用模型内在行为解决现有方法在多LoRA组合中的不足，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多LoRA组合中表现不佳，尤其是在开放环境下，无法预先知道所需技能的数量和性质。

Method: 通过空间分块计算余弦相似度，构建空间感知权重矩阵，加权聚合LoRA输出，并改进分类器自由引导以解决领域漂移。

Result: LoRAtorio在ClipScore上提升1.3%，在GPT-4V评估中胜率达72.43%，且适用于多种潜在扩散模型。

Conclusion: LoRAtorio在多LoRA组合任务中表现出色，具有广泛适用性和高效性能。

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted technique in
text-to-image diffusion models, enabling the personalisation of visual concepts
such as characters, styles, and objects. However, existing approaches struggle
to effectively compose multiple LoRA adapters, particularly in open-ended
settings where the number and nature of required skills are not known in
advance. In this work, we present LoRAtorio, a novel train-free framework for
multi-LoRA composition that leverages intrinsic model behaviour. Our method is
motivated by two key observations: (1) LoRA adapters trained on narrow domains
produce denoised outputs that diverge from the base model, and (2) when
operating out-of-distribution, LoRA outputs show behaviour closer to the base
model than when conditioned in distribution. The balance between these two
observations allows for exceptional performance in the single LoRA scenario,
which nevertheless deteriorates when multiple LoRAs are loaded. Our method
operates in the latent space by dividing it into spatial patches and computing
cosine similarity between each patch's predicted noise and that of the base
model. These similarities are used to construct a spatially-aware weight
matrix, which guides a weighted aggregation of LoRA outputs. To address domain
drift, we further propose a modification to classifier-free guidance that
incorporates the base model's unconditional score into the composition. We
extend this formulation to a dynamic module selection setting, enabling
inference-time selection of relevant LoRA adapters from a large pool. LoRAtorio
achieves state-of-the-art performance, showing up to a 1.3% improvement in
ClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, and generalises
effectively to multiple latent diffusion models.

</details>


### [93] [Is ChatGPT-5 Ready for Mammogram VQA?](https://arxiv.org/abs/2508.11628)
*Qiang Li,Shansong Wang,Mingzhe Hu,Mojtaba Safari,Zachary Eidex,Xiaofeng Yang*

Main category: cs.CV

TL;DR: GPT-5在乳腺X光片视觉问答任务中表现优于GPT-4o，但仍不及人类专家和领域专用模型。


<details>
  <summary>Details</summary>
Motivation: 探索通用大语言模型（如GPT-5）在乳腺X光片筛查任务中的应用潜力。

Method: 在四个公开乳腺X光片数据集（EMBED、InBreast、CMMD、CBIS-DDSM）上评估GPT-5和GPT-4o的性能，涵盖BI-RADS评估、异常检测和恶性分类任务。

Result: GPT-5在各项任务中表现最佳，但敏感性和特异性低于人类专家。具体表现为：EMBED数据集上密度分类56.8%，InBreast数据集上BI-RADS准确率36.9%。

Conclusion: GPT-5在乳腺X光片筛查任务中展现出潜力，但仍需领域适配和优化才能用于高风险临床应用。

Abstract: Mammogram visual question answering (VQA) integrates image interpretation
with clinical reasoning and has potential to support breast cancer screening.
We systematically evaluated the GPT-5 family and GPT-4o model on four public
mammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment,
abnormality detection, and malignancy classification tasks. GPT-5 consistently
was the best performing model but lagged behind both human experts and
domain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scores
among GPT variants in density (56.8%), distortion (52.5%), mass (64.5%),
calcification (63.5%), and malignancy (52.8%) classification. On InBreast, it
attained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0%
malignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detection
and 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADS
accuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Compared
with human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) and
specificity (52.3%). While GPT-5 exhibits promising capabilities for screening
tasks, its performance remains insufficient for high-stakes clinical imaging
applications without targeted domain adaptation and optimization. However, the
tremendous improvements in performance from GPT-4o to GPT-5 show a promising
trend in the potential for general large language models (LLMs) to assist with
mammography VQA tasks.

</details>


### [94] [Thyme: Think Beyond Images](https://arxiv.org/abs/2508.11630)
*Yi-Fan Zhang,Xingyu Lu,Shukang Yin,Chaoyou Fu,Wei Chen,Xiao Hu,Bin Wen,Kaiyu Jiang,Changyi Liu,Tianke Zhang,Haonan Fan,Kaibing Chen,Jiankang Chen,Haojie Ding,Kaiyu Tang,Zhang Zhang,Liang Wang,Fan Yang,Tingting Gao,Guorui Zhou*

Main category: cs.CV

TL;DR: Thyme提出了一种新范式，通过生成和执行代码来增强多模态大语言模型（MLLMs）的图像处理和逻辑推理能力，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有开源模型在图像处理和逻辑推理能力上不如专有模型，Thyme旨在填补这一空白。

Method: 采用两阶段训练策略：先通过SFT训练代码生成，再通过RL优化决策，并提出GRPO-ATS算法平衡推理与代码执行。

Result: 在近20个基准测试中，Thyme在感知和复杂推理任务上表现显著优于现有方法。

Conclusion: Thyme通过代码生成与执行，为MLLMs提供了更丰富的图像处理和推理能力，具有广泛的应用潜力。

Abstract: Following OpenAI's introduction of the ``thinking with images'' concept,
recent efforts have explored stimulating the use of visual information in the
reasoning process to enhance model performance in perception and reasoning
tasks. However, to the best of our knowledge, no open-source work currently
offers a feature set as rich as proprietary models (O3), which can perform
diverse image manipulations and simultaneously enhance logical reasoning
capabilities through code. In this paper, we make a preliminary attempt in this
direction by introducing Thyme (Think Beyond Images), a novel paradigm for
enabling MLLMs to transcend existing ``think with images'' approaches by
autonomously generating and executing diverse image processing and
computational operations via executable code. This approach not only
facilitates a rich, on-the-fly set of image manipulations (e.g., cropping,
rotation, contrast enhancement) but also allows for mathematical computations,
all while maintaining high autonomy in deciding when and how to apply these
operations. We activate this capability through a two-stage training strategy:
an initial SFT on a curated dataset of 500K samples to teach code generation,
followed by a RL phase to refine decision-making. For the RL stage, we manually
collect and design high-resolution question-answer pairs to increase the
learning difficulty, and we propose GRPO-ATS (Group Relative Policy
Optimization with Adaptive Temperature Sampling), an algorithm that applies
distinct temperatures to text and code generation to balance reasoning
exploration with code execution precision. We conduct extensive experimental
analysis and ablation studies. Comprehensive evaluations on nearly 20
benchmarks show that Thyme yields significant and consistent performance gains,
particularly in challenging high-resolution perception and complex reasoning
tasks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [95] [A2HCoder: An LLM-Driven Coding Agent for Hierarchical Algorithm-to-HDL Translation](https://arxiv.org/abs/2508.10904)
*Jie Lei,Ruofan Jia,J. Andrew Zhang,Hao Zhang*

Main category: cs.CL

TL;DR: A2HCoder是一种基于大型语言模型的分层算法到HDL编码代理，旨在高效可靠地实现算法到硬件的翻译。


<details>
  <summary>Details</summary>
Motivation: 无线通信系统对超低延迟和功耗的严格要求，导致算法设计与硬件实现之间存在显著差距，传统方法需要大量专业知识和手动开发。

Method: A2HCoder采用分层框架，水平维度分解算法为模块化功能块，垂直维度逐步细粒度翻译，并利用外部工具链调试和合成。

Result: 通过5G无线通信领域的实际部署案例验证了A2HCoder的实用性、可靠性和部署效率。

Conclusion: A2HCoder通过结构化流程显著减少了幻觉问题，确保了硬件级正确性。

Abstract: In wireless communication systems, stringent requirements such as ultra-low
latency and power consumption have significantly increased the demand for
efficient algorithm-to-hardware deployment. However, a persistent and
substantial gap remains between algorithm design and hardware implementation.
Bridging this gap traditionally requires extensive domain expertise and
time-consuming manual development, due to fundamental mismatches between
high-level programming languages like MATLAB and hardware description languages
(HDLs) such as Verilog-in terms of memory access patterns, data processing
manners, and datatype representations. To address this challenge, we propose
A2HCoder: a Hierarchical Algorithm-to-HDL Coding Agent, powered by large
language models (LLMs), designed to enable agile and reliable
algorithm-to-hardware translation. A2HCoder introduces a hierarchical framework
that enhances both robustness and interpretability while suppressing common
hallucination issues in LLM-generated code. In the horizontal dimension,
A2HCoder decomposes complex algorithms into modular functional blocks,
simplifying code generation and improving consistency. In the vertical
dimension, instead of relying on end-to-end generation, A2HCoder performs
step-by-step, fine-grained translation, leveraging external toolchains such as
MATLAB and Vitis HLS for debugging and circuit-level synthesis. This structured
process significantly mitigates hallucinations and ensures hardware-level
correctness. We validate A2HCoder through a real-world deployment case in the
5G wireless communication domain, demonstrating its practicality, reliability,
and deployment efficiency.

</details>


### [96] [PersonaTwin: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins](https://arxiv.org/abs/2508.10906)
*Sihan Chen,John P. Lalor,Yi Yang,Ahmed Abbasi*

Main category: cs.CL

TL;DR: PersonaTwin框架通过整合多维度数据构建自适应数字孪生，提升LLM在用户建模中的准确性和无偏性，实验证明其仿真效果接近理想状态。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在捕捉用户多维细微差异上的不足，提供更精准的用户模拟工具。

Method: 提出PersonaTwin框架，结合人口统计、行为和心理测量数据，通过多层级提示条件生成数字孪生。

Result: 实验显示PersonaTwin在仿真保真度上与理想设置相当，且下游模型在预测和公平性指标上接近真实用户数据训练结果。

Conclusion: PersonaTwin为个性化数字用户建模和行为分析提供了高效工具，展现了LLM数字孪生方法的潜力。

Abstract: While large language models (LLMs) afford new possibilities for user modeling
and approximation of human behaviors, they often fail to capture the
multidimensional nuances of individual users. In this work, we introduce
PersonaTwin, a multi-tier prompt conditioning framework that builds adaptive
digital twins by integrating demographic, behavioral, and psychometric data.
Using a comprehensive data set in the healthcare context of more than 8,500
individuals, we systematically benchmark PersonaTwin against standard LLM
outputs, and our rigorous evaluation unites state-of-the-art text similarity
metrics with dedicated demographic parity assessments, ensuring that generated
responses remain accurate and unbiased. Experimental results show that our
framework produces simulation fidelity on par with oracle settings. Moreover,
downstream models trained on persona-twins approximate models trained on
individuals in terms of prediction and fairness metrics across both
GPT-4o-based and Llama-based models. Together, these findings underscore the
potential for LLM digital twin-based approaches in producing realistic and
emotionally nuanced user simulations, offering a powerful tool for personalized
digital user modeling and behavior analysis.

</details>


### [97] [gpt-oss-120b & gpt-oss-20b Model Card](https://arxiv.org/abs/2508.10925)
*OpenAI,:,Sandhini Agarwal,Lama Ahmad,Jason Ai,Sam Altman,Andy Applebaum,Edwin Arbus,Rahul K. Arora,Yu Bai,Bowen Baker,Haiming Bao,Boaz Barak,Ally Bennett,Tyler Bertao,Nivedita Brett,Eugene Brevdo,Greg Brockman,Sebastien Bubeck,Che Chang,Kai Chen,Mark Chen,Enoch Cheung,Aidan Clark,Dan Cook,Marat Dukhan,Casey Dvorak,Kevin Fives,Vlad Fomenko,Timur Garipov,Kristian Georgiev,Mia Glaese,Tarun Gogineni,Adam Goucher,Lukas Gross,Katia Gil Guzman,John Hallman,Jackie Hehir,Johannes Heidecke,Alec Helyar,Haitang Hu,Romain Huet,Jacob Huh,Saachi Jain,Zach Johnson,Chris Koch,Irina Kofman,Dominik Kundel,Jason Kwon,Volodymyr Kyrylov,Elaine Ya Le,Guillaume Leclerc,James Park Lennon,Scott Lessans,Mario Lezcano-Casado,Yuanzhi Li,Zhuohan Li,Ji Lin,Jordan Liss,Lily,Liu,Jiancheng Liu,Kevin Lu,Chris Lu,Zoran Martinovic,Lindsay McCallum,Josh McGrath,Scott McKinney,Aidan McLaughlin,Song Mei,Steve Mostovoy,Tong Mu,Gideon Myles,Alexander Neitz,Alex Nichol,Jakub Pachocki,Alex Paino,Dana Palmie,Ashley Pantuliano,Giambattista Parascandolo,Jongsoo Park,Leher Pathak,Carolina Paz,Ludovic Peran,Dmitry Pimenov,Michelle Pokrass,Elizabeth Proehl,Huida Qiu,Gaby Raila,Filippo Raso,Hongyu Ren,Kimmy Richardson,David Robinson,Bob Rotsted,Hadi Salman,Suvansh Sanjeev,Max Schwarzer,D. Sculley,Harshit Sikchi,Kendal Simon,Karan Singhal,Yang Song,Dane Stuckey,Zhiqing Sun,Philippe Tillet,Sam Toizer,Foivos Tsimpourlas,Nikhil Vyas,Eric Wallace,Xin Wang,Miles Wang,Olivia Watkins,Kevin Weil,Amy Wendling,Kevin Whinnery,Cedric Whitney,Hannah Wong,Lin Yang,Yu Yang,Michihiro Yasunaga,Kristen Ying,Wojciech Zaremba,Wenting Zhan,Cyril Zhang,Brian Zhang,Eddie Zhang,Shengjia Zhao*

Main category: cs.CL

TL;DR: 介绍了两个开源推理模型gpt-oss-120b和gpt-oss-20b，通过混合专家架构和强化学习训练，优化了代理能力和指令跟随能力，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 推动推理模型的准确性和成本效益，同时支持广泛的开发和研究用途。

Method: 采用混合专家Transformer架构，结合大规模蒸馏和强化学习训练。

Result: 在数学、编程和安全等基准测试中表现优异，并开源了模型权重和相关工具。

Conclusion: 通过开源模型和工具，促进广泛使用和进一步研究。

Abstract: We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models
that push the frontier of accuracy and inference cost. The models use an
efficient mixture-of-expert transformer architecture and are trained using
large-scale distillation and reinforcement learning. We optimize the models to
have strong agentic capabilities (deep research browsing, python tool use, and
support for developer-provided functions), all while using a rendered chat
format that enables clear instruction following and role delineation. Both
models achieve strong results on benchmarks ranging from mathematics, coding,
and safety. We release the model weights, inference implementations, tool
environments, and tokenizers under an Apache 2.0 license to enable broad use
and further research.

</details>


### [98] [Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News](https://arxiv.org/abs/2508.10927)
*Jiaxin Pei,Soumya Vadlamannati,Liang-Kang Huang,Daniel Preotiuc-Pietro,Xinyu Hua*

Main category: cs.CL

TL;DR: 研究提出了一种自动从新闻中提取公司风险因素的计算框架，并比较了不同机器学习模型的性能，发现微调预训练模型优于零样本或少样本提示的大语言模型。


<details>
  <summary>Details</summary>
Motivation: 识别公司风险因素对投资者和金融市场的健康至关重要，但现有方法可能不够高效或准确。

Method: 构建了一个包含七个方面的风险因素分类框架，标注了744篇新闻文章，并测试了多种机器学习模型，包括大语言模型和微调预训练模型。

Result: 实验表明，微调预训练模型在大多数风险因素上表现更好，而零样本或少样本提示的大语言模型（如LLaMA-2）表现中等或较低。

Conclusion: 从新闻中识别风险因素能为公司和行业运营提供深入见解，微调预训练模型是更有效的工具。

Abstract: Identifying risks associated with a company is important to investors and the
well-being of the overall financial market. In this study, we build a
computational framework to automatically extract company risk factors from news
articles. Our newly proposed schema comprises seven distinct aspects, such as
supply chain, regulations, and competitions. We sample and annotate 744 news
articles and benchmark various machine learning models. While large language
models have achieved huge progress in various types of NLP tasks, our
experiment shows that zero-shot and few-shot prompting state-of-the-art LLMs
(e.g. LLaMA-2) can only achieve moderate to low performances in identifying
risk factors. And fine-tuned pre-trained language models are performing better
on most of the risk factors. Using this model, we analyze over 277K Bloomberg
news articles and demonstrate that identifying risk factors from news could
provide extensive insight into the operations of companies and industries.

</details>


### [99] [Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules](https://arxiv.org/abs/2508.10971)
*Nasim Shirvani-Mahdavi,Chengkai Li*

Main category: cs.CL

TL;DR: Rule2Text框架利用大语言模型为知识图谱的逻辑规则生成自然语言解释，提升可理解性和可用性。通过多数据集实验和人类评估，证明其有效性，并开源代码和数据。


<details>
  <summary>Details</summary>
Motivation: 知识图谱的逻辑规则复杂且难以解释，Rule2Text旨在通过自然语言生成改善其可访问性。

Method: 使用多种大语言模型（如Gemini 2.0 Flash）和提示策略（如零样本、少样本、链式推理）生成解释，并通过人类和LLM评估验证效果。

Result: 微调后的模型在解释质量上显著提升，尤其在特定领域数据集上表现突出。

Conclusion: Rule2Text成功提高了知识图谱规则的解释能力，并通过开源促进进一步研究。

Abstract: Knowledge graphs (KGs) can be enhanced through rule mining; however, the
resulting logical rules are often difficult for humans to interpret due to
their inherent complexity and the idiosyncratic labeling conventions of
individual KGs. This work presents Rule2Text, a comprehensive framework that
leverages large language models (LLMs) to generate natural language
explanations for mined logical rules, thereby improving KG accessibility and
usability. We conduct extensive experiments using multiple datasets, including
Freebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the
ogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically
evaluate several LLMs across a comprehensive range of prompting strategies,
including zero-shot, few-shot, variable type incorporation, and
Chain-of-Thought reasoning. To systematically assess models' performance, we
conduct a human evaluation of generated explanations on correctness and
clarity. To address evaluation scalability, we develop and validate an
LLM-as-a-judge framework that demonstrates strong agreement with human
evaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge,
and human-in-the-loop feedback, we construct high-quality ground truth
datasets, which we use to fine-tune the open-source Zephyr model. Our results
demonstrate significant improvements in explanation quality after fine-tuning,
with particularly strong gains in the domain-specific dataset. Additionally, we
integrate a type inference module to support KGs lacking explicit type
information. All code and data are publicly available at
https://github.com/idirlab/KGRule2NL.

</details>


### [100] [Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling](https://arxiv.org/abs/2508.10995)
*Tejomay Kishor Padole,Suyash P Awate,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本文提出了一种基于验证器的推理时间扩展方法，用于提升掩码扩散语言模型（MDM）的生成质量，实验表明MDM在文本风格转换任务中优于自回归语言模型。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型（MDM）因其可扩展性和易训练性成为离散数据生成的前沿方法，但如何进一步提升其生成质量仍需探索。

Method: 提出了一种基于验证器的推理时间扩展方法，结合预训练嵌入模型，优化MDM的去噪过程。

Result: 实验证明该方法显著提升了生成质量，MDM在文本风格转换任务中表现优于自回归模型。

Conclusion: 验证器辅助的MDM是一种高效且高质量的生成框架，适用于离散数据生成任务。

Abstract: Masked diffusion language models (MDMs) have recently gained traction as a
viable generative framework for natural language. This can be attributed to its
scalability and ease of training compared to other diffusion model paradigms
for discrete data, establishing itself as the state-of-the-art
non-autoregressive generator for discrete data. Diffusion models, in general,
have shown excellent ability to improve the generation quality by leveraging
inference-time scaling either by increasing the number of denoising steps or by
using external verifiers on top of the outputs of each step to guide the
generation. In this work, we propose a verifier-based inference-time scaling
method that aids in finding a better candidate generation during the denoising
process of the MDM. Our experiments demonstrate the application of MDMs for
standard text-style transfer tasks and establish MDMs as a better alternative
to autoregressive language models. Additionally, we show that a simple
soft-value-based verifier setup for MDMs using off-the-shelf pre-trained
embedding models leads to significant gains in generation quality even when
used on top of typical classifier-free guidance setups in the existing
literature.

</details>


### [101] [SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth](https://arxiv.org/abs/2508.11009)
*Wenpeng Xing,Lanyi Wei,Haixiao Hu,Rongchang Li,Mohan Li,Changting Lin,Meng Han*

Main category: cs.CL

TL;DR: 论文提出SproutBench，一个针对儿童和青少年的大型语言模型（LLM）安全评估套件，填补了现有安全框架的不足。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全框架主要针对成人，忽视了儿童和青少年在认知、情感和社交方面的独特风险。

Method: 开发了包含1,283个发展性对抗提示的SproutBench，评估47种LLM的安全漏洞。

Result: 发现LLM在情感依赖、隐私侵犯和危险行为模仿等方面存在显著漏洞，并揭示了安全性与风险预防之间的相关性。

Conclusion: 研究为儿童为中心的AI设计和部署提供了实用指南。

Abstract: The rapid proliferation of large language models (LLMs) in applications
targeting children and adolescents necessitates a fundamental reassessment of
prevailing AI safety frameworks, which are largely tailored to adult users and
neglect the distinct developmental vulnerabilities of minors. This paper
highlights key deficiencies in existing LLM safety benchmarks, including their
inadequate coverage of age-specific cognitive, emotional, and social risks
spanning early childhood (ages 0--6), middle childhood (7--12), and adolescence
(13--18). To bridge these gaps, we introduce SproutBench, an innovative
evaluation suite comprising 1,283 developmentally grounded adversarial prompts
designed to probe risks such as emotional dependency, privacy violations, and
imitation of hazardous behaviors. Through rigorous empirical evaluation of 47
diverse LLMs, we uncover substantial safety vulnerabilities, corroborated by
robust inter-dimensional correlations (e.g., between Safety and Risk
Prevention) and a notable inverse relationship between Interactivity and Age
Appropriateness. These insights yield practical guidelines for advancing
child-centric AI design and deployment.

</details>


### [102] [Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics](https://arxiv.org/abs/2508.11017)
*Carter Blum,Katja Filipova,Ann Yuan,Asma Ghandeharioun,Julian Zimmert,Fred Zhang,Jessica Hoffmann,Tal Linzen,Martin Wattenberg,Lucas Dixon,Mor Geva*

Main category: cs.CL

TL;DR: 研究大型语言模型在跨语言知识迁移中的问题，通过小规模Transformer模型和合成数据集揭示统一表示的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在跨语言知识迁移中的幻觉现象，揭示其成因和动态过程。

Method: 使用小型Transformer模型和合成多语言数据集，分析事实在不同语言中的表示方式。

Result: 发现统一表示对跨语言迁移至关重要，且统一程度受事实与语言互信息及语言提取难度影响。

Conclusion: 通过数据分布和分词调控跨语言迁移，为改进LLMs的跨语言能力提供新方向。

Abstract: Large language models (LLMs) struggle with cross-lingual knowledge transfer:
they hallucinate when asked in one language about facts expressed in a
different language during training. This work introduces a controlled setting
to study the causes and dynamics of this phenomenon by training small
Transformer models from scratch on synthetic multilingual datasets. We identify
a learning phase wherein a model develops either separate or unified
representations of the same facts across languages, and show that unification
is essential for cross-lingual transfer. We also show that the degree of
unification depends on mutual information between facts and training data
language, and on how easy it is to extract that language. Based on these
insights, we develop methods to modulate the level of cross-lingual transfer by
manipulating data distribution and tokenization, and we introduce metrics and
visualizations to formally characterize their effects on unification. Our work
shows how controlled settings can shed light on pre-training dynamics and
suggests new directions for improving cross-lingual transfer in LLMs.

</details>


### [103] [Hell or High Water: Evaluating Agentic Recovery from External Failures](https://arxiv.org/abs/2508.11027)
*Andrew Wang,Sophia Hager,Adi Asija,Daniel Khashabi,Nicholas Andrews*

Main category: cs.CL

TL;DR: 论文研究了语言模型代理在复杂任务中面对外部失败时的规划能力，发现其难以适应环境反馈并制定备用计划。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型代理在复杂任务中面对外部失败时的规划能力，以评估其适应性和鲁棒性。

Method: 设计了一个专门的规划基准测试，通过函数调用组合解决问题，并引入外部失败（如函数不可用）来测试代理的适应性。

Result: 发现语言模型代理难以适应环境反馈并制定备用计划，即使搜索空间受限。

Conclusion: 当前生成模型在适应性和规划能力上存在挑战，未来工作需改进这些方面。

Abstract: As language model agents are applied to real world problems of increasing
complexity, they will be expected to formulate plans across large search
spaces. If those plans fail for reasons beyond their control, how well do
language agents search for alternative ways to achieve their goals? We devise a
specialized agentic planning benchmark to study this question. Each planning
problem is solved via combinations of function calls. The agent searches for
relevant functions from a set of over four thousand possibilities, and observes
environmental feedback in the form of function outputs or error messages. Our
benchmark confronts the agent with external failures in its workflow, such as
functions that suddenly become unavailable. At the same time, even with the
introduction of these failures, we guarantee that the task remains solvable.
Ideally, an agent's performance on the planning task should not be affected by
the presence of external failures. Overall, we find that language agents
struggle to formulate and execute backup plans in response to environment
feedback. While state-of-the-art models are often able to identify the correct
function to use in the right context, they struggle to adapt to feedback from
the environment and often fail to pursue alternate courses of action, even when
the search space is artificially restricted. We provide a systematic analysis
of the failures of both open-source and commercial models, examining the
effects of search space size, as well as the benefits of scaling model size in
our setting. Our analysis identifies key challenges for current generative
models as well as promising directions for future work.

</details>


### [104] [BIPOLAR: Polarization-based granular framework for LLM bias evaluation](https://arxiv.org/abs/2508.11061)
*Martin Pavlíček,Tomáš Filip,Petr Sosík*

Main category: cs.CL

TL;DR: 该研究提出了一种可重用、细粒度且主题无关的框架，用于评估大语言模型（LLM）中与极化相关的偏见，并通过俄罗斯-乌克兰战争的案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在下游任务中表现出偏见，尤其是在敏感话题上，现有技术虽在偏见检测和缓解方面取得进展，但仍存在未充分探索的挑战。

Method: 结合极化敏感的情感指标与合成的平衡数据集，使用预定义的语义类别评估LLM的偏见。

Result: 研究发现LLM对乌克兰的情感普遍更积极，不同语义类别间存在显著差异，且模型对提示修改表现出进一步的偏见。

Conclusion: 该框架支持自动化数据集生成和细粒度偏见评估，适用于多种极化驱动场景，并与其他偏见评估策略互补。

Abstract: Large language models (LLMs) are known to exhibit biases in downstream tasks,
especially when dealing with sensitive topics such as political discourse,
gender identity, ethnic relations, or national stereotypes. Although
significant progress has been made in bias detection and mitigation techniques,
certain challenges remain underexplored. This study proposes a reusable,
granular, and topic-agnostic framework to evaluate polarisation-related biases
in LLM (both open-source and closed-source). Our approach combines
polarisation-sensitive sentiment metrics with a synthetically generated
balanced dataset of conflict-related statements, using a predefined set of
semantic categories.
  As a case study, we created a synthetic dataset that focusses on the
Russia-Ukraine war, and we evaluated the bias in several LLMs: Llama-3,
Mistral, GPT-4, Claude 3.5, and Gemini 1.0. Beyond aggregate bias scores, with
a general trend for more positive sentiment toward Ukraine, the framework
allowed fine-grained analysis with considerable variation between semantic
categories, uncovering divergent behavioural patterns among models. Adaptation
to prompt modifications showed further bias towards preconceived language and
citizenship modification.
  Overall, the framework supports automated dataset generation and fine-grained
bias assessment, is applicable to a variety of polarisation-driven scenarios
and topics, and is orthogonal to many other bias-evaluation strategies.

</details>


### [105] [Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs](https://arxiv.org/abs/2508.11068)
*Nicolas Goulet,Alexandre Blondin Massé,Moussa Abdendi*

Main category: cs.CL

TL;DR: 将数字词典嵌入AMR有向图，利用预训练语言模型，并通过保空间变换简化图，分析其性质与符号接地问题的关系。


<details>
  <summary>Details</summary>
Motivation: 探索如何将数字词典嵌入AMR有向图，以解决语义表示与符号接地问题。

Method: 使用预训练语言模型嵌入词典到AMR图，通过保空间变换简化图。

Result: 分析了简化图的特性，并讨论了其与符号接地问题的关联。

Conclusion: 该方法为语义表示和符号接地问题提供了新的研究视角。

Abstract: Abstract meaning representation (AMR) is a semantic formalism used to
represent the meaning of sentences as directed acyclic graphs. In this paper,
we describe how real digital dictionaries can be embedded into AMR directed
graphs (digraphs), using state-of-the-art pre-trained large language models.
Then, we reduce those graphs in a confluent manner, i.e. with transformations
that preserve their circuit space. Finally, the properties of these reduces
digraphs are analyzed and discussed in relation to the symbol grounding
problem.

</details>


### [106] [Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning](https://arxiv.org/abs/2508.11120)
*Lorenzo Jaime Yu Flores,Junyi Shen,Xiaoyuan Gu*

Main category: cs.CL

TL;DR: 论文介绍了一个多智能体框架RAMP，用于营销任务中的受众筛选，通过迭代规划、工具调用、输出验证和生成改进建议提高质量，并结合长期记忆存储提升准确性。


<details>
  <summary>Details</summary>
Motivation: 现有关于大型语言模型（LLM）在真实应用中的可靠性研究有限，本文旨在探索其在动态行业环境中的实际部署。

Method: 提出RAMP框架，结合迭代规划、工具调用、验证和反思机制，并引入长期记忆存储以存储客户特定信息。

Result: 在88个评估查询中，准确性提高了28个百分点；在模糊查询中，迭代验证和反思显著提升了召回率（约20个百分点）和用户满意度。

Conclusion: RAMP框架为在动态行业环境中部署可靠的基于LLM的系统提供了实用见解。

Abstract: Recent advances in large language models (LLMs) enabled the development of AI
agents that can plan and interact with tools to complete complex tasks.
However, literature on their reliability in real-world applications remains
limited. In this paper, we introduce a multi-agent framework for a marketing
task: audience curation. To solve this, we introduce a framework called RAMP
that iteratively plans, calls tools, verifies the output, and generates
suggestions to improve the quality of the audience generated. Additionally, we
equip the model with a long-term memory store, which is a knowledge base of
client-specific facts and past queries. Overall, we demonstrate the use of LLM
planning and memory, which increases accuracy by 28 percentage points on a set
of 88 evaluation queries. Moreover, we show the impact of iterative
verification and reflection on more ambiguous queries, showing progressively
better recall (roughly +20 percentage points) with more verify/reflect
iterations on a smaller challenge set, and higher user satisfaction. Our
results provide practical insights for deploying reliable LLM-based systems in
dynamic, industry-facing environments.

</details>


### [107] [MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents](https://arxiv.org/abs/2508.11133)
*Tomer Wolfson,Harsh Trivedi,Mor Geva,Yoav Goldberg,Dan Roth,Tushar Khot,Ashish Sabharwal,Reut Tsarfaty*

Main category: cs.CL

TL;DR: MoNaCo是一个包含1,315个自然且复杂问题的基准测试，旨在填补当前LLM基准测试中缺乏耗时问题的空白。前沿LLM在MoNaCo上的表现仅为61.2% F1，显示出其在处理复杂信息检索问题时的不足。


<details>
  <summary>Details</summary>
Motivation: 当前LLM基准测试缺乏自然且耗时的问题，无法充分评估模型在复杂信息检索任务中的表现。

Method: 通过分解标注流程，手动构建了1,315个需要多步推理的自然问题。

Result: 前沿LLM在MoNaCo上的最高F1得分为61.2%，召回率低且存在幻觉问题。

Conclusion: MoNaCo为评估和改进LLM在复杂信息检索任务中的表现提供了有效资源。

Abstract: Large language models (LLMs) are emerging as a go-to tool for querying
information. However, current LLM benchmarks rarely feature natural questions
that are both information-seeking as well as genuinely time-consuming for
humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural
and complex questions that require dozens, and at times hundreds, of
intermediate steps to solve -- far more than any existing QA benchmark. To
build MoNaCo, we developed a decomposed annotation pipeline to elicit and
manually answer natural time-consuming questions at scale. Frontier LLMs
evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and
hallucinations. Our results underscore the need for reasoning models that
better handle the complexity and sheer breadth of real-world
information-seeking questions -- with MoNaCo providing an effective resource
for tracking such progress. The MONACO benchmark, codebase, prompts and models
predictions are publicly available at: https://tomerwolgithub.github.io/monaco

</details>


### [108] [MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering](https://arxiv.org/abs/2508.11163)
*Hikaru Asano,Hiroki Ouchi,Akira Kasuga,Ryo Yonetani*

Main category: cs.CL

TL;DR: MobQA是一个评估大语言模型（LLMs）对人类移动数据语义理解能力的基准数据集，包含5800个高质量问答对，涵盖三种问题类型。


<details>
  <summary>Details</summary>
Motivation: 现有模型在预测人类移动模式上表现优异，但其对模式背后原因或语义的理解能力尚不明确。

Method: MobQA通过自然语言问答评估LLMs对多样化GPS轨迹的语义理解能力，问题类型包括事实检索、多选推理和自由解释。

Result: 评估显示LLMs在事实检索上表现良好，但在语义推理和解释问答上存在显著局限，轨迹长度对模型效果有较大影响。

Conclusion: MobQA揭示了当前LLMs在语义移动理解上的成就与局限，为未来研究提供了方向。

Abstract: This paper presents MobQA, a benchmark dataset designed to evaluate the
semantic understanding capabilities of large language models (LLMs) for human
mobility data through natural language question answering.
  While existing models excel at predicting human movement patterns, it remains
unobvious how much they can interpret the underlying reasons or semantic
meaning of those patterns. MobQA provides a comprehensive evaluation framework
for LLMs to answer questions about diverse human GPS trajectories spanning
daily to weekly granularities. It comprises 5,800 high-quality question-answer
pairs across three complementary question types: factual retrieval (precise
data extraction), multiple-choice reasoning (semantic inference), and free-form
explanation (interpretive description), which all require spatial, temporal,
and semantic reasoning. Our evaluation of major LLMs reveals strong performance
on factual retrieval but significant limitations in semantic reasoning and
explanation question answering, with trajectory length substantially impacting
model effectiveness. These findings demonstrate the achievements and
limitations of state-of-the-art LLMs for semantic mobility
understanding.\footnote{MobQA dataset is available at
https://github.com/CyberAgentAILab/mobqa.}

</details>


### [109] [Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification](https://arxiv.org/abs/2508.11166)
*Anusha M D,Deepthi Vikram,Bharathi Raja Chakravarthi,Parameshwar R Hegde*

Main category: cs.CL

TL;DR: 该研究为低资源的德拉维达语Tulu创建了首个社交媒体中混合代码的冒犯性语言识别基准数据集，并评估了多种深度学习模型，发现BiGRU模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: Tulu作为一种低资源语言，缺乏计算资源，但其数字存在感逐渐增强，因此需要建立相关基准数据集以支持NLP研究。

Method: 研究收集了YouTube评论，构建了包含3,845条注释数据的基准数据集，并评估了GRU、LSTM、BiGRU、BiLSTM、CNN、注意力机制及Transformer模型。

Result: BiGRU模型表现最佳，准确率达82%，宏F1分数为0.81；Transformer模型表现不佳，突显了多语言预训练在低资源混合代码环境中的局限性。

Conclusion: 该研究为Tulu及其他低资源混合代码语言的NLP研究奠定了基础。

Abstract: Tulu, a low-resource Dravidian language predominantly spoken in southern
India, has limited computational resources despite its growing digital
presence. This study presents the first benchmark dataset for Offensive
Language Identification (OLI) in code-mixed Tulu social media content,
collected from YouTube comments across various domains. The dataset, annotated
with high inter-annotator agreement (Krippendorff's alpha = 0.984), includes
3,845 comments categorized into four classes: Not Offensive, Not Tulu,
Offensive Untargeted, and Offensive Targeted. We evaluate a suite of deep
learning models, including GRU, LSTM, BiGRU, BiLSTM, CNN, and attention-based
variants, alongside transformer architectures (mBERT, XLM-RoBERTa). The BiGRU
model with self-attention achieves the best performance with 82% accuracy and a
0.81 macro F1-score. Transformer models underperform, highlighting the
limitations of multilingual pretraining in code-mixed, under-resourced
contexts. This work lays the foundation for further NLP research in Tulu and
similar low-resource, code-mixed languages.

</details>


### [110] [Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction](https://arxiv.org/abs/2508.11184)
*Tao Wu,Jingyuan Chen,Wang Lin,Jian Zhan,Mengze Li,Kun Kuang,Fei Wu*

Main category: cs.CL

TL;DR: 论文提出了一种个性化干扰项生成方法，通过分析学生的答题记录生成针对性干扰项，解决传统方法无法捕捉个体错误的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于大语言模型的干扰项生成方法无法捕捉学生个体错误，限制了诊断效果。

Method: 提出两阶段无训练框架：1）用蒙特卡洛树搜索从错误答案中恢复学生推理轨迹；2）基于原型生成个性化干扰项。

Result: 实验表明，该方法在140名学生中生成个性化干扰项效果最佳，且能推广到群体层面。

Conclusion: 该方法能有效生成个性化干扰项，提升诊断效果，并具有鲁棒性和适应性。

Abstract: Distractors, incorrect but plausible answer choices in multiple-choice
questions (MCQs), play a critical role in educational assessment by diagnosing
student misconceptions. Recent work has leveraged large language models (LLMs)
to generate shared, group-level distractors by learning common error patterns
across large student populations. However, such distractors often fail to
capture the diverse reasoning errors of individual students, limiting their
diagnostic effectiveness. To address this limitation, we introduce the task of
personalized distractor generation, which aims to generate tailored distractors
based on individual misconceptions inferred from each student's past
question-answering (QA) records, ensuring every student receives options that
effectively exposes their specific reasoning errors. While promising, this task
is challenging because each student typically has only a few QA records, which
often lack the student's underlying reasoning processes, making training-based
group-level approaches infeasible. To overcome this, we propose a training-free
two-stage framework. In the first stage, we construct a student-specific
misconception prototype by applying Monte Carlo Tree Search (MCTS) to recover
the student's reasoning trajectories from past incorrect answers. In the second
stage, this prototype guides the simulation of the student's reasoning on new
questions, enabling the generation of personalized distractors that align with
the student's recurring misconceptions. Experiments show that our approach
achieves the best performance in generating plausible, personalized distractors
for 140 students, and also effectively generalizes to group-level settings,
highlighting its robustness and adaptability.

</details>


### [111] [Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation](https://arxiv.org/abs/2508.11189)
*Chenyang Le,Yinfeng Xia,Huiyan Li,Manhong Wang,Yutao Sun,Xingyang Ma,Yanmin Qian*

Main category: cs.CL

TL;DR: 提出了一种名为Parasitic Dual-Scale Approach的新方法，结合推测采样、模型压缩和知识蒸馏技术，显著提升了多语言语音翻译模型的推理效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多语言语音翻译模型参数量大，难以在本地部署中平衡推理效率和性能。

Method: 基于Whisper Medium模型，引入KVSPN模块，结合推测采样、模型压缩和知识蒸馏技术，构建whisperM2M模型。

Result: 在六种流行语言上实现SOTA性能，推理速度提升40%（无BLEU分数下降），结合蒸馏方法后速度提升2.6倍。

Conclusion: 提出的方法有效解决了多语言语音翻译模型的效率与性能平衡问题，具有实际应用潜力。

Abstract: Recent advancements in speech-to-text translation have led to the development
of multilingual models capable of handling multiple language pairs
simultaneously. However, these unified models often suffer from large parameter
sizes, making it challenging to balance inference efficiency and performance,
particularly in local deployment scenarios. We propose an innovative Parasitic
Dual-Scale Approach, which combines an enhanced speculative sampling method
with model compression and knowledge distillation techniques. Building on the
Whisper Medium model, we enhance it for multilingual speech translation into
whisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art
(SOTA) performance across six popular languages with improved inference
efficiency. KVSPN enables a 40\% speedup with no BLEU score degradation.
Combined with distillation methods, it represents a 2.6$\times$ speedup over
the original Whisper Medium with superior performance.

</details>


### [112] [E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection](https://arxiv.org/abs/2508.11197)
*Ahmad Mousavi,Yeganeh Abdollahinejad,Roberto Corizzo,Nathalie Japkowicz,Zois Boukouvalas*

Main category: cs.CL

TL;DR: E-CaTCH是一个可解释且可扩展的框架，用于检测社交媒体上的多模态虚假信息，通过聚类伪事件、跨模态对齐和时间建模，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的多模态虚假信息检测面临模态不一致、时间模式变化和类别不平衡等挑战，现有方法未能捕捉事件级结构。

Method: E-CaTCH通过聚类伪事件、BERT和ResNet提取特征、跨模态注意力对齐、趋势感知LSTM建模时间演化，并结合自适应损失优化。

Result: 在Fakeddit、IND和COVID-19 MISINFOGRAPH数据集上，E-CaTCH表现优于现有基线，并展现出跨数据集的鲁棒性和泛化能力。

Conclusion: E-CaTCH通过事件级分类和多模态时间建模，有效解决了虚假信息检测的挑战，具有实际应用价值。

Abstract: Detecting multimodal misinformation on social media remains challenging due
to inconsistencies between modalities, changes in temporal patterns, and
substantial class imbalance. Many existing methods treat posts independently
and fail to capture the event-level structure that connects them across time
and modality. We propose E-CaTCH, an interpretable and scalable framework for
robustly detecting misinformation. If needed, E-CaTCH clusters posts into
pseudo-events based on textual similarity and temporal proximity, then
processes each event independently. Within each event, textual and visual
features are extracted using pre-trained BERT and ResNet encoders, refined via
intra-modal self-attention, and aligned through bidirectional cross-modal
attention. A soft gating mechanism fuses these representations to form
contextualized, content-aware embeddings of each post. To model temporal
evolution, E-CaTCH segments events into overlapping time windows and uses a
trend-aware LSTM, enhanced with semantic shift and momentum signals, to encode
narrative progression over time. Classification is performed at the event
level, enabling better alignment with real-world misinformation dynamics. To
address class imbalance and promote stable learning, the model integrates
adaptive class weighting, temporal consistency regularization, and hard-example
mining. The total loss is aggregated across all events. Extensive experiments
on Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH
consistently outperforms state-of-the-art baselines. Cross-dataset evaluations
further demonstrate its robustness, generalizability, and practical
applicability across diverse misinformation scenarios.

</details>


### [113] [Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2508.11247)
*Changjian Wang,Weihong Deng,Weili Guan,Quan Lu,Ning Jiang*

Main category: cs.CL

TL;DR: 提出了一种新的多跳问答（MHQA）方法HGRAG，通过超图整合结构和语义信息，提升问答性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在MHQA中未能充分利用结构关联或过度依赖结构信息，导致效果受限。

Method: 构建实体超图，结合细粒度实体和粗粒度段落，设计超图检索方法，并通过检索增强模块优化结果。

Result: 在基准数据集上表现优于现有方法，检索效率提升6倍。

Conclusion: HGRAG通过跨粒度整合结构和语义信息，显著提升了MHQA任务的性能。

Abstract: Multi-hop question answering (MHQA) requires integrating knowledge scattered
across multiple passages to derive the correct answer. Traditional
retrieval-augmented generation (RAG) methods primarily focus on coarse-grained
textual semantic similarity and ignore structural associations among dispersed
knowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods
address this by leveraging knowledge graphs (KGs) to capture structural
associations, but they tend to overly rely on structural information and
fine-grained word- or phrase-level retrieval, resulting in an underutilization
of textual semantics. In this paper, we propose a novel RAG approach called
HGRAG for MHQA that achieves cross-granularity integration of structural and
semantic information via hypergraphs. Structurally, we construct an entity
hypergraph where fine-grained entities serve as nodes and coarse-grained
passages as hyperedges, and establish knowledge association through shared
entities. Semantically, we design a hypergraph retrieval method that integrates
fine-grained entity similarity and coarse-grained passage similarity via
hypergraph diffusion. Finally, we employ a retrieval enhancement module, which
further refines the retrieved results both semantically and structurally, to
obtain the most relevant passages as context for answer generation with the
LLM. Experimental results on benchmark datasets demonstrate that our approach
outperforms state-of-the-art methods in QA performance, and achieves a
6$\times$ speedup in retrieval efficiency.

</details>


### [114] [UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?](https://arxiv.org/abs/2508.11260)
*Mukund Choudhary,KV Aditya Srivatsa,Gaurja Aeron,Antara Raaghavi Bhattacharya,Dang Khoa Dang Dinh,Ikhlasul Akmal Hanif,Daria Kotova,Ekaterina Kochmar,Monojit Choudhury*

Main category: cs.CL

TL;DR: LLMs在语言学谜题上表现不佳，尤其是在形态复杂的语言中，但通过预处理（如词素分割）可以提升表现。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在低资源语言中的语言学推理能力，揭示其弱点。

Method: 分析629个问题，标注语言学特征，测试预处理方法（如词素分割）。

Result: LLMs在形态复杂的谜题中表现差，但在与英语相似的特征上表现较好；词素分割显著提升表现。

Conclusion: LLMs在语言学推理和低资源语言建模中面临挑战，需改进分词方法。

Abstract: Large language models (LLMs) have demonstrated potential in reasoning tasks,
but their performance on linguistics puzzles remains consistently poor. These
puzzles, often derived from Linguistics Olympiad (LO) contests, provide a
minimal contamination environment to assess LLMs' linguistic reasoning
abilities across low-resource languages. This work analyses LLMs' performance
on 629 problems across 41 low-resource languages by labelling each with
linguistically informed features to unveil weaknesses. Our analyses show that
LLMs struggle with puzzles involving higher morphological complexity and
perform better on puzzles involving linguistic features that are also found in
English. We also show that splitting words into morphemes as a pre-processing
step improves solvability, indicating a need for more informed and
language-specific tokenisers. These findings thus offer insights into some
challenges in linguistic reasoning and modelling of low-resource languages.

</details>


### [115] [LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought](https://arxiv.org/abs/2508.11280)
*Ruiyan Qi,Congding Wen,Weibo Zhou,Shangsong Liang,Lingbo Li*

Main category: cs.CL

TL;DR: LETToT框架通过专家推理结构评估旅游领域的大语言模型，无需标注数据，效果优于基线，并揭示了模型规模与推理能力的关系。


<details>
  <summary>Details</summary>
Motivation: 由于标注数据成本高和幻觉问题，评估旅游领域的大语言模型具有挑战性。

Method: 提出LETToT框架，利用专家推理结构（ToT）替代标注数据，通过迭代优化和专家反馈验证。

Result: 专家ToT相对基线有4.99-14.15%的质量提升；小规模模型通过推理增强缩小与大规模模型的差距。

Conclusion: LETToT为领域特定LLM评估提供了可扩展的无标注范式，优于传统标注基准。

Abstract: Evaluating large language models (LLMs) in specific domain like tourism
remains challenging due to the prohibitive cost of annotated benchmarks and
persistent issues like hallucinations. We propose $\textbf{L}$able-Free
$\textbf{E}$valuation of LLM on $\textbf{T}$ourism using Expert
$\textbf{T}$ree-$\textbf{o}$f-$\textbf{T}$hought (LETToT), a framework that
leverages expert-derived reasoning structures-instead of labeled data-to access
LLMs in tourism. First, we iteratively refine and validate hierarchical ToT
components through alignment with generic quality dimensions and expert
feedback. Results demonstrate the effectiveness of our systematically optimized
expert ToT with 4.99-14.15\% relative quality gains over baselines. Second, we
apply LETToT's optimized expert ToT to evaluate models of varying scales
(32B-671B parameters), revealing: (1) Scaling laws persist in specialized
domains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,
DeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit
reasoning architectures outperform counterparts in accuracy and conciseness
($p<0.05$). Our work established a scalable, label-free paradigm for
domain-specific LLM evaluation, offering a robust alternative to conventional
annotated benchmarks.

</details>


### [116] [ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection](https://arxiv.org/abs/2508.11281)
*Axel Delaval,Shujian Yang,Haicheng Wang,Han Qiu,Jialiang Lu*

Main category: cs.CL

TL;DR: 论文介绍了TOXIFRENCH，一个法语毒性内容检测的新基准数据集，并提出了一种高效的半自动化标注方法。研究发现小型语言模型（SLMs）在任务中表现优于大型模型，并提出了动态加权损失的CoT微调策略，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 法语毒性内容检测因缺乏大规模数据集而发展不足，亟需解决这一问题。

Method: 构建TOXIFRENCH数据集，采用半自动化标注；提出动态加权损失的CoT微调策略。

Result: 微调的4B模型性能最优，F1分数提升13%，并展示了跨语言能力。

Conclusion: 该方法可扩展至其他语言和安全关键任务，具有广泛应用潜力。

Abstract: Detecting toxic content using language models is crucial yet challenging.
While substantial progress has been made in English, toxicity detection in
French remains underdeveloped, primarily due to the lack of culturally
relevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new
public benchmark of 53,622 French online comments, constructed via a
semi-automated annotation pipeline that reduces manual labeling to only 10%
through high-confidence LLM-based pre-annotation and human verification. Then,
we benchmark a broad range of models and uncover a counterintuitive insight:
Small Language Models (SLMs) outperform many larger models in robustness and
generalization under the toxicity detection task. Motivated by this finding, we
propose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic
weighted loss that progressively emphasizes the model's final decision,
significantly improving faithfulness. Our fine-tuned 4B model achieves
state-of-the-art performance, improving its F1 score by 13% over its baseline
and outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a
cross-lingual toxicity benchmark demonstrates strong multilingual ability,
suggesting that our methodology can be effectively extended to other languages
and safety-critical classification tasks.

</details>


### [117] [AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries](https://arxiv.org/abs/2508.11285)
*Arya VarastehNezhad,Reza Tavasoli,Soroush Elyasi,MohammadHossein LotfiNia,Hamed Farbeh*

Main category: cs.CL

TL;DR: 研究分析了8种大型语言模型（LLMs）对抑郁、焦虑和压力问题的回答，发现不同模型在情感表达上差异显著，且问题类型对情感反应影响较大，而用户人口统计特征影响较小。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在心理健康问题回答中的情感表达差异，以指导模型选择和应用。

Method: 对8种LLMs生成2,880个回答，使用先进工具进行情感和情绪评分。

Result: 不同模型情感表达差异显著，问题类型（抑郁、焦虑、压力）对情感反应影响大，人口统计特征影响小。

Conclusion: 模型选择对心理健康应用至关重要，因其情感表达可能显著影响用户体验和结果。

Abstract: Depression, anxiety, and stress are widespread mental health concerns that
increasingly drive individuals to seek information from Large Language Models
(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini
Pro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty
pragmatic questions about depression, anxiety, and stress when those questions
are framed for six user profiles (baseline, woman, man, young, old, and
university student). The models generated 2,880 answers, which we scored for
sentiment and emotions using state-of-the-art tools. Our analysis revealed that
optimism, fear, and sadness dominated the emotional landscape across all
outputs, with neutral sentiment maintaining consistently high values.
Gratitude, joy, and trust appeared at moderate levels, while emotions such as
anger, disgust, and love were rarely expressed. The choice of LLM significantly
influenced emotional expression patterns. Mixtral exhibited the highest levels
of negative emotions including disapproval, annoyance, and sadness, while Llama
demonstrated the most optimistic and joyful responses. The type of mental
health condition dramatically shaped emotional responses: anxiety prompts
elicited extraordinarily high fear scores (0.974), depression prompts generated
elevated sadness (0.686) and the highest negative sentiment, while
stress-related queries produced the most optimistic responses (0.755) with
elevated joy and trust. In contrast, demographic framing of queries produced
only marginal variations in emotional tone. Statistical analyses confirmed
significant model-specific and condition-specific differences, while
demographic influences remained minimal. These findings highlight the critical
importance of model selection in mental health applications, as each LLM
exhibits a distinct emotional signature that could significantly impact user
experience and outcomes.

</details>


### [118] [SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory](https://arxiv.org/abs/2508.11290)
*Utsav Maskey,Sumit Yadav,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 论文提出SafeConstellations方法，通过跟踪任务特定的轨迹模式，减少LLMs对无害指令的过度拒绝，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: LLMs的安全机制导致模型拒绝表面上类似有害内容的无害指令，降低了生产应用中的实用性。

Method: 通过嵌入空间中的轨迹分析，提出SafeConstellations方法，选择性引导模型行为以减少过度拒绝。

Result: 方法将过度拒绝率降低高达73%，且对实用性影响最小。

Conclusion: SafeConstellations提供了一种减少过度拒绝的原则性方法，同时保持模型的一般行为。

Abstract: LLMs increasingly exhibit over-refusal behavior, where safety mechanisms
cause models to reject benign instructions that superficially resemble harmful
content. This phenomena diminishes utility in production applications that
repeatedly rely on common prompt templates or applications that frequently rely
on LLMs for specific tasks (e.g. sentiment analysis, language translation).
Through comprehensive evaluation, we demonstrate that LLMs still tend to refuse
responses to harmful instructions when those instructions are reframed to
appear as benign tasks. Our mechanistic analysis reveal that LLMs follow
distinct "constellation" patterns in embedding space as representations
traverse layers, with each task maintaining consistent trajectories that shift
predictably between refusal and non-refusal cases. We introduce
SafeConstellations, an inference-time trajectory-shifting approach that tracks
task-specific trajectory patterns and guides representations toward non-refusal
pathways. By selectively guiding model behavior only on tasks prone to
over-refusal, and by preserving general model behavior, our method reduces
over-refusal rates by up to 73% with minimal impact on utility-offering a
principled approach to mitigating over-refusals.

</details>


### [119] [SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems](https://arxiv.org/abs/2508.11310)
*Beichen Guo,Zhiyuan Wen,Yu Yang,Peng Gao,Ruosong Yang,Jiaxing Shen*

Main category: cs.CL

TL;DR: SGSimEval是一个用于自动生成调查的全面评估基准，结合了LLM评分和定量指标，并引入人类偏好指标。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法存在偏差、缺乏人类偏好和过度依赖LLM的问题，需要更全面的评估框架。

Method: 提出SGSimEval，整合大纲、内容和参考文献的评估，结合LLM评分与定量指标。

Result: 当前ASG系统在大纲生成上表现优异，但在内容和参考文献生成上仍需改进，评估指标与人类评估一致性强。

Conclusion: SGSimEval为自动调查生成提供了多方面的评估方法，未来需进一步优化内容和参考文献生成。

Abstract: The growing interest in automatic survey generation (ASG), a task that
traditionally required considerable time and effort, has been spurred by recent
advances in large language models (LLMs). With advancements in
retrieval-augmented generation (RAG) and the rising popularity of multi-agent
systems (MASs), synthesizing academic surveys using LLMs has become a viable
approach, thereby elevating the need for robust evaluation methods in this
domain. However, existing evaluation methods suffer from several limitations,
including biased metrics, a lack of human preference, and an over-reliance on
LLMs-as-judges. To address these challenges, we propose SGSimEval, a
comprehensive benchmark for Survey Generation with Similarity-Enhanced
Evaluation that evaluates automatic survey generation systems by integrating
assessments of the outline, content, and references, and also combines
LLM-based scoring with quantitative metrics to provide a multifaceted
evaluation framework. In SGSimEval, we also introduce human preference metrics
that emphasize both inherent quality and similarity to humans. Extensive
experiments reveal that current ASG systems demonstrate human-comparable
superiority in outline generation, while showing significant room for
improvement in content and reference generation, and our evaluation metrics
maintain strong consistency with human assessments.

</details>


### [120] [LLM Compression: How Far Can We Go in Balancing Size and Performance?](https://arxiv.org/abs/2508.11318)
*Sahil Sk,Debasish Dhal,Sonal Khosla,Sk Shahid,Sambit Shekhar,Akash Dhaka,Shantipriya Parida,Dilip K. Prasad,Ondřej Bojar*

Main category: cs.CL

TL;DR: 该研究探讨了4位组缩放量化（GSQ）和生成预训练变换器量化（GPTQ）对LLaMA 1B、Qwen 0.5B和PHI 1.5B模型的影响，评估了它们在多个NLP任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 通过量化技术减少大型语言模型的内存占用和计算成本，同时保持性能，以提高其实际部署的可行性。

Method: 应用GSQ和GPTQ技术对模型进行4位量化，并在MS MARCO、BoolQ和GSM8K等数据集上评估准确性和效率。

Result: 研究量化了模型压缩与任务性能之间的权衡，分析了准确性、推理延迟和吞吐量等关键指标。

Conclusion: 研究结果为低比特量化在实际部署中的适用性提供了见解，并可作为未来实验的基准。

Abstract: Quantization is an essential and popular technique for improving the
accessibility of large language models (LLMs) by reducing memory usage and
computational costs while maintaining performance. In this study, we apply
4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer
Quantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their
impact across multiple NLP tasks. We benchmark these models on MS MARCO
(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K
(Mathematical Reasoning) datasets, assessing both accuracy and efficiency
across various tasks. The study measures the trade-offs between model
compression and task performance, analyzing key evaluation metrics, namely
accuracy, inference latency, and throughput (total output tokens generated per
second), providing insights into the suitability of low-bit quantization for
real-world deployment. Using the results, users can then make suitable
decisions based on the specifications that need to be met. We discuss the pros
and cons of GSQ and GPTQ techniques on models of different sizes, which also
serve as a benchmark for future experiments.

</details>


### [121] [SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis](https://arxiv.org/abs/2508.11343)
*Haitong Luo,Weiyao Zhang,Suhang Wang,Wenji Zou,Chungang Lin,Xuying Meng,Yujun Zhang*

Main category: cs.CL

TL;DR: 论文提出了一种基于信号处理的新方法，通过分析文本生成过程中的频谱特性来检测LLM生成的文本，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于高质量LLM生成文本的泛滥，需要可靠且高效的检测方法。现有方法多依赖表面统计特征，忽略了文本生成过程的信号特性。

Method: 将检测问题转化为信号处理问题，利用全局离散傅里叶变换（DFT）和局部短时傅里叶变换（STFT）分析文本的频谱特性，发现人类文本具有更高的频谱能量。

Result: 基于DFT总能量构建的检测器SpecDetect及其增强版SpecDetect++在实验中表现优异，运行时间减半。

Conclusion: 信号处理方法为LLM生成文本检测提供了高效且可解释的新途径。

Abstract: The proliferation of high-quality text from Large Language Models (LLMs)
demands reliable and efficient detection methods. While existing training-free
approaches show promise, they often rely on surface-level statistics and
overlook fundamental signal properties of the text generation process. In this
work, we reframe detection as a signal processing problem, introducing a novel
paradigm that analyzes the sequence of token log-probabilities in the frequency
domain. By systematically analyzing the signal's spectral properties using the
global Discrete Fourier Transform (DFT) and the local Short-Time Fourier
Transform (STFT), we find that human-written text consistently exhibits
significantly higher spectral energy. This higher energy reflects the
larger-amplitude fluctuations inherent in human writing compared to the
suppressed dynamics of LLM-generated text. Based on this key insight, we
construct SpecDetect, a detector built on a single, robust feature from the
global DFT: DFT total energy. We also propose an enhanced version,
SpecDetect++, which incorporates a sampling discrepancy mechanism to further
boost robustness. Extensive experiments demonstrate that our approach
outperforms the state-of-the-art model while running in nearly half the time.
Our work introduces a new, efficient, and interpretable pathway for
LLM-generated text detection, showing that classical signal processing
techniques offer a surprisingly powerful solution to this modern challenge.

</details>


### [122] [Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning](https://arxiv.org/abs/2508.11364)
*Sylvio Rüdian,Yassin Elsir,Marvin Kretschmer,Sabine Cayrou,Niels Pinkwart*

Main category: cs.CL

TL;DR: 论文探讨了利用大语言模型Llama 3.1从学生提交的语言学习作业中提取反馈指标，并与人工评分对比，结果显示强相关性。


<details>
  <summary>Details</summary>
Motivation: 自动化反馈生成可提升学生学习效率并优化教师时间分配，但需先提取高质量指标作为基础。

Method: 使用Llama 3.1从学生作业中提取反馈指标，并与人工评分进行对比分析。

Result: 发现LLM生成的指标与人工评分在多种反馈标准上具有显著强相关性。

Conclusion: 该方法为利用LLMs提取指标并生成透明反馈提供了基础，未来研究可进一步应用。

Abstract: Automated feedback generation has the potential to enhance students' learning
progress by providing timely and targeted feedback. Moreover, it can assist
teachers in optimizing their time, allowing them to focus on more strategic and
personalized aspects of teaching. To generate high-quality, information-rich
formative feedback, it is essential first to extract relevant indicators, as
these serve as the foundation upon which the feedback is constructed. Teachers
often employ feedback criteria grids composed of various indicators that they
evaluate systematically. This study examines the initial phase of extracting
such indicators from students' submissions of a language learning course using
the large language model Llama 3.1. Accordingly, the alignment between
indicators generated by the LLM and human ratings across various feedback
criteria is investigated. The findings demonstrate statistically significant
strong correlations, even in cases involving unanticipated combinations of
indicators and criteria. The methodology employed in this paper offers a
promising foundation for extracting indicators from students' submissions using
LLMs. Such indicators can potentially be utilized to auto-generate explainable
and transparent formative feedback in future research.

</details>


### [123] [When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs](https://arxiv.org/abs/2508.11383)
*Mikhail Seleznyov,Mikhail Chaichuk,Gleb Ershov,Alexander Panchenko,Elena Tutubalina,Oleg Somov*

Main category: cs.CL

TL;DR: 本文系统评估了5种提升提示鲁棒性的方法，并在统一框架下测试了8个模型在52个任务上的表现，涵盖微调和上下文学习范式，并扩展到前沿模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）对提示的细微变化非常敏感，因此需要系统评估提升鲁棒性的方法。

Method: 在统一实验框架下，评估了5种鲁棒性方法，测试了8个模型在52个任务上的表现，涵盖微调和上下文学习范式，并扩展到GPT-4.1和DeepSeek V3。

Result: 研究提供了关于不同鲁棒性方法相对有效性的实用见解，帮助实践者在实际应用中实现稳定的LLM性能。

Conclusion: 本文为提升LLM提示鲁棒性提供了系统评估和实用建议，适用于多种模型和任务。

Abstract: Large Language Models (LLMs) are highly sensitive to subtle, non-semantic
variations in prompt phrasing and formatting. In this work, we present the
first systematic evaluation of 5 methods for improving prompt robustness within
a unified experimental framework. We benchmark these techniques on 8 models
from Llama, Qwen and Gemma families across 52 tasks from Natural Instructions
dataset. Our evaluation covers robustness methods from both fine-tuned and
in-context learning paradigms, and tests their generalization against multiple
types of distribution shifts. Finally, we extend our analysis to GPT-4.1 and
DeepSeek V3 to assess frontier models' current robustness to format
perturbations. Our findings offer actionable insights into the relative
effectiveness of these robustness methods, enabling practitioners to make
informed decisions when aiming for stable and reliable LLM performance in
real-world applications. Code:
https://github.com/AIRI-Institute/when-punctuation-matters.

</details>


### [124] [Retrieval-augmented reasoning with lean language models](https://arxiv.org/abs/2508.11386)
*Ryan Sze-Yin Chan,Federico Nanni,Tomas Lazauskas,Rosie Wood,Penelope Yong,Lionel Tarassenko,Mark Girolami,James Geddes,Andrew Duncan*

Main category: cs.CL

TL;DR: 提出了一种结合推理与检索增强生成（RAG）的轻量级语言模型架构，适用于资源受限或安全敏感环境。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统依赖大规模模型和外部API，无法满足高效、隐私保护的需求。

Method: 采用密集检索器与微调的Qwen2.5-Instruct模型，结合合成查询生成和推理轨迹，优化文档压缩与数据设计。

Result: 在特定领域微调显著提升了答案准确性和一致性，接近前沿模型性能。

Conclusion: 该方法在本地部署可行，代码公开以支持跨领域复现与适配。

Abstract: This technical report details a novel approach to combining reasoning and
retrieval augmented generation (RAG) within a single, lean language model
architecture. While existing RAG systems typically rely on large-scale models
and external APIs, our work addresses the increasing demand for performant and
privacy-preserving solutions deployable in resource-constrained or secure
environments. Building on recent developments in test-time scaling and
small-scale reasoning models, we develop a retrieval augmented conversational
agent capable of interpreting complex, domain-specific queries using a
lightweight backbone model. Our system integrates a dense retriever with
fine-tuned Qwen2.5-Instruct models, using synthetic query generation and
reasoning traces derived from frontier models (e.g., DeepSeek-R1) over a
curated corpus, in this case, the NHS A-to-Z condition pages. We explore the
impact of summarisation-based document compression, synthetic data design, and
reasoning-aware fine-tuning on model performance. Evaluation against both
non-reasoning and general-purpose lean models demonstrates that our
domain-specific fine-tuning approach yields substantial gains in answer
accuracy and consistency, approaching frontier-level performance while
remaining feasible for local deployment. All implementation details and code
are publicly released to support reproducibility and adaptation across domains.

</details>


### [125] [Model Interpretability and Rationale Extraction by Input Mask Optimization](https://arxiv.org/abs/2508.11388)
*Marc Brinner,Sina Zarriess*

Main category: cs.CL

TL;DR: 提出一种基于梯度优化和正则化的新方法，为神经网络预测生成提取式解释，适用于文本和图像输入。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络模型在自然语言处理和计算机视觉领域的快速发展，对其预测提供解释的需求日益增长。

Method: 通过梯度优化和新的正则化方案，掩蔽输入中模型认为不重要的部分，生成满足充分性、全面性和紧凑性的解释。

Result: 该方法无需训练专门模型，仅基于分类器即可生成高质量解释，并成功应用于图像分类。

Conclusion: 证明了自然语言处理中的解释条件可泛化至其他输入类型，弥合了模型可解释性与解释提取之间的差距。

Abstract: Concurrent to the rapid progress in the development of neural-network based
models in areas like natural language processing and computer vision, the need
for creating explanations for the predictions of these black-box models has
risen steadily. We propose a new method to generate extractive explanations for
predictions made by neural networks, that is based on masking parts of the
input which the model does not consider to be indicative of the respective
class. The masking is done using gradient-based optimization combined with a
new regularization scheme that enforces sufficiency, comprehensiveness and
compactness of the generated explanation, three properties that are known to be
desirable from the related field of rationale extraction in natural language
processing. In this way, we bridge the gap between model interpretability and
rationale extraction, thereby proving that the latter of which can be performed
without training a specialized model, only on the basis of a trained
classifier. We further apply the same method to image inputs and obtain high
quality explanations for image classifications, which indicates that the
conditions proposed for rationale extraction in natural language processing are
more broadly applicable to different input types.

</details>


### [126] [Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training](https://arxiv.org/abs/2508.11393)
*Marc Brinner,Sina Zarrieß*

Main category: cs.CL

TL;DR: 提出一种端到端的可微分训练范式，用于稳定训练理性化Transformer分类器，简化现有方法并提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有理性化模型训练中的不稳定性和复杂性，同时提升与人类标注的对齐效果。

Method: 通过单一模型同时实现分类、输入标记评分和补充分类，简化三玩家游戏范式。

Result: 显著提升与人类标注的对齐效果，达到最先进水平。

Conclusion: 该方法简化训练流程，提高稳定性，并在无显式监督下实现更好的理性化效果。

Abstract: We propose an end-to-end differentiable training paradigm for stable training
of a rationalized transformer classifier. Our approach results in a single
model that simultaneously classifies a sample and scores input tokens based on
their relevance to the classification. To this end, we build on the widely-used
three-player-game for training rationalized models, which typically relies on
training a rationale selector, a classifier and a complement classifier. We
simplify this approach by making a single model fulfill all three roles,
leading to a more efficient training paradigm that is not susceptible to the
common training instabilities that plague existing approaches. Further, we
extend this paradigm to produce class-wise rationales while incorporating
recent advances in parameterizing and regularizing the resulting rationales,
thus leading to substantially improved and state-of-the-art alignment with
human annotations without any explicit supervision.

</details>


### [127] [Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions](https://arxiv.org/abs/2508.11414)
*Shangrui Nie,Florian Mai,David Kaczér,Charles Welch,Zhixue Zhao,Lucie Flek*

Main category: cs.CL

TL;DR: 通过微调语言模型回答价值观调查问题，可以显著改变其在下游任务中的行为表现。


<details>
  <summary>Details</summary>
Motivation: 研究是否可以通过简单的微调方法调整语言模型的价值观系统，而无需大量训练数据。

Method: 构建价值观调查问卷作为基线，通过微调模型回答问卷问题，评估其在域内和域外任务中的行为变化。

Result: 微调不仅能改变模型对问卷的回答，还能显著影响其在情境化任务中的行为表现。

Conclusion: 简单的微调方法可以有效调整语言模型的价值观系统，实现价值对齐。

Abstract: Large language models implicitly encode preferences over human values, yet
steering them often requires large training data. In this work, we investigate
a simple approach: Can we reliably modify a model's value system in downstream
behavior by training it to answer value survey questions accordingly? We first
construct value profiles of several open-source LLMs by asking them to rate a
series of value-related descriptions spanning 20 distinct human values, which
we use as a baseline for subsequent experiments. We then investigate whether
the value system of a model can be governed by fine-tuning on the value
surveys. We evaluate the effect of finetuning on the model's behavior in two
ways; first, we assess how answers change on in-domain, held-out survey
questions. Second, we evaluate whether the model's behavior changes in
out-of-domain settings (situational scenarios). To this end, we construct a
contextualized moral judgment dataset based on Reddit posts and evaluate
changes in the model's behavior in text-based adventure games. We demonstrate
that our simple approach can not only change the model's answers to in-domain
survey questions, but also produces substantial shifts (value alignment) in
implicit downstream task behavior.

</details>


### [128] [HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor](https://arxiv.org/abs/2508.11429)
*Shivam Dubey*

Main category: cs.CL

TL;DR: HumorPlanSearch通过模块化流程（包括策略规划、文化推理、知识图谱等）提升LLM生成幽默的上下文敏感性和喜剧质量，实验显示HGS提升15.4%。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成幽默时因缺乏上下文和文化敏感性导致的通用、重复或不合时宜的问题。

Method: 提出HumorPlanSearch，包含策略搜索、文化推理模板、知识图谱、新颖性过滤和迭代修订。

Result: 实验表明，完整流程（知识图谱+修订）将HGS均值提升15.4%（p < 0.05）。

Conclusion: HumorPlanSearch通过强调上下文，推动AI幽默生成向更具连贯性、适应性和文化敏感性的方向发展。

Abstract: Automated humor generation with Large Language Models (LLMs) often yields
jokes that feel generic, repetitive, or tone-deaf because humor is deeply
situated and hinges on the listener's cultural background, mindset, and
immediate context. We introduce HumorPlanSearch, a modular pipeline that
explicitly models context through: (1) Plan-Search for diverse, topic-tailored
strategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and
stylistic reasoning; (3) a Knowledge Graph to retrieve and adapt
high-performing historical strategies; (4) novelty filtering via semantic
embeddings; and (5) an iterative judge-driven revision loop. To evaluate
context sensitivity and comedic quality, we propose the Humor Generation Score
(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,
and topic relevance. In experiments across nine topics with feedback from 13
human judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent
(p < 0.05) over a strong baseline. By foregrounding context at every stage from
strategy planning to multi-signal evaluation, HumorPlanSearch advances
AI-driven humor toward more coherent, adaptive, and culturally attuned comedy.

</details>


### [129] [Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse](https://arxiv.org/abs/2508.11434)
*Aditi Dutta,Susan Banducci*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型（LLMs）在分类反性别歧视言论时容易将其误判为有害内容，尤其在政治敏感事件中。建议改进内容审核设计，避免二元分类，并纳入人类审核和反言论训练数据。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在区分性别歧视言论与反性别歧视言论时的表现，揭示自动化内容审核系统可能对反性别歧视言论的误判问题。

Method: 分析五个LLMs对英国2022年涉及女性议员的推文（性别歧视、反性别歧视和中性）的分类表现，重点关注高显著性触发事件。

Result: 模型常将反性别歧视言论误判为有害内容，尤其在政治敏感事件中，导致边缘化声音被压制。

Conclusion: 内容审核需超越二元分类，纳入人类审核和反言论训练数据，以保护数字政治空间中的抵抗性言论。

Abstract: Anti-sexist speech, i.e., public expressions that challenge or resist
gendered abuse and sexism, plays a vital role in shaping democratic debate
online. Yet automated content moderation systems, increasingly powered by large
language models (LLMs), may struggle to distinguish such resistance from the
sexism it opposes. This study examines how five LLMs classify sexist,
anti-sexist, and neutral political tweets from the UK, focusing on
high-salience trigger events involving female Members of Parliament in the year
2022. Our analysis show that models frequently misclassify anti-sexist speech
as harmful, particularly during politically charged events where rhetorical
styles of harm and resistance converge. These errors risk silencing those who
challenge sexism, with disproportionate consequences for marginalised voices.
We argue that moderation design must move beyond binary harmful/not-harmful
schemas, integrate human-in-the-loop review during sensitive events, and
explicitly include counter-speech in training data. By linking feminist
scholarship, event-based analysis, and model evaluation, this work highlights
the sociotechnical challenges of safeguarding resistance speech in digital
political spaces.

</details>


### [130] [CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity](https://arxiv.org/abs/2508.11442)
*Bowen Zhang,Zixin Song,Chunquan Chen,Qian-Wen Zhang,Di Yin,Xing Sun*

Main category: cs.CL

TL;DR: CoDiEmb框架通过任务专用目标、动态采样和模型融合策略，解决了信息检索（IR）和语义文本相似性（STS）任务联合训练中的负迁移问题。


<details>
  <summary>Details</summary>
Motivation: 联合训练IR和STS任务时，负迁移问题导致性能下降，需要一种系统化解耦任务信号的方法。

Method: CoDiEmb采用任务专用目标、动态采样、delta引导的模型融合策略，以及高效的单阶段训练流程。

Result: 在15个基准测试中，CoDiEmb不仅缓解了跨任务性能权衡，还提升了嵌入空间的几何特性。

Conclusion: CoDiEmb为多任务文本嵌入学习提供了一种有效且稳定的解决方案。

Abstract: Learning unified text embeddings that excel across diverse downstream tasks
is a central goal in representation learning, yet negative transfer remains a
persistent obstacle. This challenge is particularly pronounced when jointly
training a single encoder for Information Retrieval (IR) and Semantic Textual
Similarity (STS), two essential but fundamentally disparate tasks for which
naive co-training typically yields steep performance trade-offs. We argue that
resolving this conflict requires systematically decoupling task-specific
learning signals throughout the training pipeline. To this end, we introduce
CoDiEmb, a unified framework that reconciles the divergent requirements of IR
and STS in a collaborative yet distinct manner. CoDiEmb integrates three key
innovations for effective joint optimization: (1) Task-specialized objectives
paired with a dynamic sampler that forms single-task batches and balances
per-task updates, thereby preventing gradient interference. For IR, we employ a
contrastive loss with multiple positives and hard negatives, augmented by
cross-device sampling. For STS, we adopt order-aware objectives that directly
optimize correlation and ranking consistency. (2) A delta-guided model fusion
strategy that computes fine-grained merging weights for checkpoints by
analyzing each parameter's deviation from its pre-trained initialization,
proving more effective than traditional Model Soups. (3) An efficient,
single-stage training pipeline that is simple to implement and converges
stably. Extensive experiments on 15 standard IR and STS benchmarks across three
base encoders validate CoDiEmb. Our results and analysis demonstrate that the
framework not only mitigates cross-task trade-offs but also measurably improves
the geometric properties of the embedding space.

</details>


### [131] [Reference Points in LLM Sentiment Analysis: The Role of Structured Context](https://arxiv.org/abs/2508.11454)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 研究表明，通过结构化提示（如JSON格式）补充额外信息，小型语言模型（3B参数）在情感分析任务中表现优于传统方法，无需微调即可提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析主要依赖评论文本，但营销理论指出客户评价还受其他参考点影响。本研究探讨补充信息的内容和格式如何影响LLMs的情感分析效果。

Method: 比较自然语言（NL）和JSON格式提示，使用轻量级3B参数模型，并在Yelp的餐厅和夜生活类别上进行实验。

Result: JSON提示显著提升性能：Macro-F1分别提高1.6%和4%，RMSE降低16%和9.1%，适合资源受限的边缘设备。

Conclusion: 结构化提示使小型模型具备竞争力，为大规模模型部署提供了实用替代方案。

Abstract: Large language models (LLMs) are now widely used across many fields,
including marketing research. Sentiment analysis, in particular, helps firms
understand consumer preferences. While most NLP studies classify sentiment from
review text alone, marketing theories, such as prospect theory and
expectation--disconfirmation theory, point out that customer evaluations are
shaped not only by the actual experience but also by additional reference
points. This study therefore investigates how the content and format of such
supplementary information affect sentiment analysis using LLMs. We compare
natural language (NL) and JSON-formatted prompts using a lightweight 3B
parameter model suitable for practical marketing applications. Experiments on
two Yelp categories (Restaurant and Nightlife) show that the JSON prompt with
additional information outperforms all baselines without fine-tuning: Macro-F1
rises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it
deployable in resource-constrained edge devices. Furthermore, a follow-up
analysis confirms that performance gains stem from genuine contextual reasoning
rather than label proxying. This work demonstrates that structured prompting
can enable smaller models to achieve competitive performance, offering a
practical alternative to large-scale model deployment.

</details>


### [132] [Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models](https://arxiv.org/abs/2508.11534)
*Monika Jotautaitė,Lucius Caviola,David A. Brewster,Thilo Hagendorff*

Main category: cs.CL

TL;DR: 研究探讨大型语言模型（LLMs）是否表现出物种主义偏见，发现其能识别但很少谴责物种主义观点，且在权衡中更倾向人类。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs广泛应用，需评估其伦理倾向，尤其是对非人类动物的价值观。

Method: 通过三个范式研究：SpeciesismBench基准测试、心理测量比较、文本生成任务。

Result: LLMs能识别物种主义但较少谴责，在权衡中更倾向人类；若动物能力更强，则倾向动物。

Conclusion: 需扩展AI公平性框架以涵盖非人类道德主体，减少物种主义偏见。

Abstract: As large language models (LLMs) become more widely deployed, it is crucial to
examine their ethical tendencies. Building on research on fairness and
discrimination in AI, we investigate whether LLMs exhibit speciesist bias --
discrimination based on species membership -- and how they value non-human
animals. We systematically examine this issue across three paradigms: (1)
SpeciesismBench, a 1,003-item benchmark assessing recognition and moral
evaluation of speciesist statements; (2) established psychological measures
comparing model responses with those of human participants; (3) text-generation
tasks probing elaboration on, or resistance to, speciesist rationalizations. In
our benchmark, LLMs reliably detected speciesist statements but rarely
condemned them, often treating speciesist attitudes as morally acceptable. On
psychological measures, results were mixed: LLMs expressed slightly lower
explicit speciesism than people, yet in direct trade-offs they more often chose
to save one human over multiple animals. A tentative interpretation is that
LLMs may weight cognitive capacity rather than species per se: when capacities
were equal, they showed no species preference, and when an animal was described
as more capable, they tended to prioritize it over a less capable human. In
open-ended text generation tasks, LLMs frequently normalized or rationalized
harm toward farmed animals while refusing to do so for non-farmed animals.
These findings suggest that while LLMs reflect a mixture of progressive and
mainstream human views, they nonetheless reproduce entrenched cultural norms
around animal exploitation. We argue that expanding AI fairness and alignment
frameworks to explicitly include non-human moral patients is essential for
reducing these biases and preventing the entrenchment of speciesist attitudes
in AI systems and the societies they influence.

</details>


### [133] [Language models align with brain regions that represent concepts across modalities](https://arxiv.org/abs/2508.11536)
*Maria Ryskina,Greta Tuckute,Alexander Fung,Ashley Malkin,Evelina Fedorenko*

Main category: cs.CL

TL;DR: 研究探讨了语言模型（LMs）与大脑对齐的关系，发现语言模型可能内部表征跨模态的概念意义。


<details>
  <summary>Details</summary>
Motivation: 认知科学和神经科学长期面临区分语言表征与概念意义表征的挑战，这一问题也存在于现代语言模型中。

Method: 通过两个神经指标研究LM与大脑对齐：句子处理时的大脑激活水平（针对语言处理）和跨输入模态的意义一致性新度量。

Result: 实验表明，语言模型在意义一致性更强的脑区预测信号更好，即使这些区域对语言处理不敏感。

Conclusion: 语言模型可能内部表征跨模态的概念意义。

Abstract: Cognitive science and neuroscience have long faced the challenge of
disentangling representations of language from representations of conceptual
meaning. As the same problem arises in today's language models (LMs), we
investigate the relationship between LM--brain alignment and two neural
metrics: (1) the level of brain activation during processing of sentences,
targeting linguistic processing, and (2) a novel measure of meaning consistency
across input modalities, which quantifies how consistently a brain region
responds to the same concept across paradigms (sentence, word cloud, image)
using an fMRI dataset (Pereira et al., 2018). Our experiments show that both
language-only and language-vision models predict the signal better in more
meaning-consistent areas of the brain, even when these areas are not strongly
sensitive to language processing, suggesting that LMs might internally
represent cross-modal conceptual meaning.

</details>


### [134] [AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment](https://arxiv.org/abs/2508.11567)
*Jinpeng Hu,Ao Wang,Qianqian Xie,Hui Ma,Zhuo Li,Dan Guo*

Main category: cs.CL

TL;DR: 提出了一种基于多代理框架的心理健康评估方法，通过模拟医患对话和动态提问机制提升评估效果。


<details>
  <summary>Details</summary>
Motivation: 传统心理健康评估依赖专业医生，资源有限；现有AI方法多基于静态文本分析，无法捕捉动态交互中的深层信息。

Method: 采用多代理框架，模拟医患对话，引入动态提问机制和树状记忆结构，优化信息提取和上下文跟踪。

Result: 在DAIC-WOZ数据集上表现优于现有方法。

Conclusion: 该方法通过动态交互和自适应提问，显著提升了心理健康评估的准确性和效率。

Abstract: Mental health assessment is crucial for early intervention and effective
treatment, yet traditional clinician-based approaches are limited by the
shortage of qualified professionals. Recent advances in artificial intelligence
have sparked growing interest in automated psychological assessment, yet most
existing approaches are constrained by their reliance on static text analysis,
limiting their ability to capture deeper and more informative insights that
emerge through dynamic interaction and iterative questioning. Therefore, in
this paper, we propose a multi-agent framework for mental health evaluation
that simulates clinical doctor-patient dialogues, with specialized agents
assigned to questioning, adequacy evaluation, scoring, and updating. We
introduce an adaptive questioning mechanism in which an evaluation agent
assesses the adequacy of user responses to determine the necessity of
generating targeted follow-up queries to address ambiguity and missing
information. Additionally, we employ a tree-structured memory in which the root
node encodes the user's basic information, while child nodes (e.g., topic and
statement) organize key information according to distinct symptom categories
and interaction turns. This memory is dynamically updated throughout the
interaction to reduce redundant questioning and further enhance the information
extraction and contextual tracking capabilities. Experimental results on the
DAIC-WOZ dataset illustrate the effectiveness of our proposed method, which
achieves better performance than existing approaches.

</details>


### [135] [Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models](https://arxiv.org/abs/2508.11582)
*Qiguang Chen,Dengyun Peng,Jinhao Liu,HuiKang Su,Jiannan Guan,Libo Qin,Wanxiang Che*

Main category: cs.CL

TL;DR: 论文提出DR. SAF框架，通过动态调整推理深度提升LLM效率，减少冗余，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有长链思维方法导致冗余和效率低下，且人工定义的难度先验与模型自感知难度不匹配。

Method: DR. SAF框架包含边界自感知对齐、自适应奖励管理和边界保护机制，动态优化推理过程。

Result: 实验显示，DR. SAF减少49.27%的响应token，效率提升6.59倍，训练时间减少5倍，极端训练下准确性提升16%。

Conclusion: DR. SAF显著提升LLM的推理效率和准确性，适用于资源受限场景。

Abstract: Recent advancements in large language models (LLMs) have greatly improved
their capabilities on complex reasoning tasks through Long Chain-of-Thought
(CoT). However, this approach often results in substantial redundancy,
impairing computational efficiency and causing significant delays in real-time
applications. To improve the efficiency, current methods often rely on
human-defined difficulty priors, which do not align with the LLM's self-awared
difficulty, leading to inefficiencies. In this paper, we introduce the Dynamic
Reasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to
dynamically assess and adjust their reasoning depth in response to problem
complexity. DR. SAF integrates three key components: Boundary Self-Awareness
Alignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.
These components allow models to optimize their reasoning processes, balancing
efficiency and accuracy without compromising performance. Our experimental
results demonstrate that DR. SAF achieves a 49.27% reduction in total response
tokens with minimal loss in accuracy. The framework also delivers a 6.59x gain
in token efficiency and a 5x reduction in training time, making it well-suited
to resource-limited settings. During extreme training, DR. SAF can even surpass
traditional instruction-based models in token efficiency with more than 16%
accuracy improvement.

</details>


### [136] [Representing Speech Through Autoregressive Prediction of Cochlear Tokens](https://arxiv.org/abs/2508.11598)
*Greta Tuckute,Klemen Kotar,Evelina Fedorenko,Daniel L. K. Yamins*

Main category: cs.CL

TL;DR: AuriStream是一个受生物启发的两阶段语音编码模型，模拟人类听觉处理层次结构，在SUPERB语音任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 开发更接近人类听觉处理的语音模型，以高效处理多种语音任务。

Method: 第一阶段将原始音频转换为基于人类耳蜗的时频表示，提取离散的耳蜗标记；第二阶段对耳蜗标记应用自回归序列模型。

Result: AuriStream学习到有意义的音素和单词表示，并在SUPERB任务中表现出色，还能生成音频延续。

Conclusion: AuriStream为语音表示学习提供了一个两阶段框架，推动了更接近人类听觉的模型发展。

Abstract: We introduce AuriStream, a biologically inspired model for encoding speech
via a two-stage framework inspired by the human auditory processing hierarchy.
The first stage transforms raw audio into a time-frequency representation based
on the human cochlea, from which we extract discrete \textbf{cochlear tokens}.
The second stage applies an autoregressive sequence model over the cochlear
tokens. AuriStream learns meaningful phoneme and word representations, and
state-of-the-art lexical semantics. AuriStream shows competitive performance on
diverse downstream SUPERB speech tasks. Complementing AuriStream's strong
representational capabilities, it generates continuations of audio which can be
visualized in a spectrogram space and decoded back into audio, providing
insights into the model's predictions. In summary, we present a two-stage
framework for speech representation learning to advance the development of more
human-like models that efficiently handle a range of speech-based tasks.

</details>


### [137] [Dataset Creation for Visual Entailment using Generative AI](https://arxiv.org/abs/2508.11605)
*Rob Reijtenbach,Suzan Verberne,Gijs Wijnholds*

Main category: cs.CL

TL;DR: 提出并验证了一种基于SNLI数据集生成的视觉蕴含合成数据集，通过Stable Diffusion生成图像，实验表明合成数据在训练视觉蕴含模型时效果接近真实数据。


<details>
  <summary>Details</summary>
Motivation: 现有视觉蕴含数据集规模小且稀疏，手动创建耗时费力，需探索合成数据的可行性。

Method: 利用SNLI文本前提作为输入，通过Stable Diffusion生成图像，构建合成数据集，并通过CLIP特征向量训练分类器进行内外评估。

Result: 合成数据训练的分类器在SNLI-VE和SICK-VTE上F-score略低于真实数据（0.686 vs 0.703；0.384 vs 0.400），但差距较小。

Conclusion: 在数据稀疏场景下，合成数据是训练视觉蕴含模型的有效替代方案。

Abstract: In this paper we present and validate a new synthetic dataset for training
visual entailment models. Existing datasets for visual entailment are small and
sparse compared to datasets for textual entailment. Manually creating datasets
is labor-intensive. We base our synthetic dataset on the SNLI dataset for
textual entailment. We take the premise text from SNLI as input prompts in a
generative image model, Stable Diffusion, creating an image to replace each
textual premise. We evaluate our dataset both intrinsically and extrinsically.
For extrinsic evaluation, we evaluate the validity of the generated images by
using them as training data for a visual entailment classifier based on CLIP
feature vectors. We find that synthetic training data only leads to a slight
drop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when
trained on real data. We also compare the quality of our generated training
data to original training data on another dataset: SICK-VTE. Again, there is
only a slight drop in F-score: from 0.400 to 0.384. These results indicate that
in settings with data sparsity, synthetic data can be a promising solution for
training visual entailment models.

</details>


### [138] [TinyTim: A Family of Language Models for Divergent Generation](https://arxiv.org/abs/2508.11607)
*Christopher J. Agostino*

Main category: cs.CL

TL;DR: TinyTim是一个基于James Joyce《Finnegans Wake》微调的大型语言模型家族，其生成内容具有高词汇多样性和低语义连贯性。


<details>
  <summary>Details</summary>
Motivation: 探索专门化语言模型在创造性架构中作为知识源的可能性，以支持自动化发现机制。

Method: 通过定量评估对比基线模型，分析TinyTim V1的生成特性。

Result: TinyTim V1展现出高词汇多样性和低语义连贯性的独特生成特征。

Conclusion: 此类专门化模型可作为创造性架构中的发散知识源，推动自动化发现。

Abstract: This work introduces TinyTim, a family of large language models fine-tuned on
James Joyce's `Finnegans Wake'. Through quantitative evaluation against
baseline models, we demonstrate that TinyTim V1 produces a statistically
distinct generative profile characterized by high lexical diversity and low
semantic coherence. These findings are interpreted through theories of
creativity and complex problem-solving, arguing that such specialized models
can function as divergent knowledge sources within more extensive creative
architectures, powering automated discovery mechanisms in diverse settings.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [139] [Developing and Validating a High-Throughput Robotic System for the Accelerated Development of Porous Membranes](https://arxiv.org/abs/2508.10973)
*Hongchen Wang,Sima Zeinali Danalou,Jiahao Zhu,Kenneth Sulimro,Chaewon Lim,Smita Basak,Aimee Tai,Usan Siriwardana,Jason Hattrick-Simpers,Jay Werber*

Main category: cs.RO

TL;DR: 开发了一种全自动平台，用于通过非溶剂诱导相分离（NIPS）制备和表征多孔聚合物膜，提高了实验效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 传统多孔聚合物膜的开发过程耗时且依赖试错，需要更高效、可重复的方法。

Method: 集成自动化溶液制备、刮涂、控制浸没和压缩测试，精确控制参数如聚合物浓度和环境湿度。

Result: 系统成功验证了聚合物浓度和湿度对膜性能的影响，支持高通量实验。

Conclusion: 该自动化平台为数据驱动的多孔膜优化提供了可扩展、可重复的基础。

Abstract: The development of porous polymeric membranes remains a labor-intensive
process, often requiring extensive trial and error to identify optimal
fabrication parameters. In this study, we present a fully automated platform
for membrane fabrication and characterization via nonsolvent-induced phase
separation (NIPS). The system integrates automated solution preparation, blade
casting, controlled immersion, and compression testing, allowing precise
control over fabrication parameters such as polymer concentration and ambient
humidity. The modular design allows parallel processing and reproducible
handling of samples, reducing experimental time and increasing consistency.
Compression testing is introduced as a sensitive mechanical characterization
method for estimating membrane stiffness and as a proxy to infer porosity and
intra-sample uniformity through automated analysis of stress-strain curves. As
a proof of concept to demonstrate the effectiveness of the system, NIPS was
carried out with polysulfone, the green solvent PolarClean, and water as the
polymer, solvent, and nonsolvent, respectively. Experiments conducted with the
automated system reproduced expected effects of polymer concentration and
ambient humidity on membrane properties, namely increased stiffness and
uniformity with increasing polymer concentration and humidity variations in
pore morphology and mechanical response. The developed automated platform
supports high-throughput experimentation and is well-suited for integration
into self-driving laboratory workflows, offering a scalable and reproducible
foundation for data-driven optimization of porous polymeric membranes through
NIPS.

</details>


### [140] [Robust Online Calibration for UWB-Aided Visual-Inertial Navigation with Bias Correction](https://arxiv.org/abs/2508.10999)
*Yizhi Zhou,Jie Xu,Jiawei Xia,Zechen Hu,Weizi Li,Xuan Wang*

Main category: cs.RO

TL;DR: 提出了一种新型的UWB锚点在线校准框架，解决了现有方法对机器人定位误差和初始猜测敏感的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有UWB校准方法依赖准确的机器人定位和初始猜测，实际应用中鲁棒性不足。

Method: 通过考虑机器人定位不确定性，提出基于SKF的紧密耦合在线优化方法。

Result: 仿真和实验验证了方法的准确性和鲁棒性。

Conclusion: 该方法显著提升了UWB校准的实用性和鲁棒性。

Abstract: This paper presents a novel robust online calibration framework for
Ultra-Wideband (UWB) anchors in UWB-aided Visual-Inertial Navigation Systems
(VINS). Accurate anchor positioning, a process known as calibration, is crucial
for integrating UWB ranging measurements into state estimation. While several
prior works have demonstrated satisfactory results by using robot-aided systems
to autonomously calibrate UWB systems, there are still some limitations: 1)
these approaches assume accurate robot localization during the initialization
step, ignoring localization errors that can compromise calibration robustness,
and 2) the calibration results are highly sensitive to the initial guess of the
UWB anchors' positions, reducing the practical applicability of these methods
in real-world scenarios. Our approach addresses these challenges by explicitly
incorporating the impact of robot localization uncertainties into the
calibration process, ensuring robust initialization. To further enhance the
robustness of the calibration results against initialization errors, we propose
a tightly-coupled Schmidt Kalman Filter (SKF)-based online refinement method,
making the system suitable for practical applications. Simulations and
real-world experiments validate the improved accuracy and robustness of our
approach.

</details>


### [141] [3D FlowMatch Actor: Unified 3D Policy for Single- and Dual-Arm Manipulation](https://arxiv.org/abs/2508.11002)
*Nikolaos Gkanatsios,Jiahe Xu,Matthew Bronars,Arsalan Mousavian,Tsung-Wei Ke,Katerina Fragkiadaki*

Main category: cs.RO

TL;DR: 3D FlowMatch Actor (3DFA) 是一种结合流匹配和3D预训练视觉场景表示的机器人操作策略架构，显著提升了训练和推理速度，并在多项任务中达到最新性能。


<details>
  <summary>Details</summary>
Motivation: 通过结合流匹配和3D视觉表示，提升机器人操作策略的学习效率和性能，解决现有3D扩散策略速度慢的问题。

Method: 利用3D相对注意力机制在动作去噪过程中结合动作和视觉标记，并通过流匹配和系统级优化实现高效训练和推理。

Result: 在PerAct2基准测试中，性能提升41.4%；在真实世界评估中超越基线；在单臂任务中直接预测轨迹，无需运动规划。

Conclusion: 3DFA通过设计优化显著提升了策略的效率和性能，为机器人操作提供了高效解决方案。

Abstract: We present 3D FlowMatch Actor (3DFA), a 3D policy architecture for robot
manipulation that combines flow matching for trajectory prediction with 3D
pretrained visual scene representations for learning from demonstration. 3DFA
leverages 3D relative attention between action and visual tokens during action
denoising, building on prior work in 3D diffusion-based single-arm policy
learning. Through a combination of flow matching and targeted system-level and
architectural optimizations, 3DFA achieves over 30x faster training and
inference than previous 3D diffusion-based policies, without sacrificing
performance. On the bimanual PerAct2 benchmark, it establishes a new state of
the art, outperforming the next-best method by an absolute margin of 41.4%. In
extensive real-world evaluations, it surpasses strong baselines with up to
1000x more parameters and significantly more pretraining. In unimanual
settings, it sets a new state of the art on 74 RLBench tasks by directly
predicting dense end-effector trajectories, eliminating the need for motion
planning. Comprehensive ablation studies underscore the importance of our
design choices for both policy effectiveness and efficiency.

</details>


### [142] [GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning](https://arxiv.org/abs/2508.11049)
*Kelin Yu,Sheng Zhang,Harshit Soora,Furong Huang,Heng Huang,Pratap Tokekar,Ruohan Gao*

Main category: cs.RO

TL;DR: GenFlowRL通过从多样化跨体现数据集中提取生成的流来设计奖励，从而学习通用且鲁棒的策略。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型依赖生成数据质量且缺乏环境反馈，难以处理精细操作，且视频强化学习受限于视频生成的不确定性和大规模数据集收集的挑战。

Method: GenFlowRL从多样化跨体现数据集中提取生成的流，设计奖励函数，利用低维、以对象为中心的特征学习策略。

Result: 在10个操作任务的仿真和现实跨体现评估中，GenFlowRL表现优异，能够有效利用生成的以对象为中心的流特征。

Conclusion: GenFlowRL通过利用生成的流特征，在多样化场景中实现了高性能，解决了现有方法的局限性。

Abstract: Recent advances have shown that video generation models can enhance robot
learning by deriving effective robot actions through inverse dynamics. However,
these methods heavily depend on the quality of generated data and struggle with
fine-grained manipulation due to the lack of environment feedback. While
video-based reinforcement learning improves policy robustness, it remains
constrained by the uncertainty of video generation and the challenges of
collecting large-scale robot datasets for training diffusion models. To address
these limitations, we propose GenFlowRL, which derives shaped rewards from
generated flow trained from diverse cross-embodiment datasets. This enables
learning generalizable and robust policies from diverse demonstrations using
low-dimensional, object-centric features. Experiments on 10 manipulation tasks,
both in simulation and real-world cross-embodiment evaluations, demonstrate
that GenFlowRL effectively leverages manipulation features extracted from
generated object-centric flow, consistently achieving superior performance
across diverse and challenging scenarios. Our Project Page:
https://colinyu1.github.io/genflowrl

</details>


### [143] [Utilizing Vision-Language Models as Action Models for Intent Recognition and Assistance](https://arxiv.org/abs/2508.11093)
*Cesar Alan Contreras,Manolis Chiou,Alireza Rastegarpanah,Michal Szulik,Rustam Stolkin*

Main category: cs.RO

TL;DR: GUIDER框架通过结合视觉语言模型（VLM）和纯文本语言模型（LLM）增强语义先验，以快速推断用户意图并辅助目标达成。


<details>
  <summary>Details</summary>
Motivation: 提升人机协作中机器人对用户意图的快速推断能力，并提供透明的推理过程。

Method: 结合VLM和LLM形成语义先验，利用YOLO和Segment Anything Model进行视觉处理，通过评分机制筛选相关目标。

Result: 系统能够选择上下文相关目标并适应意图变化，实现导航和抓取任务。

Conclusion: 未来将在Isaac Sim中评估系统实时辅助性能。

Abstract: Human-robot collaboration requires robots to quickly infer user intent,
provide transparent reasoning, and assist users in achieving their goals. Our
recent work introduced GUIDER, our framework for inferring navigation and
manipulation intents. We propose augmenting GUIDER with a vision-language model
(VLM) and a text-only language model (LLM) to form a semantic prior that
filters objects and locations based on the mission prompt. A vision pipeline
(YOLO for object detection and the Segment Anything Model for instance
segmentation) feeds candidate object crops into the VLM, which scores their
relevance given an operator prompt; in addition, the list of detected object
labels is ranked by a text-only LLM. These scores weight the existing
navigation and manipulation layers of GUIDER, selecting context-relevant
targets while suppressing unrelated objects. Once the combined belief exceeds a
threshold, autonomy changes occur, enabling the robot to navigate to the
desired area and retrieve the desired object, while adapting to any changes in
the operator's intent. Future work will evaluate the system on Isaac Sim using
a Franka Emika arm on a Ridgeback base, with a focus on real-time assistance.

</details>


### [144] [Robot Policy Evaluation for Sim-to-Real Transfer: A Benchmarking Perspective](https://arxiv.org/abs/2508.11117)
*Xuning Yang,Clemens Eppner,Jonathan Tremblay,Dieter Fox,Stan Birchfield,Fabio Ramos*

Main category: cs.RO

TL;DR: 论文讨论了设计通用机器人操作策略基准的挑战与需求，提出了高视觉保真度仿真、任务复杂性评估和性能对齐量化的方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉的机器人仿真基准在真实世界应用中评估通用策略时存在不足，需改进以支持仿真到现实的策略迁移。

Method: 1) 使用高视觉保真度仿真；2) 通过增加任务复杂性和扰动评估策略鲁棒性；3) 量化仿真与真实性能的对齐。

Result: 提出了改进仿真到现实迁移的基准设计方法。

Conclusion: 论文为通用机器人操作策略的仿真到现实迁移提供了实用的基准设计指导。

Abstract: Current vision-based robotics simulation benchmarks have significantly
advanced robotic manipulation research. However, robotics is fundamentally a
real-world problem, and evaluation for real-world applications has lagged
behind in evaluating generalist policies. In this paper, we discuss challenges
and desiderata in designing benchmarks for generalist robotic manipulation
policies for the goal of sim-to-real policy transfer. We propose 1) utilizing
high visual-fidelity simulation for improved sim-to-real transfer, 2)
evaluating policies by systematically increasing task complexity and scenario
perturbation to assess robustness, and 3) quantifying performance alignment
between real-world performance and its simulation counterparts.

</details>


### [145] [Geometry-Aware Predictive Safety Filters on Humanoids: From Poisson Safety Functions to CBF Constrained MPC](https://arxiv.org/abs/2508.11129)
*Ryan M. Bena,Gilbert Bahati,Blake Werner,Ryan K. Cosner,Lizhi Yang,Aaron D. Ames*

Main category: cs.RO

TL;DR: 提出了一种基于控制屏障函数（CBFs）的非线性模型预测控制（MPC）算法，用于在线轨迹生成，并结合几何感知安全约束，适用于动态环境中的腿式机器人导航。


<details>
  <summary>Details</summary>
Motivation: 解决腿式机器人在非结构化和动态环境中导航时的几何不对称性和安全轨迹规划问题。

Method: 利用泊松安全函数从感知数据中数值合成CBF约束，扩展理论框架以处理动态边界问题，并使用Minkowski集合操作考虑机器人几何。

Result: 在多种安全关键场景中实现了实时预测安全过滤器，验证了泊松安全函数的通用性和CBF约束MPC控制器的优势。

Conclusion: 该方法为动态环境中的机器人导航提供了一种高效且安全的解决方案。

Abstract: Autonomous navigation through unstructured and dynamically-changing
environments is a complex task that continues to present many challenges for
modern roboticists. In particular, legged robots typically possess manipulable
asymmetric geometries which must be considered during safety-critical
trajectory planning. This work proposes a predictive safety filter: a nonlinear
model predictive control (MPC) algorithm for online trajectory generation with
geometry-aware safety constraints based on control barrier functions (CBFs).
Critically, our method leverages Poisson safety functions to numerically
synthesize CBF constraints directly from perception data. We extend the
theoretical framework for Poisson safety functions to incorporate temporal
changes in the domain by reformulating the static Dirichlet problem for
Poisson's equation as a parameterized moving boundary value problem.
Furthermore, we employ Minkowski set operations to lift the domain into a
configuration space that accounts for robot geometry. Finally, we implement our
real-time predictive safety filter on humanoid and quadruped robots in various
safety-critical scenarios. The results highlight the versatility of Poisson
safety functions, as well as the benefit of CBF constrained model predictive
safety-critical controllers.

</details>


### [146] [Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward](https://arxiv.org/abs/2508.11143)
*Jiarui Yang,Bin Zhu,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.RO

TL;DR: AC3是一种新型强化学习框架，通过稳定机制高效学习连续动作序列，解决了长时程稀疏奖励任务中的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在长时程稀疏奖励的机器人操作任务中表现不佳，直接学习连续动作块存在稳定性和数据效率问题。

Method: AC3结合了非对称更新规则和自监督模块，稳定了演员和评论家的学习过程。

Result: 在BiGym和RLBench的25个任务中，AC3仅需少量演示即取得高成功率。

Conclusion: AC3通过高效设计和稳定机制，显著提升了长时程稀疏奖励任务的性能。

Abstract: Existing reinforcement learning (RL) methods struggle with long-horizon
robotic manipulation tasks, particularly those involving sparse rewards. While
action chunking is a promising paradigm for robotic manipulation, using RL to
directly learn continuous action chunks in a stable and data-efficient manner
remains a critical challenge. This paper introduces AC3 (Actor-Critic for
Continuous Chunks), a novel RL framework that learns to generate
high-dimensional, continuous action sequences. To make this learning process
stable and data-efficient, AC3 incorporates targeted stabilization mechanisms
for both the actor and the critic. First, to ensure reliable policy
improvement, the actor is trained with an asymmetric update rule, learning
exclusively from successful trajectories. Second, to enable effective value
learning despite sparse rewards, the critic's update is stabilized using
intra-chunk $n$-step returns and further enriched by a self-supervised module
providing intrinsic rewards at anchor points aligned with each action chunk. We
conducted extensive experiments on 25 tasks from the BiGym and RLBench
benchmarks. Results show that by using only a few demonstrations and a simple
model architecture, AC3 achieves superior success rates on most tasks,
validating its effective design.

</details>


### [147] [Visuomotor Grasping with World Models for Surgical Robots](https://arxiv.org/abs/2508.11200)
*Hongbin Lin,Bin Li,Kwok Wai Samuel Au*

Main category: cs.RO

TL;DR: 论文提出GASv2框架，通过视觉运动学习实现手术中的物体抓取，解决了模拟到现实的迁移、单摄像头输入和泛化能力等挑战。


<details>
  <summary>Details</summary>
Motivation: 自动化手术抓取任务可提升效率与安全性，但现有方法泛化性差且难以适应复杂手术环境。

Method: 采用基于世界模型的架构和混合控制系统，通过域随机化训练模拟策略并部署到真实场景。

Result: 在模拟和真实手术环境中达到65%成功率，泛化性强且适应性强。

Conclusion: GASv2框架在手术抓取任务中表现出高性能、泛化性和鲁棒性。

Abstract: Grasping is a fundamental task in robot-assisted surgery (RAS), and
automating it can reduce surgeon workload while enhancing efficiency, safety,
and consistency beyond teleoperated systems. Most prior approaches rely on
explicit object pose tracking or handcrafted visual features, limiting their
generalization to novel objects, robustness to visual disturbances, and the
ability to handle deformable objects. Visuomotor learning offers a promising
alternative, but deploying it in RAS presents unique challenges, such as low
signal-to-noise ratio in visual observations, demands for high safety and
millimeter-level precision, as well as the complex surgical environment. This
paper addresses three key challenges: (i) sim-to-real transfer of visuomotor
policies to ex vivo surgical scenes, (ii) visuomotor learning using only a
single stereo camera pair -- the standard RAS setup, and (iii) object-agnostic
grasping with a single policy that generalizes to diverse, unseen surgical
objects without retraining or task-specific models. We introduce Grasp Anything
for Surgery V2 (GASv2), a visuomotor learning framework for surgical grasping.
GASv2 leverages a world-model-based architecture and a surgical perception
pipeline for visual observations, combined with a hybrid control system for
safe execution. We train the policy in simulation using domain randomization
for sim-to-real transfer and deploy it on a real robot in both phantom-based
and ex vivo surgical settings, using only a single pair of endoscopic cameras.
Extensive experiments show our policy achieves a 65% success rate in both
settings, generalizes to unseen objects and grippers, and adapts to diverse
disturbances, demonstrating strong performance, generality, and robustness.

</details>


### [148] [Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation](https://arxiv.org/abs/2508.11204)
*Hongbin Lin,Juan Rojas,Kwok Wai Samuel Au*

Main category: cs.RO

TL;DR: 论文提出了一种利用非等距对称性提升机器人视觉运动学习采样效率的方法，通过多群等变性增强（MEA）和数据增强技术，结合离线强化学习，显著提高了任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要局限于等距对称性，限制了采样效率的提升。本文探索非等距对称性，以更灵活的方式利用对称性结构，从而优化学习过程。

Method: 提出了一种新的部分可观测马尔可夫决策过程（POMDP）框架，结合非等距对称性结构，并设计了多群等变性增强（MEA）方法。此外，引入了一种基于体素的视觉表示方法，保持平移等变性。

Result: 在仿真和真实机器人实验中，该方法在两个操作任务中表现出显著的效果，验证了其采样效率和性能提升。

Conclusion: 非等距对称性是一种有效的归纳偏置，结合MEA和离线强化学习，可以显著提升机器人视觉运动学习的效率。

Abstract: Sampling efficiency is critical for deploying visuomotor learning in
real-world robotic manipulation. While task symmetry has emerged as a promising
inductive bias to improve efficiency, most prior work is limited to isometric
symmetries -- applying the same group transformation to all task objects across
all timesteps. In this work, we explore non-isometric symmetries, applying
multiple independent group transformations across spatial and temporal
dimensions to relax these constraints. We introduce a novel formulation of the
partially observable Markov decision process (POMDP) that incorporates the
non-isometric symmetry structures, and propose a simple yet effective data
augmentation method, Multi-Group Equivariance Augmentation (MEA). We integrate
MEA with offline reinforcement learning to enhance sampling efficiency, and
introduce a voxel-based visual representation that preserves translational
equivariance. Extensive simulation and real-robot experiments across two
manipulation domains demonstrate the effectiveness of our approach.

</details>


### [149] [Embodied Edge Intelligence Meets Near Field Communication: Concept, Design, and Verification](https://arxiv.org/abs/2508.11232)
*Guoliang Li,Xibin Jin,Yujie Wan,Chenxuan Liu,Tong Zhang,Shuai Wang,Chengzhong Xu*

Main category: cs.RO

TL;DR: 论文提出了一种结合边缘智能（EEI）和近场通信（NFC）的新范式NEEI，以解决大模型在嵌入式人工智能中的计算需求问题，并通过联合优化提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决大模型在嵌入式人工智能中的高计算需求问题，同时满足实时推理、高效频谱利用、通信安全和低干扰的要求。

Method: 提出NEEI范式，结合EEI和NFC，设计无线电友好的规划方法和波束聚焦技术，并通过协作导航实现资源高效利用。

Result: 实验结果表明，所提技术优于多种基准方法。

Conclusion: NEEI范式通过联合优化EEI和NFC，有效解决了嵌入式人工智能中的挑战，为未来研究提供了新方向。

Abstract: Realizing embodied artificial intelligence is challenging due to the huge
computation demands of large models (LMs). To support LMs while ensuring
real-time inference, embodied edge intelligence (EEI) is a promising paradigm,
which leverages an LM edge to provide computing powers in close proximity to
embodied robots. Due to embodied data exchange, EEI requires higher spectral
efficiency, enhanced communication security, and reduced inter-user
interference. To meet these requirements, near-field communication (NFC), which
leverages extremely large antenna arrays as its hardware foundation, is an
ideal solution. Therefore, this paper advocates the integration of EEI and NFC,
resulting in a near-field EEI (NEEI) paradigm. However, NEEI also introduces
new challenges that cannot be adequately addressed by isolated EEI or NFC
designs, creating research opportunities for joint optimization of both
functionalities. To this end, we propose radio-friendly embodied planning for
EEI-assisted NFC scenarios and view-guided beam-focusing for NFC-assisted EEI
scenarios. We also elaborate how to realize resource-efficient NEEI through
opportunistic collaborative navigation. Experimental results are provided to
confirm the superiority of the proposed techniques compared with various
benchmarks.

</details>


### [150] [Tactile Robotics: An Outlook](https://arxiv.org/abs/2508.11261)
*Shan Luo,Nathan F. Lepora,Wenzhen Yuan,Kaspar Althoefer,Gordon Cheng,Ravinder Dahiya*

Main category: cs.RO

TL;DR: 本文探讨了触觉机器人技术的发展现状、挑战及未来方向，强调了触觉感知在机器人交互中的重要性。


<details>
  <summary>Details</summary>
Motivation: 触觉感知对机器人实现与人类的紧密交互至关重要，尤其是在新兴应用中。

Method: 综述了多种触觉传感技术（如压阻、压电、电容、磁性和光学传感器）及其集成方法，并讨论了仿真工具和多模态融合的作用。

Result: 触觉技术的进步为机器人提供了更有效的物理交互能力，但仍面临挑战。

Conclusion: 未来需采取整体方法解决当前挑战，推动触觉机器人在制造、医疗、回收和农业等领域的创新。

Abstract: Robotics research has long sought to give robots the ability to perceive the
physical world through touch in an analogous manner to many biological systems.
Developing such tactile capabilities is important for numerous emerging
applications that require robots to co-exist and interact closely with humans.
Consequently, there has been growing interest in tactile sensing, leading to
the development of various technologies, including piezoresistive and
piezoelectric sensors, capacitive sensors, magnetic sensors, and optical
tactile sensors. These diverse approaches utilise different transduction
methods and materials to equip robots with distributed sensing capabilities,
enabling more effective physical interactions. These advances have been
supported in recent years by simulation tools that generate large-scale tactile
datasets to support sensor designs and algorithms to interpret and improve the
utility of tactile data. The integration of tactile sensing with other
modalities, such as vision, as well as with action strategies for active
tactile perception highlights the growing scope of this field. To further the
transformative progress in tactile robotics, a holistic approach is essential.
In this outlook article, we examine several challenges associated with the
current state of the art in tactile robotics and explore potential solutions to
inspire innovations across multiple domains, including manufacturing,
healthcare, recycling and agriculture.

</details>


### [151] [Learning Differentiable Reachability Maps for Optimization-based Humanoid Motion Generation](https://arxiv.org/abs/2508.11275)
*Masaki Murooka,Iori Kumagai,Mitsuharu Morisawa,Fumio Kanehiro*

Main category: cs.RO

TL;DR: 提出了一种可微分可达性地图的新方法，用于降低人形机器人运动生成的算力成本。


<details>
  <summary>Details</summary>
Motivation: 减少人形机器人运动生成的计算成本。

Method: 通过神经网络或支持向量机学习可微分可达性地图，并将其作为约束用于连续优化运动规划。

Result: 该方法在足步规划、多接触运动规划和操作-移动规划中表现高效。

Conclusion: 可微分可达性地图为连续优化提供了有效约束，显著提升了运动规划效率。

Abstract: To reduce the computational cost of humanoid motion generation, we introduce
a new approach to representing robot kinematic reachability: the differentiable
reachability map. This map is a scalar-valued function defined in the task
space that takes positive values only in regions reachable by the robot's
end-effector. A key feature of this representation is that it is continuous and
differentiable with respect to task-space coordinates, enabling its direct use
as constraints in continuous optimization for humanoid motion planning. We
describe a method to learn such differentiable reachability maps from a set of
end-effector poses generated using a robot's kinematic model, using either a
neural network or a support vector machine as the learning model. By
incorporating the learned reachability map as a constraint, we formulate
humanoid motion generation as a continuous optimization problem. We demonstrate
that the proposed approach efficiently solves various motion planning problems,
including footstep planning, multi-contact motion planning, and
loco-manipulation planning for humanoid robots.

</details>


### [152] [Scene Graph-Guided Proactive Replanning for Failure-Resilient Embodied Agent](https://arxiv.org/abs/2508.11286)
*Che Rin Yu,Daewon Chae,Dabin Seo,Sangwon Lee,Hyeongwoo Im,Jinkyu Kim*

Main category: cs.RO

TL;DR: 论文提出了一种主动重新规划框架，通过比较当前场景与参考场景的差异，在子任务边界检测并修正潜在失败，提升机器人任务的成功率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有自主机器人缺乏环境适应性，常因忽略场景变化而失败，且现有重新规划方法多为被动响应。主动重新规划虽能预防失败，但依赖人工规则和监督。

Method: 通过构建当前RGB-D观测的场景图与成功演示的参考图对比，在子任务边界检测不匹配，并激活轻量级推理模块调整计划。

Result: 在AI2-THOR模拟器中验证，该方法能提前检测语义和空间不匹配，显著提高任务成功率和鲁棒性。

Conclusion: 提出的主动重新规划框架有效预防执行失败，为机器人自主性提供了更可靠的解决方案。

Abstract: When humans perform everyday tasks, we naturally adjust our actions based on
the current state of the environment. For instance, if we intend to put
something into a drawer but notice it is closed, we open it first. However,
many autonomous robots lack this adaptive awareness. They often follow
pre-planned actions that may overlook subtle yet critical changes in the scene,
which can result in actions being executed under outdated assumptions and
eventual failure. While replanning is critical for robust autonomy, most
existing methods respond only after failures occur, when recovery may be
inefficient or infeasible. While proactive replanning holds promise for
preventing failures in advance, current solutions often rely on manually
designed rules and extensive supervision. In this work, we present a proactive
replanning framework that detects and corrects failures at subtask boundaries
by comparing scene graphs constructed from current RGB-D observations against
reference graphs extracted from successful demonstrations. When the current
scene fails to align with reference trajectories, a lightweight reasoning
module is activated to diagnose the mismatch and adjust the plan. Experiments
in the AI2-THOR simulator demonstrate that our approach detects semantic and
spatial mismatches before execution failures occur, significantly improving
task success and robustness.

</details>


### [153] [A Recursive Total Least Squares Solution for Bearing-Only Target Motion Analysis and Circumnavigation](https://arxiv.org/abs/2508.11289)
*Lin Li,Xueming Liu,Zhoujingzi Qiu,Tianjiang Hu,Qingrui Zhang*

Main category: cs.RO

TL;DR: 提出了一种基于递归总体最小二乘法（RTLS）的在线目标定位与跟踪方法，解决了仅用方位角的目标运动分析（TMA）中的非线性问题和观测性不足。


<details>
  <summary>Details</summary>
Motivation: 仅用方位角的TMA技术因测量模型非线性和缺乏距离信息而面临观测性和估计器收敛性挑战。

Method: 采用递归总体最小二乘法（RTLS）进行目标定位与跟踪，并设计环绕控制器提升系统观测性。

Result: 仿真和实验验证了方法的有效性和鲁棒性，其性能优于现有方法。

Conclusion: RTLS方法在精度和稳定性上优于伪线性卡尔曼滤波（PLKF）等现有技术。

Abstract: Bearing-only Target Motion Analysis (TMA) is a promising technique for
passive tracking in various applications as a bearing angle is easy to measure.
Despite its advantages, bearing-only TMA is challenging due to the nonlinearity
of the bearing measurement model and the lack of range information, which
impairs observability and estimator convergence. This paper addresses these
issues by proposing a Recursive Total Least Squares (RTLS) method for online
target localization and tracking using mobile observers. The RTLS approach,
inspired by previous results on Total Least Squares (TLS), mitigates biases in
position estimation and improves computational efficiency compared to
pseudo-linear Kalman filter (PLKF) methods. Additionally, we propose a
circumnavigation controller to enhance system observability and estimator
convergence by guiding the mobile observer in orbit around the target.
Extensive simulations and experiments are performed to demonstrate the
effectiveness and robustness of the proposed method. The proposed algorithm is
also compared with the state-of-the-art approaches, which confirms its superior
performance in terms of both accuracy and stability.

</details>


### [154] [Pedestrian Dead Reckoning using Invariant Extended Kalman Filter](https://arxiv.org/abs/2508.11396)
*Jingran Zhang,Zhengzhang Yan,Yiming Chen,Zeqiang He,Jiahao Chen*

Main category: cs.RO

TL;DR: 本文提出了一种成本效益高的惯性行人航位推算方法，适用于双足机器人在GPS缺失环境中的定位。通过伪测量校正IMU预测，并采用基于矩阵李群的InEKF方法，实验验证了其优于标准EKF的可行性。


<details>
  <summary>Details</summary>
Motivation: 在GPS缺失环境下，双足机器人需要可靠的定位方法。传统EKF调参复杂，而InEKF在理论和实际应用中更具优势。

Method: 利用IMU在支撑脚时的伪测量校正预测，采用基于矩阵李群的InEKF方法，并通过三种实验验证其性能。

Result: 实验表明，InEKF在运动捕捉、大规模多楼层行走及双足机器人实验中均优于标准EKF，且调参更简单。

Conclusion: InEKF方法在双足机器人定位中具有更高的可行性和易用性，适用于实际机器人系统。

Abstract: This paper presents a cost-effective inertial pedestrian dead reckoning
method for the bipedal robot in the GPS-denied environment. Each time when the
inertial measurement unit (IMU) is on the stance foot, a stationary
pseudo-measurement can be executed to provide innovation to the IMU measurement
based prediction. The matrix Lie group based theoretical development of the
adopted invariant extended Kalman filter (InEKF) is set forth for tutorial
purpose. Three experiments are conducted to compare between InEKF and standard
EKF, including motion capture benchmark experiment, large-scale multi-floor
walking experiment, and bipedal robot experiment, as an effort to show our
method's feasibility in real-world robot system. In addition, a sensitivity
analysis is included to show that InEKF is much easier to tune than EKF.

</details>


### [155] [An Exploratory Study on Crack Detection in Concrete through Human-Robot Collaboration](https://arxiv.org/abs/2508.11404)
*Junyeon Kim,Tianshu Ruan,Cesar Alan Contreras,Manolis Chiou*

Main category: cs.RO

TL;DR: AI与机器人技术结合，通过人机协作（HRC）提升核设施结构检查的准确性和效率，减少人工负担。


<details>
  <summary>Details</summary>
Motivation: 传统人工检查存在安全风险、认知负担高和准确性不足的问题，AI和机器人技术为改进检查方法提供了新可能。

Method: 研究将AI辅助视觉裂纹检测集成到移动Jackal机器人平台，探索HRC的效果。

Result: 实验表明，HRC提高了检查准确性并减少了操作员负担，性能优于传统人工方法。

Conclusion: AI辅助的HRC在核设施检查中具有潜力，可替代传统方法。

Abstract: Structural inspection in nuclear facilities is vital for maintaining
operational safety and integrity. Traditional methods of manual inspection pose
significant challenges, including safety risks, high cognitive demands, and
potential inaccuracies due to human limitations. Recent advancements in
Artificial Intelligence (AI) and robotic technologies have opened new
possibilities for safer, more efficient, and accurate inspection methodologies.
Specifically, Human-Robot Collaboration (HRC), leveraging robotic platforms
equipped with advanced detection algorithms, promises significant improvements
in inspection outcomes and reductions in human workload. This study explores
the effectiveness of AI-assisted visual crack detection integrated into a
mobile Jackal robot platform. The experiment results indicate that HRC enhances
inspection accuracy and reduces operator workload, resulting in potential
superior performance outcomes compared to traditional manual methods.

</details>


### [156] [Open, Reproducible and Trustworthy Robot-Based Experiments with Virtual Labs and Digital-Twin-Based Execution Tracing](https://arxiv.org/abs/2508.11406)
*Benjamin Alt,Mareike Picklum,Sorin Arion,Franklin Kenghagho Kenfack,Michael Beetz*

Main category: cs.RO

TL;DR: 论文提出了一种语义执行追踪框架和云平台AICOR VRB，旨在实现透明、可复现的机器人自主科学实验。


<details>
  <summary>Details</summary>
Motivation: 为了实现机器人科学实验的透明、可信和可重复性。

Method: 开发了语义执行追踪框架和AICOR VRB云平台。

Result: 工具实现了机器人任务的确定性执行、语义记忆和开放知识表示。

Conclusion: 为自主系统参与科学发现奠定了基础。

Abstract: We envision a future in which autonomous robots conduct scientific
experiments in ways that are not only precise and repeatable, but also open,
trustworthy, and transparent. To realize this vision, we present two key
contributions: a semantic execution tracing framework that logs sensor data
together with semantically annotated robot belief states, ensuring that
automated experimentation is transparent and replicable; and the AICOR Virtual
Research Building (VRB), a cloud-based platform for sharing, replicating, and
validating robot task executions at scale. Together, these tools enable
reproducible, robot-driven science by integrating deterministic execution,
semantic memory, and open knowledge representation, laying the foundation for
autonomous systems to participate in scientific discovery.

</details>


### [157] [EvoPSF: Online Evolution of Autonomous Driving Models via Planning-State Feedback](https://arxiv.org/abs/2508.11453)
*Jiayue Jin,Lang Qian,Jingyu Zhang,Chuanyu Ju,Liang Song*

Main category: cs.RO

TL;DR: 论文提出EvoPSF框架，通过在线进化机制解决自动驾驶中环境适应性问题，利用规划状态反馈提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统多为离线训练，缺乏对新环境的适应能力，导致泛化性不足。

Method: 提出EvoPSF框架，以规划器不确定性为触发信号，通过目标自监督损失在线更新模型。

Result: 实验表明EvoPSF在nuScenes数据集上显著提升了规划性能。

Conclusion: EvoPSF通过在线进化机制有效提升了自动驾驶系统的适应性和规划准确性。

Abstract: Recent years have witnessed remarkable progress in autonomous driving, with
systems evolving from modular pipelines to end-to-end architectures. However,
most existing methods are trained offline and lack mechanisms to adapt to new
environments during deployment. As a result, their generalization ability
diminishes when faced with unseen variations in real-world driving scenarios.
In this paper, we break away from the conventional "train once, deploy forever"
paradigm and propose EvoPSF, a novel online Evolution framework for autonomous
driving based on Planning-State Feedback. We argue that planning failures are
primarily caused by inaccurate object-level motion predictions, and such
failures are often reflected in the form of increased planner uncertainty. To
address this, we treat planner uncertainty as a trigger for online evolution,
using it as a diagnostic signal to initiate targeted model updates. Rather than
performing blind updates, we leverage the planner's agent-agent attention to
identify the specific objects that the ego vehicle attends to most, which are
primarily responsible for the planning failures. For these critical objects, we
compute a targeted self-supervised loss by comparing their predicted waypoints
from the prediction module with their actual future positions, selected from
the perception module's outputs with high confidence scores. This loss is then
backpropagated to adapt the model online. As a result, our method improves the
model's robustness to environmental changes, leads to more precise motion
predictions, and therefore enables more accurate and stable planning behaviors.
Experiments on both cross-region and corrupted variants of the nuScenes dataset
demonstrate that EvoPSF consistently improves planning performance under
challenging conditions.

</details>


### [158] [OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation](https://arxiv.org/abs/2508.11479)
*Tatiana Zemskova,Aleksei Staroverov,Dmitry Yudin,Aleksandr Panov*

Main category: cs.RO

TL;DR: OVSegDT是一种轻量级Transformer策略，通过语义分支和熵自适应损失调制，解决了开放词汇目标导航中的过拟合和碰撞问题，显著提升了泛化性能和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端策略在小规模模拟数据集上过拟合，泛化能力差且行为不安全（频繁碰撞）。

Method: OVSegDT包含语义分支（目标掩码编码器和辅助分割损失函数）和熵自适应损失调制（动态平衡模仿与强化信号）。

Result: 训练样本复杂度降低33%，碰撞次数减少一半，在未见类别上性能与已见类别相当（40.1% SR，20.9% SPL）。

Conclusion: OVSegDT在无需深度、里程计或大型视觉语言模型的情况下，实现了开放词汇目标导航的先进性能。

Abstract: Open-vocabulary Object Goal Navigation requires an embodied agent to reach
objects described by free-form language, including categories never seen during
training. Existing end-to-end policies overfit small simulator datasets,
achieving high success on training scenes but failing to generalize and
exhibiting unsafe behaviour (frequent collisions). We introduce OVSegDT, a
lightweight transformer policy that tackles these issues with two synergistic
components. The first component is the semantic branch, which includes an
encoder for the target binary mask and an auxiliary segmentation loss function,
grounding the textual goal and providing precise spatial cues. The second
component consists of a proposed Entropy-Adaptive Loss Modulation, a per-sample
scheduler that continuously balances imitation and reinforcement signals
according to the policy entropy, eliminating brittle manual phase switches.
These additions cut the sample complexity of training by 33%, and reduce
collision count in two times while keeping inference cost low (130M parameters,
RGB-only input). On HM3D-OVON, our model matches the performance on unseen
categories to that on seen ones and establishes state-of-the-art results (40.1%
SR, 20.9% SPL on val unseen) without depth, odometry, or large vision-language
models. Code is available at https://github.com/CognitiveAISystems/OVSegDT.

</details>


### [159] [i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor Fusion Navigation and Mapping](https://arxiv.org/abs/2508.11485)
*Hailiang Tang,Tisheng Zhang,Liqiang Wang,Xin Ding,Man Yuan,Zhiyu Xiang,Jujin Chen,Yuhan Bian,Shuangyan Liu,Yuqing Wang,Guan Wang,Xiaoji Niu*

Main category: cs.RO

TL;DR: i2Nav-Robot是一个大规模多传感器融合数据集，旨在解决UGV导航和地图构建中的传感器配置、同步和场景多样性问题。


<details>
  <summary>Details</summary>
Motivation: 当前UGV数据集在传感器配置、时间同步、地面真实性和场景多样性方面存在不足，限制了导航和地图构建技术的发展。

Method: 通过集成多种传感器（如固态LiDAR、4D雷达、立体相机等），并采用硬件同步和离线校准，构建了覆盖室内外场景的17060米数据集。

Result: 数据集通过十多种开源多传感器融合系统验证，具有厘米级精度的高质量地面真实数据。

Conclusion: i2Nav-Robot为UGV导航和地图构建提供了高质量、多样化的数据支持。

Abstract: Accurate and reliable navigation is crucial for autonomous unmanned ground
vehicle (UGV). However, current UGV datasets fall short in meeting the demands
for advancing navigation and mapping techniques due to limitations in sensor
configuration, time synchronization, ground truth, and scenario diversity. To
address these challenges, we present i2Nav-Robot, a large-scale dataset
designed for multi-sensor fusion navigation and mapping in indoor-outdoor
environments. We integrate multi-modal sensors, including the newest front-view
and 360-degree solid-state LiDARs, 4-dimensional (4D) radar, stereo cameras,
odometer, global navigation satellite system (GNSS) receiver, and inertial
measurement units (IMU) on an omnidirectional wheeled robot. Accurate
timestamps are obtained through both online hardware synchronization and
offline calibration for all sensors. The dataset comprises ten larger-scale
sequences covering diverse UGV operating scenarios, such as outdoor streets,
and indoor parking lots, with a total length of about 17060 meters.
High-frequency ground truth, with centimeter-level accuracy for position, is
derived from post-processing integrated navigation methods using a
navigation-grade IMU. The proposed i2Nav-Robot dataset is evaluated by more
than ten open-sourced multi-sensor fusion systems, and it has proven to have
superior data quality.

</details>


### [160] [Relative Position Matters: Trajectory Prediction and Planning with Polar Representation](https://arxiv.org/abs/2508.11492)
*Bozhou Zhang,Nan Song,Bingzhao Gao,Li Zhang*

Main category: cs.RO

TL;DR: 论文提出了一种基于极坐标的新方法Polaris，用于自动驾驶中的轨迹预测与规划，优于传统笛卡尔坐标方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在笛卡尔坐标系中建模车辆与周围交通元素的关系不够直观，无法有效捕捉距离和方向的影响。

Method: 采用极坐标系统，通过半径和角度表示位置，并设计了专门的编码和优化模块来建模距离和方向变化。

Result: 在Argoverse 2和nuPlan等基准测试中，Polaris实现了最先进的性能。

Conclusion: 极坐标表示能更直观地建模空间关系，提升轨迹预测与规划的准确性。

Abstract: Trajectory prediction and planning in autonomous driving are highly
challenging due to the complexity of predicting surrounding agents' movements
and planning the ego agent's actions in dynamic environments. Existing methods
encode map and agent positions and decode future trajectories in Cartesian
coordinates. However, modeling the relationships between the ego vehicle and
surrounding traffic elements in Cartesian space can be suboptimal, as it does
not naturally capture the varying influence of different elements based on
their relative distances and directions. To address this limitation, we adopt
the Polar coordinate system, where positions are represented by radius and
angle. This representation provides a more intuitive and effective way to model
spatial changes and relative relationships, especially in terms of distance and
directional influence. Based on this insight, we propose Polaris, a novel
method that operates entirely in Polar coordinates, distinguishing itself from
conventional Cartesian-based approaches. By leveraging the Polar
representation, this method explicitly models distance and direction variations
and captures relative relationships through dedicated encoding and refinement
modules, enabling more structured and spatially aware trajectory prediction and
planning. Extensive experiments on the challenging prediction (Argoverse 2) and
planning benchmarks (nuPlan) demonstrate that Polaris achieves state-of-the-art
performance.

</details>


### [161] [Swarm-in-Blocks: Simplifying Drone Swarm Programming with Block-Based Language](https://arxiv.org/abs/2508.11498)
*Agnes Bressan de Almeida,Joao Aires Correa Fernandes Marsicano*

Main category: cs.RO

TL;DR: Swarm in Blocks是一个基于块语言的无人机群编程高级接口，简化了群管理，适合初学者和教育用途。


<details>
  <summary>Details</summary>
Motivation: 随着无人机群在配送、农业和监控等领域的应用增加，管理复杂性也随之提高，Atena团队开发此工具以降低门槛。

Method: 基于Clover平台，使用块语言构建功能（如循环和条件结构），2023年推出2.0版本进一步优化。

Result: 工具简化了群控制，无需ROS或编程专业知识，同时扩展了编程教育机会。

Conclusion: Swarm in Blocks通过块语言降低了无人机群编程的门槛，提升了易用性和教育价值。

Abstract: Swarm in Blocks, originally developed for CopterHack 2022, is a high-level
interface that simplifies drone swarm programming using a block-based language.
Building on the Clover platform, this tool enables users to create
functionalities like loops and conditional structures by assembling code
blocks. In 2023, we introduced Swarm in Blocks 2.0, further refining the
platform to address the complexities of swarm management in a user-friendly
way. As drone swarm applications grow in areas like delivery, agriculture, and
surveillance, the challenge of managing them, especially for beginners, has
also increased. The Atena team developed this interface to make swarm handling
accessible without requiring extensive knowledge of ROS or programming. The
block-based approach not only simplifies swarm control but also expands
educational opportunities in programming.

</details>


### [162] [Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media](https://arxiv.org/abs/2508.11503)
*Andrej Orsula,Matthieu Geist,Miguel Olivares-Mendez,Carol Martinez*

Main category: cs.RO

TL;DR: 论文提出了一种从仿真到现实的框架，用于在复杂地形上训练和验证强化学习控制策略，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决学习型控制器在行星表面复杂地形导航中的仿真与现实差距问题。

Method: 利用大规模并行仿真训练强化学习代理，并在真实轮式漫游车上进行零样本迁移。

Result: 实验表明，通过程序多样性训练的代理在零样本性能上优于静态场景训练的代理。

Conclusion: 该框架为开发可靠的自主导航系统提供了有效方法，推动了太空探索中机器人部署的进展。

Abstract: Reliable autonomous navigation across the unstructured terrains of distant
planetary surfaces is a critical enabler for future space exploration. However,
the deployment of learning-based controllers is hindered by the inherent
sim-to-real gap, particularly for the complex dynamics of wheel interactions
with granular media. This work presents a complete sim-to-real framework for
developing and validating robust control policies for dynamic waypoint tracking
on such challenging surfaces. We leverage massively parallel simulation to
train reinforcement learning agents across a vast distribution of procedurally
generated environments with randomized physics. These policies are then
transferred zero-shot to a physical wheeled rover operating in a lunar-analogue
facility. Our experiments systematically compare multiple reinforcement
learning algorithms and action smoothing filters to identify the most effective
combinations for real-world deployment. Crucially, we provide strong empirical
evidence that agents trained with procedural diversity achieve superior
zero-shot performance compared to those trained on static scenarios. We also
analyze the trade-offs of fine-tuning with high-fidelity particle physics,
which offers minor gains in low-speed precision at a significant computational
cost. Together, these contributions establish a validated workflow for creating
reliable learning-based navigation systems, marking a critical step towards
deploying autonomous robots in the final frontier.

</details>


### [163] [A Comparative Study of Floating-Base Space Parameterizations for Agile Whole-Body Motion Planning](https://arxiv.org/abs/2508.11520)
*Evangelos Tsiatsianas,Chairi Kiourt,Konstantinos Chatzilygeroudis*

Main category: cs.RO

TL;DR: 本文比较了不同参数化方法在腿式机器人敏捷运动轨迹优化中的表现，并提出了一种基于SE(3)切空间的新方法。


<details>
  <summary>Details</summary>
Motivation: 解决腿式和人形机器人敏捷运动生成中浮动基空间参数化选择缺乏明确指导的问题。

Method: 通过直接转录轨迹优化方法，系统比较多种参数化方法，并提出基于SE(3)切空间的新参数化方法。

Result: 新方法无需专用流形优化技术，可直接使用成熟数值求解器，为敏捷运动生成提供了新选择。

Conclusion: 研究为选择浮动基表示提供了实用指导，并展示了新方法的潜力。

Abstract: Automatically generating agile whole-body motions for legged and humanoid
robots remains a fundamental challenge in robotics. While numerous trajectory
optimization approaches have been proposed, there is no clear guideline on how
the choice of floating-base space parameterization affects performance,
especially for agile behaviors involving complex contact dynamics. In this
paper, we present a comparative study of different parameterizations for direct
transcription-based trajectory optimization of agile motions in legged systems.
We systematically evaluate several common choices under identical optimization
settings to ensure a fair comparison. Furthermore, we introduce a novel
formulation based on the tangent space of SE(3) for representing the robot's
floating-base pose, which, to our knowledge, has not received attention from
the literature. This approach enables the use of mature off-the-shelf numerical
solvers without requiring specialized manifold optimization techniques. We hope
that our experiments and analysis will provide meaningful insights for
selecting the appropriate floating-based representation for agile whole-body
motion generation.

</details>


### [164] [MultiPark: Multimodal Parking Transformer with Next-Segment Prediction](https://arxiv.org/abs/2508.11537)
*Han Zheng,Zikang Zhou,Guli Zhang,Zhepei Wang,Kaixuan Wang,Peiliang Li,Shaojie Shen,Ming Yang,Tong Qin*

Main category: cs.RO

TL;DR: 论文提出MultiPark，一种基于自回归Transformer的多模态停车方法，解决了现有模仿学习在停车任务中的局限性，实现了空间泛化和时间外推。


<details>
  <summary>Details</summary>
Motivation: 停车在高度受限空间中仍具挑战性，现有模仿学习方法忽略了停车行为的多模态性，且存在因果混淆问题。

Method: 采用数据高效的下一段预测范式，设计可学习的停车查询（分解为档位、纵向和横向组件），并使用目标中心位姿和自中心碰撞作为损失函数。

Result: 在真实数据集上表现优异，并在实际车辆上验证了其鲁棒性。

Conclusion: MultiPark在多模态停车任务中实现了最先进性能，解决了现有方法的不足。

Abstract: Parking accurately and safely in highly constrained spaces remains a critical
challenge. Unlike structured driving environments, parking requires executing
complex maneuvers such as frequent gear shifts and steering saturation. Recent
attempts to employ imitation learning (IL) for parking have achieved promising
results. However, existing works ignore the multimodal nature of parking
behavior in lane-free open space, failing to derive multiple plausible
solutions under the same situation. Notably, IL-based methods encompass
inherent causal confusion, so enabling a neural network to generalize across
diverse parking scenarios is particularly difficult. To address these
challenges, we propose MultiPark, an autoregressive transformer for multimodal
parking. To handle paths filled with abrupt turning points, we introduce a
data-efficient next-segment prediction paradigm, enabling spatial
generalization and temporal extrapolation. Furthermore, we design learnable
parking queries factorized into gear, longitudinal, and lateral components,
parallelly decoding diverse parking behaviors. To mitigate causal confusion in
IL, our method employs target-centric pose and ego-centric collision as
outcome-oriented loss across all modalities beyond pure imitation loss.
Evaluations on real-world datasets demonstrate that MultiPark achieves
state-of-the-art performance across various scenarios. We deploy MultiPark on a
production vehicle, further confirming our approach's robustness in real-world
parking environments.

</details>


### [165] [Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads](https://arxiv.org/abs/2508.11547)
*Martin Jiroušek,Tomáš Báča,Martin Saska*

Main category: cs.RO

TL;DR: 提出一种仅使用标准机载传感器（RTK-GNSS和IMU）的框架，用于估计和控制无人机悬挂负载的位置，性能接近真实测量，仅轻微下降（<6%）。


<details>
  <summary>Details</summary>
Motivation: 解决无人机悬挂负载位置跟踪问题，减少对额外硬件（如运动捕捉系统或额外摄像头）的依赖，实现实际部署和最小硬件需求。

Method: 结合线性卡尔曼滤波进行状态估计，模型预测轮廓控制规划器和增量模型预测控制器，建模无人机与负载的耦合动力学。

Result: 仿真显示性能接近真实测量（仅<6%下降），对负载参数变化具有强鲁棒性；户外实验验证了其实际适用性。

Conclusion: 该框架在仅使用现成硬件的情况下，实现了可靠的负载位置跟踪，适用于实际户外环境。

Abstract: This paper addresses the problem of tracking the position of a
cable-suspended payload carried by an unmanned aerial vehicle, with a focus on
real-world deployment and minimal hardware requirements. In contrast to many
existing approaches that rely on motion-capture systems, additional onboard
cameras, or instrumented payloads, we propose a framework that uses only
standard onboard sensors--specifically, real-time kinematic global navigation
satellite system measurements and data from the onboard inertial measurement
unit--to estimate and control the payload's position. The system models the
full coupled dynamics of the aerial vehicle and payload, and integrates a
linear Kalman filter for state estimation, a model predictive contouring
control planner, and an incremental model predictive controller. The control
architecture is designed to remain effective despite sensing limitations and
estimation uncertainty. Extensive simulations demonstrate that the proposed
system achieves performance comparable to control based on ground-truth
measurements, with only minor degradation (< 6%). The system also shows strong
robustness to variations in payload parameters. Field experiments further
validate the framework, confirming its practical applicability and reliable
performance in outdoor environments using only off-the-shelf aerial vehicle
hardware.

</details>


### [166] [Nominal Evaluation Of Automatic Multi-Sections Control Potential In Comparison To A Simpler One- Or Two-Sections Alternative With Predictive Spray Switching](https://arxiv.org/abs/2508.11573)
*Mogens Plessen*

Main category: cs.RO

TL;DR: 论文比较了自动多段控制方法与更简单的一或两段预测喷雾切换方法，提出了一种低成本、无需传感器的优选方案。


<details>
  <summary>Details</summary>
Motivation: 农业喷雾中自动分段控制（ASC）复杂且依赖传感器，研究旨在寻找更简单的替代方法。

Method: 比较了48段、2段和单段控制方法，结合路径规划和喷雾切换逻辑，评估了10种实际农田场景。

Result: 优选方法在路径长度、喷雾重叠和成本方面表现良好，适合手动驾驶且无需传感器。

Conclusion: 提出了一种低成本、无需传感器的喷雾控制方法，适用于复杂农田条件。

Abstract: Automatic Section Control (ASC) is a long-standing trend for spraying in
agriculture. It promises to minimise spray overlap areas. The core idea is to
(i) switch off spray nozzles on areas that have already been sprayed, and (ii)
to dynamically adjust nozzle flow rates along the boom bar that holds the spray
nozzles when velocities of boom sections vary during turn maneuvers. ASC is not
possible without sensors, in particular for accurate positioning data. Spraying
and the movement of modern wide boom bars are highly dynamic processes. In
addition, many uncertainty factors have an effect such as cross wind drift,
boom height, nozzle clogging in open-field conditions, and so forth. In view of
this complexity, the natural question arises if a simpler alternative exist.
Therefore, an Automatic Multi-Sections Control method is compared to a proposed
simpler one- or two-sections alternative that uses predictive spray switching.
The comparison is provided under nominal conditions. Agricultural spraying is
intrinsically linked to area coverage path planning and spray switching logic.
Combinations of two area coverage path planning and switching logics as well as
three sections-setups are compared. The three sections-setups differ by
controlling 48 sections, 2 sections or controlling all nozzles uniformly with
the same control signal as one single section. Methods are evaluated on 10
diverse real-world field examples, including non-convex field contours,
freeform mainfield lanes and multiple obstacle areas. A preferred method is
suggested that (i) minimises area coverage pathlength, (ii) offers intermediate
overlap, (iii) is suitable for manual driving by following a pre-planned
predictive spray switching logic for an area coverage path plan, and (iv) and
in contrast to ASC can be implemented sensor-free and therefore at low cost.

</details>


### [167] [Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks](https://arxiv.org/abs/2508.11584)
*Jakub Łucki,Jonathan Becktor,Georgios Georgakis,Robert Royce,Shehryar Khattak*

Main category: cs.RO

TL;DR: VPEngine是一个模块化框架，旨在通过共享基础模型和并行任务头实现高效GPU利用，解决机器人平台上多模型部署的计算冗余和内存问题。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限的机器人平台上部署多个机器学习模型时出现的计算冗余、内存占用大和集成复杂的问题。

Method: 利用共享基础模型提取图像表示，并通过并行任务头高效共享这些表示，避免不必要的GPU-CPU内存传输，同时支持动态任务优先级调整。

Result: 使用DINOv2作为基础模型，实现了3倍的速度提升，并在NVIDIA Jetson Orin AGX上达到≥50 Hz的实时性能。

Conclusion: VPEngine通过高效GPU利用和动态任务管理，为机器人视觉多任务处理提供了可行的解决方案。

Abstract: Deploying multiple machine learning models on resource-constrained robotic
platforms for different perception tasks often results in redundant
computations, large memory footprints, and complex integration challenges. In
response, this work presents Visual Perception Engine (VPEngine), a modular
framework designed to enable efficient GPU usage for visual multitasking while
maintaining extensibility and developer accessibility. Our framework
architecture leverages a shared foundation model backbone that extracts image
representations, which are efficiently shared, without any unnecessary GPU-CPU
memory transfers, across multiple specialized task-specific model heads running
in parallel. This design eliminates the computational redundancy inherent in
feature extraction component when deploying traditional sequential models while
enabling dynamic task prioritization based on application demands. We
demonstrate our framework's capabilities through an example implementation
using DINOv2 as the foundation model with multiple task (depth, object
detection and semantic segmentation) heads, achieving up to 3x speedup compared
to sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine
offers efficient GPU utilization and maintains a constant memory footprint
while allowing per-task inference frequencies to be adjusted dynamically during
runtime. The framework is written in Python and is open source with ROS2 C++
(Humble) bindings for ease of use by the robotics community across diverse
robotic platforms. Our example implementation demonstrates end-to-end real-time
performance at $\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized
models.

</details>


### [168] [Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation](https://arxiv.org/abs/2508.11588)
*Benjamin Walt,Jordan Westphal,Girish Krishnan*

Main category: cs.RO

TL;DR: 论文研究了农业采摘中抓取状态的分类方法，通过集成多种传感器并使用随机森林和LSTM模型，发现随机森林在实验室和实际环境中表现优异，准确率达100%。


<details>
  <summary>Details</summary>
Motivation: 农业环境的复杂性和遮挡问题使得准确理解抓取状态成为关键挑战，需选择合适的传感器和建模技术以提高采摘效率和可靠性。

Method: 研究集成了IMU、红外反射、张力、触觉传感器和RGB摄像头，使用随机森林和LSTM模型分类抓取状态。

Result: 随机森林在实验室和实际樱桃番茄植株测试中达到100%准确率，优于基线性能。IMU和张力传感器组合被确定为最小可行方案。

Conclusion: 该分类器能基于实时反馈规划纠正动作，显著提升水果采摘的效率和可靠性。

Abstract: Effective and efficient agricultural manipulation and harvesting depend on
accurately understanding the current state of the grasp. The agricultural
environment presents unique challenges due to its complexity, clutter, and
occlusion. Additionally, fruit is physically attached to the plant, requiring
precise separation during harvesting. Selecting appropriate sensors and
modeling techniques is critical for obtaining reliable feedback and correctly
identifying grasp states. This work investigates a set of key sensors, namely
inertial measurement units (IMUs), infrared (IR) reflectance, tension, tactile
sensors, and RGB cameras, integrated into a compliant gripper to classify grasp
states. We evaluate the individual contribution of each sensor and compare the
performance of two widely used classification models: Random Forest and Long
Short-Term Memory (LSTM) networks. Our results demonstrate that a Random Forest
classifier, trained in a controlled lab environment and tested on real cherry
tomato plants, achieved 100% accuracy in identifying slip, grasp failure, and
successful picks, marking a substantial improvement over baseline performance.
Furthermore, we identify a minimal viable sensor combination, namely IMU and
tension sensors that effectively classifies grasp states. This classifier
enables the planning of corrective actions based on real-time feedback, thereby
enhancing the efficiency and reliability of fruit harvesting operations.

</details>
