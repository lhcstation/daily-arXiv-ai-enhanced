<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 89]
- [cs.CL](#cs.CL) [Total: 45]
- [cs.RO](#cs.RO) [Total: 17]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Quality-Guided Mixture of Score-Fusion Experts Framework for Human Recognition](https://arxiv.org/abs/2508.00053)
*Jie Zhu,Yiyang Su,Minchul Kim,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: 提出了一种名为QME的新框架，通过可学习的分数融合策略提升全身生物识别性能，解决了传统方法中分数分布差异和数据质量变化的问题。


<details>
  <summary>Details</summary>
Motivation: 传统全身生物识别方法通过分数融合处理多模态数据，但忽视了各模态分数分布的差异，限制了性能提升。

Method: QME框架结合了质量估计器（QE）和专家混合（MoE），引入了伪质量损失和分数三元组损失。

Result: 在多个数据集上验证了QME的有效性，性能优于基线方法，达到了最先进水平。

Conclusion: QME框架在多模态和多模型场景中表现出色，解决了相似性分数域中的模型不对齐和数据质量变化等关键挑战。

Abstract: Whole-body biometric recognition is a challenging multimodal task that
integrates various biometric modalities, including face, gait, and body. This
integration is essential for overcoming the limitations of unimodal systems.
Traditionally, whole-body recognition involves deploying different models to
process multiple modalities, achieving the final outcome by score-fusion (e.g.,
weighted averaging of similarity matrices from each model). However, these
conventional methods may overlook the variations in score distributions of
individual modalities, making it challenging to improve final performance. In
this work, we present \textbf{Q}uality-guided \textbf{M}ixture of score-fusion
\textbf{E}xperts (QME), a novel framework designed for improving whole-body
biometric recognition performance through a learnable score-fusion strategy
using a Mixture of Experts (MoE). We introduce a novel pseudo-quality loss for
quality estimation with a modality-specific Quality Estimator (QE), and a score
triplet loss to improve the metric performance. Extensive experiments on
multiple whole-body biometric datasets demonstrate the effectiveness of our
proposed approach, achieving state-of-the-art results across various metrics
compared to baseline methods. Our method is effective for multimodal and
multi-model, addressing key challenges such as model misalignment in the
similarity score domain and variability in data quality.

</details>


### [2] [Punching Bag vs. Punching Person: Motion Transferability in Videos](https://arxiv.org/abs/2508.00085)
*Raiyaan Abdullah,Jared Claypoole,Michael Cogswell,Ajay Divakaran,Yogesh Rawat*

Main category: cs.CV

TL;DR: 论文探讨了动作识别模型在跨上下文迁移高级运动概念时的表现，发现模型在新场景中识别高级动作时性能显著下降，并提出了改进方法。


<details>
  <summary>Details</summary>
Motivation: 研究动作识别模型是否能有效迁移高级运动概念到不同上下文，尤其是类似分布中的新变体。

Method: 引入了一个运动迁移性框架，使用三个数据集（Syn-TA、Kinetics400-TA、Something-Something-v2-TA）评估了13个先进模型。

Result: 模型在新场景中识别高级动作时性能显著下降，多模态模型在细粒度动作上表现更差，大模型在空间主导任务中表现更好但时间推理能力不足。

Conclusion: 研究为动作识别中的运动迁移性评估提供了重要基准，并提出了通过解耦粗粒度和细粒度运动改进识别的方法。

Abstract: Action recognition models demonstrate strong generalization, but can they
effectively transfer high-level motion concepts across diverse contexts, even
within similar distributions? For example, can a model recognize the broad
action "punching" when presented with an unseen variation such as "punching
person"? To explore this, we introduce a motion transferability framework with
three datasets: (1) Syn-TA, a synthetic dataset with 3D object motions; (2)
Kinetics400-TA; and (3) Something-Something-v2-TA, both adapted from natural
video datasets. We evaluate 13 state-of-the-art models on these benchmarks and
observe a significant drop in performance when recognizing high-level actions
in novel contexts. Our analysis reveals: 1) Multimodal models struggle more
with fine-grained unknown actions than with coarse ones; 2) The bias-free
Syn-TA proves as challenging as real-world datasets, with models showing
greater performance drops in controlled settings; 3) Larger models improve
transferability when spatial cues dominate but struggle with intensive temporal
reasoning, while reliance on object and background cues hinders generalization.
We further explore how disentangling coarse and fine motions can improve
recognition in temporally challenging datasets. We believe this study
establishes a crucial benchmark for assessing motion transferability in action
recognition. Datasets and relevant code:
https://github.com/raiyaan-abdullah/Motion-Transfer.

</details>


### [3] [The Monado SLAM Dataset for Egocentric Visual-Inertial Tracking](https://arxiv.org/abs/2508.00088)
*Mateo de Mayo,Daniel Cremers,Taihú Pire*

Main category: cs.CV

TL;DR: 论文介绍了Monado SLAM数据集，旨在解决现有VIO/SLAM系统在头戴设备使用场景中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有VIO/SLAM系统在头戴设备的高强度运动、动态遮挡等复杂场景中表现不佳，缺乏相关数据集。

Method: 通过多个虚拟现实头戴设备采集真实序列，构建Monado SLAM数据集。

Result: 发布了Monado SLAM数据集，采用CC BY 4.0许可，推动VIO/SLAM研究发展。

Conclusion: Monado SLAM数据集填补了现有研究的空白，有助于解决实际应用中的挑战。

Abstract: Humanoid robots and mixed reality headsets benefit from the use of
head-mounted sensors for tracking. While advancements in visual-inertial
odometry (VIO) and simultaneous localization and mapping (SLAM) have produced
new and high-quality state-of-the-art tracking systems, we show that these are
still unable to gracefully handle many of the challenging settings presented in
the head-mounted use cases. Common scenarios like high-intensity motions,
dynamic occlusions, long tracking sessions, low-textured areas, adverse
lighting conditions, saturation of sensors, to name a few, continue to be
covered poorly by existing datasets in the literature. In this way, systems may
inadvertently overlook these essential real-world issues. To address this, we
present the Monado SLAM dataset, a set of real sequences taken from multiple
virtual reality headsets. We release the dataset under a permissive CC BY 4.0
license, to drive advancements in VIO/SLAM research and development.

</details>


### [4] [Exploring the Feasibility of Deep Learning Techniques for Accurate Gender Classification from Eye Images](https://arxiv.org/abs/2508.00135)
*Basna Mohammed Salih Hasan,Ramadhan J. Mstafa*

Main category: cs.CV

TL;DR: 论文提出了一种基于卷积神经网络（CNN）的模型，利用眼周区域的颜色图像进行性别分类，并在两个数据集上取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 性别分类在安全、人机交互等领域至关重要，但化妆品和伪装等因素可能影响分类准确性。研究专注于眼周区域以解决这一问题。

Method: 使用CNN模型分析眼周区域的颜色图像，提取关键特征进行性别分类。

Result: 在CVBL数据集上达到99%的准确率，在（Female and Male）数据集上达到96%的准确率。

Conclusion: 模型表现优异，适用于安全和监控等实际应用。

Abstract: Gender classification has emerged as a crucial aspect in various fields,
including security, human-machine interaction, surveillance, and advertising.
Nonetheless, the accuracy of this classification can be influenced by factors
such as cosmetics and disguise. Consequently, our study is dedicated to
addressing this concern by concentrating on gender classification using color
images of the periocular region. The periocular region refers to the area
surrounding the eye, including the eyelids, eyebrows, and the region between
them. It contains valuable visual cues that can be used to extract key features
for gender classification. This paper introduces a sophisticated Convolutional
Neural Network (CNN) model that utilizes color image databases to evaluate the
effectiveness of the periocular region for gender classification. To validate
the model's performance, we conducted tests on two eye datasets, namely CVBL
and (Female and Male). The recommended architecture achieved an outstanding
accuracy of 99% on the previously unused CVBL dataset while attaining a
commendable accuracy of 96% with a small number of learnable parameters
(7,235,089) on the (Female and Male) dataset. To ascertain the effectiveness of
our proposed model for gender classification using the periocular region, we
evaluated its performance through an extensive range of metrics and compared it
with other state-of-the-art approaches. The results unequivocally demonstrate
the efficacy of our model, thereby suggesting its potential for practical
application in domains such as security and surveillance.

</details>


### [5] [World Consistency Score: A Unified Metric for Video Generation Quality](https://arxiv.org/abs/2508.00144)
*Akshat Rakheja,Aarsh Ashdhir,Aryan Bhattacharjee,Vanshika Sharma*

Main category: cs.CV

TL;DR: World Consistency Score (WCS) 是一种新的视频生成模型评估指标，强调生成视频的内部世界一致性，整合了四个可解释的子组件，并通过学习权重公式与人类判断对齐。


<details>
  <summary>Details</summary>
Motivation: 现有视频评估指标主要关注视觉保真度或提示对齐，而忽略了时间与物理一致性。WCS旨在填补这一空白，提供更全面的评估框架。

Method: WCS 包含四个子指标（物体持久性、关系稳定性、因果合规性和闪烁惩罚），通过开源工具计算，并通过人类偏好数据学习权重。

Result: WCS 在多个基准测试中验证了与人类评估的相关性，并与其他指标（如FVD、CLIPScore）进行了比较。

Conclusion: WCS 提供了一个全面且可解释的框架，用于评估视频生成模型在时间一致性方面的表现。

Abstract: We introduce World Consistency Score (WCS), a novel unified evaluation metric
for generative video models that emphasizes internal world consistency of the
generated videos. WCS integrates four interpretable sub-components - object
permanence, relation stability, causal compliance, and flicker penalty - each
measuring a distinct aspect of temporal and physical coherence in a video.
These submetrics are combined via a learned weighted formula to produce a
single consistency score that aligns with human judgments. We detail the
motivation for WCS in the context of existing video evaluation metrics,
formalize each submetric and how it is computed with open-source tools
(trackers, action recognizers, CLIP embeddings, optical flow), and describe how
the weights of the WCS combination are trained using human preference data. We
also outline an experimental validation blueprint: using benchmarks like
VBench-2.0, EvalCrafter, and LOVE to test WCS's correlation with human
evaluations, performing sensitivity analyses, and comparing WCS against
established metrics (FVD, CLIPScore, VBench, FVMD). The proposed WCS offers a
comprehensive and interpretable framework for evaluating video generation
models on their ability to maintain a coherent "world" over time, addressing
gaps left by prior metrics focused only on visual fidelity or prompt alignment.

</details>


### [6] [GeoExplorer: Active Geo-localization with Curiosity-Driven Exploration](https://arxiv.org/abs/2508.00152)
*Li Mi,Manon Bechaz,Zeming Chen,Antoine Bosselut,Devis Tuia*

Main category: cs.CV

TL;DR: GeoExplorer提出了一种基于好奇心驱动探索的主动地理定位方法，通过内在奖励提升鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于距离奖励的强化学习方法在距离估计困难或面对新目标和环境时表现不佳，缺乏可靠探索策略。

Method: 引入好奇心驱动的内在奖励，实现目标无关的多样化探索，并通过环境建模提升探索效果。

Result: 在四个AGL基准测试中验证了GeoExplorer的有效性，尤其是在定位陌生目标和环境时表现优异。

Conclusion: GeoExplorer通过好奇心驱动探索显著提升了AGL任务的鲁棒性和泛化能力。

Abstract: Active Geo-localization (AGL) is the task of localizing a goal, represented
in various modalities (e.g., aerial images, ground-level images, or text),
within a predefined search area. Current methods approach AGL as a
goal-reaching reinforcement learning (RL) problem with a distance-based reward.
They localize the goal by implicitly learning to minimize the relative distance
from it. However, when distance estimation becomes challenging or when
encountering unseen targets and environments, the agent exhibits reduced
robustness and generalization ability due to the less reliable exploration
strategy learned during training. In this paper, we propose GeoExplorer, an AGL
agent that incorporates curiosity-driven exploration through intrinsic rewards.
Unlike distance-based rewards, our curiosity-driven reward is goal-agnostic,
enabling robust, diverse, and contextually relevant exploration based on
effective environment modeling. These capabilities have been proven through
extensive experiments across four AGL benchmarks, demonstrating the
effectiveness and generalization ability of GeoExplorer in diverse settings,
particularly in localizing unfamiliar targets and environments.

</details>


### [7] [Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs](https://arxiv.org/abs/2508.00169)
*Bhavya Goyal,Felipe Gutierrez-Barragan,Wei Lin,Andreas Velten,Yin Li,Mohit Gupta*

Main category: cs.CV

TL;DR: 论文提出了一种名为概率点云（PPC）的新型3D场景表示方法，通过为每个点添加概率属性来封装原始数据中的测量不确定性，从而提升3D物体检测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现代LiDAR在远距离或低反射率物体等场景中会产生稀疏或错误的点云，这些误差会传播到下游感知模型中，导致准确性下降。传统3D处理流程未保留原始测量中的不确定性信息。

Method: 提出PPC方法，为每个点添加概率属性以表示测量不确定性，并设计轻量级推理模块，用于鲁棒的3D物体检测。

Result: 通过仿真和实际数据验证，PPC方法在室内外场景中优于LiDAR及相机-LiDAR融合模型，尤其在处理小、远、低反射率物体及强环境光时表现突出。

Conclusion: PPC通过保留原始测量不确定性，显著提升了3D物体检测的鲁棒性和准确性，适用于多种挑战性场景。

Abstract: LiDAR-based 3D sensors provide point clouds, a canonical 3D representation
used in various scene understanding tasks. Modern LiDARs face key challenges in
several real-world scenarios, such as long-distance or low-albedo objects,
producing sparse or erroneous point clouds. These errors, which are rooted in
the noisy raw LiDAR measurements, get propagated to downstream perception
models, resulting in potentially severe loss of accuracy. This is because
conventional 3D processing pipelines do not retain any uncertainty information
from the raw measurements when constructing point clouds.
  We propose Probabilistic Point Clouds (PPC), a novel 3D scene representation
where each point is augmented with a probability attribute that encapsulates
the measurement uncertainty (or confidence) in the raw data. We further
introduce inference approaches that leverage PPC for robust 3D object
detection; these methods are versatile and can be used as computationally
lightweight drop-in modules in 3D inference pipelines. We demonstrate, via both
simulations and real captures, that PPC-based 3D inference methods outperform
several baselines using LiDAR as well as camera-LiDAR fusion models, across
challenging indoor and outdoor scenarios involving small, distant, and
low-albedo objects, as well as strong ambient light.
  Our project webpage is at https://bhavyagoyal.github.io/ppc .

</details>


### [8] [On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI](https://arxiv.org/abs/2508.00171)
*David Restrepo,Ira Ktena,Maria Vakalopoulou,Stergios Christodoulidis,Enzo Ferrante*

Main category: cs.CV

TL;DR: 论文提出了一种基于扰动的选择性模态转移（SMS）方法，用于量化视觉语言模型（VLMs）在二元分类任务中对图像和文本模态的依赖程度，揭示了模型对文本输入的显著偏好。


<details>
  <summary>Details</summary>
Motivation: 临床决策依赖于医学图像和相关临床报告的综合分析，但现有的视觉语言模型（VLMs）往往偏向于单一模态（尤其是文本），忽略了关键的视觉信息。

Method: 通过选择性模态转移（SMS）方法，系统地交换样本中的图像或文本，以暴露模型对特定模态的依赖。评估了六种开源VLMs在两种医学影像数据集上的表现。

Result: 研究发现，模型在未扰动和扰动设置下均表现出对文本输入的显著依赖，图像内容常被文本细节掩盖。

Conclusion: 强调设计和评估真正整合视觉和文本线索的多模态医学模型的重要性，而非依赖单一模态信号。

Abstract: Clinical decision-making relies on the integrated analysis of medical images
and the associated clinical reports. While Vision-Language Models (VLMs) can
offer a unified framework for such tasks, they can exhibit strong biases toward
one modality, frequently overlooking critical visual cues in favor of textual
information. In this work, we introduce Selective Modality Shifting (SMS), a
perturbation-based approach to quantify a model's reliance on each modality in
binary classification tasks. By systematically swapping images or text between
samples with opposing labels, we expose modality-specific biases. We assess six
open-source VLMs-four generalist models and two fine-tuned for medical data-on
two medical imaging datasets with distinct modalities: MIMIC-CXR (chest X-ray)
and FairVLMed (scanning laser ophthalmoscopy). By assessing model performance
and the calibration of every model in both unperturbed and perturbed settings,
we reveal a marked dependency on text input, which persists despite the
presence of complementary visual information. We also perform a qualitative
attention-based analysis which further confirms that image content is often
overshadowed by text details. Our findings highlight the importance of
designing and evaluating multimodal medical models that genuinely integrate
visual and textual cues, rather than relying on single-modality signals.

</details>


### [9] [Graph Lineages and Skeletal Graph Products](https://arxiv.org/abs/2508.00197)
*Eric Mjolsness,Cory B. Scott*

Main category: cs.CV

TL;DR: 论文定义了结构化图“谱系”，通过层次化增长支持高效代数操作，适用于机器学习和计算科学中的层次模型架构。


<details>
  <summary>Details</summary>
Motivation: 为机器学习和计算科学中的层次模型架构提供一种高效的图表示和操作方法。

Method: 定义层次化增长的图谱系，引入延长映射和骨架代数操作，推导空间高效的一元操作符。

Result: 提出了一种适用于层次图谱系的代数类型理论，并展示了在深度神经网络和多网格数值方法中的应用。

Conclusion: 该方法为层次模型架构和局部算法提供了高效的理论基础，具有广泛的应用潜力。

Abstract: Graphs, and sequences of growing graphs, can be used to specify the
architecture of mathematical models in many fields including machine learning
and computational science. Here we define structured graph "lineages" (ordered
by level number) that grow in a hierarchical fashion, so that: (1) the number
of graph vertices and edges increases exponentially in level number; (2)
bipartite graphs connect successive levels within a graph lineage and, as in
multigrid methods, can constrain matrices relating successive levels; (3) using
prolongation maps within a graph lineage, process-derived distance measures
between graphs at successive levels can be defined; (4) a category of "graded
graphs" can be defined, and using it low-cost "skeletal" variants of standard
algebraic graph operations and type constructors (cross product, box product,
disjoint sum, and function types) can be derived for graded graphs and hence
hierarchical graph lineages; (5) these skeletal binary operators have similar
but not identical algebraic and category-theoretic properties to their standard
counterparts; (6) graph lineages and their skeletal product constructors can
approach continuum limit objects. Additional space-efficient unary operators on
graded graphs are also derived: thickening, which creates a graph lineage of
multiscale graphs, and escalation to a graph lineage of search frontiers
(useful as a generalization of adaptive grids and in defining "skeletal"
functions). The result is an algebraic type theory for graded graphs and
(hierarchical) graph lineages. The approach is expected to be well suited to
defining hierarchical model architectures - "hierarchitectures" - and local
sampling, search, or optimization algorithms on them. We demonstrate such
application to deep neural networks (including visual and feature scale spaces)
and to multigrid numerical methods.

</details>


### [10] [Learning Personalised Human Internal Cognition from External Expressive Behaviours for Real Personality Recognition](https://arxiv.org/abs/2508.00205)
*Xiangyu Kong,Hengde Zhu,Haoqin Sun,Zhihao Guo,Jiayan Gu,Xinyi Ni,Wei Zhang,Shizhe Liu,Siyang Song*

Main category: cs.CV

TL;DR: 提出了一种基于个性化内部认知模拟的自动真实人格识别方法，通过短音频视频行为模拟认知，并利用2D-GNN进行人格推断。


<details>
  <summary>Details</summary>
Motivation: 现有方法多基于外部观察者推断人格印象，与真实人格偏差较大，性能不佳。本文受真实人格与内部认知关联启发，提出新方法。

Method: 模拟个性化内部认知，将其表示为网络权重，构建包含二维节点和边特征的图，提出2D-GNN进行人格推断，采用端到端训练策略。

Result: 通过模拟个性化认知和2D-GNN，提高了真实人格识别的性能。

Conclusion: 该方法通过模拟内部认知和2D-GNN，有效提升了真实人格识别的准确性。

Abstract: Automatic real personality recognition (RPR) aims to evaluate human real
personality traits from their expressive behaviours. However, most existing
solutions generally act as external observers to infer observers' personality
impressions based on target individuals' expressive behaviours, which
significantly deviate from their real personalities and consistently lead to
inferior recognition performance. Inspired by the association between real
personality and human internal cognition underlying the generation of
expressive behaviours, we propose a novel RPR approach that efficiently
simulates personalised internal cognition from easy-accessible external short
audio-visual behaviours expressed by the target individual. The simulated
personalised cognition, represented as a set of network weights that enforce
the personalised network to reproduce the individual-specific facial reactions,
is further encoded as a novel graph containing two-dimensional node and edge
feature matrices, with a novel 2D Graph Neural Network (2D-GNN) proposed for
inferring real personality traits from it. To simulate real personality-related
cognition, an end-to-end strategy is designed to jointly train our cognition
simulation, 2D graph construction, and personality recognition modules.

</details>


### [11] [SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters](https://arxiv.org/abs/2508.00213)
*Shayan Jalilian,Abdul Bais*

Main category: cs.CV

TL;DR: SAM-PTx通过轻量级适配器设计，将CLIP文本嵌入引入SAM模型，提升语义引导的分割性能。


<details>
  <summary>Details</summary>
Motivation: 探索语义文本提示在分割任务中的潜力，弥补传统空间提示的不足。

Method: 提出Parallel-Text适配器，将冻结的CLIP文本嵌入注入SAM的图像编码器，仅修改MLP并行分支。

Result: 在COD10K、COCO和ADE20K数据集上，语义提示显著优于纯空间提示基线。

Conclusion: 语义条件集成是高效适应SAM的可行路径，计算复杂度低。

Abstract: The Segment Anything Model (SAM) has demonstrated impressive generalization
in prompt-based segmentation. Yet, the potential of semantic text prompts
remains underexplored compared to traditional spatial prompts like points and
boxes. This paper introduces SAM-PTx, a parameter-efficient approach for
adapting SAM using frozen CLIP-derived text embeddings as class-level semantic
guidance. Specifically, we propose a lightweight adapter design called
Parallel-Text that injects text embeddings into SAM's image encoder, enabling
semantics-guided segmentation while keeping most of the original architecture
frozen. Our adapter modifies only the MLP-parallel branch of each transformer
block, preserving the attention pathway for spatial reasoning. Through
supervised experiments and ablations on the COD10K dataset as well as low-data
subsets of COCO and ADE20K, we show that incorporating fixed text embeddings as
input improves segmentation performance over purely spatial prompt baselines.
To our knowledge, this is the first work to use text prompts for segmentation
on the COD10K dataset. These results suggest that integrating semantic
conditioning into SAM's architecture offers a practical and scalable path for
efficient adaptation with minimal computational complexity.

</details>


### [12] [Object-Centric Cropping for Visual Few-Shot Classification](https://arxiv.org/abs/2508.00218)
*Aymane Abdali,Bartosz Boguslawski,Lucas Drumetz,Vincent Gripon*

Main category: cs.CV

TL;DR: 在少样本图像分类中，通过加入物体局部位置信息显著提升性能，Segment Anything Model和无需监督的前景提取方法也能实现大部分改进。


<details>
  <summary>Details</summary>
Motivation: 少样本图像分类中，图像模糊（如多物体或复杂背景）会显著降低性能，因此需要改进方法。

Method: 利用物体在图像中的局部位置信息，结合Segment Anything Model或无需监督的前景提取方法。

Result: 分类性能在多个基准测试中显著提升。

Conclusion: 局部位置信息和简单标注或无需监督的方法能有效提升少样本分类性能。

Abstract: In the domain of Few-Shot Image Classification, operating with as little as
one example per class, the presence of image ambiguities stemming from multiple
objects or complex backgrounds can significantly deteriorate performance. Our
research demonstrates that incorporating additional information about the local
positioning of an object within its image markedly enhances classification
across established benchmarks. More importantly, we show that a significant
fraction of the improvement can be achieved through the use of the Segment
Anything Model, requiring only a pixel of the object of interest to be pointed
out, or by employing fully unsupervised foreground object extraction methods.

</details>


### [13] [Guided Depth Map Super-Resolution via Multi-Scale Fusion U-shaped Mamba Network](https://arxiv.org/abs/2508.00248)
*Chenggang Guo,Hao Xu,XianMing Wan*

Main category: cs.CV

TL;DR: 提出了一种多尺度融合U形Mamba模型（MSF-UM），用于深度图超分辨率，结合Mamba的高效状态空间建模能力和多尺度融合策略，显著提升重建精度并减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 传统卷积神经网络在处理长距离依赖和全局上下文信息时存在局限，而Transformer的计算复杂度和内存消耗较高，限制了其在高分辨率深度图处理中的应用。

Method: 设计了一种结合残差密集通道注意力块和Mamba状态空间模块的结构，利用多尺度跨模态融合策略，通过彩色图像的高频纹理信息引导深度图超分辨率。

Result: 在多个公开数据集上的实验表明，MSF-UM在减少参数数量的同时，显著提升了重建精度，并展现出优异的泛化能力。

Conclusion: MSF-UM模型通过结合局部特征提取和长距离依赖建模，以及多尺度融合策略，为深度图超分辨率任务提供了一种高效且准确的解决方案。

Abstract: Depth map super-resolution technology aims to improve the spatial resolution
of low-resolution depth maps and effectively restore high-frequency detail
information. Traditional convolutional neural network has limitations in
dealing with long-range dependencies and are unable to fully model the global
contextual information in depth maps. Although transformer can model global
dependencies, its computational complexity and memory consumption are
quadratic, which significantly limits its ability to process high-resolution
depth maps. In this paper, we propose a multi-scale fusion U-shaped Mamba
(MSF-UM) model, a novel guided depth map super-resolution framework. The core
innovation of this model is to integrate Mamba's efficient state-space modeling
capabilities into a multi-scale U-shaped fusion structure guided by a color
image. The structure combining the residual dense channel attention block and
the Mamba state space module is designed, which combines the local feature
extraction capability of the convolutional layer with the modeling advantage of
the state space model for long-distance dependencies. At the same time, the
model adopts a multi-scale cross-modal fusion strategy to make full use of the
high-frequency texture information from the color image to guide the
super-resolution process of the depth map. Compared with existing mainstream
methods, the proposed MSF-UM significantly reduces the number of model
parameters while achieving better reconstruction accuracy. Extensive
experiments on multiple publicly available datasets validate the effectiveness
of the model, especially showing excellent generalization ability in the task
of large-scale depth map super-resolution.

</details>


### [14] [PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting](https://arxiv.org/abs/2508.00259)
*Wentao Sun,Hanqing Xu,Quanyun Wu,Dedong Zhang,Yiping Chen,Lingfei Ma,John S. Zelek,Jonathan Li*

Main category: cs.CV

TL;DR: PointGauss是一种基于点云引导的实时多目标分割框架，通过直接解析高斯基元实现高效3D分割，显著提升了多视图一致性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在初始化时间长和多视图一致性差的问题，PointGauss旨在通过点云分割驱动的流程解决这些问题。

Method: 提出点云高斯基元解码器和GPU加速的2D掩码渲染系统，实现快速3D分割和多视图一致性。

Result: 实验显示在多视图mIoU上性能提升1.89%至31.78%，同时保持高效计算。

Conclusion: PointGauss在3D分割中表现出色，并提出了新数据集DesktopObjects-360以解决现有基准的局限性。

Abstract: We introduce PointGauss, a novel point cloud-guided framework for real-time
multi-object segmentation in Gaussian Splatting representations. Unlike
existing methods that suffer from prolonged initialization and limited
multi-view consistency, our approach achieves efficient 3D segmentation by
directly parsing Gaussian primitives through a point cloud segmentation-driven
pipeline. The key innovation lies in two aspects: (1) a point cloud-based
Gaussian primitive decoder that generates 3D instance masks within 1 minute,
and (2) a GPU-accelerated 2D mask rendering system that ensures multi-view
consistency. Extensive experiments demonstrate significant improvements over
previous state-of-the-art methods, achieving performance gains of 1.89 to
31.78% in multi-view mIoU, while maintaining superior computational efficiency.
To address the limitations of current benchmarks (single-object focus,
inconsistent 3D evaluation, small scale, and partial coverage), we present
DesktopObjects-360, a novel comprehensive dataset for 3D segmentation in
radiance fields, featuring: (1) complex multi-object scenes, (2) globally
consistent 2D annotations, (3) large-scale training data (over 27 thousand 2D
masks), (4) full 360{\deg} coverage, and (5) 3D evaluation masks.

</details>


### [15] [Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models](https://arxiv.org/abs/2508.00260)
*Hyundong Jin,Hyung Jin Chang,Eunwoo Kim*

Main category: cs.CV

TL;DR: 提出了一种新框架，通过混合视觉投影器和专家推荐策略，解决生成式视觉语言模型在持续学习中忽视语言指令的问题，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在持续学习中可能过度关注视觉输入而忽视语言指令，尤其是在重复文本指令任务中。

Method: 引入混合视觉投影器作为专家，基于指令上下文翻译视觉信息，并提出专家推荐和修剪策略。

Result: 在多样化视觉语言任务中，该方法生成更符合指令的响应，优于现有持续学习方法。

Conclusion: 该框架有效平衡了视觉和语言信息的处理，提升了持续学习性能。

Abstract: Continual learning enables pre-trained generative vision-language models
(VLMs) to incorporate knowledge from new tasks without retraining data from
previous ones. Recent methods update a visual projector to translate visual
information for new tasks, connecting pre-trained vision encoders with large
language models. However, such adjustments may cause the models to prioritize
visual inputs over language instructions, particularly learning tasks with
repetitive types of textual instructions. To address the neglect of language
instructions, we propose a novel framework that grounds the translation of
visual information on instructions for language models. We introduce a mixture
of visual projectors, each serving as a specialized visual-to-language
translation expert based on the given instruction context to adapt to new
tasks. To avoid using experts for irrelevant instruction contexts, we propose
an expert recommendation strategy that reuses experts for tasks similar to
those previously learned. Additionally, we introduce expert pruning to
alleviate interference from the use of experts that cumulatively activated in
previous tasks. Extensive experiments on diverse vision-language tasks
demonstrate that our method outperforms existing continual learning approaches
by generating instruction-following responses.

</details>


### [16] [Multimodal Referring Segmentation: A Survey](https://arxiv.org/abs/2508.00265)
*Henghui Ding,Song Tang,Shuting He,Chang Liu,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文综述了多模态指代分割任务，涵盖其背景、方法、性能比较及实际应用。


<details>
  <summary>Details</summary>
Motivation: 多模态指代分割在基于用户指令的精确对象感知应用中至关重要，近年来因卷积神经网络、Transformer和大语言模型的进步而备受关注。

Method: 总结了统一的元架构，并回顾了图像、视频和3D场景中的代表性方法，同时讨论了广义指代表达（GREx）方法。

Result: 提供了标准基准上的性能比较，并持续跟踪相关研究。

Conclusion: 多模态指代分割在理论和应用上均有重要意义，未来仍需应对现实世界的复杂性挑战。

Abstract: Multimodal referring segmentation aims to segment target objects in visual
scenes, such as images, videos, and 3D scenes, based on referring expressions
in text or audio format. This task plays a crucial role in practical
applications requiring accurate object perception based on user instructions.
Over the past decade, it has gained significant attention in the multimodal
community, driven by advances in convolutional neural networks, transformers,
and large language models, all of which have substantially improved multimodal
perception capabilities. This paper provides a comprehensive survey of
multimodal referring segmentation. We begin by introducing this field's
background, including problem definitions and commonly used datasets. Next, we
summarize a unified meta architecture for referring segmentation and review
representative methods across three primary visual scenes, including images,
videos, and 3D scenes. We further discuss Generalized Referring Expression
(GREx) methods to address the challenges of real-world complexity, along with
related tasks and practical applications. Extensive performance comparisons on
standard benchmarks are also provided. We continually track related works at
https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.

</details>


### [17] [Towards Robust Semantic Correspondence: A Benchmark and Insights](https://arxiv.org/abs/2508.00272)
*Wenyue Chong*

Main category: cs.CV

TL;DR: 论文提出了一个评估语义对应在恶劣条件下鲁棒性的新基准，发现现有方法在挑战性场景中表现下降，并提出了一些关键见解。


<details>
  <summary>Details</summary>
Motivation: 语义对应是计算机视觉中的基础任务，但在恶劣条件下的鲁棒性研究较少，因此需要建立一个新基准。

Method: 通过构建包含14种挑战性场景的数据集，评估现有方法的鲁棒性，并分析大规模视觉模型的效果。

Result: 现有方法在恶劣条件下表现下降，DINO模型优于Stable Diffusion，融合两者可提升绝对鲁棒性。

Conclusion: 任务特定的设计对提升鲁棒性至关重要，通用数据增强效果有限。

Abstract: Semantic correspondence aims to identify semantically meaningful
relationships between different images and is a fundamental challenge in
computer vision. It forms the foundation for numerous tasks such as 3D
reconstruction, object tracking, and image editing. With the progress of
large-scale vision models, semantic correspondence has achieved remarkable
performance in controlled and high-quality conditions. However, the robustness
of semantic correspondence in challenging scenarios is much less investigated.
In this work, we establish a novel benchmark for evaluating semantic
correspondence in adverse conditions. The benchmark dataset comprises 14
distinct challenging scenarios that reflect commonly encountered imaging
issues, including geometric distortion, image blurring, digital artifacts, and
environmental occlusion. Through extensive evaluations, we provide several key
insights into the robustness of semantic correspondence approaches: (1) All
existing methods suffer from noticeable performance drops under adverse
conditions; (2) Using large-scale vision models can enhance overall robustness,
but fine-tuning on these models leads to a decline in relative robustness; (3)
The DINO model outperforms the Stable Diffusion in relative robustness, and
their fusion achieves better absolute robustness; Moreover, We evaluate common
robustness enhancement strategies for semantic correspondence and find that
general data augmentations are ineffective, highlighting the need for
task-specific designs. These results are consistent across both our dataset and
real-world benchmarks.

</details>


### [18] [Privacy-Preserving Driver Drowsiness Detection with Spatial Self-Attention and Federated Learning](https://arxiv.org/abs/2508.00287)
*Tran Viet Khoa,Do Hai Son,Mohammad Abu Alsheikh,Yibeltal F Alem,Dinh Thai Hoang*

Main category: cs.CV

TL;DR: 提出了一种基于空间自注意力机制和LSTM的驾驶员疲劳检测框架，结合联邦学习和梯度相似性比较，实现了89.9%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 驾驶员疲劳是交通事故的主要原因之一，但在现实场景中，分散且多样化的面部数据使得准确检测具有挑战性。

Method: 开发了空间自注意力机制（SSA）与LSTM结合的网络，并采用梯度相似性比较（GSC）优化联邦学习模型聚合。

Result: 在联邦学习环境中实现了89.9%的检测准确率，优于现有方法。

Conclusion: 该框架能有效处理现实数据多样性，有望应用于智能交通系统以提升道路安全。

Abstract: Driver drowsiness is one of the main causes of road accidents and is
recognized as a leading contributor to traffic-related fatalities. However,
detecting drowsiness accurately remains a challenging task, especially in
real-world settings where facial data from different individuals is
decentralized and highly diverse. In this paper, we propose a novel framework
for drowsiness detection that is designed to work effectively with
heterogeneous and decentralized data. Our approach develops a new Spatial
Self-Attention (SSA) mechanism integrated with a Long Short-Term Memory (LSTM)
network to better extract key facial features and improve detection
performance. To support federated learning, we employ a Gradient Similarity
Comparison (GSC) that selects the most relevant trained models from different
operators before aggregation. This improves the accuracy and robustness of the
global model while preserving user privacy. We also develop a customized tool
that automatically processes video data by extracting frames, detecting and
cropping faces, and applying data augmentation techniques such as rotation,
flipping, brightness adjustment, and zooming. Experimental results show that
our framework achieves a detection accuracy of 89.9% in the federated learning
settings, outperforming existing methods under various deployment scenarios.
The results demonstrate the effectiveness of our approach in handling
real-world data variability and highlight its potential for deployment in
intelligent transportation systems to enhance road safety through early and
reliable drowsiness detection.

</details>


### [19] [Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence](https://arxiv.org/abs/2508.00299)
*Danzhen Fu,Jiagao Hu,Daiguo Zhou,Fei Wang,Zepeng Wang,Wenhua Liao*

Main category: cs.CV

TL;DR: 提出了一种多视角行人视频编辑框架，通过视频修复和运动控制技术增强自动驾驶训练数据的危险场景表示。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶系统中行人检测模型因训练数据缺乏危险场景表示而鲁棒性不足的问题。

Method: 识别多视角行人区域，扩展检测框并拼接为统一画布，通过姿态序列控制条件进行行人编辑（插入、替换、移除）。

Result: 实验表明框架能高质量完成行人编辑，具有视觉真实感、时空一致性和多视角一致性。

Conclusion: 该方法为多视角行人视频生成提供了鲁棒且通用的解决方案，适用于数据增强和场景模拟。

Abstract: Pedestrian detection models in autonomous driving systems often lack
robustness due to insufficient representation of dangerous pedestrian scenarios
in training datasets. To address this limitation, we present a novel framework
for controllable pedestrian video editing in multi-view driving scenarios by
integrating video inpainting and human motion control techniques. Our approach
begins by identifying pedestrian regions of interest across multiple camera
views, expanding detection bounding boxes with a fixed ratio, and resizing and
stitching these regions into a unified canvas while preserving cross-view
spatial relationships. A binary mask is then applied to designate the editable
area, within which pedestrian editing is guided by pose sequence control
conditions. This enables flexible editing functionalities, including pedestrian
insertion, replacement, and removal. Extensive experiments demonstrate that our
framework achieves high-quality pedestrian editing with strong visual realism,
spatiotemporal coherence, and cross-view consistency. These results establish
the proposed method as a robust and versatile solution for multi-view
pedestrian video generation, with broad potential for applications in data
augmentation and scenario simulation in autonomous driving.

</details>


### [20] [TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models](https://arxiv.org/abs/2508.00289)
*Christian Simon,Masato Ishii,Akio Hayakawa,Zhi Zhong,Shusuke Takahashi,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 提出了一种名为TITAN-Guide的训练无关引导方法，优化了文本到视频扩散模型的内存使用和控制效果。


<details>
  <summary>Details</summary>
Motivation: 现有训练无关引导框架存在内存占用高或控制效果不佳的问题，限制了其在计算密集型任务（如文本到视频扩散模型）中的应用。

Method: 开发了一种无需反向传播的高效扩散潜在优化方法，研究了前向梯度下降及其方向性指导选项。

Result: 实验表明，该方法在内存管理和潜在优化方面优于现有方法，显著提升了文本到视频扩散模型的性能。

Conclusion: TITAN-Guide方法不仅降低了内存需求，还在多个扩散引导基准测试中表现出色。

Abstract: In the recent development of conditional diffusion models still require heavy
supervised fine-tuning for performing control on a category of tasks.
Training-free conditioning via guidance with off-the-shelf models is a
favorable alternative to avoid further fine-tuning on the base model. However,
the existing training-free guidance frameworks either have heavy memory
requirements or offer sub-optimal control due to rough estimation. These
shortcomings limit the applicability to control diffusion models that require
intense computation, such as Text-to-Video (T2V) diffusion models. In this
work, we propose Taming Inference Time Alignment for Guided Text-to-Video
Diffusion Model, so-called TITAN-Guide, which overcomes memory space issues,
and provides more optimal control in the guidance process compared to the
counterparts. In particular, we develop an efficient method for optimizing
diffusion latents without backpropagation from a discriminative guiding model.
In particular, we study forward gradient descents for guided diffusion tasks
with various options on directional directives. In our experiments, we
demonstrate the effectiveness of our approach in efficiently managing memory
during latent optimization, while previous methods fall short. Our proposed
approach not only minimizes memory requirements but also significantly enhances
T2V performance across a range of diffusion guidance benchmarks. Code, models,
and demo are available at https://titanguide.github.io.

</details>


### [21] [Reducing the gap between general purpose data and aerial images in concentrated solar power plants](https://arxiv.org/abs/2508.00440)
*M. A. Pérez-Cutiño,J. Valverde,J. Capitán,J. M. Díaz-Báñez*

Main category: cs.CV

TL;DR: 提出AerialCSP虚拟数据集，用于CSP电站的无人机图像分析，减少对大量标注数据的需求。


<details>
  <summary>Details</summary>
Motivation: CSP电站的无人机图像具有高反射性和领域特殊性，传统数据集训练的模型难以泛化，标注成本高。

Method: 创建AerialCSP合成数据集，模拟真实CSP电站图像，用于模型预训练。

Result: AerialCSP显著提升真实故障检测效果，减少标注需求。

Conclusion: AerialCSP为CSP电站视觉任务提供高效解决方案。

Abstract: In the context of Concentrated Solar Power (CSP) plants, aerial images
captured by drones present a unique set of challenges. Unlike urban or natural
landscapes commonly found in existing datasets, solar fields contain highly
reflective surfaces, and domain-specific elements that are uncommon in
traditional computer vision benchmarks. As a result, machine learning models
trained on generic datasets struggle to generalize to this setting without
extensive retraining and large volumes of annotated data. However, collecting
and labeling such data is costly and time-consuming, making it impractical for
rapid deployment in industrial applications.
  To address this issue, we propose a novel approach: the creation of
AerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By
generating synthetic data that closely mimic real-world conditions, our
objective is to facilitate pretraining of models before deployment,
significantly reducing the need for extensive manual labeling. Our main
contributions are threefold: (1) we introduce AerialCSP, a high-quality
synthetic dataset for aerial inspection of CSP plants, providing annotated data
for object detection and image segmentation; (2) we benchmark multiple models
on AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we
demonstrate that pretraining on AerialCSP significantly improves real-world
fault detection, particularly for rare and small defects, reducing the need for
extensive manual labeling. AerialCSP is made publicly available at
https://mpcutino.github.io/aerialcsp/.

</details>


### [22] [AniMer+: Unified Pose and Shape Estimation Across Mammalia and Aves via Family-Aware Transformer](https://arxiv.org/abs/2508.00298)
*Jin Lyu,Liang An,Li Lin,Pujin Cheng,Yebin Liu,Xiaoying Tang*

Main category: cs.CV

TL;DR: AniMer+ 是一个扩展的框架，通过高容量的 Vision Transformer 和 Mixture-of-Experts 设计，实现了对哺乳动物和鸟类的统一姿态与形状重建。为解决数据稀缺问题，提出了基于扩散模型的合成数据生成方法，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 在基础模型时代，通过单一网络实现对不同动态物体的统一理解具有重要意义。同时，准确估计动物姿态和形状对生物学研究至关重要，但现有方法受限于网络容量和数据稀缺。

Method: AniMer+ 采用高容量的 Vision Transformer 结合 Mixture-of-Experts 设计，将网络层分为物种特定和共享部分。为解决数据不足，提出了扩散模型生成合成数据集 CtrlAni3D 和 CtrlAVES3D。

Result: 在包含 41.3k 哺乳动物和 12.4k 鸟类图像的混合数据集上训练后，AniMer+ 在多个基准测试中表现优于现有方法，包括具有挑战性的 Animal Kingdom 数据集。

Conclusion: AniMer+ 的网络架构和合成数据集有效提升了真实场景下的性能，为多物种姿态与形状重建提供了新思路。

Abstract: In the era of foundation models, achieving a unified understanding of
different dynamic objects through a single network has the potential to empower
stronger spatial intelligence. Moreover, accurate estimation of animal pose and
shape across diverse species is essential for quantitative analysis in
biological research. However, this topic remains underexplored due to the
limited network capacity of previous methods and the scarcity of comprehensive
multi-species datasets. To address these limitations, we introduce AniMer+, an
extended version of our scalable AniMer framework. In this paper, we focus on a
unified approach for reconstructing mammals (mammalia) and birds (aves). A key
innovation of AniMer+ is its high-capacity, family-aware Vision Transformer
(ViT) incorporating a Mixture-of-Experts (MoE) design. Its architecture
partitions network layers into taxa-specific components (for mammalia and aves)
and taxa-shared components, enabling efficient learning of both distinct and
common anatomical features within a single model. To overcome the critical
shortage of 3D training data, especially for birds, we introduce a
diffusion-based conditional image generation pipeline. This pipeline produces
two large-scale synthetic datasets: CtrlAni3D for quadrupeds and CtrlAVES3D for
birds. To note, CtrlAVES3D is the first large-scale, 3D-annotated dataset for
birds, which is crucial for resolving single-view depth ambiguities. Trained on
an aggregated collection of 41.3k mammalian and 12.4k avian images (combining
real and synthetic data), our method demonstrates superior performance over
existing approaches across a wide range of benchmarks, including the
challenging out-of-domain Animal Kingdom dataset. Ablation studies confirm the
effectiveness of both our novel network architecture and the generated
synthetic datasets in enhancing real-world application performance.

</details>


### [23] [Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving](https://arxiv.org/abs/2508.00589)
*Stefan Englmeier,Max A. Büttner,Katharina Winter,Fabian B. Flohr*

Main category: cs.CV

TL;DR: 提出了一种基于上下文感知的运动检索框架，用于从大规模数据集中高效检索罕见的人类行为场景，以支持自动驾驶系统的评估。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需在涉及弱势道路使用者（VRUs）的复杂场景中可靠运行，但现有数据集中罕见行为的检索具有挑战性。

Method: 结合SMPL运动序列和视频帧，编码到与自然语言对齐的多模态嵌入空间，支持通过文本查询检索人类行为及其上下文。

Result: 在WayMoCo数据集上，方法在运动-上下文检索中的准确率比现有最优模型高27.5%。

Conclusion: 提出的框架和数据集WayMoCo为自动驾驶系统在多样化人类行为场景中的评估提供了有效工具。

Abstract: Autonomous driving systems must operate reliably in safety-critical
scenarios, particularly those involving unusual or complex behavior by
Vulnerable Road Users (VRUs). Identifying these edge cases in driving datasets
is essential for robust evaluation and generalization, but retrieving such rare
human behavior scenarios within the long tail of large-scale datasets is
challenging. To support targeted evaluation of autonomous driving systems in
diverse, human-centered scenarios, we propose a novel context-aware motion
retrieval framework. Our method combines Skinned Multi-Person Linear
(SMPL)-based motion sequences and corresponding video frames before encoding
them into a shared multimodal embedding space aligned with natural language.
Our approach enables the scalable retrieval of human behavior and their context
through text queries. This work also introduces our dataset WayMoCo, an
extension of the Waymo Open Dataset. It contains automatically labeled motion
and scene context descriptions derived from generated pseudo-ground-truth SMPL
sequences and corresponding image data. Our approach outperforms
state-of-the-art models by up to 27.5% accuracy in motion-context retrieval,
when evaluated on the WayMoCo dataset.

</details>


### [24] [IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation](https://arxiv.org/abs/2508.00823)
*Wenxuan Guo,Xiuwei Xu,Hang Yin,Ziwei Wang,Jianjiang Feng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: IGL-Nav提出了一种基于可渲染3D高斯表示（3DGS）的增量式3D高斯定位框架，用于高效且3D感知的图像目标导航。


<details>
  <summary>Details</summary>
Motivation: 传统方法无法充分建模探索的3D环境与目标图像之间的几何关系，且计算效率低。

Method: 通过增量更新场景表示、粗定位目标（利用几何信息进行离散空间匹配）和精细优化目标姿态（通过可微分渲染）实现导航。

Result: IGL-Nav在多种实验配置中大幅优于现有方法，并能处理更具挑战性的自由视角图像目标设置。

Conclusion: IGL-Nav是一种高效且3D感知的图像目标导航方法，适用于实际机器人平台。

Abstract: Visual navigation with an image as goal is a fundamental and challenging
problem. Conventional methods either rely on end-to-end RL learning or
modular-based policy with topological graph or BEV map as memory, which cannot
fully model the geometric relationship between the explored 3D environment and
the goal image. In order to efficiently and accurately localize the goal image
in 3D space, we build our navigation system upon the renderable 3D gaussian
(3DGS) representation. However, due to the computational intensity of 3DGS
optimization and the large search space of 6-DoF camera pose, directly
leveraging 3DGS for image localization during agent exploration process is
prohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D
Gaussian Localization framework for efficient and 3D-aware image-goal
navigation. Specifically, we incrementally update the scene representation as
new images arrive with feed-forward monocular prediction. Then we coarsely
localize the goal by leveraging the geometric information for discrete space
matching, which can be equivalent to efficient 3D convolution. When the agent
is close to the goal, we finally solve the fine target pose with optimization
via differentiable rendering. The proposed IGL-Nav outperforms existing
state-of-the-art methods by a large margin across diverse experimental
configurations. It can also handle the more challenging free-view image-goal
setting and be deployed on real-world robotic platform using a cellphone to
capture goal image at arbitrary pose. Project page:
https://gwxuan.github.io/IGL-Nav/.

</details>


### [25] [Exploring Fourier Prior and Event Collaboration for Low-Light Image Enhancement](https://arxiv.org/abs/2508.00308)
*Chunyan She,Fujun Han,Chengyu Fang,Shukai Duan,Lidan Wang*

Main category: cs.CV

TL;DR: 论文提出了一种基于事件相机的低光图像增强方法，通过解耦增强流程为可见性恢复和结构细化两阶段，利用动态对齐和对比损失提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分利用事件相机和帧相机的模态优势，限制了性能。

Method: 1. 可见性恢复网络（利用傅里叶空间的振幅-相位关系）；2. 动态对齐融合策略；3. 空间频率插值生成负样本，设计对比损失。

Result: 实验表明，该方法优于现有最优模型。

Conclusion: 通过解耦增强流程和动态对齐策略，显著提升了低光图像增强的性能。

Abstract: The event camera, benefiting from its high dynamic range and low latency,
provides performance gain for low-light image enhancement. Unlike frame-based
cameras, it records intensity changes with extremely high temporal resolution,
capturing sufficient structure information. Currently, existing event-based
methods feed a frame and events directly into a single model without fully
exploiting modality-specific advantages, which limits their performance.
Therefore, by analyzing the role of each sensing modality, the enhancement
pipeline is decoupled into two stages: visibility restoration and structure
refinement. In the first stage, we design a visibility restoration network with
amplitude-phase entanglement by rethinking the relationship between amplitude
and phase components in Fourier space. In the second stage, a fusion strategy
with dynamic alignment is proposed to mitigate the spatial mismatch caused by
the temporal resolution discrepancy between two sensing modalities, aiming to
refine the structure information of the image enhanced by the visibility
restoration network. In addition, we utilize spatial-frequency interpolation to
simulate negative samples with diverse illumination, noise and artifact
degradations, thereby developing a contrastive loss that encourages the model
to learn discriminative representations. Experiments demonstrate that the
proposed method outperforms state-of-the-art models.

</details>


### [26] [DocTron-Formula: Generalized Formula Recognition in Complex and Structured Scenarios](https://arxiv.org/abs/2508.00311)
*Yufeng Zhong,Zhixiong Zeng,Lei Chen,Longrong Yang,Liming Zheng,Jing Huang,Siqi Yang,Lin Ma*

Main category: cs.CV

TL;DR: DocTron-Formula是一个基于通用视觉语言模型的统一框架，用于数学公式OCR，无需专用架构。结合CSFormula数据集，通过监督微调实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 数学公式OCR在科学文献智能分析中至关重要，但现有模型难以处理数学内容的结构多样性和复杂性。

Method: 提出DocTron-Formula框架，利用通用视觉语言模型，并引入CSFormula数据集进行监督微调。

Result: 方法在多种风格、科学领域和复杂布局中达到最先进性能，超越专用模型。

Conclusion: DocTron-Formula为复杂科学文档的自动化理解提供了新范式。

Abstract: Optical Character Recognition (OCR) for mathematical formula is essential for
the intelligent analysis of scientific literature. However, both task-specific
and general vision-language models often struggle to handle the structural
diversity, complexity, and real-world variability inherent in mathematical
content. In this work, we present DocTron-Formula, a unified framework built
upon general vision-language models, thereby eliminating the need for
specialized architectures. Furthermore, we introduce CSFormula, a large-scale
and challenging dataset that encompasses multidisciplinary and structurally
complex formulas at the line, paragraph, and page levels. Through
straightforward supervised fine-tuning, our approach achieves state-of-the-art
performance across a variety of styles, scientific domains, and complex
layouts. Experimental results demonstrate that our method not only surpasses
specialized models in terms of accuracy and robustness, but also establishes a
new paradigm for the automated understanding of complex scientific documents.

</details>


### [27] [GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.00312)
*Suhang Cai,Xiaohao Peng,Chong Wang,Xiaojie Cai,Jiangbo Qian*

Main category: cs.CV

TL;DR: 提出了一种基于生成视频的弱监督视频异常检测框架（GV-VAD），通过文本条件视频生成模型生成可控且真实的合成视频，低成本增强训练数据，并在UCF-Crime数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决真实异常数据的稀缺性、不可预测性和高标注成本问题，提升视频异常检测模型的性能和泛化能力。

Method: 利用文本条件视频生成模型生成可控且真实的合成视频，并通过合成样本损失缩放策略控制其对训练的影响。

Result: 在UCF-Crime数据集上表现优于现有方法。

Conclusion: GV-VAD框架通过低成本生成合成视频增强训练数据，显著提升了视频异常检测的性能和泛化能力。

Abstract: Video anomaly detection (VAD) plays a critical role in public safety
applications such as intelligent surveillance. However, the rarity,
unpredictability, and high annotation cost of real-world anomalies make it
difficult to scale VAD datasets, which limits the performance and
generalization ability of existing models. To address this challenge, we
propose a generative video-enhanced weakly-supervised video anomaly detection
(GV-VAD) framework that leverages text-conditioned video generation models to
produce semantically controllable and physically plausible synthetic videos.
These virtual videos are used to augment training data at low cost. In
addition, a synthetic sample loss scaling strategy is utilized to control the
influence of generated synthetic samples for efficient training. The
experiments show that the proposed framework outperforms state-of-the-art
methods on UCF-Crime datasets. The code is available at
https://github.com/Sumutan/GV-VAD.git.

</details>


### [28] [Steering Guidance for Personalized Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.00319)
*Sunghyun Park,Seokeon Choi,Hyoungwoo Park,Sungrack Yun*

Main category: cs.CV

TL;DR: 提出了一种个性化引导方法，通过未学习的弱模型和动态权重插值，平衡目标分布对齐和文本编辑能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如CFG和AG）在个性化文本到图像扩散模型中无法有效平衡目标分布对齐和原始模型知识保留。

Method: 利用未学习的弱模型和动态权重插值，在推理过程中控制未学习程度。

Result: 实验表明，该方法能提升文本对齐和目标分布保真度，且无需额外计算开销。

Conclusion: 个性化引导方法简单有效，适用于多种微调策略，显著优于现有方法。

Abstract: Personalizing text-to-image diffusion models is crucial for adapting the
pre-trained models to specific target concepts, enabling diverse image
generation. However, fine-tuning with few images introduces an inherent
trade-off between aligning with the target distribution (e.g., subject
fidelity) and preserving the broad knowledge of the original model (e.g., text
editability). Existing sampling guidance methods, such as classifier-free
guidance (CFG) and autoguidance (AG), fail to effectively guide the output
toward well-balanced space: CFG restricts the adaptation to the target
distribution, while AG compromises text alignment. To address these
limitations, we propose personalization guidance, a simple yet effective method
leveraging an unlearned weak model conditioned on a null text prompt. Moreover,
our method dynamically controls the extent of unlearning in a weak model
through weight interpolation between pre-trained and fine-tuned models during
inference. Unlike existing guidance methods, which depend solely on guidance
scales, our method explicitly steers the outputs toward a balanced latent space
without additional computational overhead. Experimental results demonstrate
that our proposed guidance can improve text alignment and target distribution
fidelity, integrating seamlessly with various fine-tuning strategies.

</details>


### [29] [Spectral Sensitivity Estimation with an Uncalibrated Diffraction Grating](https://arxiv.org/abs/2508.00330)
*Lilika Makabe,Hiroaki Santo,Fumio Okura,Michael S. Brown,Yasuyuki Matsushita*

Main category: cs.CV

TL;DR: 提出了一种使用衍射光栅校准相机光谱灵敏度的实用方法，无需窄带滤光片或已知光谱反射率的目标。


<details>
  <summary>Details</summary>
Motivation: 相机光谱灵敏度的精确校准对颜色校正、光照估计和材料分析等计算机视觉任务至关重要。现有方法需要特殊设备，而本方法仅需未校准的衍射光栅片。

Method: 通过捕获直接光照及其通过衍射光栅片的衍射图案图像，以闭式方法估计相机光谱灵敏度和光栅参数。

Result: 在合成和真实数据上的实验表明，该方法优于传统的基于参考目标的方法。

Conclusion: 该方法高效实用，为相机光谱灵敏度校准提供了一种简便的解决方案。

Abstract: This paper introduces a practical and accurate calibration method for camera
spectral sensitivity using a diffraction grating. Accurate calibration of
camera spectral sensitivity is crucial for various computer vision tasks,
including color correction, illumination estimation, and material analysis.
Unlike existing approaches that require specialized narrow-band filters or
reference targets with known spectral reflectances, our method only requires an
uncalibrated diffraction grating sheet, readily available off-the-shelf. By
capturing images of the direct illumination and its diffracted pattern through
the grating sheet, our method estimates both the camera spectral sensitivity
and the diffraction grating parameters in a closed-form manner. Experiments on
synthetic and real-world data demonstrate that our method outperforms
conventional reference target-based methods, underscoring its effectiveness and
practicality.

</details>


### [30] [Analyze-Prompt-Reason: A Collaborative Agent-Based Framework for Multi-Image Vision-Language Reasoning](https://arxiv.org/abs/2508.00356)
*Angelos Vlachos,Giorgos Filandrianos,Maria Lymperaiou,Nikolaos Spanos,Ilias Mitsouras,Vasileios Karampinis,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: 提出了一种基于双代理的多图像推理框架，通过语言提示和视觉推理模型实现自动化、模块化的多任务处理。


<details>
  <summary>Details</summary>
Motivation: 解决多模态数据和多任务格式下的跨图像推理挑战。

Method: 采用双代理系统：PromptEngineer生成任务提示，VisionReasoner进行视觉推理。

Result: 在18个数据集上表现优异，Claude 3.7在TQA、DocVQA等任务中接近天花板性能。

Conclusion: 提示引导的视觉语言模型能有效实现多图像推理，设计选择对性能有显著影响。

Abstract: We present a Collaborative Agent-Based Framework for Multi-Image Reasoning.
Our approach tackles the challenge of interleaved multimodal reasoning across
diverse datasets and task formats by employing a dual-agent system: a
language-based PromptEngineer, which generates context-aware, task-specific
prompts, and a VisionReasoner, a large vision-language model (LVLM) responsible
for final inference. The framework is fully automated, modular, and
training-free, enabling generalization across classification, question
answering, and free-form generation tasks involving one or multiple input
images. We evaluate our method on 18 diverse datasets from the 2025 MIRAGE
Challenge (Track A), covering a broad spectrum of visual reasoning tasks
including document QA, visual comparison, dialogue-based understanding, and
scene-level inference. Our results demonstrate that LVLMs can effectively
reason over multiple images when guided by informative prompts. Notably, Claude
3.7 achieves near-ceiling performance on challenging tasks such as TQA (99.13%
accuracy), DocVQA (96.87%), and MMCoQA (75.28 ROUGE-L). We also explore how
design choices-such as model selection, shot count, and input length-influence
the reasoning performance of different LVLMs.

</details>


### [31] [Stable at Any Speed: Speed-Driven Multi-Object Tracking with Learnable Kalman Filtering](https://arxiv.org/abs/2508.00358)
*Yan Gong,Mengjun Chen,Hao Liu,Gao Yongsheng,Lei Yang,Naibang Wang,Ziying Song,Haoqun Ma*

Main category: cs.CV

TL;DR: 论文提出了一种速度引导的可学习卡尔曼滤波器（SG-LKF），通过动态调整不确定性建模以适应自车速度，显著提高了动态场景下的多目标跟踪稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的基于检测的跟踪方法忽视了自车速度对观测噪声和参考系变化的影响，导致在高动态场景中跟踪性能下降。

Method: 提出了SG-LKF，其核心是MotionScaleNet（MSNet），一种解耦的token-mixing和channel-mixing MLP，用于自适应预测SG-LKF的关键参数。同时引入了自监督轨迹一致性损失以增强帧间关联和轨迹连续性。

Result: SG-LKF在KITTI 2D MOT上以79.59%的HOTA排名第一，在KITTI 3D MOT上达到82.03% HOTA，并在nuScenes 3D MOT上比SimpleTrack高出2.2% AMOTA。

Conclusion: SG-LKF通过动态适应自车速度，显著提升了多目标跟踪在高动态场景中的性能。

Abstract: Multi-object tracking (MOT) enables autonomous vehicles to continuously
perceive dynamic objects, supplying essential temporal cues for prediction,
behavior understanding, and safe planning. However, conventional
tracking-by-detection methods typically rely on static coordinate
transformations based on ego-vehicle poses, disregarding ego-vehicle
speed-induced variations in observation noise and reference frame changes,
which degrades tracking stability and accuracy in dynamic, high-speed
scenarios. In this paper, we investigate the critical role of ego-vehicle speed
in MOT and propose a Speed-Guided Learnable Kalman Filter (SG-LKF) that
dynamically adapts uncertainty modeling to ego-vehicle speed, significantly
improving stability and accuracy in highly dynamic scenarios. Central to SG-LKF
is MotionScaleNet (MSNet), a decoupled token-mixing and channel-mixing MLP that
adaptively predicts key parameters of SG-LKF. To enhance inter-frame
association and trajectory continuity, we introduce a self-supervised
trajectory consistency loss jointly optimized with semantic and positional
constraints. Extensive experiments show that SG-LKF ranks first among all
vision-based methods on KITTI 2D MOT with 79.59% HOTA, delivers strong results
on KITTI 3D MOT with 82.03% HOTA, and outperforms SimpleTrack by 2.2% AMOTA on
nuScenes 3D MOT.

</details>


### [32] [CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective](https://arxiv.org/abs/2508.00359)
*Zongheng Tang,Yi Liu,Yifan Sun,Yulu Gao,Jinyu Chen,Runsheng Xu,Si Liu*

Main category: cs.CV

TL;DR: 本文提出了一种高效的协作感知方法CoST，通过统一时空空间同时聚合多智能体和多时间观测，提高了特征传输效率和融合性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法将多智能体融合和多时间融合分离为两步的问题，提升协作感知的效率和准确性。

Method: 提出协作感知时空变换器（CoST），在统一时空空间中同时聚合多智能体和多时间观测。

Result: CoST在效率和准确性上均有提升，且兼容多数现有方法，减少传输带宽需求。

Conclusion: CoST通过统一时空融合，显著提升了协作感知的性能和效率，具有广泛适用性。

Abstract: Collaborative perception shares information among different agents and helps
solving problems that individual agents may face, e.g., occlusions and small
sensing range. Prior methods usually separate the multi-agent fusion and
multi-time fusion into two consecutive steps. In contrast, this paper proposes
an efficient collaborative perception that aggregates the observations from
different agents (space) and different times into a unified spatio-temporal
space simultanesouly. The unified spatio-temporal space brings two benefits,
i.e., efficient feature transmission and superior feature fusion. 1) Efficient
feature transmission: each static object yields a single observation in the
spatial temporal space, and thus only requires transmission only once (whereas
prior methods re-transmit all the object features multiple times). 2) superior
feature fusion: merging the multi-agent and multi-time fusion into a unified
spatial-temporal aggregation enables a more holistic perspective, thereby
enhancing perception performance in challenging scenarios. Consequently, our
Collaborative perception with Spatio-temporal Transformer (CoST) gains
improvement in both efficiency and accuracy. Notably, CoST is not tied to any
specific method and is compatible with a majority of previous methods,
enhancing their accuracy while reducing the transmission bandwidth.

</details>


### [33] [Honey Classification using Hyperspectral Imaging and Machine Learning](https://arxiv.org/abs/2508.00361)
*Mokhtar A. Al-Awadhi,Ratnadeep R. Deshmukh*

Main category: cs.CV

TL;DR: 提出了一种基于机器学习的蜂蜜植物来源自动分类方法，包括数据集准备、特征提取和分类三个步骤，使用LDA和SVM/KNN模型，在标准数据集上取得了95.13%和92.80%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 解决蜂蜜植物来源自动分类问题，提高分类准确率。

Method: 数据集准备阶段采用类转换方法，特征提取使用LDA，分类阶段采用SVM和KNN模型。

Result: 在标准HSI数据集上，分类准确率分别达到95.13%（图像）和92.80%（实例）。

Conclusion: 提出的方法在蜂蜜植物来源分类上表现优异，达到了当前最佳水平。

Abstract: In this paper, we propose a machine learning-based method for automatically
classifying honey botanical origins. Dataset preparation, feature extraction,
and classification are the three main steps of the proposed method. We use a
class transformation method in the dataset preparation phase to maximize the
separability across classes. The feature extraction phase employs the Linear
Discriminant Analysis (LDA) technique for extracting relevant features and
reducing the number of dimensions. In the classification phase, we use Support
Vector Machines (SVM) and K-Nearest Neighbors (KNN) models to classify the
extracted features of honey samples into their botanical origins. We evaluate
our system using a standard honey hyperspectral imaging (HSI) dataset.
Experimental findings demonstrate that the proposed system produces
state-of-the-art results on this dataset, achieving the highest classification
accuracy of 95.13% for hyperspectral image-based classification and 92.80% for
hyperspectral instance-based classification.

</details>


### [34] [SparseRecon: Neural Implicit Surface Reconstruction from Sparse Views with Feature and Depth Consistencies](https://arxiv.org/abs/2508.00366)
*Liang Han,Xu Zhang,Haichuan Song,Kanle Shi,Yu-Shen Liu,Zhizhong Han*

Main category: cs.CV

TL;DR: SparseRecon提出了一种新的稀疏视图神经隐式重建方法，通过特征一致性和不确定性引导的深度约束，解决了现有方法在未见视图上泛化能力差或重建质量受限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏视图重建方法（泛化基或过拟合基）在未见视图上泛化能力差或重建质量有限，SparseRecon旨在解决这一问题。

Method: 结合基于体积渲染的特征一致性损失和不确定性引导的深度约束，优化神经隐式场，提升重建完整性和细节恢复。

Result: 实验表明，SparseRecon在稀疏视图输入下优于现有方法，尤其在视图重叠较少的场景中表现突出。

Conclusion: SparseRecon通过特征一致性和深度约束，显著提升了稀疏视图重建的质量和泛化能力。

Abstract: Surface reconstruction from sparse views aims to reconstruct a 3D shape or
scene from few RGB images. The latest methods are either generalization-based
or overfitting-based. However, the generalization-based methods do not
generalize well on views that were unseen during training, while the
reconstruction quality of overfitting-based methods is still limited by the
limited geometry clues. To address this issue, we propose SparseRecon, a novel
neural implicit reconstruction method for sparse views with volume
rendering-based feature consistency and uncertainty-guided depth constraint.
Firstly, we introduce a feature consistency loss across views to constrain the
neural implicit field. This design alleviates the ambiguity caused by
insufficient consistency information of views and ensures completeness and
smoothness in the reconstruction results. Secondly, we employ an
uncertainty-guided depth constraint to back up the feature consistency loss in
areas with occlusion and insignificant features, which recovers geometry
details for better reconstruction quality. Experimental results demonstrate
that our method outperforms the state-of-the-art methods, which can produce
high-quality geometry with sparse-view input, especially in the scenarios with
small overlapping views. Project page: https://hanl2010.github.io/SparseRecon/.

</details>


### [35] [Representation Shift: Unifying Token Compression with FlashAttention](https://arxiv.org/abs/2508.00367)
*Joonmyung Choi,Sanghyeok Lee,Byungoh Ko,Eunseo Kim,Jihyung Kil,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: 提出了一种名为Representation Shift的训练无关、模型无关的度量方法，用于衡量每个token表示的变化程度，从而在无需注意力图或重新训练的情况下实现与FlashAttention兼容的token压缩。


<details>
  <summary>Details</summary>
Motivation: 随着任务复杂度的增加，Transformer模型的规模和token数量增长，导致自注意力的二次计算成本和GPU内存访问开销增加。现有token压缩方法依赖注意力图，与FlashAttention不兼容。

Method: 提出Representation Shift度量方法，通过测量token表示的变化程度实现token压缩，兼容FlashAttention且无需注意力图或重新训练。

Result: 实验表明，该方法在视频文本检索和视频问答任务中分别实现了5.5%和4.4%的速度提升。

Conclusion: Representation Shift是一种高效且通用的token压缩方法，适用于多种模型，并与FlashAttention无缝集成。

Abstract: Transformers have demonstrated remarkable success across vision, language,
and video. Yet, increasing task complexity has led to larger models and more
tokens, raising the quadratic cost of self-attention and the overhead of GPU
memory access. To reduce the computation cost of self-attention, prior work has
proposed token compression techniques that drop redundant or less informative
tokens. Meanwhile, fused attention kernels such as FlashAttention have been
developed to alleviate memory overhead by avoiding attention map construction
and its associated I/O to HBM. This, however, makes it incompatible with most
training-free token compression methods, which rely on attention maps to
determine token importance. Here, we propose Representation Shift, a
training-free, model-agnostic metric that measures the degree of change in each
token's representation. This seamlessly integrates token compression with
FlashAttention, without attention maps or retraining. Our method further
generalizes beyond Transformers to CNNs and state space models. Extensive
experiments show that Representation Shift enables effective token compression
compatible with FlashAttention, yielding significant speedups of up to 5.5% and
4.4% in video-text retrieval and video QA, respectively. Code is available at
https://github.com/mlvlab/Representation-Shift.

</details>


### [36] [Bidirectional Action Sequence Learning for Long-term Action Anticipation with Large Language Models](https://arxiv.org/abs/2508.00374)
*Yuji Sato,Yasunori Ishii,Takayoshi Yamashita*

Main category: cs.CV

TL;DR: BiAnt方法通过结合前向和后向预测，利用大语言模型改进视频长期动作预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法因单向性限制性能，难以捕捉场景中的语义子动作。

Method: 结合前向和后向预测，使用大语言模型（BiAnt）。

Result: 在Ego4D数据集上，BiAnt在编辑距离指标上优于基线方法。

Conclusion: BiAnt通过双向预测提升长期动作预测性能。

Abstract: Video-based long-term action anticipation is crucial for early risk detection
in areas such as automated driving and robotics. Conventional approaches
extract features from past actions using encoders and predict future events
with decoders, which limits performance due to their unidirectional nature.
These methods struggle to capture semantically distinct sub-actions within a
scene. The proposed method, BiAnt, addresses this limitation by combining
forward prediction with backward prediction using a large language model.
Experimental results on Ego4D demonstrate that BiAnt improves performance in
terms of edit distance compared to baseline methods.

</details>


### [37] [Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis](https://arxiv.org/abs/2508.00381)
*Kamal Basha S,Athira Nambiar*

Main category: cs.CV

TL;DR: 该论文提出了一种自适应框架Adapt-WeldNet和解释性分析框架DDIA，以提高焊接缺陷检测的性能和可解释性，适用于海洋和离岸环境。


<details>
  <summary>Details</summary>
Motivation: 传统无损检测方法和现有神经网络方法在检测细微或内部缺陷时表现不佳，且缺乏可解释性，存在安全隐患。

Method: 提出Adapt-WeldNet框架，系统评估预训练架构、迁移学习策略和自适应优化器；并设计DDIA框架，结合XAI技术和专家验证。

Result: 优化了缺陷检测性能，并通过DDIA增强了系统的透明度和可信度。

Conclusion: 该研究提升了焊接缺陷检测系统的信任度、安全性和可靠性，适用于关键海洋和离岸操作。

Abstract: Weld defect detection is crucial for ensuring the safety and reliability of
piping systems in the oil and gas industry, especially in challenging marine
and offshore environments. Traditional non-destructive testing (NDT) methods
often fail to detect subtle or internal defects, leading to potential failures
and costly downtime. Furthermore, existing neural network-based approaches for
defect classification frequently rely on arbitrarily selected pretrained
architectures and lack interpretability, raising safety concerns for
deployment. To address these challenges, this paper introduces
``Adapt-WeldNet", an adaptive framework for welding defect detection that
systematically evaluates various pre-trained architectures, transfer learning
strategies, and adaptive optimizers to identify the best-performing model and
hyperparameters, optimizing defect detection and providing actionable insights.
Additionally, a novel Defect Detection Interpretability Analysis (DDIA)
framework is proposed to enhance system transparency. DDIA employs Explainable
AI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific
evaluations validated by certified ASNT NDE Level II professionals.
Incorporating a Human-in-the-Loop (HITL) approach and aligning with the
principles of Trustworthy AI, DDIA ensures the reliability, fairness, and
accountability of the defect detection system, fostering confidence in
automated decisions through expert validation. By improving both performance
and interpretability, this work enhances trust, safety, and reliability in
welding defect detection systems, supporting critical operations in offshore
and marine environments.

</details>


### [38] [$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models](https://arxiv.org/abs/2508.00383)
*Won June Cho,Hongjun Yoon,Daeky Jeong,Hyeongyeol Lim,Yosep Chong*

Main category: cs.CV

TL;DR: 论文提出了一种结合状态空间模型（SSMs）与ViT的混合架构$MV_{Hybrid}$，用于从病理图像预测空间基因表达，显著优于现有ViT模型。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学成本高且技术复杂，限制了临床应用。从常规病理图像预测基因表达是一种实用替代方案，但现有ViT模型表现不足。

Method: 提出$MV_{Hybrid}$架构，结合SSMs和ViT，利用负实特征值初始化SSMs以增强低频模式捕捉能力。

Result: 在LOSO评估中，$MV_{Hybrid}$比最佳ViT模型相关性提高57%，性能下降减少43%。

Conclusion: $MV_{Hybrid}$在基因表达预测和其他任务中表现优越，有望成为下一代病理视觉基础模型主干。

Abstract: Spatial transcriptomics reveals gene expression patterns within tissue
context, enabling precision oncology applications such as treatment response
prediction, but its high cost and technical complexity limit clinical adoption.
Predicting spatial gene expression (biomarkers) from routine histopathology
images offers a practical alternative, yet current vision foundation models
(VFMs) in pathology based on Vision Transformer (ViT) backbones perform below
clinical standards. Given that VFMs are already trained on millions of diverse
whole slide images, we hypothesize that architectural innovations beyond ViTs
may better capture the low-frequency, subtle morphological patterns correlating
with molecular phenotypes. By demonstrating that state space models initialized
with negative real eigenvalues exhibit strong low-frequency bias, we introduce
$MV_{Hybrid}$, a hybrid backbone architecture combining state space models
(SSMs) with ViT. We compare five other different backbone architectures for
pathology VFMs, all pretrained on identical colorectal cancer datasets using
the DINOv2 self-supervised learning method. We evaluate all pretrained models
using both random split and leave-one-study-out (LOSO) settings of the same
biomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher
correlation than the best-performing ViT and shows 43% smaller performance
degradation compared to random split in gene expression prediction,
demonstrating superior performance and robustness, respectively. Furthermore,
$MV_{Hybrid}$ shows equal or better downstream performance in classification,
patch retrieval, and survival prediction tasks compared to that of ViT, showing
its promise as a next-generation pathology VFM backbone. Our code is publicly
available at: https://github.com/deepnoid-ai/MVHybrid.

</details>


### [39] [Cued-Agent: A Collaborative Multi-Agent System for Automatic Cued Speech Recognition](https://arxiv.org/abs/2508.00391)
*Guanjie Huang,Danny H. K. Tsang,Shan Yang,Guangzhi Lei,Li Liu*

Main category: cs.CV

TL;DR: 提出了一种名为Cued-Agent的多智能体系统，用于自动识别手语和唇语，通过多模态融合和语义优化显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法因数据有限和手唇动作异步性导致多模态融合效果不佳，多智能体系统为解决这一问题提供了新思路。

Method: Cued-Agent包含四个子智能体：基于多模态大语言模型的手势识别、预训练Transformer的唇语识别、动态整合手唇特征的提示解码，以及语义优化的音素到单词转换。

Result: 实验表明，Cued-Agent在正常和听力障碍场景下均优于现有方法。

Conclusion: Cued-Agent通过多智能体协作和语义优化，显著提升了手语和唇语的自动识别性能。

Abstract: Cued Speech (CS) is a visual communication system that combines lip-reading
with hand coding to facilitate communication for individuals with hearing
impairments. Automatic CS Recognition (ACSR) aims to convert CS hand gestures
and lip movements into text via AI-driven methods. Traditionally, the temporal
asynchrony between hand and lip movements requires the design of complex
modules to facilitate effective multimodal fusion. However, constrained by
limited data availability, current methods demonstrate insufficient capacity
for adequately training these fusion mechanisms, resulting in suboptimal
performance. Recently, multi-agent systems have shown promising capabilities in
handling complex tasks with limited data availability. To this end, we propose
the first collaborative multi-agent system for ACSR, named Cued-Agent. It
integrates four specialized sub-agents: a Multimodal Large Language Model-based
Hand Recognition agent that employs keyframe screening and CS expert prompt
strategies to decode hand movements, a pretrained Transformer-based Lip
Recognition agent that extracts lip features from the input video, a Hand
Prompt Decoding agent that dynamically integrates hand prompts with lip
features during inference in a training-free manner, and a Self-Correction
Phoneme-to-Word agent that enables post-process and end-to-end conversion from
phoneme sequences to natural language sentences for the first time through
semantic refinement. To support this study, we expand the existing Mandarin CS
dataset by collecting data from eight hearing-impaired cuers, establishing a
mixed dataset of fourteen subjects. Extensive experiments demonstrate that our
Cued-Agent performs superbly in both normal and hearing-impaired scenarios
compared with state-of-the-art methods. The implementation is available at
https://github.com/DennisHgj/Cued-Agent.

</details>


### [40] [Decouple before Align: Visual Disentanglement Enhances Prompt Tuning](https://arxiv.org/abs/2508.00395)
*Fei Zhang,Tianfei Zhou,Jiangchao Yao,Ya Zhang,Ivor W. Tsang,Yanfeng Wang*

Main category: cs.CV

TL;DR: 论文提出DAPT框架，通过解耦视觉模态为前景和背景表示，并分别与文本对齐，解决了提示调优中的信息不对称问题。


<details>
  <summary>Details</summary>
Motivation: 解决提示调优（PT）中视觉模态与文本模态信息不对称的问题，避免模型仅关注上下文区域。

Method: 提出DAPT框架，先解耦视觉模态为前景和背景表示，再分别与文本对齐，并引入视觉拉推正则化以增强视觉注意力。

Result: 在少样本学习、基类到新类泛化和数据高效学习中表现优异。

Conclusion: DAPT通过解耦和对齐视觉模态，有效提升了模型性能，解决了信息不对称问题。

Abstract: Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm,
has showcased remarkable effectiveness in improving the task-specific
transferability of vision-language models. This paper delves into a previously
overlooked information asymmetry issue in PT, where the visual modality mostly
conveys more context than the object-oriented textual modality.
Correspondingly, coarsely aligning these two modalities could result in the
biased attention, driving the model to merely focus on the context area. To
address this, we propose DAPT, an effective PT framework based on an intuitive
decouple-before-align concept. First, we propose to explicitly decouple the
visual modality into the foreground and background representation via
exploiting coarse-and-fine visual segmenting cues, and then both of these
decoupled patterns are aligned with the original foreground texts and the
hand-crafted background classes, thereby symmetrically strengthening the modal
alignment. To further enhance the visual concentration, we propose a visual
pull-push regularization tailored for the foreground-background patterns,
directing the original visual representation towards unbiased attention on the
region-of-interest object. We demonstrate the power of architecture-free DAPT
through few-shot learning, base-to-novel generalization, and data-efficient
learning, all of which yield superior performance across prevailing benchmarks.
Our code will be released at https://github.com/Ferenas/DAPT.

</details>


### [41] [Video Forgery Detection with Optical Flow Residuals and Spatial-Temporal Consistency](https://arxiv.org/abs/2508.00397)
*Xi Xue,Kunio Suzuki,Nabarun Goswami,Takuya Shintate*

Main category: cs.CV

TL;DR: 提出了一种基于RGB外观特征和光流残差的双分支检测框架，用于检测AI生成视频中的伪造内容，实验证明其具有鲁棒性和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着基于扩散的视频生成模型的快速发展，合成内容越来越逼真，给视频伪造检测带来了新挑战。现有方法难以捕捉细粒度的时间不一致性。

Method: 采用双分支架构，一支分析RGB帧检测外观伪影，另一支处理光流残差揭示时间合成不完美导致的运动异常。

Result: 在十种不同生成模型的文本到视频和图像到视频任务上进行了广泛实验，证明了方法的鲁棒性和强泛化能力。

Conclusion: 通过结合RGB外观特征和光流残差，该方法能有效检测多种伪造视频。

Abstract: The rapid advancement of diffusion-based video generation models has led to
increasingly realistic synthetic content, presenting new challenges for video
forgery detection. Existing methods often struggle to capture fine-grained
temporal inconsistencies, particularly in AI-generated videos with high visual
fidelity and coherent motion. In this work, we propose a detection framework
that leverages spatial-temporal consistency by combining RGB appearance
features with optical flow residuals. The model adopts a dual-branch
architecture, where one branch analyzes RGB frames to detect appearance-level
artifacts, while the other processes flow residuals to reveal subtle motion
anomalies caused by imperfect temporal synthesis. By integrating these
complementary features, the proposed method effectively detects a wide range of
forged videos. Extensive experiments on text-to-video and image-to-video tasks
across ten diverse generative models demonstrate the robustness and strong
generalization ability of the proposed approach.

</details>


### [42] [iSafetyBench: A video-language benchmark for safety in industrial environment](https://arxiv.org/abs/2508.00399)
*Raiyaan Abdullah,Yogesh Singh Rawat,Shruti Vyas*

Main category: cs.CV

TL;DR: iSafetyBench是一个新的视频语言基准测试，用于评估工业环境中视觉语言模型（VLMs）在常规和危险场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs在工业领域的高风险场景中表现不足，需要专门评估其识别常规操作和安全关键异常的能力。

Method: 构建了包含1,100个工业视频片段的iSafetyBench数据集，标注了98种常规和67种危险动作类别，并设计了多标签评估问题。

Result: 在零样本条件下测试了8种先进视频语言模型，发现它们在识别危险活动和多标签场景中表现不佳。

Conclusion: iSafetyBench揭示了现有模型的性能差距，为开发更鲁棒、安全感知的多模态模型提供了测试平台。

Abstract: Recent advances in vision-language models (VLMs) have enabled impressive
generalization across diverse video understanding tasks under zero-shot
settings. However, their capabilities in high-stakes industrial domains-where
recognizing both routine operations and safety-critical anomalies is
essential-remain largely underexplored. To address this gap, we introduce
iSafetyBench, a new video-language benchmark specifically designed to evaluate
model performance in industrial environments across both normal and hazardous
scenarios. iSafetyBench comprises 1,100 video clips sourced from real-world
industrial settings, annotated with open-vocabulary, multi-label action tags
spanning 98 routine and 67 hazardous action categories. Each clip is paired
with multiple-choice questions for both single-label and multi-label
evaluation, enabling fine-grained assessment of VLMs in both standard and
safety-critical contexts. We evaluate eight state-of-the-art video-language
models under zero-shot conditions. Despite their strong performance on existing
video benchmarks, these models struggle with iSafetyBench-particularly in
recognizing hazardous activities and in multi-label scenarios. Our results
reveal significant performance gaps, underscoring the need for more robust,
safety-aware multimodal models for industrial applications. iSafetyBench
provides a first-of-its-kind testbed to drive progress in this direction. The
dataset is available at: https://github.com/raiyaan-abdullah/iSafety-Bench.

</details>


### [43] [Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents](https://arxiv.org/abs/2508.00400)
*Janika Deborah Gajo,Gerarld Paul Merales,Jerome Escarcha,Brenden Ashley Molina,Gian Nartea,Emmanuel G. Maminta,Juan Carlos Roldan,Rowel O. Atienza*

Main category: cs.CV

TL;DR: Sari Sandbox是一个高保真、逼真的3D零售店模拟环境，用于评估具身代理在购物任务中与人类表现的对比。


<details>
  <summary>Details</summary>
Motivation: 解决零售领域缺乏专门用于具身代理训练的模拟环境的问题。

Method: 开发了一个包含250多种交互式杂货商品的3D零售店模拟环境，支持VR和VLM驱动的具身代理，并提供了SariBench数据集。

Result: 提供了基准测试和性能分析，展示了具身代理在导航、检查和操作零售商品方面的表现。

Conclusion: Sari Sandbox为具身代理训练提供了高保真的零售环境，并提出了提升真实性和可扩展性的建议。

Abstract: We present Sari Sandbox, a high-fidelity, photorealistic 3D retail store
simulation for benchmarking embodied agents against human performance in
shopping tasks. Addressing a gap in retail-specific sim environments for
embodied agent training, Sari Sandbox features over 250 interactive grocery
items across three store configurations, controlled via an API. It supports
both virtual reality (VR) for human interaction and a vision language model
(VLM)-powered embodied agent. We also introduce SariBench, a dataset of
annotated human demonstrations across varied task difficulties. Our sandbox
enables embodied agents to navigate, inspect, and manipulate retail items,
providing baselines against human performance. We conclude with benchmarks,
performance analysis, and recommendations for enhancing realism and
scalability. The source code can be accessed via
https://github.com/upeee/sari-sandbox-env.

</details>


### [44] [PMR: Physical Model-Driven Multi-Stage Restoration of Turbulent Dynamic Videos](https://arxiv.org/abs/2508.00406)
*Tao Wu,Jingyuan Ye,Ying Fu*

Main category: cs.CV

TL;DR: 论文提出了一种动态效率指数（DEI）和多阶段视频恢复框架（PMR），用于解决大气湍流导致的几何失真和模糊问题，显著提升了高动态湍流条件下的视频恢复质量。


<details>
  <summary>Details</summary>
Motivation: 大气湍流引起的几何失真和模糊降低了远距离动态场景视频的质量，现有方法难以恢复边缘细节和消除混合失真，尤其是在强湍流和复杂动态条件下。

Method: 提出动态效率指数（DEI）量化视频动态强度，并构建高动态湍流训练数据集；设计物理模型驱动的多阶段视频恢复框架（PMR），包括去倾斜、动态区域增强和去模糊三个阶段。

Result: 实验表明，PMR能有效抑制运动拖尾伪影，恢复边缘细节，并在高湍流和复杂动态的真实场景中表现出强泛化能力。

Conclusion: 该方法显著提升了视频恢复质量，代码和数据集将公开。

Abstract: Geometric distortions and blurring caused by atmospheric turbulence degrade
the quality of long-range dynamic scene videos. Existing methods struggle with
restoring edge details and eliminating mixed distortions, especially under
conditions of strong turbulence and complex dynamics. To address these
challenges, we introduce a Dynamic Efficiency Index ($DEI$), which combines
turbulence intensity, optical flow, and proportions of dynamic regions to
accurately quantify video dynamic intensity under varying turbulence conditions
and provide a high-dynamic turbulence training dataset. Additionally, we
propose a Physical Model-Driven Multi-Stage Video Restoration ($PMR$) framework
that consists of three stages: \textbf{de-tilting} for geometric stabilization,
\textbf{motion segmentation enhancement} for dynamic region refinement, and
\textbf{de-blurring} for quality restoration. $PMR$ employs lightweight
backbones and stage-wise joint training to ensure both efficiency and high
restoration quality. Experimental results demonstrate that the proposed method
effectively suppresses motion trailing artifacts, restores edge details and
exhibits strong generalization capability, especially in real-world scenarios
characterized by high-turbulence and complex dynamics. We will make the code
and datasets openly available.

</details>


### [45] [Sortblock: Similarity-Aware Feature Reuse for Diffusion Model](https://arxiv.org/abs/2508.00412)
*Hanqi Chen,Xu Zhang,Xiaoliu Guan,Lielin Jiang,Guanzhong Wang,Zeyu Chen,Yi Liu*

Main category: cs.CV

TL;DR: Sortblock是一种无需训练的推理加速框架，通过动态缓存块级特征并选择性跳过冗余计算，显著提升DiTs的推理速度，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: DiTs的序列去噪过程导致高推理延迟，限制了实时应用。现有加速方法未考虑去噪阶段和Transformer块的语义变化。

Method: 提出Sortblock框架，动态缓存块级特征，基于相邻时间步的相似性排名自适应确定重计算比例，并引入轻量级线性预测机制减少误差。

Result: 实验表明，Sortblock在多种任务和DiT架构中实现超过2倍的推理加速，且输出质量下降极小。

Conclusion: Sortblock为基于扩散的生成模型提供了一种高效且通用的加速解决方案。

Abstract: Diffusion Transformers (DiTs) have demonstrated remarkable generative
capabilities, particularly benefiting from Transformer architectures that
enhance visual and artistic fidelity. However, their inherently sequential
denoising process results in high inference latency, limiting their deployment
in real-time scenarios. Existing training-free acceleration approaches
typically reuse intermediate features at fixed timesteps or layers, overlooking
the evolving semantic focus across denoising stages and Transformer blocks.To
address this, we propose Sortblock, a training-free inference acceleration
framework that dynamically caches block-wise features based on their similarity
across adjacent timesteps. By ranking the evolution of residuals, Sortblock
adaptively determines a recomputation ratio, selectively skipping redundant
computations while preserving generation quality. Furthermore, we incorporate a
lightweight linear prediction mechanism to reduce accumulated errors in skipped
blocks.Extensive experiments across various tasks and DiT architectures
demonstrate that Sortblock achieves over 2$\times$ inference speedup with
minimal degradation in output quality, offering an effective and generalizable
solution for accelerating diffusion-based generative models.

</details>


### [46] [DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space](https://arxiv.org/abs/2508.00413)
*Junyu Chen,Dongyun Zou,Wenkun He,Junsong Chen,Enze Xie,Song Han,Han Cai*

Main category: cs.CV

TL;DR: DC-AE 1.5是一种新型深度压缩自编码器，通过结构化潜在空间和增强扩散训练，解决了高分辨率扩散模型中潜在通道增加导致的收敛慢和生成质量下降问题。


<details>
  <summary>Details</summary>
Motivation: 高分辨率扩散模型中，增加自编码器的潜在通道数虽能提升重建质量，但会导致扩散模型收敛慢，生成质量下降，限制了潜在扩散模型的质量上限。

Method: 提出两种创新：1) 结构化潜在空间，通过训练在潜在空间中实现通道级结构；2) 增强扩散训练，通过额外扩散训练目标加速收敛。

Result: DC-AE 1.5在ImageNet 512x512上比DC-AE-f32c32生成质量更好且快4倍。

Conclusion: DC-AE 1.5通过结构化潜在空间和增强扩散训练，显著提升了高分辨率扩散模型的性能和效率。

Abstract: We present DC-AE 1.5, a new family of deep compression autoencoders for
high-resolution diffusion models. Increasing the autoencoder's latent channel
number is a highly effective approach for improving its reconstruction quality.
However, it results in slow convergence for diffusion models, leading to poorer
generation quality despite better reconstruction quality. This issue limits the
quality upper bound of latent diffusion models and hinders the employment of
autoencoders with higher spatial compression ratios. We introduce two key
innovations to address this challenge: i) Structured Latent Space, a
training-based approach to impose a desired channel-wise structure on the
latent space with front latent channels capturing object structures and latter
latent channels capturing image details; ii) Augmented Diffusion Training, an
augmented diffusion training strategy with additional diffusion training
objectives on object latent channels to accelerate convergence. With these
techniques, DC-AE 1.5 delivers faster convergence and better diffusion scaling
results than DC-AE. On ImageNet 512x512, DC-AE-1.5-f64c128 delivers better
image generation quality than DC-AE-f32c32 while being 4x faster. Code:
https://github.com/dc-ai-projects/DC-Gen.

</details>


### [47] [Fine-grained Spatiotemporal Grounding on Egocentric Videos](https://arxiv.org/abs/2508.00518)
*Shuo Liang,Yiwu Zhong,Zi-Yuan Hu,Yeyao Tao,Liwei Wang*

Main category: cs.CV

TL;DR: 论文提出了EgoMask，首个用于自我中心视频中细粒度时空定位的像素级基准，并揭示了自我中心与外部中心视频的关键差异。


<details>
  <summary>Details</summary>
Motivation: 自我中心视频在增强现实和机器人等应用中日益重要，但相关研究较少，现有模型在此类视频中表现不佳。

Method: 通过自动标注管道构建EgoMask基准，并创建大规模训练数据集EgoMask-Train，用于模型开发和微调。

Result: 实验表明，现有模型在EgoMask上表现不佳，但通过微调EgoMask-Train可显著提升性能。

Conclusion: 研究为自我中心视频理解提供了关键资源和见解，推动了该领域的发展。

Abstract: Spatiotemporal video grounding aims to localize target entities in videos
based on textual queries. While existing research has made significant progress
in exocentric videos, the egocentric setting remains relatively underexplored,
despite its growing importance in applications such as augmented reality and
robotics. In this work, we conduct a systematic analysis of the discrepancies
between egocentric and exocentric videos, revealing key challenges such as
shorter object durations, sparser trajectories, smaller object sizes, and
larger positional shifts. To address these challenges, we introduce EgoMask,
the first pixel-level benchmark for fine-grained spatiotemporal grounding in
egocentric videos. It is constructed by our proposed automatic annotation
pipeline, which annotates referring expressions and object masks across short-,
medium-, and long-term videos. Additionally, we create EgoMask-Train, a
large-scale training dataset to facilitate model development. Experiments
demonstrate that the state-of-the-art spatiotemporal grounding models perform
poorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields
significant improvements, while preserving performance on exocentric datasets.
Our work thus provides essential resources and insights for advancing
egocentric video understanding. Our code is available at
https://github.com/LaVi-Lab/EgoMask .

</details>


### [48] [IN2OUT: Fine-Tuning Video Inpainting Model for Video Outpainting Using Hierarchical Discriminator](https://arxiv.org/abs/2508.00418)
*Sangwoo Youn,Minji Lee,Nokap Tony Park,Yeonggyoo Jeon,Taeyoung Na*

Main category: cs.CV

TL;DR: 论文提出了一种改进的视频外绘方法，通过引入分层判别器和专用损失函数，解决了现有方法直接应用修复模型导致的模糊问题。


<details>
  <summary>Details</summary>
Motivation: 视频外绘的挑战在于扩展边界时保持内容一致性，现有方法仅生成背景效果不佳。

Method: 采用视频修复模型，设计分层判别器区分全局和局部目标，并开发专用外绘损失函数。

Result: 方法在定量和定性上均优于现有技术。

Conclusion: 通过改进判别器和损失函数，实现了视觉吸引力和全局一致性的外绘效果。

Abstract: Video outpainting presents a unique challenge of extending the borders while
maintaining consistency with the given content. In this paper, we suggest the
use of video inpainting models that excel in object flow learning and
reconstruction in outpainting rather than solely generating the background as
in existing methods. However, directly applying or fine-tuning inpainting
models to outpainting has shown to be ineffective, often leading to blurry
results. Our extensive experiments on discriminator designs reveal that a
critical component missing in the outpainting fine-tuning process is a
discriminator capable of effectively assessing the perceptual quality of the
extended areas. To tackle this limitation, we differentiate the objectives of
adversarial training into global and local goals and introduce a hierarchical
discriminator that meets both objectives. Additionally, we develop a
specialized outpainting loss function that leverages both local and global
features of the discriminator. Fine-tuning on this adversarial loss function
enhances the generator's ability to produce both visually appealing and
globally coherent outpainted scenes. Our proposed method outperforms
state-of-the-art methods both quantitatively and qualitatively. Supplementary
materials including the demo video and the code are available in SigPort.

</details>


### [49] [UIS-Mamba: Exploring Mamba for Underwater Instance Segmentation via Dynamic Tree Scan and Hidden State Weaken](https://arxiv.org/abs/2508.00421)
*Runmin Cong,Zongji Yu,Hao Fang,Haoyan Sun,Sam Kwong*

Main category: cs.CV

TL;DR: 提出了一种基于Mamba的水下实例分割模型UIS-Mamba，通过动态树扫描（DTS）和隐藏状态弱化（HSW）模块解决水下场景中的挑战，实现了高性能和低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 水下实例分割任务面临颜色失真和边界模糊等挑战，现有固定补丁扫描机制无法保持实例连续性，复杂背景也会干扰实例理解。

Method: 设计了DTS模块（动态调整补丁偏移和缩放）和HSW模块（基于Ncut的隐藏状态弱化机制），以迁移Mamba模型至水下任务。

Result: 在UIIS和USIS10K数据集上达到最先进性能，同时保持低参数量和计算复杂度。

Conclusion: UIS-Mamba成功解决了水下实例分割的挑战，为相关任务提供了高效解决方案。

Abstract: Underwater Instance Segmentation (UIS) tasks are crucial for underwater
complex scene detection. Mamba, as an emerging state space model with
inherently linear complexity and global receptive fields, is highly suitable
for processing image segmentation tasks with long sequence features. However,
due to the particularity of underwater scenes, there are many challenges in
applying Mamba to UIS. The existing fixed-patch scanning mechanism cannot
maintain the internal continuity of scanned instances in the presence of
severely underwater color distortion and blurred instance boundaries, and the
hidden state of the complex underwater background can also inhibit the
understanding of instance objects. In this work, we propose the first
Mamba-based underwater instance segmentation model UIS-Mamba, and design two
innovative modules, Dynamic Tree Scan (DTS) and Hidden State Weaken (HSW), to
migrate Mamba to the underwater task. DTS module maintains the continuity of
the internal features of the instance objects by allowing the patches to
dynamically offset and scale, thereby guiding the minimum spanning tree and
providing dynamic local receptive fields. HSW module suppresses the
interference of complex backgrounds and effectively focuses the information
flow of state propagation to the instances themselves through the Ncut-based
hidden state weakening mechanism. Experimental results show that UIS-Mamba
achieves state-of-the-art performance on both UIIS and USIS10K datasets, while
maintaining a low number of parameters and computational complexity. Code is
available at https://github.com/Maricalce/UIS-Mamba.

</details>


### [50] [Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting](https://arxiv.org/abs/2508.00427)
*Seunggeun Chi,Enna Sachdeva,Pin-Hao Huang,Kwonjoon Lee*

Main category: cs.CV

TL;DR: 提出了一种结合物理先验知识和多区域修复技术的新方法，用于动态场景中的遮挡物体补全，显著提升了生成结果的准确性和真实性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态场景中因对人类-物体交互（HOI）理解有限，难以生成合理的补全结果。

Method: 结合人类拓扑和接触信息的物理约束，定义主次区域，并在扩散模型中采用定制化的去噪策略进行多区域修复。

Result: 在HOI场景中显著优于现有方法，且无需真实接触标注也能保持鲁棒性。

Conclusion: 该方法使机器感知更接近人类对动态环境的理解，适用于3D重建和新视角/姿态合成等任务。

Abstract: Amodal completion, which is the process of inferring the full appearance of
objects despite partial occlusions, is crucial for understanding complex
human-object interactions (HOI) in computer vision and robotics. Existing
methods, such as those that use pre-trained diffusion models, often struggle to
generate plausible completions in dynamic scenarios because they have a limited
understanding of HOI. To solve this problem, we've developed a new approach
that uses physical prior knowledge along with a specialized multi-regional
inpainting technique designed for HOI. By incorporating physical constraints
from human topology and contact information, we define two distinct regions:
the primary region, where occluded object parts are most likely to be, and the
secondary region, where occlusions are less probable. Our multi-regional
inpainting method uses customized denoising strategies across these regions
within a diffusion model. This improves the accuracy and realism of the
generated completions in both their shape and visual detail. Our experimental
results show that our approach significantly outperforms existing methods in
HOI scenarios, moving machine perception closer to a more human-like
understanding of dynamic environments. We also show that our pipeline is robust
even without ground-truth contact annotations, which broadens its applicability
to tasks like 3D reconstruction and novel view/pose synthesis.

</details>


### [51] [TopoTTA: Topology-Enhanced Test-Time Adaptation for Tubular Structure Segmentation](https://arxiv.org/abs/2508.00442)
*Jiale Zhou,Wenhan Wang,Shikun Li,Xiaolei Qu,Xin Guo,Yizhong Liu,Wenzhong Tang,Xun Lin,Yefeng Zheng*

Main category: cs.CV

TL;DR: 论文提出TopoTTA，一种针对管状结构分割（TSS）的测试时适应框架，通过两阶段方法解决域偏移问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 管状结构分割对领域偏移敏感，传统方法性能下降明显，需专门解决方案。

Method: TopoTTA分两阶段：1. 使用TopoMDCs适应拓扑差异；2. 通过TopoHG生成硬样本并优化拓扑连续性。

Result: 在四个场景和十个数据集上，TopoTTA平均提升31.81%的clDice指标。

Conclusion: TopoTTA是一种高效的即插即用解决方案，适用于基于CNN的TSS模型。

Abstract: Tubular structure segmentation (TSS) is important for various applications,
such as hemodynamic analysis and route navigation. Despite significant progress
in TSS, domain shifts remain a major challenge, leading to performance
degradation in unseen target domains. Unlike other segmentation tasks, TSS is
more sensitive to domain shifts, as changes in topological structures can
compromise segmentation integrity, and variations in local features
distinguishing foreground from background (e.g., texture and contrast) may
further disrupt topological continuity. To address these challenges, we propose
Topology-enhanced Test-Time Adaptation (TopoTTA), the first test-time
adaptation framework designed specifically for TSS. TopoTTA consists of two
stages: Stage 1 adapts models to cross-domain topological discrepancies using
the proposed Topological Meta Difference Convolutions (TopoMDCs), which enhance
topological representation without altering pre-trained parameters; Stage 2
improves topological continuity by a novel Topology Hard sample Generation
(TopoHG) strategy and prediction alignment on hard samples with pseudo-labels
in the generated pseudo-break regions. Extensive experiments across four
scenarios and ten datasets demonstrate TopoTTA's effectiveness in handling
topological distribution shifts, achieving an average improvement of 31.81% in
clDice. TopoTTA also serves as a plug-and-play TTA solution for CNN-based TSS
models.

</details>


### [52] [SDMatte: Grafting Diffusion Models for Interactive Matting](https://arxiv.org/abs/2508.00443)
*Longfei Huang,Yu Liang,Hao Zhang,Jinwei Chen,Wei Dong,Lunde Chen,Wanyu Liu,Bo Li,Pengtao Jiang*

Main category: cs.CV

TL;DR: SDMatte利用扩散模型的强大先验，将文本驱动交互能力转化为视觉提示驱动交互能力，提升交互式抠图的精细度。


<details>
  <summary>Details</summary>
Motivation: 现有交互式抠图方法在边缘区域提取精细细节方面表现不足，扩散模型因其复杂数据分布建模和纹理细节合成能力，成为潜在解决方案。

Method: 提出SDMatte，结合扩散模型先验，通过坐标嵌入和透明度嵌入增强U-Net，并引入掩码自注意力机制聚焦视觉提示区域。

Result: 多数据集实验验证了SDMatte在交互式抠图中的优越性能。

Conclusion: SDMatte通过扩散模型和视觉提示驱动，显著提升了交互式抠图的精细度和效果。

Abstract: Recent interactive matting methods have shown satisfactory performance in
capturing the primary regions of objects, but they fall short in extracting
fine-grained details in edge regions. Diffusion models trained on billions of
image-text pairs, demonstrate exceptional capability in modeling highly complex
data distributions and synthesizing realistic texture details, while exhibiting
robust text-driven interaction capabilities, making them an attractive solution
for interactive matting. To this end, we propose SDMatte, a diffusion-driven
interactive matting model, with three key contributions. First, we exploit the
powerful priors of diffusion models and transform the text-driven interaction
capability into visual prompt-driven interaction capability to enable
interactive matting. Second, we integrate coordinate embeddings of visual
prompts and opacity embeddings of target objects into U-Net, enhancing
SDMatte's sensitivity to spatial position information and opacity information.
Third, we propose a masked self-attention mechanism that enables the model to
focus on areas specified by visual prompts, leading to better performance.
Extensive experiments on multiple datasets demonstrate the superior performance
of our method, validating its effectiveness in interactive matting. Our code
and model are available at https://github.com/vivoCameraResearch/SDMatte.

</details>


### [53] [AutoDebias: Automated Framework for Debiasing Text-to-Image Models](https://arxiv.org/abs/2508.00445)
*Hongyi Cai,Mohammad Mahdinur Rahman,Mingkang Dong,Jie Li,Muxin Pu,Zhili Fang,Yinan Peng,Hanjun Luo,Yang Liu*

Main category: cs.CV

TL;DR: AutoDebias是一种自动识别和减轻文本到图像模型中社会偏见的框架，无需预先了解具体偏见类型。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理复杂或重叠的偏见，而AutoDebias旨在解决这一问题。

Method: 利用视觉语言模型检测偏见，生成包容性提示，并通过CLIP引导训练优化模型。

Result: 在25种偏见场景中，AutoDebias准确检测91.6%的偏见，将偏见输出从90%降至可忽略水平。

Conclusion: AutoDebias能有效处理复杂偏见，同时保持图像质量和多样性。

Abstract: Text-to-Image (T2I) models generate high-quality images from text prompts but
often exhibit unintended social biases, such as gender or racial stereotypes,
even when these attributes are not mentioned. Existing debiasing methods work
well for simple or well-known cases but struggle with subtle or overlapping
biases. We propose AutoDebias, a framework that automatically identifies and
mitigates harmful biases in T2I models without prior knowledge of specific bias
types. Specifically, AutoDebias leverages vision-language models to detect
biased visual patterns and constructs fairness guides by generating inclusive
alternative prompts that reflect balanced representations. These guides drive a
CLIP-guided training process that promotes fairer outputs while preserving the
original model's image quality and diversity. Unlike existing methods,
AutoDebias effectively addresses both subtle stereotypes and multiple
interacting biases. We evaluate the framework on a benchmark covering over 25
bias scenarios, including challenging cases where multiple biases occur
simultaneously. AutoDebias detects harmful patterns with 91.6% accuracy and
reduces biased outputs from 90% to negligible levels, while preserving the
visual fidelity of the original model.

</details>


### [54] [CLIPTime: Time-Aware Multimodal Representation Learning from Images and Text](https://arxiv.org/abs/2508.00447)
*Anju Rani,Daniel Ortiz-Arroyo,Petar Durdevic*

Main category: cs.CV

TL;DR: CLIPTime是一个多模态、多任务框架，基于CLIP架构，用于从图像和文本输入预测真菌生长的发育阶段和时间戳，无需显式时间输入。


<details>
  <summary>Details</summary>
Motivation: 理解生物生长的时序动态在微生物学、农业和生物降解研究中至关重要，但现有视觉语言模型在捕捉时序进展方面效果有限。

Method: 提出CLIPTime框架，学习视觉-文本联合嵌入，支持时间感知推理；引入合成真菌生长数据集，标注时间戳和阶段标签；联合分类和回归任务。

Result: 实验表明，CLIPTime能有效建模生物进展，生成可解释的时间相关输出，展示了视觉语言模型在生物监测中的潜力。

Conclusion: CLIPTime为生物时序动态分析提供了新工具，扩展了视觉语言模型的应用范围。

Abstract: Understanding the temporal dynamics of biological growth is critical across
diverse fields such as microbiology, agriculture, and biodegradation research.
Although vision-language models like Contrastive Language Image Pretraining
(CLIP) have shown strong capabilities in joint visual-textual reasoning, their
effectiveness in capturing temporal progression remains limited. To address
this, we propose CLIPTime, a multimodal, multitask framework designed to
predict both the developmental stage and the corresponding timestamp of fungal
growth from image and text inputs. Built upon the CLIP architecture, our model
learns joint visual-textual embeddings and enables time-aware inference without
requiring explicit temporal input during testing. To facilitate training and
evaluation, we introduce a synthetic fungal growth dataset annotated with
aligned timestamps and categorical stage labels. CLIPTime jointly performs
classification and regression, predicting discrete growth stages alongside
continuous timestamps. We also propose custom evaluation metrics, including
temporal accuracy and regression error, to assess the precision of time-aware
predictions. Experimental results demonstrate that CLIPTime effectively models
biological progression and produces interpretable, temporally grounded outputs,
highlighting the potential of vision-language models in real-world biological
monitoring applications.

</details>


### [55] [PIF-Net: Ill-Posed Prior Guided Multispectral and Hyperspectral Image Fusion via Invertible Mamba and Fusion-Aware LoRA](https://arxiv.org/abs/2508.00453)
*Baisong Li,Xingwang Wang,Haixiao Xu*

Main category: cs.CV

TL;DR: PIF-Net 是一个用于多光谱和高光谱图像融合的框架，通过引入病态先验和可逆 Mamba 架构，解决了数据对齐问题，并在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多光谱和高光谱图像融合任务因光谱与空间信息的固有权衡和数据对齐问题而具有病态性，现有方法未能有效解决。

Method: 提出 PIF-Net 框架，结合病态先验和可逆 Mamba 架构，设计 Fusion-Aware Low-Rank Adaptation 模块动态校准特征。

Result: 在多个基准数据集上，PIF-Net 显著优于现有方法，同时保持模型高效。

Conclusion: PIF-Net 通过创新的架构和模块设计，有效解决了图像融合中的病态性问题，提升了性能。

Abstract: The goal of multispectral and hyperspectral image fusion (MHIF) is to
generate high-quality images that simultaneously possess rich spectral
information and fine spatial details. However, due to the inherent trade-off
between spectral and spatial information and the limited availability of
observations, this task is fundamentally ill-posed. Previous studies have not
effectively addressed the ill-posed nature caused by data misalignment. To
tackle this challenge, we propose a fusion framework named PIF-Net, which
explicitly incorporates ill-posed priors to effectively fuse multispectral
images and hyperspectral images. To balance global spectral modeling with
computational efficiency, we design a method based on an invertible Mamba
architecture that maintains information consistency during feature
transformation and fusion, ensuring stable gradient flow and process
reversibility. Furthermore, we introduce a novel fusion module called the
Fusion-Aware Low-Rank Adaptation module, which dynamically calibrates spectral
and spatial features while keeping the model lightweight. Extensive experiments
on multiple benchmark datasets demonstrate that PIF-Net achieves significantly
better image restoration performance than current state-of-the-art methods
while maintaining model efficiency.

</details>


### [56] [Semantic and Temporal Integration in Latent Diffusion Space for High-Fidelity Video Super-Resolution](https://arxiv.org/abs/2508.00471)
*Yiwen Wang,Xinning Chai,Yuhong Zhang,Zhengxue Cheng,Jun Zhao,Rong Xie,Li Song*

Main category: cs.CV

TL;DR: SeTe-VSR是一种结合语义和时空引导的视频超分辨率方法，显著提升了细节恢复和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频超分辨率模型在生成过程中难以同时保证高保真度和时间一致性。

Method: 通过潜在扩散空间中的语义和时空引导，平衡细节恢复和时间一致性。

Result: 实验表明，SeTe-VSR在细节恢复和感知质量上优于现有方法。

Conclusion: SeTe-VSR为复杂视频超分辨率任务提供了有效解决方案。

Abstract: Recent advancements in video super-resolution (VSR) models have demonstrated
impressive results in enhancing low-resolution videos. However, due to
limitations in adequately controlling the generation process, achieving high
fidelity alignment with the low-resolution input while maintaining temporal
consistency across frames remains a significant challenge. In this work, we
propose Semantic and Temporal Guided Video Super-Resolution (SeTe-VSR), a novel
approach that incorporates both semantic and temporal-spatio guidance in the
latent diffusion space to address these challenges. By incorporating high-level
semantic information and integrating spatial and temporal information, our
approach achieves a seamless balance between recovering intricate details and
ensuring temporal coherence. Our method not only preserves high-reality visual
content but also significantly enhances fidelity. Extensive experiments
demonstrate that SeTe-VSR outperforms existing methods in terms of detail
recovery and perceptual quality, highlighting its effectiveness for complex
video super-resolution tasks.

</details>


### [57] [HyPCV-Former: Hyperbolic Spatio-Temporal Transformer for 3D Point Cloud Video Anomaly Detection](https://arxiv.org/abs/2508.00473)
*Jiaping Cao,Kangkang Zhou,Juan Du*

Main category: cs.CV

TL;DR: 提出了一种名为HyPCV-Former的双曲时空变换器，用于3D点云视频中的异常检测，通过双曲空间建模层次结构和时空连续性，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在RGB或深度域中使用欧几里得表示，无法有效捕捉事件的层次结构和时空连续性。

Method: 提取点云序列的空间特征，嵌入洛伦兹双曲空间，并引入双曲多头自注意力机制（HMHA）建模时间动态。

Result: 在TIMo和DAD数据集上分别取得7%和5.6%的性能提升，达到最先进水平。

Conclusion: HyPCV-Former通过双曲空间建模显著提升了异常检测性能，展示了非欧几里得几何的潜力。

Abstract: Video anomaly detection is a fundamental task in video surveillance, with
broad applications in public safety and intelligent monitoring systems.
Although previous methods leverage Euclidean representations in RGB or depth
domains, such embeddings are inherently limited in capturing hierarchical event
structures and spatio-temporal continuity. To address these limitations, we
propose HyPCV-Former, a novel hyperbolic spatio-temporal transformer for
anomaly detection in 3D point cloud videos. Our approach first extracts
per-frame spatial features from point cloud sequences via point cloud
extractor, and then embeds them into Lorentzian hyperbolic space, which better
captures the latent hierarchical structure of events. To model temporal
dynamics, we introduce a hyperbolic multi-head self-attention (HMHA) mechanism
that leverages Lorentzian inner products and curvature-aware softmax to learn
temporal dependencies under non-Euclidean geometry. Our method performs all
feature transformations and anomaly scoring directly within full Lorentzian
space rather than via tangent space approximation. Extensive experiments
demonstrate that HyPCV-Former achieves state-of-the-art performance across
multiple anomaly categories, with a 7\% improvement on the TIMo dataset and a
5.6\% gain on the DAD dataset compared to benchmarks. The code will be released
upon paper acceptance.

</details>


### [58] [LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer](https://arxiv.org/abs/2508.00477)
*Yuzhuo Chen,Zehua Ma,Jianhua Wang,Kai Kang,Shunyu Yao,Weiming Zhang*

Main category: cs.CV

TL;DR: LAMIC是一个无需训练的布局感知多图像合成框架，通过两种注意力机制实现多参考场景下的图像生成，并在多个指标上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多参考图像合成中布局一致性和背景一致性的挑战。

Method: 基于MMDiT模型，引入Group Isolation Attention和Region-Modulated Attention两种注意力机制。

Result: 在ID-S、BG-S、IN-R和AVG等指标上优于现有方法，展示了零样本泛化能力。

Conclusion: LAMIC为可控多图像合成提供了一种无需训练的新范式，具有强大的扩展潜力。

Abstract: In controllable image synthesis, generating coherent and consistent images
from multiple references with spatial layout awareness remains an open
challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework
that, for the first time, extends single-reference diffusion models to
multi-reference scenarios in a training-free manner. Built upon the MMDiT
model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group
Isolation Attention (GIA) to enhance entity disentanglement; and 2)
Region-Modulated Attention (RMA) to enable layout-aware generation. To
comprehensively evaluate model capabilities, we further introduce three
metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout
control; and 2) Background Similarity (BG-S) for measuring background
consistency. Extensive experiments show that LAMIC achieves state-of-the-art
performance across most major metrics: it consistently outperforms existing
multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all
settings, and achieves the best DPG in complex composition tasks. These results
demonstrate LAMIC's superior abilities in identity keeping, background
preservation, layout control, and prompt-following, all achieved without any
training or fine-tuning, showcasing strong zero-shot generalization ability. By
inheriting the strengths of advanced single-reference models and enabling
seamless extension to multi-image scenarios, LAMIC establishes a new
training-free paradigm for controllable multi-image composition. As foundation
models continue to evolve, LAMIC's performance is expected to scale
accordingly. Our implementation is available at:
https://github.com/Suchenl/LAMIC.

</details>


### [59] [SAMSA 2.0: Prompting Segment Anything with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation](https://arxiv.org/abs/2508.00493)
*Alfie Roddan,Tobias Czempiel,Chi Xu,Daniel S. Elson,Stamatia Giannarou*

Main category: cs.CV

TL;DR: SAMSA 2.0结合光谱角度提示与空间线索，提升高光谱医学图像分割的准确性和鲁棒性，无需重新训练即可优于RGB模型和现有光谱融合方法。


<details>
  <summary>Details</summary>
Motivation: 解决高光谱医学图像分割中光谱信息利用不足的问题，提升在低数据和噪声场景下的性能。

Method: 引入光谱角度提示，将光谱相似性与空间线索结合，指导Segment Anything Model (SAM)进行分割。

Result: 相比仅RGB模型，Dice分数提升最高3.8%；优于现有光谱融合方法最高3.1%。

Conclusion: SAMSA 2.0在少样本和零样本任务中表现优异，适用于临床成像中的复杂场景。

Abstract: We present SAMSA 2.0, an interactive segmentation framework for hyperspectral
medical imaging that introduces spectral angle prompting to guide the Segment
Anything Model (SAM) using spectral similarity alongside spatial cues. This
early fusion of spectral information enables more accurate and robust
segmentation across diverse spectral datasets. Without retraining, SAMSA 2.0
achieves up to +3.8% higher Dice scores compared to RGB-only models and up to
+3.1% over prior spectral fusion methods. Our approach enhances few-shot and
zero-shot performance, demonstrating strong generalization in challenging
low-data and noisy scenarios common in clinical imaging.

</details>


### [60] [LesiOnTime -- Joint Temporal and Clinical Modeling for Small Breast Lesion Segmentation in Longitudinal DCE-MRI](https://arxiv.org/abs/2508.00496)
*Mohammed Kamran,Maria Bernathova,Raoul Varga,Christian Singer,Zsuzsanna Bago-Horvath,Thomas Helbich,Georg Langs,Philipp Seeböck*

Main category: cs.CV

TL;DR: LesiOnTime是一种新型3D分割方法，通过结合纵向影像和BI-RADS评分，模拟临床诊断流程，显著提升小病灶分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法主要针对大病灶，忽略了纵向和临床信息，而早期癌症检测需要这些信息。

Method: 提出Temporal Prior Attention (TPA)块动态整合历史扫描信息，以及BI-RADS Consistency Regularization (BCR)损失函数嵌入临床知识。

Result: 在内部纵向数据集上，Dice分数比现有方法提升5%，TPA和BCR均贡献性能提升。

Conclusion: 结合时间和临床背景对乳腺癌筛查中的早期病灶分割至关重要。

Abstract: Accurate segmentation of small lesions in Breast Dynamic Contrast-Enhanced
MRI (DCE-MRI) is critical for early cancer detection, especially in high-risk
patients. While recent deep learning methods have advanced lesion segmentation,
they primarily target large lesions and neglect valuable longitudinal and
clinical information routinely used by radiologists. In real-world screening,
detecting subtle or emerging lesions requires radiologists to compare across
timepoints and consider previous radiology assessments, such as the BI-RADS
score. We propose LesiOnTime, a novel 3D segmentation approach that mimics
clinical diagnostic workflows by jointly leveraging longitudinal imaging and
BIRADS scores. The key components are: (1) a Temporal Prior Attention (TPA)
block that dynamically integrates information from previous and current scans;
and (2) a BI-RADS Consistency Regularization (BCR) loss that enforces latent
space alignment for scans with similar radiological assessments, thus embedding
domain knowledge into the training process. Evaluated on a curated in-house
longitudinal dataset of high-risk patients with DCE-MRI, our approach
outperforms state-of-the-art single-timepoint and longitudinal baselines by 5%
in terms of Dice. Ablation studies demonstrate that both TPA and BCR contribute
complementary performance gains. These results highlight the importance of
incorporating temporal and clinical context for reliable early lesion
segmentation in real-world breast cancer screening. Our code is publicly
available at https://github.com/cirmuw/LesiOnTime

</details>


### [61] [Leveraging Convolutional and Graph Networks for an Unsupervised Remote Sensing Labelling Tool](https://arxiv.org/abs/2508.00506)
*Tulsi Patel,Mark W. Jones,Thomas Redfern*

Main category: cs.CV

TL;DR: 提出了一种无监督的遥感图像标注方法，结合卷积和图神经网络，提高了标注的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 遥感图像标注耗时且成本高，现有方法依赖预标注数据，限制了灵活性。

Method: 使用卷积和图神经网络进行图像分割，生成更鲁棒的特征空间，并通过图神经网络聚合邻域信息。

Result: 减少了标注中的异常值，支持细粒度标注，并实现了旋转不变的语义关系。

Conclusion: 该方法为遥感图像标注提供了一种高效且灵活的无监督解决方案。

Abstract: Machine learning for remote sensing imaging relies on up-to-date and accurate
labels for model training and testing. Labelling remote sensing imagery is time
and cost intensive, requiring expert analysis. Previous labelling tools rely on
pre-labelled data for training in order to label new unseen data. In this work,
we define an unsupervised pipeline for finding and labelling geographical areas
of similar context and content within Sentinel-2 satellite imagery. Our
approach removes limitations of previous methods by utilising segmentation with
convolutional and graph neural networks to encode a more robust feature space
for image comparison. Unlike previous approaches we segment the image into
homogeneous regions of pixels that are grouped based on colour and spatial
similarity. Graph neural networks are used to aggregate information about the
surrounding segments enabling the feature representation to encode the local
neighbourhood whilst preserving its own local information. This reduces
outliers in the labelling tool, allows users to label at a granular level, and
allows a rotationally invariant semantic relationship at the image level to be
formed within the encoding space.

</details>


### [62] [EPANet: Efficient Path Aggregation Network for Underwater Fish Detection](https://arxiv.org/abs/2508.00528)
*Jinsong Yang,Zeyuan Hu,Yichen Li*

Main category: cs.CV

TL;DR: 提出了一种高效路径聚合网络（EPANet），用于解决水下鱼类检测中的低分辨率、背景干扰和目标相似性问题，通过互补特征集成实现轻量级且准确的检测。


<details>
  <summary>Details</summary>
Motivation: 水下鱼类检测因低分辨率、背景干扰和目标相似性而具有挑战性，现有方法常因复杂注意力机制导致模型效率低下。

Method: EPANet包含高效路径聚合特征金字塔网络（EPA-FPN）和多尺度多样化短路径瓶颈（MS-DDSP瓶颈），前者通过跨尺度跳连提升特征互补性，后者通过细粒度特征分割增强局部多样性。

Result: 在基准数据集上，EPANet在检测精度和推理速度上优于现有方法，同时保持较低参数复杂度。

Conclusion: EPANet通过高效特征集成和多样化操作，实现了轻量级且高性能的水下鱼类检测。

Abstract: Underwater fish detection (UFD) remains a challenging task in computer vision
due to low object resolution, significant background interference, and high
visual similarity between targets and surroundings. Existing approaches
primarily focus on local feature enhancement or incorporate complex attention
mechanisms to highlight small objects, often at the cost of increased model
complexity and reduced efficiency. To address these limitations, we propose an
efficient path aggregation network (EPANet), which leverages complementary
feature integration to achieve accurate and lightweight UFD. EPANet consists of
two key components: an efficient path aggregation feature pyramid network
(EPA-FPN) and a multi-scale diverse-division short path bottleneck (MS-DDSP
bottleneck). The EPA-FPN introduces long-range skip connections across
disparate scales to improve semantic-spatial complementarity, while cross-layer
fusion paths are adopted to enhance feature integration efficiency. The MS-DDSP
bottleneck extends the conventional bottleneck structure by introducing
finer-grained feature division and diverse convolutional operations, thereby
increasing local feature diversity and representation capacity. Extensive
experiments on benchmark UFD datasets demonstrate that EPANet outperforms
state-of-the-art methods in terms of detection accuracy and inference speed,
while maintaining comparable or even lower parameter complexity.

</details>


### [63] [Video Color Grading via Look-Up Table Generation](https://arxiv.org/abs/2508.00548)
*Seunghyun Shin,Dongmin Shin,Jisu Shin,Hae-Gon Jeon,Joon-Young Lee*

Main category: cs.CV

TL;DR: 提出了一种基于参考的视频色彩分级框架，通过扩散模型生成查找表（LUT）实现色彩属性对齐，支持用户通过文本提示调整低层特征。


<details>
  <summary>Details</summary>
Motivation: 视频色彩分级通常需要专业技能，限制了非专业人士的使用，因此需要一种自动化且易于操作的方法。

Method: 使用扩散模型生成LUT，对齐参考场景与输入视频的色彩属性，并通过文本提示增强低层特征。

Result: 实验和用户研究表明，该方法能有效进行视频色彩分级，且不损失结构细节。

Conclusion: 该框架为视频色彩分级提供了一种高效且用户友好的解决方案。

Abstract: Different from color correction and transfer, color grading involves
adjusting colors for artistic or storytelling purposes in a video, which is
used to establish a specific look or mood. However, due to the complexity of
the process and the need for specialized editing skills, video color grading
remains primarily the domain of professional colorists. In this paper, we
present a reference-based video color grading framework. Our key idea is
explicitly generating a look-up table (LUT) for color attribute alignment
between reference scenes and input video via a diffusion model. As a training
objective, we enforce that high-level features of the reference scenes like
look, mood, and emotion should be similar to that of the input video. Our
LUT-based approach allows for color grading without any loss of structural
details in the whole video frames as well as achieving fast inference. We
further build a pipeline to incorporate a user-preference via text prompts for
low-level feature enhancement such as contrast and brightness, etc.
Experimental results, including extensive user studies, demonstrate the
effectiveness of our approach for video color grading. Codes are publicly
available at https://github.com/seunghyuns98/VideoColorGrading.

</details>


### [64] [Your other Left! Vision-Language Models Fail to Identify Relative Positions in Medical Images](https://arxiv.org/abs/2508.00549)
*Daniel Wolf,Heiko Hillenhagen,Billurvan Taskin,Alex Bäuerle,Meinrad Beer,Michael Götz,Timo Ropinski*

Main category: cs.CV

TL;DR: 论文探讨了视觉语言模型（VLMs）在医学图像中精确定位解剖结构相对位置的能力，发现现有模型表现不佳，并尝试通过视觉提示改进。最终提出了MIRP基准数据集以推动相关研究。


<details>
  <summary>Details</summary>
Motivation: 临床决策依赖解剖结构的相对位置信息，但目前VLMs在此任务上表现不足，亟需探索改进方法。

Method: 评估了GPT-4o、Llama3.2等VLMs的表现，并测试了视觉提示（如标记）的效果。

Result: 所有模型均表现不佳，视觉提示仅带来有限改进，医学图像上的表现显著低于自然图像。

Conclusion: VLMs在医学图像中更依赖先验知识而非图像内容，需进一步研究。MIRP数据集为此提供了工具。

Abstract: Clinical decision-making relies heavily on understanding relative positions
of anatomical structures and anomalies. Therefore, for Vision-Language Models
(VLMs) to be applicable in clinical practice, the ability to accurately
determine relative positions on medical images is a fundamental prerequisite.
Despite its importance, this capability remains highly underexplored. To
address this gap, we evaluate the ability of state-of-the-art VLMs, GPT-4o,
Llama3.2, Pixtral, and JanusPro, and find that all models fail at this
fundamental task. Inspired by successful approaches in computer vision, we
investigate whether visual prompts, such as alphanumeric or colored markers
placed on anatomical structures, can enhance performance. While these markers
provide moderate improvements, results remain significantly lower on medical
images compared to observations made on natural images. Our evaluations suggest
that, in medical imaging, VLMs rely more on prior anatomical knowledge than on
actual image content for answering relative position questions, often leading
to incorrect conclusions. To facilitate further research in this area, we
introduce the MIRP , Medical Imaging Relative Positioning, benchmark dataset,
designed to systematically evaluate the capability to identify relative
positions in medical images.

</details>


### [65] [DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable Adversarial Purification](https://arxiv.org/abs/2508.00552)
*Chihan Huang,Belal Alsinglawi,Islam Al-qudah*

Main category: cs.CV

TL;DR: 本文提出了一种名为DBLP的高效扩散对抗净化框架，通过噪声桥蒸馏和自适应语义增强，显著提升了对抗净化的速度和效果。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络（DNNs）对对抗性扰动的敏感性是一个关键漏洞，现有扩散对抗净化方法需要大量迭代去噪，限制了实际应用。

Method: 提出噪声桥蒸馏目标，构建对抗噪声分布与干净数据分布之间的对齐，并引入自适应语义增强，利用多尺度金字塔边缘图指导净化过程。

Result: 实验表明，DBLP在多个数据集上实现了最先进的鲁棒准确性、卓越的图像质量，且推理时间约为0.2秒。

Conclusion: DBLP为实时对抗净化迈出了重要一步。

Abstract: Recent advances in deep neural networks (DNNs) have led to remarkable success
across a wide range of tasks. However, their susceptibility to adversarial
perturbations remains a critical vulnerability. Existing diffusion-based
adversarial purification methods often require intensive iterative denoising,
severely limiting their practical deployment. In this paper, we propose
Diffusion Bridge Distillation for Purification (DBLP), a novel and efficient
diffusion-based framework for adversarial purification. Central to our approach
is a new objective, noise bridge distillation, which constructs a principled
alignment between the adversarial noise distribution and the clean data
distribution within a latent consistency model (LCM). To further enhance
semantic fidelity, we introduce adaptive semantic enhancement, which fuses
multi-scale pyramid edge maps as conditioning input to guide the purification
process. Extensive experiments across multiple datasets demonstrate that DBLP
achieves state-of-the-art (SOTA) robust accuracy, superior image quality, and
around 0.2s inference time, marking a significant step toward real-time
adversarial purification.

</details>


### [66] [HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention in Vision-Language Models](https://arxiv.org/abs/2508.00553)
*Jizhihui Liu,Feiyi Du,Guangdao Zhu,Niu Lian,Jun Li,Bin Chen*

Main category: cs.CV

TL;DR: HiPrune是一种无需训练、模型无关的视觉令牌剪枝框架，利用视觉编码器的分层注意力结构，显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型中视觉令牌序列过长导致的计算开销大和推理效率低的问题。

Method: 通过分层注意力结构选择三类信息令牌：锚定令牌、缓冲令牌和注册令牌，无需重新训练。

Result: 在多个模型上实现最先进的剪枝性能，保留高达99.3%的任务准确率，同时减少9倍的推理计算开销。

Conclusion: HiPrune是一种高效、通用的剪枝方法，适用于多种视觉语言模型和任务。

Abstract: Vision-Language Models (VLMs) encode images into lengthy sequences of visual
tokens, leading to excessive computational overhead and limited inference
efficiency. While prior efforts prune or merge tokens to address this issue,
they often rely on special tokens (e.g., CLS) or require task-specific
training, hindering scalability across architectures. In this paper, we propose
HiPrune, a training-free and model-agnostic token Pruning framework that
exploits the Hierarchical attention structure within vision encoders. We
identify that middle layers attend to object-centric regions, while deep layers
capture global contextual features. Based on this observation, HiPrune selects
three types of informative tokens: (1) Anchor tokens with high attention in
object-centric layers, (2) Buffer tokens adjacent to anchors for spatial
continuity, and (3) Register tokens with strong attention in deep layers for
global summarization. Our method requires no retraining and integrates
seamlessly with any ViT-based VLM. Extensive experiments on LLaVA-1.5,
LLaVA-NeXT, and Qwen2.5-VL demonstrate that HiPrune achieves state-of-the-art
pruning performance, preserving up to 99.3% task accuracy with only 33.3%
tokens, and maintaining 99.5% accuracy with just 11.1% tokens. Meanwhile, it
reduces inference FLOPs and latency by up to 9$\times$, showcasing strong
generalization across models and tasks. Code is available at
https://github.com/Danielement321/HiPrune.

</details>


### [67] [Training-Free Class Purification for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2508.00557)
*Qi Chen,Lingxiao Yang,Yun Chen,Nailong Zhao,Jianhuang Lai,Jie Shao,Xiaohua Xie*

Main category: cs.CV

TL;DR: FreeCP是一种无需训练的类别净化框架，旨在解决开放词汇语义分割中的类别冗余和视觉语言模糊性问题，显著提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有无需训练方法忽略了类别冗余和视觉语言模糊性带来的挑战，导致次优的类别激活图。FreeCP旨在解决这些问题。

Method: FreeCP通过净化语义类别并纠正冗余和模糊性引起的错误，利用净化后的类别表示生成最终分割预测。

Result: 在八个基准测试中，FreeCP作为即插即用模块，显著提升了与其他开放词汇语义分割方法的性能。

Conclusion: FreeCP有效解决了无需训练方法中的关键问题，提升了开放词汇语义分割的性能。

Abstract: Fine-tuning pre-trained vision-language models has emerged as a powerful
approach for enhancing open-vocabulary semantic segmentation (OVSS). However,
the substantial computational and resource demands associated with training on
large datasets have prompted interest in training-free methods for OVSS.
Existing training-free approaches primarily focus on modifying model
architectures and generating prototypes to improve segmentation performance.
However, they often neglect the challenges posed by class redundancy, where
multiple categories are not present in the current test image, and
visual-language ambiguity, where semantic similarities among categories create
confusion in class activation. These issues can lead to suboptimal class
activation maps and affinity-refined activation maps. Motivated by these
observations, we propose FreeCP, a novel training-free class purification
framework designed to address these challenges. FreeCP focuses on purifying
semantic categories and rectifying errors caused by redundancy and ambiguity.
The purified class representations are then leveraged to produce final
segmentation predictions. We conduct extensive experiments across eight
benchmarks to validate FreeCP's effectiveness. Results demonstrate that FreeCP,
as a plug-and-play module, significantly boosts segmentation performance when
combined with other OVSS methods.

</details>


### [68] [Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints](https://arxiv.org/abs/2508.00558)
*Jens U. Kreber,Joerg Stueckler*

Main category: cs.CV

TL;DR: PhysNAP是一种基于扩散模型的新方法，用于生成与部分点云对齐且物理合理的铰接物体。


<details>
  <summary>Details</summary>
Motivation: 铰接物体是日常环境中重要的交互对象，但现有方法在物理合理性和点云对齐方面存在不足。

Method: 使用符号距离函数（SDFs）表示部件形状，通过点云对齐损失和非穿透性、移动性约束指导反向扩散过程。

Result: 在PartNet-Mobility数据集上验证，PhysNAP在约束一致性和生成能力之间提供了平衡。

Conclusion: PhysNAP能够生成更物理合理的铰接物体，并在点云对齐方面优于无指导的扩散模型。

Abstract: Articulated objects are an important type of interactable objects in everyday
environments. In this paper, we propose PhysNAP, a novel diffusion model-based
approach for generating articulated objects that aligns them with partial point
clouds and improves their physical plausibility. The model represents part
shapes by signed distance functions (SDFs). We guide the reverse diffusion
process using a point cloud alignment loss computed using the predicted SDFs.
Additionally, we impose non-penetration and mobility constraints based on the
part SDFs for guiding the model to generate more physically plausible objects.
We also make our diffusion approach category-aware to further improve point
cloud alignment if category information is available. We evaluate the
generative ability and constraint consistency of samples generated with PhysNAP
using the PartNet-Mobility dataset. We also compare it with an unguided
baseline diffusion model and demonstrate that PhysNAP can improve constraint
consistency and provides a tradeoff with generative ability.

</details>


### [69] [Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images](https://arxiv.org/abs/2508.00563)
*Hannah Kniesel,Leon Sick,Tristan Payer,Tim Bergner,Kavitha Shaga Devan,Clarissa Read,Paul Walther,Timo Ropinski*

Main category: cs.CV

TL;DR: 提出了一种基于图像级标注的弱监督目标检测算法，通过预训练模型生成伪标签，优化方法提取病毒颗粒，优于现有弱标注方法。


<details>
  <summary>Details</summary>
Motivation: 获取目标检测的边界框标注成本高且耗时，尤其是需要领域专家知识。图像级标注更易获取，因此提出弱监督方法。

Method: 利用预训练模型预测图像中病毒存在与否，生成伪标签；采用优化方法和缩小感受野提取病毒颗粒。

Result: 伪标签更易获取，且在标注时间有限时，性能优于现有弱标注方法甚至真实标注。

Conclusion: 该方法显著降低了标注成本，同时保持了高性能，适用于领域特定的目标检测任务。

Abstract: Current state-of-the-art methods for object detection rely on annotated
bounding boxes of large data sets for training. However, obtaining such
annotations is expensive and can require up to hundreds of hours of manual
labor. This poses a challenge, especially since such annotations can only be
provided by experts, as they require knowledge about the scientific domain. To
tackle this challenge, we propose a domain-specific weakly supervised object
detection algorithm that only relies on image-level annotations, which are
significantly easier to acquire. Our method distills the knowledge of a
pre-trained model, on the task of predicting the presence or absence of a virus
in an image, to obtain a set of pseudo-labels that can be used to later train a
state-of-the-art object detection model. To do so, we use an optimization
approach with a shrinking receptive field to extract virus particles directly
without specific network architectures. Through a set of extensive studies, we
show how the proposed pseudo-labels are easier to obtain, and, more
importantly, are able to outperform other existing weak labeling methods, and
even ground truth labels, in cases where the time to obtain the annotation is
limited.

</details>


### [70] [CoProU-VO: Combining Projected Uncertainty for End-to-End Unsupervised Monocular Visual Odometry](https://arxiv.org/abs/2508.00568)
*Jingchao Xie,Oussema Dhaouadi,Weirong Chen,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: CoProU-VO通过跨帧不确定性传播改进视觉里程计，在动态场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 动态物体和遮挡导致传统无监督视觉里程计方法在静态场景假设下失效，需要更鲁棒的不确定性建模。

Method: 提出CoProU-VO，结合目标帧和参考帧的不确定性，基于概率公式和视觉Transformer架构，联合学习深度、不确定性和相机位姿。

Result: 在KITTI和nuScenes数据集上显著优于现有方法，尤其在高速公路场景中表现突出。

Conclusion: 跨帧不确定性传播有效提升动态场景下的视觉里程计性能。

Abstract: Visual Odometry (VO) is fundamental to autonomous navigation, robotics, and
augmented reality, with unsupervised approaches eliminating the need for
expensive ground-truth labels. However, these methods struggle when dynamic
objects violate the static scene assumption, leading to erroneous pose
estimations. We tackle this problem by uncertainty modeling, which is a
commonly used technique that creates robust masks to filter out dynamic objects
and occlusions without requiring explicit motion segmentation. Traditional
uncertainty modeling considers only single-frame information, overlooking the
uncertainties across consecutive frames. Our key insight is that uncertainty
must be propagated and combined across temporal frames to effectively identify
unreliable regions, particularly in dynamic scenes. To address this challenge,
we introduce Combined Projected Uncertainty VO (CoProU-VO), a novel end-to-end
approach that combines target frame uncertainty with projected reference frame
uncertainty using a principled probabilistic formulation. Built upon vision
transformer backbones, our model simultaneously learns depth, uncertainty
estimation, and camera poses. Consequently, experiments on the KITTI and
nuScenes datasets demonstrate significant improvements over previous
unsupervised monocular end-to-end two-frame-based methods and exhibit strong
performance in challenging highway scenes where other approaches often fail.
Additionally, comprehensive ablation studies validate the effectiveness of
cross-frame uncertainty propagation.

</details>


### [71] [Uncertainty-Aware Likelihood Ratio Estimation for Pixel-Wise Out-of-Distribution Detection](https://arxiv.org/abs/2508.00587)
*Marc Hölle,Walter Kellermann,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: 论文提出了一种基于不确定性感知的似然比估计方法，用于区分语义分割模型中的已知和未知像素特征，显著降低了误报率。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在复杂场景中难以区分罕见类别与真正未知物体的问题。

Method: 使用证据分类器结合似然比检验，明确考虑不确定性，输出概率分布而非点估计。

Result: 在五个标准数据集上，误报率最低（2.5%），平均精度高（90.91%），计算开销可忽略。

Conclusion: 通过不确定性建模，有效提升了未知物体检测的准确性，且计算效率高。

Abstract: Semantic segmentation models trained on known object classes often fail in
real-world autonomous driving scenarios by confidently misclassifying unknown
objects. While pixel-wise out-of-distribution detection can identify unknown
objects, existing methods struggle in complex scenes where rare object classes
are often confused with truly unknown objects. We introduce an
uncertainty-aware likelihood ratio estimation method that addresses these
limitations. Our approach uses an evidential classifier within a likelihood
ratio test to distinguish between known and unknown pixel features from a
semantic segmentation model, while explicitly accounting for uncertainty.
Instead of producing point estimates, our method outputs probability
distributions that capture uncertainty from both rare training examples and
imperfect synthetic outliers. We show that by incorporating uncertainty in this
way, outlier exposure can be leveraged more effectively. Evaluated on five
standard benchmark datasets, our method achieves the lowest average false
positive rate (2.5%) among state-of-the-art while maintaining high average
precision (90.91%) and incurring only negligible computational overhead. Code
is available at https://github.com/glasbruch/ULRE.

</details>


### [72] [A Novel Modeling Framework and Data Product for Extended VIIRS-like Artificial Nighttime Light Image Reconstruction (1986-2024)](https://arxiv.org/abs/2508.00590)
*Yihe Tian,Kwan Man Cheng,Zhengbo Zhang,Tao Zhang,Suju Li,Dongmei Yan,Bing Xu*

Main category: cs.CV

TL;DR: 论文提出了一种新的夜间灯光数据重建框架（EVAL），解决了现有方法在光强度低估和结构缺失上的不足，将VIIRS数据时间序列扩展至1986年。


<details>
  <summary>Details</summary>
Motivation: 现有VIIRS数据始于2012年，限制了长期时间序列研究，且现有方法存在光强度低估和结构缺失问题。

Method: 采用两阶段重建框架：构建阶段使用分层融合解码器（HFD）提高初始重建保真度，细化阶段利用双特征细化器（DFR）结合高分辨率不透水面掩膜增强细节。

Result: EVAL产品将时间序列扩展至1986年，R²从0.68提升至0.80，RMSE从1.27降至0.99，且与经济社会参数高度相关。

Conclusion: EVAL数据集为研究社区提供了可靠的长期分析资源，并公开可用。

Abstract: Artificial Night-Time Light (NTL) remote sensing is a vital proxy for
quantifying the intensity and spatial distribution of human activities.
Although the NPP-VIIRS sensor provides high-quality NTL observations, its
temporal coverage, which begins in 2012, restricts long-term time-series
studies that extend to earlier periods. Despite the progress in extending
VIIRS-like NTL time-series, current methods still suffer from two significant
shortcomings: the underestimation of light intensity and the structural
omission. To overcome these limitations, we propose a novel reconstruction
framework consisting of a two-stage process: construction and refinement. The
construction stage features a Hierarchical Fusion Decoder (HFD) designed to
enhance the fidelity of the initial reconstruction. The refinement stage
employs a Dual Feature Refiner (DFR), which leverages high-resolution
impervious surface masks to guide and enhance fine-grained structural details.
Based on this framework, we developed the Extended VIIRS-like Artificial
Nighttime Light (EVAL) product for China, extending the standard data record
backwards by 26 years to begin in 1986. Quantitative evaluation shows that EVAL
significantly outperforms existing state-of-the-art products, boosting the
$\text{R}^2$ from 0.68 to 0.80 while lowering the RMSE from 1.27 to 0.99.
Furthermore, EVAL exhibits excellent temporal consistency and maintains a high
correlation with socioeconomic parameters, confirming its reliability for
long-term analysis. The resulting EVAL dataset provides a valuable new resource
for the research community and is publicly available at
https://doi.org/10.11888/HumanNat.tpdc.302930.

</details>


### [73] [Wukong Framework for Not Safe For Work Detection in Text-to-Image systems](https://arxiv.org/abs/2508.00591)
*Mingrui Liu,Sixiao Zhang,Cheng Long*

Main category: cs.CV

TL;DR: Wukong是一种基于Transformer的NSFW检测框架，利用扩散模型的早期去噪步骤和预训练参数，显著优于文本过滤器，并与图像过滤器精度相当，同时更高效。


<details>
  <summary>Details</summary>
Motivation: 现有外部保护措施（文本过滤器和图像过滤器）存在效率低或易受攻击的问题，需要一种更高效的NSFW内容检测方法。

Method: 利用扩散模型的早期去噪步骤和预训练交叉注意力参数，提出Wukong框架，在生成过程中实现早期检测。

Result: Wukong显著优于文本过滤器，与图像过滤器精度相当，同时效率更高。

Conclusion: Wukong提供了一种高效且准确的NSFW检测方法，适用于现代T2I系统。

Abstract: Text-to-Image (T2I) generation is a popular AI-generated content (AIGC)
technology enabling diverse and creative image synthesis. However, some outputs
may contain Not Safe For Work (NSFW) content (e.g., violence), violating
community guidelines. Detecting NSFW content efficiently and accurately, known
as external safeguarding, is essential. Existing external safeguards fall into
two types: text filters, which analyze user prompts but overlook T2I
model-specific variations and are prone to adversarial attacks; and image
filters, which analyze final generated images but are computationally costly
and introduce latency. Diffusion models, the foundation of modern T2I systems
like Stable Diffusion, generate images through iterative denoising using a
U-Net architecture with ResNet and Transformer blocks. We observe that: (1)
early denoising steps define the semantic layout of the image, and (2)
cross-attention layers in U-Net are crucial for aligning text and image
regions. Based on these insights, we propose Wukong, a transformer-based NSFW
detection framework that leverages intermediate outputs from early denoising
steps and reuses U-Net's pre-trained cross-attention parameters. Wukong
operates within the diffusion process, enabling early detection without waiting
for full image generation. We also introduce a new dataset containing prompts,
seeds, and image-specific NSFW labels, and evaluate Wukong on this and two
public benchmarks. Results show that Wukong significantly outperforms
text-based safeguards and achieves comparable accuracy of image filters, while
offering much greater efficiency.

</details>


### [74] [GeoMoE: Divide-and-Conquer Motion Field Modeling with Mixture-of-Experts for Two-View Geometry](https://arxiv.org/abs/2508.00592)
*Jiajun Le,Jiayi Ma*

Main category: cs.CV

TL;DR: GeoMoE利用混合专家模型（MoE）分解和建模两视图几何中的异质运动场，通过概率先验引导分解和MoE增强的双路径校正器，显著提升了运动场估计的精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 复杂真实场景中的运动场具有多样性和异质性，现有方法缺乏针对性建模策略，导致估计结果偏离真实结构。

Method: 提出GeoMoE框架，包括概率先验引导分解策略和MoE增强的双路径校正器，分别用于分解运动场和针对性建模。

Result: 在相对位姿和单应性估计任务中优于现有方法，并表现出强泛化能力。

Conclusion: GeoMoE通过分而治之的策略有效解决了异质运动场建模问题，为两视图几何提供了高效解决方案。

Abstract: Recent progress in two-view geometry increasingly emphasizes enforcing
smoothness and global consistency priors when estimating motion fields between
pairs of images. However, in complex real-world scenes, characterized by
extreme viewpoint and scale changes as well as pronounced depth
discontinuities, the motion field often exhibits diverse and heterogeneous
motion patterns. Most existing methods lack targeted modeling strategies and
fail to explicitly account for this variability, resulting in estimated motion
fields that diverge from their true underlying structure and distribution. We
observe that Mixture-of-Experts (MoE) can assign dedicated experts to motion
sub-fields, enabling a divide-and-conquer strategy for heterogeneous motion
patterns. Building on this insight, we re-architect motion field modeling in
two-view geometry with GeoMoE, a streamlined framework. Specifically, we first
devise a Probabilistic Prior-Guided Decomposition strategy that exploits inlier
probability signals to perform a structure-aware decomposition of the motion
field into heterogeneous sub-fields, sharply curbing outlier-induced bias.
Next, we introduce an MoE-Enhanced Bi-Path Rectifier that enhances each
sub-field along spatial-context and channel-semantic paths and routes it to a
customized expert for targeted modeling, thereby decoupling heterogeneous
motion regimes, suppressing cross-sub-field interference and representational
entanglement, and yielding fine-grained motion-field rectification. With this
minimalist design, GeoMoE outperforms prior state-of-the-art methods in
relative pose and homography estimation and shows strong generalization. The
source code and pre-trained models are available at
https://github.com/JiajunLe/GeoMoE.

</details>


### [75] [DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior](https://arxiv.org/abs/2508.00599)
*Junzhe Lu,Jing Lin,Hongkun Dou,Ailing Zeng,Yue Deng,Xian Liu,Zhongang Cai,Lei Yang,Yulun Zhang,Haoqian Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: DPoser-X是一种基于扩散的3D全身人体姿态先验模型，通过变分扩散采样解决姿态任务，结合截断时间步调度和掩码训练机制，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 构建多功能且鲁棒的全身人体姿态先验模型面临挑战，主要由于姿态的复杂性和高质量数据稀缺。

Method: 提出扩散模型DPoser-X，统一姿态任务为逆问题，采用变分扩散采样，引入截断时间步调度和掩码训练机制。

Result: 在多个基准测试中表现优异，优于现有方法，为全身姿态建模设定了新标准。

Conclusion: DPoser-X展示了强大的鲁棒性和多功能性，为全身姿态先验建模提供了新思路。

Abstract: We present DPoser-X, a diffusion-based prior model for 3D whole-body human
poses. Building a versatile and robust full-body human pose prior remains
challenging due to the inherent complexity of articulated human poses and the
scarcity of high-quality whole-body pose datasets. To address these
limitations, we introduce a Diffusion model as body Pose prior (DPoser) and
extend it to DPoser-X for expressive whole-body human pose modeling. Our
approach unifies various pose-centric tasks as inverse problems, solving them
through variational diffusion sampling. To enhance performance on downstream
applications, we introduce a novel truncated timestep scheduling method
specifically designed for pose data characteristics. We also propose a masked
training mechanism that effectively combines whole-body and part-specific
datasets, enabling our model to capture interdependencies between body parts
while avoiding overfitting to specific actions. Extensive experiments
demonstrate DPoser-X's robustness and versatility across multiple benchmarks
for body, hand, face, and full-body pose modeling. Our model consistently
outperforms state-of-the-art alternatives, establishing a new benchmark for
whole-body human pose prior modeling.

</details>


### [76] [Backdoor Attacks on Deep Learning Face Detection](https://arxiv.org/abs/2508.00620)
*Quentin Le Roux,Yannick Teglia,Teddy Furon,Philippe Loubet-Moundi*

Main category: cs.CV

TL;DR: 论文研究了针对人脸检测系统的生成攻击，首次提出了一种能够干扰坐标回归任务的Landmark Shift Attack，并提出了防御措施。


<details>
  <summary>Details</summary>
Motivation: 在无约束环境中，人脸检测系统面临光照、姿态等挑战，需要准确的边界框和关键点回归。然而，这些任务可能受到攻击，因此研究其脆弱性和防御方法至关重要。

Method: 通过生成攻击（Face Generation Attacks）干扰人脸检测系统，特别是首次提出Landmark Shift Attack，针对坐标回归任务进行后门攻击。

Result: 展示了攻击的有效性，成功干扰了人脸检测系统的坐标回归任务。

Conclusion: 论文揭示了人脸检测系统的脆弱性，并提出了相应的防御措施，为系统安全性提供了改进方向。

Abstract: Face Recognition Systems that operate in unconstrained environments capture
images under varying conditions,such as inconsistent lighting, or diverse face
poses. These challenges require including a Face Detection module that
regresses bounding boxes and landmark coordinates for proper Face Alignment.
This paper shows the effectiveness of Object Generation Attacks on Face
Detection, dubbed Face Generation Attacks, and demonstrates for the first time
a Landmark Shift Attack that backdoors the coordinate regression task performed
by face detectors. We then offer mitigations against these vulnerabilities.

</details>


### [77] [Minimum Data, Maximum Impact: 20 annotated samples for explainable lung nodule classification](https://arxiv.org/abs/2508.00639)
*Luisa Gallée,Catharina Silvia Lisson,Christoph Gerhard Lisson,Daniela Drees,Felix Weig,Daniel Vogele,Meinrad Beer,Michael Götz*

Main category: cs.CV

TL;DR: 论文提出了一种通过生成模型合成属性标注数据的方法，以解决医学图像中属性标注数据稀缺的问题，并提升可解释模型的性能。


<details>
  <summary>Details</summary>
Motivation: 增强医学图像诊断中可解释模型的透明性和临床适用性，解决属性标注数据稀缺的挑战。

Method: 利用扩散模型生成属性标注数据，仅需少量真实标注样本（如20个肺结节样本），并将其用于训练可解释模型。

Result: 合成数据显著提升了模型性能，属性预测准确率提高13.4%，目标预测准确率提高1.8%。

Conclusion: 合成数据能有效克服数据集限制，提升可解释模型在医学图像分析中的应用潜力。

Abstract: Classification models that provide human-interpretable explanations enhance
clinicians' trust and usability in medical image diagnosis. One research focus
is the integration and prediction of pathology-related visual attributes used
by radiologists alongside the diagnosis, aligning AI decision-making with
clinical reasoning. Radiologists use attributes like shape and texture as
established diagnostic criteria and mirroring these in AI decision-making both
enhances transparency and enables explicit validation of model outputs.
However, the adoption of such models is limited by the scarcity of large-scale
medical image datasets annotated with these attributes. To address this
challenge, we propose synthesizing attribute-annotated data using a generative
model. We enhance the Diffusion Model with attribute conditioning and train it
using only 20 attribute-labeled lung nodule samples from the LIDC-IDRI dataset.
Incorporating its generated images into the training of an explainable model
boosts performance, increasing attribute prediction accuracy by 13.4% and
target prediction accuracy by 1.8% compared to training with only the small
real attribute-annotated dataset. This work highlights the potential of
synthetic data to overcome dataset limitations, enhancing the applicability of
explainable models in medical image analysis.

</details>


### [78] [Revisiting Adversarial Patch Defenses on Object Detectors: Unified Evaluation, Large-Scale Dataset, and New Insights](https://arxiv.org/abs/2508.00649)
*Junhao Zheng,Jiahao Sun,Chenhao Lin,Zhengyu Zhao,Chen Ma,Chong Zhang,Cong Wang,Qian Wang,Chao Shen*

Main category: cs.CV

TL;DR: 本文提出了首个针对目标检测器补丁攻击的防御评估基准，包含多种攻击目标、攻击方式、检测器和评估指标，揭示了防御性能的新见解。


<details>
  <summary>Details</summary>
Motivation: 现有防御评估缺乏统一框架，导致评估不一致且不完整。

Method: 重新评估11种代表性防御方法，构建大规模补丁攻击数据集，涉及2种攻击目标、13种补丁攻击、11种目标检测器和4种评估指标。

Result: 发现防御自然补丁的难点在于数据分布而非高频特征；新数据集可提升现有防御性能15.09%；自适应攻击可绕过现有防御。

Conclusion: 研究为补丁攻击/防御的评估和设计提供了指导，数据集和代码将持续更新。

Abstract: Developing reliable defenses against patch attacks on object detectors has
attracted increasing interest. However, we identify that existing defense
evaluations lack a unified and comprehensive framework, resulting in
inconsistent and incomplete assessments of current methods. To address this
issue, we revisit 11 representative defenses and present the first patch
defense benchmark, involving 2 attack goals, 13 patch attacks, 11 object
detectors, and 4 diverse metrics. This leads to the large-scale adversarial
patch dataset with 94 types of patches and 94,000 images. Our comprehensive
analyses reveal new insights: (1) The difficulty in defending against
naturalistic patches lies in the data distribution, rather than the commonly
believed high frequencies. Our new dataset with diverse patch distributions can
be used to improve existing defenses by 15.09% AP@0.5. (2) The average
precision of the attacked object, rather than the commonly pursued patch
detection accuracy, shows high consistency with defense performance. (3)
Adaptive attacks can substantially bypass existing defenses, and defenses with
complex/stochastic models or universal patch properties are relatively robust.
We hope that our analyses will serve as guidance on properly evaluating patch
attacks/defenses and advancing their design. Code and dataset are available at
https://github.com/Gandolfczjh/APDE, where we will keep integrating new
attacks/defenses.

</details>


### [79] [Can Large Pretrained Depth Estimation Models Help With Image Dehazing?](https://arxiv.org/abs/2508.00698)
*Hongfei Zhang,Kun Zhou,Ruizheng Wu,Jiangbo Lu*

Main category: cs.CV

TL;DR: 论文提出了一种基于预训练深度特征的图像去雾方法，通过RGB-D融合模块提升去雾效果。


<details>
  <summary>Details</summary>
Motivation: 现有去雾方法受限于特定架构设计，难以适应不同场景的需求。

Method: 利用预训练的深度特征，设计了一个即插即用的RGB-D融合模块。

Result: 实验验证了该方法在多个基准数据集上的有效性和广泛适用性。

Conclusion: 预训练的深度特征在去雾任务中具有显著的一致性和泛化能力。

Abstract: Image dehazing remains a challenging problem due to the spatially varying
nature of haze in real-world scenes. While existing methods have demonstrated
the promise of large-scale pretrained models for image dehazing, their
architecture-specific designs hinder adaptability across diverse scenarios with
different accuracy and efficiency requirements. In this work, we systematically
investigate the generalization capability of pretrained depth
representations-learned from millions of diverse images-for image dehazing. Our
empirical analysis reveals that the learned deep depth features maintain
remarkable consistency across varying haze levels. Building on this insight, we
propose a plug-and-play RGB-D fusion module that seamlessly integrates with
diverse dehazing architectures. Extensive experiments across multiple
benchmarks validate both the effectiveness and broad applicability of our
approach.

</details>


### [80] [D3: Training-Free AI-Generated Video Detection Using Second-Order Features](https://arxiv.org/abs/2508.00701)
*Chende Zheng,Ruiqi suo,Chenhao Lin,Zhengyu Zhao,Le Yang,Shuai Liu,Minghui Yang,Cong Wang,Chao Shen*

Main category: cs.CV

TL;DR: 论文提出了一种基于二阶动力学分析的训练免费检测方法D3，用于识别AI生成视频中的时间伪影，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法对合成视频中时间伪影的探索不足，导致检测效果受限。

Method: 通过牛顿力学下的二阶动力学分析建立理论框架，提出Second-order Central Difference特征，并开发了D3方法。

Result: 在4个开源数据集上验证了D3的优越性，例如在Gen-Video上平均精度提升10.39%。

Conclusion: D3在计算效率和鲁棒性方面表现优异，为合成视频检测提供了新思路。

Abstract: The evolution of video generation techniques, such as Sora, has made it
increasingly easy to produce high-fidelity AI-generated videos, raising public
concern over the dissemination of synthetic content. However, existing
detection methodologies remain limited by their insufficient exploration of
temporal artifacts in synthetic videos. To bridge this gap, we establish a
theoretical framework through second-order dynamical analysis under Newtonian
mechanics, subsequently extending the Second-order Central Difference features
tailored for temporal artifact detection. Building on this theoretical
foundation, we reveal a fundamental divergence in second-order feature
distributions between real and AI-generated videos. Concretely, we propose
Detection by Difference of Differences (D3), a novel training-free detection
method that leverages the above second-order temporal discrepancies. We
validate the superiority of our D3 on 4 open-source datasets (Gen-Video,
VideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo,
D3 outperforms the previous best method by 10.39% (absolute) mean Average
Precision. Additional experiments on time cost and post-processing operations
demonstrate D3's exceptional computational efficiency and strong robust
performance. Our code is available at https://github.com/Zig-HS/D3.

</details>


### [81] [MIHBench: Benchmarking and Mitigating Multi-Image Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2508.00726)
*Jiale Li,Mingrui Wu,Zixiang Jin,Hao Chen,Jiayi Ji,Xiaoshuai Sun,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: 该论文首次系统研究了多图像多模态大语言模型（MLLMs）中的幻觉问题，提出了专门用于评估多图像场景中对象相关幻觉的基准MIHBench，并提出了动态注意力平衡机制以减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单图像设置中的幻觉，多图像场景中的幻觉问题尚未充分探索，因此需要填补这一空白。

Method: 提出了MIHBench基准，包含三个核心任务，用于评估多图像中的对象相关幻觉；并设计了动态注意力平衡机制来调整图像间注意力分布。

Result: 实验发现多图像幻觉与图像输入数量、单图像幻觉倾向、同对象图像比例及负样本位置等因素相关；提出的方法有效减少了幻觉并提升了语义整合和推理稳定性。

Conclusion: 该研究填补了多图像MLLMs中幻觉研究的空白，提出的MIHBench和动态注意力平衡机制为未来研究提供了重要工具和方法。

Abstract: Despite growing interest in hallucination in Multimodal Large Language
Models, existing studies primarily focus on single-image settings, leaving
hallucination in multi-image scenarios largely unexplored. To address this gap,
we conduct the first systematic study of hallucinations in multi-image MLLMs
and propose MIHBench, a benchmark specifically tailored for evaluating
object-related hallucinations across multiple images. MIHBench comprises three
core tasks: Multi-Image Object Existence Hallucination, Multi-Image Object
Count Hallucination, and Object Identity Consistency Hallucination, targeting
semantic understanding across object existence, quantity reasoning, and
cross-view identity consistency. Through extensive evaluation, we identify key
factors associated with the occurrence of multi-image hallucinations,
including: a progressive relationship between the number of image inputs and
the likelihood of hallucination occurrences; a strong correlation between
single-image hallucination tendencies and those observed in multi-image
contexts; and the influence of same-object image ratios and the positional
placement of negative samples within image sequences on the occurrence of
object identity consistency hallucination. To address these challenges, we
propose a Dynamic Attention Balancing mechanism that adjusts inter-image
attention distributions while preserving the overall visual attention
proportion. Experiments across multiple state-of-the-art MLLMs demonstrate that
our method effectively reduces hallucination occurrences and enhances semantic
integration and reasoning stability in multi-image scenarios.

</details>


### [82] [YOLO-Count: Differentiable Object Counting for Text-to-Image Generation](https://arxiv.org/abs/2508.00728)
*Guanning Zeng,Xiang Zhang,Zirui Wang,Haiyang Xu,Zeyuan Chen,Bingnan Li,Zhuowen Tu*

Main category: cs.CV

TL;DR: YOLO-Count是一个可微分的开放词汇对象计数模型，用于解决通用计数问题并为文本到图像生成提供精确数量控制。


<details>
  <summary>Details</summary>
Motivation: 解决开放词汇计数与文本到图像生成控制之间的差距，提供更精确的对象数量估计和生成指导。

Method: 引入'cardinality'映射作为回归目标，结合表示对齐和混合强弱监督方案，采用完全可微分架构。

Result: 在实验中表现出最先进的计数准确性，并为文本到图像系统提供鲁棒的数量控制。

Conclusion: YOLO-Count在开放词汇计数和生成控制方面取得了显著成果，具有实际应用潜力。

Abstract: We propose YOLO-Count, a differentiable open-vocabulary object counting model
that tackles both general counting challenges and enables precise quantity
control for text-to-image (T2I) generation. A core contribution is the
'cardinality' map, a novel regression target that accounts for variations in
object size and spatial distribution. Leveraging representation alignment and a
hybrid strong-weak supervision scheme, YOLO-Count bridges the gap between
open-vocabulary counting and T2I generation control. Its fully differentiable
architecture facilitates gradient-based optimization, enabling accurate object
count estimation and fine-grained guidance for generative models. Extensive
experiments demonstrate that YOLO-Count achieves state-of-the-art counting
accuracy while providing robust and effective quantity control for T2I systems.

</details>


### [83] [Rethinking Backbone Design for Lightweight 3D Object Detection in LiDAR](https://arxiv.org/abs/2508.00744)
*Adwait Chandorkar,Hasan Tercan,Tobias Meisen*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级骨干网络Dense Backbone，用于3D目标检测，显著降低了计算成本，同时保持了高检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有3D目标检测方法多依赖复杂骨干网络，增加了模型复杂度，而轻量级设计在3D领域研究较少。

Method: 提出Dense Backbone，结合高速处理、轻量架构和鲁棒检测精度，适配多种3D检测器（如PillarNet）。

Result: DensePillarNet在nuScenes测试集上减少29%参数和28%延迟，仅损失2%检测精度。

Conclusion: Dense Backbone为3D目标检测提供了高效、轻量的解决方案，且易于集成到现有架构中。

Abstract: Recent advancements in LiDAR-based 3D object detection have significantly
accelerated progress toward the realization of fully autonomous driving in
real-world environments. Despite achieving high detection performance, most of
the approaches still rely on a VGG-based or ResNet-based backbone for feature
exploration, which increases the model complexity. Lightweight backbone design
is well-explored for 2D object detection, but research on 3D object detection
still remains limited. In this work, we introduce Dense Backbone, a lightweight
backbone that combines the benefits of high processing speed, lightweight
architecture, and robust detection accuracy. We adapt multiple SoTA 3d object
detectors, such as PillarNet, with our backbone and show that with our
backbone, these models retain most of their detection capability at a
significantly reduced computational cost. To our knowledge, this is the first
dense-layer-based backbone tailored specifically for 3D object detection from
point cloud data. DensePillarNet, our adaptation of PillarNet, achieves a 29%
reduction in model parameters and a 28% reduction in latency with just a 2%
drop in detection accuracy on the nuScenes test set. Furthermore, Dense
Backbone's plug-and-play design allows straightforward integration into
existing architectures, requiring no modifications to other network components.

</details>


### [84] [GECO: Geometrically Consistent Embedding with Lightspeed Inference](https://arxiv.org/abs/2508.00746)
*Regine Hartwig,Dominik Muhle,Riccardo Marin,Daniel Cremers*

Main category: cs.CV

TL;DR: GECO提出了一种基于最优传输的训练框架，生成几何一致的特征，显著提升了语义对应任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督视觉基础模型能捕捉语义对应，但缺乏对3D几何的感知。GECO旨在填补这一空白。

Method: 采用最优传输框架进行训练，支持超越关键点的监督，适用于遮挡和去遮挡场景。

Result: GECO在PFPascal、APK和CUB上实现了SOTA性能，PCK分别提升6.0%、6.2%和4.1%，运行速度达30 fps。

Conclusion: PCK不足以衡量几何质量，GECO提出了新指标和见解，推动更几何感知的特征学习。

Abstract: Recent advances in feature learning have shown that self-supervised vision
foundation models can capture semantic correspondences but often lack awareness
of underlying 3D geometry. GECO addresses this gap by producing geometrically
coherent features that semantically distinguish parts based on geometry (e.g.,
left/right eyes, front/back legs). We propose a training framework based on
optimal transport, enabling supervision beyond keypoints, even under occlusions
and disocclusions. With a lightweight architecture, GECO runs at 30 fps, 98.2%
faster than prior methods, while achieving state-of-the-art performance on
PFPascal, APK, and CUB, improving PCK by 6.0%, 6.2%, and 4.1%, respectively.
Finally, we show that PCK alone is insufficient to capture geometric quality
and introduce new metrics and insights for more geometry-aware feature
learning. Link to project page:
https://reginehartwig.github.io/publications/geco/

</details>


### [85] [Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos](https://arxiv.org/abs/2508.00748)
*Laura Pedrouzo-Rodriguez,Pedro Delgado-DeRobles,Luis F. Gomez,Ruben Tolosana,Ruben Vera-Rodriguez,Aythami Morales,Julian Fierrez*

Main category: cs.CV

TL;DR: 论文探讨了在逼真头像通信中，利用面部运动模式作为行为生物特征进行身份验证的可行性，并提出了一种轻量级的时空图卷积网络架构。


<details>
  <summary>Details</summary>
Motivation: 随着逼真头像在虚拟会议和社交平台中的普及，其带来的安全风险（如冒充攻击）日益突出。研究旨在验证面部运动模式是否能作为可靠的行为生物特征。

Method: 使用GAGAvatar生成真实和冒充头像视频数据集，并提出一种基于面部关键点的轻量级时空图卷积网络架构，结合时间注意力池化。

Result: 实验表明，面部运动特征可实现有效的身份验证，AUC值接近80%。

Conclusion: 研究强调了在头像通信系统中开发更先进行为生物特征防御的紧迫性，并提供了公开的基准和生物特征系统。

Abstract: Photorealistic talking-head avatars are becoming increasingly common in
virtual meetings, gaming, and social platforms. These avatars allow for more
immersive communication, but they also introduce serious security risks. One
emerging threat is impersonation: an attacker can steal a user's
avatar-preserving their appearance and voice-making it nearly impossible to
detect its fraudulent usage by sight or sound alone. In this paper, we explore
the challenge of biometric verification in such avatar-mediated scenarios. Our
main question is whether an individual's facial motion patterns can serve as
reliable behavioral biometrics to verify their identity when the avatar's
visual appearance is a facsimile of its owner. To answer this question, we
introduce a new dataset of realistic avatar videos created using a
state-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and
impostor avatar videos. We also propose a lightweight, explainable
spatio-temporal Graph Convolutional Network architecture with temporal
attention pooling, that uses only facial landmarks to model dynamic facial
gestures. Experimental results demonstrate that facial motion cues enable
meaningful identity verification with AUC values approaching 80%. The proposed
benchmark and biometric system are available for the research community in
order to bring attention to the urgent need for more advanced behavioral
biometric defenses in avatar-based communication systems.

</details>


### [86] [SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation](https://arxiv.org/abs/2508.00750)
*Prerana Ramkumar*

Main category: cs.CV

TL;DR: SU-ESRGAN是一种针对卫星图像的超分辨率框架，结合了ESRGAN、DeepLabv3分割损失和蒙特卡洛dropout，生成像素级不确定性地图，提升了语义一致性和可信度。


<details>
  <summary>Details</summary>
Motivation: GANs在图像超分辨率中缺乏语义一致性和像素级可信度，限制了其在遥感关键应用中的可靠性。

Method: 提出SU-ESRGAN框架，整合ESRGAN、DeepLabv3分割损失和蒙特卡洛dropout，生成不确定性地图。

Result: 在PSNR、SSIM和LPIPS指标上与基线ESRGAN相当，适用于卫星和无人机系统。

Conclusion: SU-ESRGAN在卫星和无人机图像处理中表现优异，强调了领域感知训练的重要性。

Abstract: Generative Adversarial Networks (GANs) have achieved realistic
super-resolution (SR) of images however, they lack semantic consistency and
per-pixel confidence, limiting their credibility in critical remote sensing
applications such as disaster response, urban planning and agriculture. This
paper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first
SR framework designed for satellite imagery to integrate the ESRGAN,
segmentation loss via DeepLabv3 for class detail preservation and Monte Carlo
dropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results
(PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This
novel model is valuable in satellite systems or UAVs that use wide
field-of-view (FoV) cameras, trading off spatial resolution for coverage. The
modular design allows integration in UAV data pipelines for on-board or
post-processing SR to enhance imagery resulting due to motion blur, compression
and sensor limitations. Further, the model is fine-tuned to evaluate its
performance on cross domain applications. The tests are conducted on two drone
based datasets which differ in altitude and imaging perspective. Performance
evaluation of the fine-tuned models show a stronger adaptation to the Aerial
Maritime Drone Dataset, whose imaging characteristics align with the training
data, highlighting the importance of domain-aware training in SR-applications.

</details>


### [87] [Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation](https://arxiv.org/abs/2508.00766)
*Irene Iele,Francesco Di Feola,Valerio Guarrasi,Paolo Soda*

Main category: cs.CV

TL;DR: 提出了一种动态测试时间适应（TTA）框架，用于医学图像转换任务，通过重建模块和动态适应块解决分布外样本的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像转换中分布外样本导致的性能下降问题。

Method: 引入重建模块量化域偏移，动态适应块选择性调整预训练模型的内部特征。

Result: 在低剂量CT去噪和T1到T2 MRI转换任务中表现优于基线模型和现有TTA方法。

Conclusion: 动态样本特定调整是提高模型鲁棒性的有效方法。

Abstract: Image-to-image translation has emerged as a powerful technique in medical
imaging, enabling tasks such as image denoising and cross-modality conversion.
However, it suffers from limitations in handling out-of-distribution samples
without causing performance degradation. To address this limitation, we propose
a novel Test-Time Adaptation (TTA) framework that dynamically adjusts the
translation process based on the characteristics of each test sample. Our
method introduces a Reconstruction Module to quantify the domain shift and a
Dynamic Adaptation Block that selectively modifies the internal features of a
pretrained translation model to mitigate the shift without compromising the
performance on in-distribution samples that do not require adaptation. We
evaluate our approach on two medical image-to-image translation tasks: low-dose
CT denoising and T1 to T2 MRI translation, showing consistent improvements over
both the baseline translation model without TTA and prior TTA methods. Our
analysis highlights the limitations of the state-of-the-art that uniformly
apply the adaptation to both out-of-distribution and in-distribution samples,
demonstrating that dynamic, sample-specific adjustment offers a promising path
to improve model resilience in real-world scenarios. The code is available at:
https://github.com/cosbidev/Sample-Aware_TTA.

</details>


### [88] [Zero-Shot Anomaly Detection with Dual-Branch Prompt Learning](https://arxiv.org/abs/2508.00777)
*Zihan Wang,Samira Ebrahimi Kahou,Narges Armanfard*

Main category: cs.CV

TL;DR: PILOT框架通过双分支提示学习和无标签测试时适应策略，解决了零样本异常检测中的领域偏移问题，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本异常检测方法在领域偏移下表现不佳，因其训练数据有限且无法泛化到新分布。

Method: PILOT采用双分支提示学习机制动态整合可学习提示与语义属性，并结合无标签测试时适应策略更新提示参数。

Result: 在13个工业和医学基准测试中，PILOT在领域偏移下实现了异常检测和定位的最先进性能。

Conclusion: PILOT通过创新方法有效解决了零样本异常检测中的领域偏移问题，具有广泛的应用潜力。

Abstract: Zero-shot anomaly detection (ZSAD) enables identifying and localizing defects
in unseen categories by relying solely on generalizable features rather than
requiring any labeled examples of anomalies. However, existing ZSAD methods,
whether using fixed or learned prompts, struggle under domain shifts because
their training data are derived from limited training domains and fail to
generalize to new distributions. In this paper, we introduce PILOT, a framework
designed to overcome these challenges through two key innovations: (1) a novel
dual-branch prompt learning mechanism that dynamically integrates a pool of
learnable prompts with structured semantic attributes, enabling the model to
adaptively weight the most relevant anomaly cues for each input image; and (2)
a label-free test-time adaptation strategy that updates the learnable prompt
parameters using high-confidence pseudo-labels from unlabeled test data.
Extensive experiments on 13 industrial and medical benchmarks demonstrate that
PILOT achieves state-of-the-art performance in both anomaly detection and
localization under domain shift.

</details>


### [89] [Cross-Dataset Semantic Segmentation Performance Analysis: Unifying NIST Point Cloud City Datasets for 3D Deep Learning](https://arxiv.org/abs/2508.00822)
*Alexander Nikitas Dimopoulos,Joseph Grasso*

Main category: cs.CV

TL;DR: 研究分析了异构标注点云数据集在公共安全应用中的语义分割性能，发现几何较大的物体分割效果较好，而小型安全关键特征识别率较低。


<details>
  <summary>Details</summary>
Motivation: 解决公共安全应用中异构标注点云数据集统一化的挑战，提升语义分割性能。

Method: 采用分级标注方案和KPConv架构，通过IoU指标评估安全相关特征的分割性能。

Result: 几何较大的物体（如楼梯、窗户）分割效果较好，小型安全关键特征识别率较低，受类别不平衡和几何区分度不足影响。

Conclusion: 公共安全领域的可靠点云语义分割需要标准化标注协议和改进标注技术，以应对数据异构性和小型安全关键特征的检测。

Abstract: This study analyzes semantic segmentation performance across heterogeneously
labeled point-cloud datasets relevant to public safety applications, including
pre-incident planning systems derived from lidar scans. Using NIST's Point
Cloud City dataset (Enfield and Memphis collections), we investigate challenges
in unifying differently labeled 3D data. Our methodology employs a graded
schema with the KPConv architecture, evaluating performance through IoU metrics
on safety-relevant features. Results indicate performance variability:
geometrically large objects (e.g. stairs, windows) achieve higher segmentation
performance, suggesting potential for navigational context, while smaller
safety-critical features exhibit lower recognition rates. Performance is
impacted by class imbalance and the limited geometric distinction of smaller
objects in typical lidar scans, indicating limitations in detecting certain
safety-relevant features using current point-cloud methods. Key identified
challenges include insufficient labeled data, difficulties in unifying class
labels across datasets, and the need for standardization. Potential directions
include automated labeling and multi-dataset learning strategies. We conclude
that reliable point-cloud semantic segmentation for public safety necessitates
standardized annotation protocols and improved labeling techniques to address
data heterogeneity and the detection of small, safety-critical elements.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [90] [PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems](https://arxiv.org/abs/2508.00079)
*Oshayer Siddique,J. M Areeb Uzair Alam,Md Jobayer Rahman Rafy,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 论文评估了前沿LLMs在解决物理问题（数学和描述性）上的表现，并采用多代理框架和推理技术提升性能。提出了新评估基准${\rm P{\small HYSICS}E{\small VAL}$。


<details>
  <summary>Details</summary>
Motivation: 物理问题是自然语言推理的关键领域，研究LLMs在此的表现有助于推动技术进步。

Method: 使用多代理框架和推理技术验证解决方案，并进行比较分析。

Result: 多代理框架显著提升了模型在初始表现不佳的问题上的性能。

Conclusion: 论文提出了新基准并展示了多代理框架的有效性，代码和数据已公开。

Abstract: The discipline of physics stands as a cornerstone of human intellect, driving
the evolution of technology and deepening our understanding of the fundamental
principles of the cosmos. Contemporary literature includes some works centered
on the task of solving physics problems - a crucial domain of natural language
reasoning. In this paper, we evaluate the performance of frontier LLMs in
solving physics problems, both mathematical and descriptive. We also employ a
plethora of inference-time techniques and agentic frameworks to improve the
performance of the models. This includes the verification of proposed solutions
in a cumulative fashion by other, smaller LLM agents, and we perform a
comparative analysis of the performance that the techniques entail. There are
significant improvements when the multi-agent framework is applied to problems
that the models initially perform poorly on. Furthermore, we introduce a new
evaluation benchmark for physics problems, ${\rm P{\small HYSICS}E{\small
VAL}}$, consisting of 19,609 problems sourced from various physics textbooks
and their corresponding correct solutions scraped from physics forums and
educational websites. Our code and data are publicly available at
https://github.com/areebuzair/PhysicsEval.

</details>


### [91] [Do LLMs produce texts with "human-like" lexical diversity?](https://arxiv.org/abs/2508.00086)
*Kelly Kendro,Jeffrey Maloney,Scott Jarvis*

Main category: cs.CL

TL;DR: 研究探讨了不同ChatGPT模型生成的文本与人类写作在词汇多样性上的差异，发现LLM生成的文本与人类写作显著不同，且新模型更不接近人类写作。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM的写作能力受到广泛关注，但其是否真正接近人类写作仍不明确。本研究从词汇多样性角度探讨这一问题。

Method: 比较了四种ChatGPT模型与240名L1/L2英语参与者的文本，测量了六个词汇多样性维度，并进行了统计分析和机器学习分类。

Result: LLM生成的文本在词汇多样性上与人类写作显著不同，尤其是ChatGPT-o4 mini和-4.5模型。新模型生成的文本更不接近人类写作。

Conclusion: LLM在词汇多样性上未能产生接近人类的文本，且新模型表现更差。这对语言教学和相关应用有重要启示。

Abstract: The degree to which LLMs produce writing that is truly human-like remains
unclear despite the extensive empirical attention that this question has
received. The present study addresses this question from the perspective of
lexical diversity. Specifically, the study investigates patterns of lexical
diversity in LLM-generated texts from four ChatGPT models (-3.5, -4, -o4 mini,
and -4.5) in comparison with texts written by L1 and L2 English participants (n
= 240) across four education levels. Six dimensions of lexical diversity were
measured in each text: volume, abundance, variety-repetition, evenness,
disparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAS, and
Support Vector Machines revealed that the LLM-generated texts differed
significantly from human-written texts for each variable, with ChatGPT-o4 mini
and -4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated
higher levels of lexical diversity despite producing fewer tokens. The human
writers' lexical diversity did not differ across subgroups (i.e., education,
language status). Altogether, the results indicate that LLMs do not produce
human-like texts in relation to lexical diversity, and the newer LLMs produce
less human-like texts than older models. We discuss the implications of these
results for language pedagogy and related applications.

</details>


### [92] [Semiotic Complexity and Its Epistemological Implications for Modeling Culture](https://arxiv.org/abs/2508.00095)
*Zachary K. Stine,James E. Deitrick*

Main category: cs.CL

TL;DR: 论文呼吁计算人文学科中方法的理论化，提出建模工作是一种翻译过程，强调理论化的重要性以避免翻译错误。


<details>
  <summary>Details</summary>
Motivation: 计算人文学科需要更多方法理论化以实现认识论和解释的清晰性，推动领域成熟。

Method: 将建模工作视为从文化、语言领域到计算、数学领域的翻译过程，并强调理论化翻译过程的必要性。

Result: 指出当前建模实践中因缺乏理论化导致的翻译错误，尤其是将符号复杂数据简化为符号简单数据的错误。

Conclusion: 提出若干建议，帮助研究者更好地处理认识论问题，避免翻译错误。

Abstract: Greater theorizing of methods in the computational humanities is needed for
epistemological and interpretive clarity, and therefore the maturation of the
field. In this paper, we frame such modeling work as engaging in translation
work from a cultural, linguistic domain into a computational, mathematical
domain, and back again. Translators benefit from articulating the theory of
their translation process, and so do computational humanists in their work --
to ensure internal consistency, avoid subtle yet consequential translation
errors, and facilitate interpretive transparency. Our contribution in this
paper is to lay out a particularly consequential dimension of the lack of
theorizing and the sorts of translation errors that emerge in our modeling
practices as a result. Along these lines we introduce the idea of semiotic
complexity as the degree to which the meaning of some text may vary across
interpretive lenses, and make the case that dominant modeling practices --
especially around evaluation -- commit a translation error by treating
semiotically complex data as semiotically simple when it seems
epistemologically convenient by conferring superficial clarity. We then lay out
several recommendations for researchers to better account for these
epistemological issues in their own work.

</details>


### [93] [FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality](https://arxiv.org/abs/2508.00109)
*Mingda Chen,Yang Li,Xilun Chen,Adina Williams,Gargi Ghosh,Scott Yih*

Main category: cs.CL

TL;DR: FACTORY是一个大规模、人工验证的提示集，用于评估模型生成准确、全面回答的能力，结果显示现有SOTA模型在FACTORY上的表现较差。


<details>
  <summary>Details</summary>
Motivation: 现有的事实性评估基准缺乏人工验证，可能导致质量问题，因此需要更可靠的评估工具。

Method: 采用模型辅助和人工精炼的方法开发FACTORY，并在6个SOTA语言模型上进行人工评估。

Result: FACTORY是一个具有挑战性的基准，约40%的SOTA模型回答不准确，而其他数据集仅为10%。

Conclusion: FACTORY在可靠性和评估长尾事实推理能力方面优于现有基准。

Abstract: Long-form factuality evaluation assesses the ability of models to generate
accurate, comprehensive responses to short prompts. Existing benchmarks often
lack human verification, leading to potential quality issues. To address this
limitation, we introduce FACTORY, a large-scale, human-verified prompt set.
Developed using a model-in-the-loop approach and refined by humans, FACTORY
includes challenging prompts that are fact-seeking, answerable, and
unambiguous. We conduct human evaluations on 6 state-of-the-art language models
using FACTORY and existing datasets. Our results show that FACTORY is a
challenging benchmark: approximately 40% of the claims made in the responses of
SOTA models are not factual, compared to only 10% for other datasets. Our
analysis identifies the strengths of FACTORY over prior benchmarks, emphasizing
its reliability and the necessity for models to reason across long-tailed
facts.

</details>


### [94] [Is neural semantic parsing good at ellipsis resolution, or isn't it?](https://arxiv.org/abs/2508.00121)
*Xiao Zhang,Johan bos*

Main category: cs.CL

TL;DR: 神经语义解析器在多种语言现象中表现良好，但在强上下文敏感现象（如英语动词短语省略）中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究神经语义解析器是否能处理强上下文敏感现象，特别是动词短语省略。

Method: 构建包含120个省略案例及其完整语义表示的语料库，用于测试多种神经语义解析器。

Result: 解析器在标准测试集上表现优异，但在省略案例中失败。

Conclusion: 数据增强可能是解决此类问题的潜在方法。

Abstract: Neural semantic parsers have shown good overall performance for a variety of
linguistic phenomena, reaching semantic matching scores of more than 90%. But
how do such parsers perform on strongly context-sensitive phenomena, where
large pieces of semantic information need to be duplicated to form a meaningful
semantic representation? A case in point is English verb phrase ellipsis, a
construct where entire verb phrases can be abbreviated by a single auxiliary
verb. Are the otherwise known as powerful semantic parsers able to deal with
ellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with
their fully resolved meaning representation and used this as a challenge set
for a large battery of neural semantic parsers. Although these parsers
performed very well on the standard test set, they failed in the instances with
ellipsis. Data augmentation

</details>


### [95] [Comparison of Large Language Models for Deployment Requirements](https://arxiv.org/abs/2508.00185)
*Alper Yaman,Jannik Schwab,Christof Nitsche,Abhirup Sinha,Marco Huber*

Main category: cs.CL

TL;DR: 本文比较了大型语言模型（LLMs）的基础和领域特定模型，帮助研究者和公司根据许可和硬件需求选择最佳模型。


<details>
  <summary>Details</summary>
Motivation: 随着开源LLM的增多，选择适合的模型变得复杂，本文旨在简化这一过程。

Method: 通过整理和比较不同模型的特性（如发布时间、许可和硬件需求），创建了一个持续更新的列表。

Result: 提供了一个公开的、持续更新的模型比较列表，发布在GitLab上。

Conclusion: 该列表有助于研究者和公司在快速发展的LLM领域做出更明智的选择。

Abstract: Large Language Models (LLMs), such as Generative Pre-trained Transformers
(GPTs) are revolutionizing the generation of human-like text, producing
contextually relevant and syntactically correct content. Despite challenges
like biases and hallucinations, these Artificial Intelligence (AI) models excel
in tasks, such as content creation, translation, and code generation.
Fine-tuning and novel architectures, such as Mixture of Experts (MoE), address
these issues. Over the past two years, numerous open-source foundational and
fine-tuned models have been introduced, complicating the selection of the
optimal LLM for researchers and companies regarding licensing and hardware
requirements. To navigate the rapidly evolving LLM landscape and facilitate LLM
selection, we present a comparative list of foundational and domain-specific
models, focusing on features, such as release year, licensing, and hardware
requirements. This list is published on GitLab and will be continuously
updated.

</details>


### [96] [Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges](https://arxiv.org/abs/2508.00217)
*Xiaofeng Wu,Alan Ritter,Wei Xu*

Main category: cs.CL

TL;DR: 本文探讨了表格在大型语言模型（LLMs）和多模态大型语言模型（MLLMs）中的重要性，提出了表格输入表示的分类法，并指出了该领域的关键研究空白。


<details>
  <summary>Details</summary>
Motivation: 表格因其复杂和灵活的结构在LLMs和MLLMs中受到关注，但缺乏通用方法，导致理解任务具有挑战性。

Method: 通过分类法总结表格输入表示，并介绍表格理解任务。

Result: 指出了三个关键研究空白：检索任务主导、复杂表格处理困难、模型泛化能力有限。

Conclusion: 需要进一步研究以解决表格理解中的挑战。

Abstract: Tables have gained significant attention in large language models (LLMs) and
multimodal large language models (MLLMs) due to their complex and flexible
structure. Unlike linear text inputs, tables are two-dimensional, encompassing
formats that range from well-structured database tables to complex,
multi-layered spreadsheets, each with different purposes. This diversity in
format and purpose has led to the development of specialized methods and tasks,
instead of universal approaches, making navigation of table understanding tasks
challenging. To address these challenges, this paper introduces key concepts
through a taxonomy of tabular input representations and an introduction of
table understanding tasks. We highlight several critical gaps in the field that
indicate the need for further research: (1) the predominance of
retrieval-focused tasks that require minimal reasoning beyond mathematical and
logical operations; (2) significant challenges faced by models when processing
complex table structures, large-scale tables, length context, or multi-table
scenarios; and (3) the limited generalization of models across different
tabular representations and formats.

</details>


### [97] [Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform](https://arxiv.org/abs/2508.00220)
*Rana Aref Salama,Abdou Youssef,Mona Diab*

Main category: cs.CL

TL;DR: 论文探讨了离散小波变换（DWT）在词和句子嵌入中的应用，展示了其在多分辨率分析和压缩嵌入表示中的能力，同时保持语义质量。


<details>
  <summary>Details</summary>
Motivation: 小波变换在信号和图像处理中表现优异，但其在NLP中的应用潜力尚未充分挖掘。本文旨在探索DWT在NLP中的实际效果。

Method: 通过将DWT应用于词和句子嵌入，分析其在多分辨率下的表现，并评估其在语义相似性任务中的有效性。

Result: DWT能将嵌入维度减少50-93%，同时在语义相似性任务中性能几乎不变，且在下游任务中表现更优。

Conclusion: DWT为改进NLP应用提供了一种有效方法，尤其是在嵌入压缩和语义信息保留方面。

Abstract: Wavelet transforms, a powerful mathematical tool, have been widely used in
different domains, including Signal and Image processing, to unravel intricate
patterns, enhance data representation, and extract meaningful features from
data. Tangible results from their application suggest that Wavelet transforms
can be applied to NLP capturing a variety of linguistic and semantic
properties. In this paper, we empirically leverage the application of Discrete
Wavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase
the capabilities of DWT in analyzing embedding representations at different
levels of resolution and compressing them while maintaining their overall
quality. We assess the effectiveness of DWT embeddings on semantic similarity
tasks to show how DWT can be used to consolidate important semantic information
in an embedding vector. We show the efficacy of the proposed paradigm using
different embedding models, including large language models, on downstream
tasks. Our results show that DWT can reduce the dimensionality of embeddings by
50-93% with almost no change in performance for semantic similarity tasks,
while achieving superior accuracy in most downstream tasks. Our findings pave
the way for applying DWT to improve NLP applications.

</details>


### [98] [Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English](https://arxiv.org/abs/2508.00238)
*Bryce Anderson,Riley Galpin,Tom S. Juzek*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）对人类语言使用的影响，发现2022年后与LLM相关的词汇使用显著增加，表明人类语言选择与LLM模式趋同。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否导致人类语言系统本身的广泛变化，而不仅仅是作为文本生成工具的直接使用。

Method: 构建了一个包含2210万单词的数据集，分析ChatGPT发布前后词汇使用趋势，重点关注LLM相关词汇。

Result: 2022年后LLM相关词汇使用显著增加，而基线同义词未出现显著变化。

Conclusion: 研究结果表明人类语言可能与LLM模式趋同，但这是自然语言变化还是AI驱动的新变化尚不明确，同时涉及伦理问题。

Abstract: In recent years, written language, particularly in science and education, has
undergone remarkable shifts in word usage. These changes are widely attributed
to the growing influence of Large Language Models (LLMs), which frequently rely
on a distinct lexical style. Divergences between model output and target
audience norms can be viewed as a form of misalignment. While these shifts are
often linked to using Artificial Intelligence (AI) directly as a tool to
generate text, it remains unclear whether the changes reflect broader changes
in the human language system itself. To explore this question, we constructed a
dataset of 22.1 million words from unscripted spoken language drawn from
conversational science and technology podcasts. We analyzed lexical trends
before and after ChatGPT's release in 2022, focusing on commonly LLM-associated
words. Our results show a moderate yet significant increase in the usage of
these words post-2022, suggesting a convergence between human word choices and
LLM-associated patterns. In contrast, baseline synonym words exhibit no
significant directional shift. Given the short time frame and the number of
words affected, this may indicate the onset of a remarkable shift in language
use. Whether this represents natural language change or a novel shift driven by
AI exposure remains an open question. Similarly, although the shifts may stem
from broader adoption patterns, it may also be that upstream training
misalignments ultimately contribute to changes in human language use. These
findings parallel ethical concerns that misaligned models may shape social and
moral beliefs.

</details>


### [99] [Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering](https://arxiv.org/abs/2508.00285)
*Peixian Li,Yu Tian,Ruiqi Tu,Chengkai Wu,Jingjing Ren,Jingsong Li*

Main category: cs.CL

TL;DR: 提出了一种病因感知注意力引导框架，通过结构化临床推理提升LLM在复杂临床场景中的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在医学文本理解和生成方面表现出色，但其在复杂临床诊断中的可靠性有限，需提升其诊断准确性和临床推理能力。

Method: 构建临床推理支架（CRS），开发病因感知头识别算法，并引入推理引导的参数高效微调方法，通过推理引导损失函数优化模型注意力。

Result: 在一致性诊断队列中，诊断准确性平均提升15.65%，推理聚焦分数提升31.6%，外部验证进一步证实其有效性。

Conclusion: 该框架通过结构化临床推理对齐模型注意力，为构建更可靠和可解释的AI诊断系统提供了实用方法。

Abstract: Objective: Large Language Models (LLMs) demonstrate significant capabilities
in medical text understanding and generation. However, their diagnostic
reliability in complex clinical scenarios remains limited. This study aims to
enhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We
propose an Etiology-Aware Attention Steering Framework to integrate structured
clinical reasoning into LLM-based diagnosis. Specifically, we first construct
Clinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines
for three representative acute abdominal emergencies: acute appendicitis, acute
pancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head
Identification algorithm to pinpoint attention heads crucial for the model's
etiology reasoning. To ensure reliable clinical reasoning alignment, we
introduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds
etiological reasoning cues into input representations and steers the selected
Etiology-Aware Heads toward critical information through a Reasoning-Guided
Loss function. Result: On the Consistent Diagnosis Cohort, our framework
improves average diagnostic accuracy by 15.65% and boosts the average Reasoning
Focus Score by 31.6% over baselines. External validation on the Discrepant
Diagnosis Cohort further confirms its effectiveness in enhancing diagnostic
accuracy. Further assessments via Reasoning Attention Frequency indicate that
our models exhibit enhanced reliability when faced with real-world complex
scenarios. Conclusion: This study presents a practical and effective approach
to enhance clinical reasoning in LLM-based diagnosis. By aligning model
attention with structured CRS, the proposed framework offers a promising
paradigm for building more interpretable and reliable AI diagnostic systems in
complex clinical settings.

</details>


### [100] [Systematic Evaluation of Optimization Techniques for Long-Context Language Models](https://arxiv.org/abs/2508.00305)
*Ammar Ahmed,Sheng Di,Franck Cappello,Zirui Liu,Jingoo Han,Ali Anwar*

Main category: cs.CL

TL;DR: 论文系统评估了大型语言模型（LLM）的优化技术（如剪枝、量化和标记丢弃）在长上下文场景中的效果，揭示了组合优化对大型模型的负面影响，并强调了系统级分析与任务特定见解的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在多种自然语言处理任务中表现出色，但其资源需求和有限上下文窗口限制了应用。现有优化技术在长上下文场景中的效果和系统评估尚未充分探索。

Method: 论文首先分析两种支持长上下文的LLM架构的个体优化方法，随后系统评估这些技术的组合效果，并在70B参数模型上研究其扩展性。

Result: 研究发现，组合优化算法可能因累积近似误差而对大型模型产生负面影响，且仅依赖F1分数会掩盖问答任务中的精度-召回权衡。

Conclusion: 通过结合系统级分析和任务特定见解，本研究为LLM实践者和研究人员提供了在效率和准确性之间平衡的指导。

Abstract: Large language models (LLMs) excel across diverse natural language processing
tasks but face resource demands and limited context windows. Although
techniques like pruning, quantization, and token dropping can mitigate these
issues, their efficacy in long-context scenarios and system evaluation remains
underexplored. This paper systematically benchmarks these optimizations,
characterizing memory usage, latency, and throughput, and studies how these
methods impact the quality of text generation. We first analyze individual
optimization methods for two LLM architectures supporting long context and then
systematically evaluate combinations of these techniques to assess how this
deeper analysis impacts performance metrics. We subsequently study the
scalability of individual optimization methods on a larger variant with 70
billion-parameter model. Our novel insights reveal that naive combination
inference optimization algorithms can adversely affect larger models due to
compounded approximation errors, as compared to their smaller counterparts.
Experiments show that relying solely on F1 obscures these effects by hiding
precision-recall trade-offs in question answering tasks. By integrating
system-level profiling with task-specific insights, this study helps LLM
practitioners and researchers explore and balance efficiency, accuracy, and
scalability across tasks and hardware configurations.

</details>


### [101] [Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment](https://arxiv.org/abs/2508.00332)
*Kaiyan Zhao,Zhongtao Miao,Yoshimasa Tsuruoka*

Main category: cs.CL

TL;DR: MCSEO通过细粒度对象-短语对齐提升多模态句子嵌入性能，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 多模态句子嵌入模型常因图像-标题对中的噪声（冗余或无关信息）而性能受限。

Method: 利用分割和对象检测模型提取精确的对象-短语对，优化对比学习目标。

Result: 在语义文本相似性任务中，MCSEO表现优于基线模型。

Conclusion: 精确的对象-短语对齐对多模态表示学习至关重要。

Abstract: Multimodal sentence embedding models typically leverage image-caption pairs
in addition to textual data during training. However, such pairs often contain
noise, including redundant or irrelevant information on either the image or
caption side. To mitigate this issue, we propose MCSEO, a method that enhances
multimodal sentence embeddings by incorporating fine-grained object-phrase
alignment alongside traditional image-caption alignment. Specifically, MCSEO
utilizes existing segmentation and object detection models to extract accurate
object-phrase pairs, which are then used to optimize a contrastive learning
objective tailored to object-phrase correspondence. Experimental results on
semantic textual similarity (STS) tasks across different backbone models
demonstrate that MCSEO consistently outperforms strong baselines, highlighting
the significance of precise object-phrase alignment in multimodal
representation learning.

</details>


### [102] [PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning](https://arxiv.org/abs/2508.00344)
*Keer Lu,Chong Chen,Bin Cui,Huang Leng,Wentao Zhang*

Main category: cs.CL

TL;DR: 论文提出了一种名为AdaPlan的自适应全局规划代理范式，结合了PilotRL训练框架，以解决LLM在复杂任务中的长期规划和执行协调问题，实验显示其性能优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理范式（如ReAct）在复杂任务中表现受限，主要因单步推理与即时执行的结合无法支持长期规划，且监督微调导致泛化能力不足。

Method: 提出AdaPlan范式，结合全局规划与执行；设计PilotRL框架，通过渐进式强化学习训练LLM代理，优化规划与执行协调。

Result: 实验表明，PilotRL性能优异，LLaMA3.1-8B-Instruct + PilotRL超越GPT-4o 3.60%，并在相近参数规模下比GPT-4o-mini提升55.78%。

Conclusion: AdaPlan与PilotRL有效解决了LLM代理在长期规划和执行协调中的挑战，显著提升了性能。

Abstract: Large Language Models (LLMs) have shown remarkable advancements in tackling
agent-oriented tasks. Despite their potential, existing work faces challenges
when deploying LLMs in agent-based environments. The widely adopted agent
paradigm ReAct centers on integrating single-step reasoning with immediate
action execution, which limits its effectiveness in complex tasks requiring
long-term strategic planning. Furthermore, the coordination between the planner
and executor during problem-solving is also a critical factor to consider in
agent design. Additionally, current approaches predominantly rely on supervised
fine-tuning, which often leads models to memorize established task completion
trajectories, thereby restricting their generalization ability when confronted
with novel problem contexts. To address these challenges, we introduce an
adaptive global plan-based agent paradigm AdaPlan, aiming to synergize
high-level explicit guidance with execution to support effective long-horizon
decision-making. Based on the proposed paradigm, we further put forward
PilotRL, a global planning-guided training framework for LLM agents driven by
progressive reinforcement learning. We first develop the model's ability to
follow explicit guidance from global plans when addressing agent tasks.
Subsequently, based on this foundation, we focus on optimizing the quality of
generated plans. Finally, we conduct joint optimization of the model's planning
and execution coordination. Experiments indicate that PilotRL could achieve
state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing
closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%
comparing to GPT-4o-mini at a comparable parameter scale.

</details>


### [103] [Lucy: edgerunning agentic web search on mobile with machine generated task vectors](https://arxiv.org/abs/2508.00360)
*Alan Dao,Dinh Bach Vu,Alex Nguyen,Norapat Buppodom*

Main category: cs.CL

TL;DR: 小型语言模型（SLMs）在知识密集型任务中表现受限，但通过动态任务向量机器和强化学习优化，Lucy（1.7B参数）在SimpleQA基准测试中达到78.3%准确率，媲美更大模型。


<details>
  <summary>Details</summary>
Motivation: 解决小型语言模型因容量限制在知识密集型任务中表现不佳的问题，探索动态推理机制提升性能。

Method: 提出动态任务向量机器，将模型内部推理过程（<think>和</think>标签内）视为动态任务向量生成和优化过程，通过RLVR训练。

Result: Lucy（1.7B参数）在SimpleQA基准测试中达到78.3%准确率，性能与更大模型相当。

Conclusion: 小型模型通过结构化、自构建的任务推理机制，可以媲美大型模型。

Abstract: Small language models (SLMs) are inherently limited in knowledge-intensive
tasks due to their constrained capacity. While test-time computation offers a
path to enhanced performance, most approaches treat reasoning as a fixed or
heuristic process. In this work, we propose a new paradigm: viewing the model's
internal reasoning, delimited by <think> and </think> tags, as a dynamic task
vector machine. Rather than treating the content inside these tags as a mere
trace of thought, we interpret the generation process itself as a mechanism
through which the model \textbf{constructs and refines its own task vectors} on
the fly. We developed a method to optimize this dynamic task vector machine
through RLVR and successfully trained an agentic web-search model. We present
Lucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with
MCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing
on par with much larger models such as DeepSeek-V3. This demonstrates that
small models can rival large ones when equipped with structured,
self-constructed task reasoning.

</details>


### [104] [EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices](https://arxiv.org/abs/2508.00370)
*Jiyu Chen,Poh Seng Lim,Shuang Peng,Daxiong Luo,JungHau Foo,Yap Deep,Timothy Lee Jun Jie,Kelvin Teh Kae Wen,Fan Yang,Danyu Feng,Hao-Yun Chen,Peng-Wen Chen,Fangyuan Li,Xiaoxin Chen,Wong Wai Mun*

Main category: cs.CL

TL;DR: EdgeInfinite-Instruct通过分段监督微调和量化优化，解决了Transformer模型在边缘设备上部署的效率和性能问题。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer大模型在资源受限的边缘设备上部署时面临的计算复杂性和内存需求问题。

Method: 采用分段监督微调（S-SFT）策略和细粒度后训练量化（PTQ），优化计算图和缓存大小。

Result: 在长上下文基准测试和实际移动任务中表现优异，同时保持高效性。

Conclusion: EdgeInfinite-Instruct在边缘设备上实现了高效且高性能的部署。

Abstract: Deploying Transformer-based large language models (LLMs) on
resource-constrained edge devices for long-sequence tasks remains challenging
due to the quadratic time complexity of self-attention and growing Key-Value
(KV) cache demands. While existing KV cache optimizations improve memory
efficiency, they often fail to reduce time to first token (TTFT) and may
degrade performance through token pruning. Alternative sequence modeling
architectures address some of these limitations, but typically require full
retraining and lack infrastructure support. EdgeInfinite offers an efficient
solution by fine-tuning only a small subset of parameters, maintaining quality
while reducing both computational and memory costs, including improved TTFT.
However, its instruction-following ability is limited, and it lacks
mobile-specific optimizations. To address these issues, we propose
EdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning
(S-SFT) strategy tailored to long-sequence tasks such as summarization and
question answering. We further optimized EdgeInfinite-Instruct for efficient
deployment on edge NPUs by employing fine-grained post-training quantization
(PTQ) to reduce computational demands while maintaining accuracy, and by
implementing a fixed-shape computation graph that balances memory usage and
on-device efficiency through scenario-specific customization of input token and
cache sizes. Experiments on long-context benchmarks and real-world mobile tasks
show that our approach improves domain-specific performance while maintaining
efficiency on NPU-accelerated edge devices.

</details>


### [105] [Multi-Layer Attention is the Amplifier of Demonstration Effectiveness](https://arxiv.org/abs/2508.00385)
*Dingzirui Wang,Xuangliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: 论文研究了上下文学习（ICL）中演示无效的原因，提出了一种基于梯度流的演示选择方法GradS，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设ICL中的演示总是有效，但实际并非如此。本文旨在探究演示无效的原因，并提出改进方法。

Method: 通过梯度流和线性自注意力模型分析，提出GradS方法，利用梯度流大小选择有效演示。

Result: 实验表明，模型层数增加会放大演示效果的差异，GradS平均相对提升6.8%。

Conclusion: GradS能有效选择演示，提升ICL性能，验证了理论推导。

Abstract: Numerous studies have investigated the underlying mechanisms of in-context
learning (ICL) effectiveness to inspire the design of related methods. However,
existing work predominantly assumes the effectiveness of the demonstrations
provided within ICL, while many research indicates that not all demonstrations
are effective, failing to yielding any performance improvement during ICL.
Therefore, in this paper, we investigate the reasons behind demonstration
ineffectiveness. Our analysis is based on gradient flow and linear
self-attention models. By setting the gradient flow to zero, we deduce that a
demonstration becomes ineffective if its information has either been learned by
the model or is irrelevant to the user query. Furthermore, we demonstrate that
in multi-layer models, the disparity in effectiveness among demonstrations is
amplified with layer increasing, causing the model to focus more on effective
ones. Considering that current demonstration selection methods primarily focus
on the relevance to the user query while overlooking the information that the
model has already assimilated, we propose a novel method called GradS, which
leverages gradient flow for demonstration selection. We use the magnitude of
the gradient flow of the demonstration with respect to a given user query as
the criterion, thereby ensuring the effectiveness of the chosen ones. We
validate our derivation and GradS on four prominent LLMs across five mainstream
datasets. The experimental results confirm that the disparity in effectiveness
among demonstrations is magnified as the model layer increases, substantiating
our derivations. Moreover, GradS achieves a relative improvement of $6.8\%$ on
average over the strongest baselines, demonstrating its effectiveness.

</details>


### [106] [SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation](https://arxiv.org/abs/2508.00390)
*Hengxing Cai,Jinhan Dong,Yijie Rao,Jingcheng Deng,Jingjun Tan,Qien Chen,Haidong Wang,Zhen Wang,Shiyu Huang,Agachai Sumalee,Renxin Zhong*

Main category: cs.CL

TL;DR: 论文提出了一种名为SA-GCS的训练框架，通过结合课程学习与强化学习，解决了现有方法在数据利用效率、收敛速度和样本难度差异上的不足，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 无人机视觉语言导航（VLN）在复杂环境中基于自然语言指令定位目标和规划路径具有广泛应用，但现有强化学习方法存在数据利用效率低、收敛慢和样本难度差异考虑不足的问题。

Method: 提出了SA-GCS框架，包括语义感知难度估计器（SA-DE）量化样本复杂度，以及高斯课程调度器（GCS）动态调整采样分布，实现从易到难的任务过渡。

Result: 在CityNav基准测试中，SA-GCS在所有指标上均优于基线，收敛更快更稳定，且在不同规模模型上泛化良好。

Conclusion: SA-GCS通过系统整合课程学习与强化学习，显著提升了训练效率和模型性能，具有鲁棒性和可扩展性。

Abstract: Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable
agents to accurately localize targets and plan flight paths in complex
environments based on natural language instructions, with broad applications in
intelligent inspection, disaster rescue, and urban monitoring. Recent progress
in Vision-Language Models (VLMs) has provided strong semantic understanding for
this task, while reinforcement learning (RL) has emerged as a promising
post-training strategy to further improve generalization. However, existing RL
methods often suffer from inefficient use of training data, slow convergence,
and insufficient consideration of the difficulty variation among training
samples, which limits further performance improvement. To address these
challenges, we propose \textbf{Semantic-Aware Gaussian Curriculum Scheduling
(SA-GCS)}, a novel training framework that systematically integrates Curriculum
Learning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator
(SA-DE) to quantify the complexity of training samples and a Gaussian
Curriculum Scheduler (GCS) to dynamically adjust the sampling distribution,
enabling a smooth progression from easy to challenging tasks. This design
significantly improves training efficiency, accelerates convergence, and
enhances overall model performance. Extensive experiments on the CityNav
benchmark demonstrate that SA-GCS consistently outperforms strong baselines
across all metrics, achieves faster and more stable convergence, and
generalizes well across models of different scales, highlighting its robustness
and scalability. The implementation of our approach is publicly available.

</details>


### [107] [Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding](https://arxiv.org/abs/2508.00420)
*Rana Salama,Abdou Youssef,Mona Diab*

Main category: cs.CL

TL;DR: 该论文探讨了离散小波变换（DWT）在词和句子嵌入中的应用，提出了一种结合DWT和离散余弦变换（DCT）的非参数化模型，用于压缩信息并提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 小波技术在图像和信号处理中的成功应用激发了其在自然语言处理（NLP）中的潜力，尤其是在捕捉语言特征方面。

Method: 论文首先评估了小波变换如何有效压缩词向量并减少维度，随后结合DWT和DCT提出了一种固定大小的句子向量压缩模型。

Result: 实验表明，该方法在下游任务中表现优异，甚至在某些任务中优于原始嵌入。

Conclusion: 小波变换在NLP中具有显著潜力，尤其是在信息压缩和特征提取方面。

Abstract: Wavelets have emerged as a cutting edge technology in a number of fields.
Concrete results of their application in Image and Signal processing suggest
that wavelets can be effectively applied to Natural Language Processing (NLP)
tasks that capture a variety of linguistic properties. In this paper, we
leverage the power of applying Discrete Wavelet Transforms (DWT) to word and
sentence embeddings. We first evaluate, intrinsically and extrinsically, how
wavelets can effectively be used to consolidate important information in a word
vector while reducing its dimensionality. We further combine DWT with Discrete
Cosine Transform (DCT) to propose a non-parameterized model that compresses a
sentence with a dense amount of information in a fixed size vector based on
locally varying word features. We show the efficacy of the proposed paradigm on
downstream applications models yielding comparable and even superior (in some
tasks) results to original embeddings.

</details>


### [108] [ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network](https://arxiv.org/abs/2508.00429)
*Minghao Guo,Xi Zhu,Jingyuan Huang,Kai Mei,Yongfeng Zhang*

Main category: cs.CL

TL;DR: ReaGAN提出了一种基于代理的图神经网络框架，通过节点级自主决策和检索增强生成技术，解决了传统GNN在节点信息不平衡和全局语义关系捕捉上的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统GNN的固定聚合机制无法处理节点信息不平衡问题，且忽略了全局语义关系，限制了模型的表达能力。

Method: ReaGAN采用代理框架，每个节点作为独立代理，基于内部记忆自主规划行动，并结合检索增强生成技术获取全局语义信息。

Result: ReaGAN在少样本上下文设置中表现出色，无需微调即可与现有方法竞争。

Conclusion: ReaGAN展示了代理规划和局部-全局检索在图学习中的潜力，为GNN设计提供了新思路。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in graph-based
learning by propagating information among neighbor nodes via predefined
aggregation mechanisms. However, such fixed schemes often suffer from two key
limitations. First, they cannot handle the imbalance in node informativeness --
some nodes are rich in information, while others remain sparse. Second,
predefined message passing primarily leverages local structural similarity
while ignoring global semantic relationships across the graph, limiting the
model's ability to capture distant but relevant information. We propose
Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework
that empowers each node with autonomous, node-level decision-making. Each node
acts as an agent that independently plans its next action based on its internal
memory, enabling node-level planning and adaptive message propagation.
Additionally, retrieval-augmented generation (RAG) allows nodes to access
semantically relevant content and build global relationships in the graph.
ReaGAN achieves competitive performance under few-shot in-context settings
using a frozen LLM backbone without fine-tuning, showcasing the potential of
agentic planning and local-global retrieval in graph learning.

</details>


### [109] [Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges](https://arxiv.org/abs/2508.00454)
*Yuqi Tang,Kehua Feng,Yunfeng Wang,Zhiwen Chen,Chengfei Lv,Gang Yu,Qiang Zhang,Keyan Ding*

Main category: cs.CL

TL;DR: 提出了一种高效的多轮对话评估方法，通过将多个LLM评委的偏好知识聚合到单一模型中，显著降低评估成本，同时保持多样性反馈的优势。


<details>
  <summary>Details</summary>
Motivation: 当前依赖单一LLM作为评委的方法存在偏见，影响评估结果的可靠性和一致性，而多评委方法虽有效但计算开销大。

Method: 提出一种高效的多轮对话评估器，通过聚合多个LLM评委的偏好知识到单一模型中，减少计算开销。

Result: 在七个对话评估基准测试中，该方法优于现有基线，展示了高效性和鲁棒性。

Conclusion: 该方法在保持多评委反馈优势的同时显著降低了评估成本，适用于快速灵活的对话质量评估。

Abstract: Evaluating the conversational abilities of large language models (LLMs)
remains a challenging task. Current mainstream approaches primarily rely on the
``LLM-as-a-judge" paradigm, where an LLM is prompted to serve as an evaluator
to assess dialogue quality. However, such methods often suffer from various
biases, which undermine the reliability and consistency of the evaluation
results. To mitigate these biases, recent methods employ multiple LLMs as
judges and aggregate their judgments to select the optimal assessment. Although
effective, this multi-judge approach incurs significant computational overhead
during inference. In this paper, we propose an efficient multi-turn dialogue
evaluator that captures the collective wisdom of multiple LLM judges by
aggregating their preference knowledge into a single model. Our approach
preserves the advantages of diverse multi-judge feedback while drastically
reducing the evaluation cost, enabling fast and flexible dialogue quality
assessment. Extensive experiments on seven single rating and pairwise
comparison dialogue evaluation benchmarks demonstrate that our method
outperforms existing baselines across diverse scenarios, showcasing its
efficiency and robustness.

</details>


### [110] [GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts](https://arxiv.org/abs/2508.00476)
*Jeongwoo Kang,Markarit Vartampetian,Felix Herron,Yongxin Zhou,Diandra Fabre,Gabriela Gonzalez-Saez*

Main category: cs.CL

TL;DR: GETALP团队在SIGDial 2025的自动会议纪要任务中提交了基于RAG和AMR的系统，结果显示AMR显著提升了35%问题的回答质量。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过结合检索增强生成（RAG）和抽象意义表示（AMR）提升会议纪要中问答任务的性能。

Method: 提出了三种结合RAG和AMR的系统，用于处理基于会议转录的问答任务。

Result: AMR显著提升了35%问题的回答质量，尤其在区分参与者的问题（如“谁”类问题）上表现突出。

Conclusion: 结合AMR的RAG系统在会议纪要问答任务中表现出色，尤其在复杂问题上有显著优势。

Abstract: This paper documents GETALP's submission to the Third Run of the Automatic
Minuting Shared Task at SIGDial 2025. We participated in Task B:
question-answering based on meeting transcripts. Our method is based on a
retrieval augmented generation (RAG) system and Abstract Meaning
Representations (AMR). We propose three systems combining these two approaches.
Our results show that incorporating AMR leads to high-quality responses for
approximately 35% of the questions and provides notable improvements in
answering questions that involve distinguishing between different participants
(e.g., who questions).

</details>


### [111] [The Missing Parts: Augmenting Fact Verification with Half-Truth Detection](https://arxiv.org/abs/2508.00489)
*Yixuan Tang,Jincheng Wang,Anthony K. H. Tung*

Main category: cs.CL

TL;DR: 论文提出了一种检测半真半假陈述的任务，并开发了一个新基准PolitiFact-Hidden和框架TRACER，用于识别因遗漏关键信息而导致的误导性陈述。


<details>
  <summary>Details</summary>
Motivation: 现有的事实核查系统难以处理半真半假的陈述，因为它们仅基于已有证据判断真实性，而忽略了遗漏信息的影响。

Method: 提出了TRACER框架，通过证据对齐、推断隐含意图和评估隐藏内容的因果影响，来检测遗漏型误导信息。

Result: TRACER显著提升了半真半假分类的F1分数（最高16分），表明建模遗漏信息对可信事实核查的重要性。

Conclusion: 该研究强调了在事实核查中考虑遗漏信息的必要性，TRACER框架为现有系统提供了有效改进。

Abstract: Fact verification systems typically assess whether a claim is supported by
retrieved evidence, assuming that truthfulness depends solely on what is
stated. However, many real-world claims are half-truths, factually correct yet
misleading due to the omission of critical context. Existing models struggle
with such cases, as they are not designed to reason about what is left unsaid.
We introduce the task of half-truth detection, and propose PolitiFact-Hidden, a
new benchmark with 15k political claims annotated with sentence-level evidence
alignment and inferred claim intent. To address this challenge, we present
TRACER, a modular re-assessment framework that identifies omission-based
misinformation by aligning evidence, inferring implied intent, and estimating
the causal impact of hidden content. TRACER can be integrated into existing
fact-checking pipelines and consistently improves performance across multiple
strong baselines. Notably, it boosts Half-True classification F1 by up to 16
points, highlighting the importance of modeling omissions for trustworthy fact
verification.

</details>


### [112] [EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond](https://arxiv.org/abs/2508.00522)
*Jiaxin Deng,Qingcheng Zhu,Junbiao Pang,Linlin Yang,Zhongqian Fu,Baochang Zhang*

Main category: cs.CL

TL;DR: 论文提出Flat-LoRA和EFlat-LoRA，通过寻找平坦最小值提升LoRA的泛化能力，实验证明其在语言和视觉语言模型中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 探索低秩适应（LoRA）的表达能力与泛化能力之间的关系，并解决现有方法在寻找平坦最小值方面的不足。

Method: 提出Flat-LoRA和EFlat-LoRA，通过理论证明全参数空间的扰动可转移到低秩子空间，避免多矩阵干扰。

Result: EFlat-LoRA在GLUE和视觉语言模型任务中表现优于LoRA和全微调，性能提升显著。

Conclusion: LoRA的泛化能力与平坦性密切相关，EFlat-LoRA为提升模型性能提供了有效工具。

Abstract: Little research explores the correlation between the expressive ability and
generalization ability of the low-rank adaptation (LoRA). Sharpness-Aware
Minimization (SAM) improves model generalization for both Convolutional Neural
Networks (CNNs) and Transformers by encouraging convergence to locally flat
minima. However, the connection between sharpness and generalization has not
been fully explored for LoRA due to the lack of tools to either empirically
seek flat minima or develop theoretical methods. In this work, we propose
Flat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for
LoRA. Concretely, we theoretically demonstrate that perturbations in the full
parameter space can be transferred to the low-rank subspace. This approach
eliminates the potential interference introduced by perturbations across
multiple matrices in the low-rank subspace. Our extensive experiments on large
language models and vision-language models demonstrate that EFlat-LoRA achieves
optimize efficiency comparable to that of LoRA while simultaneously attaining
comparable or even better performance. For example, on the GLUE dataset with
RoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and
0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat
shows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets,
respectively. These empirical results also verify that the generalization of
LoRA is closely related to sharpness, which is omitted by previous methods.

</details>


### [113] [The Prosody of Emojis](https://arxiv.org/abs/2508.00537)
*Giulio Zhou,Tsz Kin Lam,Alexandra Birch,Barry Haddow*

Main category: cs.CL

TL;DR: 研究探讨了表情符号如何影响语音中的韵律实现，以及听者如何通过韵律线索理解表情符号的含义。


<details>
  <summary>Details</summary>
Motivation: 在文本交流中，韵律特征（如音高、节奏和语调）缺失，表情符号作为视觉替代品传递情感和语用信息。研究旨在直接关联韵律与表情符号，填补此前研究的空白。

Method: 通过结构化的开放式生产和感知任务收集真实人类语音数据，分析表情符号如何塑造语音表达和听者理解。

Result: 说话者会根据表情符号调整韵律，听者能通过韵律变化识别表情符号，且表情符号语义差异越大，韵律差异越明显。

Conclusion: 表情符号能有效传递韵律意图，揭示了其在数字媒介中的交际作用。

Abstract: Prosodic features such as pitch, timing, and intonation are central to spoken
communication, conveying emotion, intent, and discourse structure. In
text-based settings, where these cues are absent, emojis act as visual
surrogates that add affective and pragmatic nuance. This study examines how
emojis influence prosodic realisation in speech and how listeners interpret
prosodic cues to recover emoji meanings. Unlike previous work, we directly link
prosody and emoji by analysing actual human speech data, collected through
structured but open-ended production and perception tasks. This provides
empirical evidence of how emoji semantics shape spoken delivery and perception.
Results show that speakers adapt their prosody based on emoji cues, listeners
can often identify the intended emoji from prosodic variation alone, and
greater semantic differences between emojis correspond to increased prosodic
divergence. These findings suggest that emojis can act as meaningful carriers
of prosodic intent, offering insight into their communicative role in digitally
mediated contexts.

</details>


### [114] [PaPaformer: Language Model from Pre-trained Paraller Paths](https://arxiv.org/abs/2508.00544)
*Joonas Tapaninaho,Mourad Oussala*

Main category: cs.CL

TL;DR: 提出了一种名为PaPaformer的并行路径解码器-变换器架构，通过训练低维路径并组合成更大模型，减少训练时间和参数数量，同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型训练耗时且计算资源密集，即使是小型模型也需要多GPU和数天时间。本文旨在缩短训练时间至小时级别。

Method: 采用并行路径结构的PaPaformer架构，分别训练低维路径后组合成完整模型，支持任务定制化。

Result: 减少了总参数数量和训练时间，同时性能有所提升，并行路径结构为任务定制提供了灵活性。

Conclusion: PaPaformer通过并行路径设计，显著提升了训练效率，并为模型定制化提供了新思路。

Abstract: The training of modern large-language models requires an increasingly amount
of computation power and time. Even smaller variants, such as small-language
models (SLMs), take several days to train in the best-case scenarios, often
requiring multiple GPUs. This paper explores methods to train and evaluate
decoder-only transformer-based language models in hours instead of days/weeks.
We introduces \textit{PaPaformer}, a decoder-only transformer architecture
variant, whose lower-dimensional parallel paths are combined into larger model.
The paper shows that these lower-dimensional paths can be trained individually
with different types of training data and then combined into one larger model.
This method gives the option to reduce the total number of model parameters and
the training time with increasing performance. Moreover, the use of parallel
path structure opens interesting possibilities to customize paths to
accommodate specific task requirements.

</details>


### [115] [SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought](https://arxiv.org/abs/2508.00574)
*Jianwei Wang,Ziming Wu,Fuming Lai,Shaobing Lian,Ziqian Zeng*

Main category: cs.CL

TL;DR: 论文提出了一种名为SynAdapt的高效推理框架，通过生成合成的连续CoT（CCoT）作为对齐目标，并结合难度分类器优化LLM性能。


<details>
  <summary>Details</summary>
Motivation: 现有连续CoT方法存在间接微调、对齐不足或目标不一致的问题，影响了推理效率。

Method: 提出SynAdapt框架，生成合成的CCoT作为对齐目标，并引入难度分类器自适应提示LLM重新思考难题。

Result: 在多个基准测试中表现出色，实现了最佳的准确性与效率平衡。

Conclusion: SynAdapt通过合成CCoT和难度分类器，显著提升了推理效率和准确性。

Abstract: While Chain-of-Thought (CoT) reasoning improves model performance, it incurs
significant time costs due to the generation of discrete CoT tokens (DCoT).
Continuous CoT (CCoT) offers a more efficient alternative, but existing CCoT
methods are hampered by indirect fine-tuning, limited alignment, or
inconsistent targets. To overcome these limitations, we propose
\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,
\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and
effective alignment target for LLMs. This synthetic CCoT explicitly guides the
LLM to learn CCoT and derive accurate answers directly. Furthermore, relying
solely on CCoT is insufficient for solving hard questions. To address this,
\textit{SynAdapt} integrates a difficulty classifier that leverages both
question context and CCoT to identify hard questions. CCoT can effectively help
identify hard questions after some brief reasoning. We then adaptively prompt
the LLM to re-think these hard questions for improved performance. Extensive
experimental results across various benchmarks from different difficulty levels
strongly demonstrate the effectiveness of our method, achieving the best
accuracy-efficiency trade-off.

</details>


### [116] [A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models](https://arxiv.org/abs/2508.00600)
*Mingruo Yuan,Shuyi Zhang,Ben Kao*

Main category: cs.CL

TL;DR: CRUX框架通过结合上下文忠实性和一致性，提出两种新指标，显著提升了大型语言模型的置信度估计效果。


<details>
  <summary>Details</summary>
Motivation: 现有置信度估计方法忽略了响应与上下文信息的相关性，而CRUX旨在填补这一空白，提升输出质量评估的可靠性。

Method: CRUX通过上下文熵减和统一一致性检查两种新指标，分别量化数据不确定性和模型不确定性。

Result: 在多个基准数据集和领域特定数据集上，CRUX的AUROC表现优于现有基线方法。

Conclusion: CRUX为大型语言模型的置信度估计提供了更可靠的解决方案，尤其在需要背景知识的场景中表现突出。

Abstract: Accurate confidence estimation is essential for trustworthy large language
models (LLMs) systems, as it empowers the user to determine when to trust
outputs and enables reliable deployment in safety-critical applications.
Current confidence estimation methods for LLMs neglect the relevance between
responses and contextual information, a crucial factor in output quality
evaluation, particularly in scenarios where background knowledge is provided.
To bridge this gap, we propose CRUX (Context-aware entropy Reduction and
Unified consistency eXamination), the first framework that integrates context
faithfulness and consistency for confidence estimation via two novel metrics.
First, contextual entropy reduction represents data uncertainty with the
information gain through contrastive sampling with and without context. Second,
unified consistency examination captures potential model uncertainty through
the global consistency of the generated answers with and without context.
Experiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two
domain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,
achieving the highest AUROC than existing baselines.

</details>


### [117] [GHTM: A Graph based Hybrid Topic Modeling Approach in Low-Resource Bengali Language](https://arxiv.org/abs/2508.00605)
*Farhana Haque,Md. Abdur Rahman,Sumon Ahmed*

Main category: cs.CL

TL;DR: 提出了一种基于图卷积网络（GCN）的混合主题模型GHTM，用于孟加拉语文本的主题建模，并在多个数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语因其形态复杂性和资源匮乏，主题建模研究较少，需要更有效的模型。

Method: 使用GCN生成语义丰富的文档嵌入，结合NMF分解得到主题表示。

Result: GHTM在主题一致性和多样性上优于传统和现代方法。

Conclusion: GHTM为孟加拉语主题建模提供了有效解决方案，并引入了新的数据集NCTBText。

Abstract: Topic modeling is a Natural Language Processing (NLP) technique that is used
to identify latent themes and extract topics from text corpora by grouping
similar documents based on their most significant keywords. Although widely
researched in English, topic modeling remains understudied in Bengali due to
its morphological complexity, lack of adequate resources and initiatives. In
this contribution, a novel Graph Convolutional Network (GCN) based model called
GHTM (Graph-Based Hybrid Topic Model) is proposed. This model represents input
vectors of documents as nodes in the graph, which GCN uses to produce
semantically rich embeddings. The embeddings are then decomposed using
Non-negative Matrix Factorization (NMF) to get the topical representations of
the underlying themes of the text corpus. This study compares the proposed
model against a wide range of Bengali topic modeling techniques, from
traditional methods such as LDA, LSA, and NMF to contemporary frameworks such
as BERTopic and Top2Vec on three Bengali datasets. The experimental results
demonstrate the effectiveness of the proposed model by outperforming other
models in topic coherence and diversity. In addition, we introduce a novel
Bengali dataset called "NCTBText" sourced from Bengali textbook materials to
enrich and diversify the predominantly newspaper-centric Bengali corpora.

</details>


### [118] [Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?](https://arxiv.org/abs/2508.00614)
*Lennart Meincke,Ethan Mollick,Lilach Mollick,Dan Shapiro*

Main category: cs.CL

TL;DR: 研究发现，威胁或奖励AI模型对基准性能无显著影响，但提示变化可能对个别问题表现有显著影响。


<details>
  <summary>Details</summary>
Motivation: 验证关于威胁和奖励AI模型的常见信念，评估其对模型性能的实际影响。

Method: 在GPQA和MMLU-Pro基准上测试威胁和奖励提示的效果。

Result: 威胁或奖励对整体性能无显著影响，但提示变化可能显著影响个别问题表现。

Conclusion: 简单提示变化对困难问题的效果有限，但对个别问题可能有显著影响。

Abstract: This is the third in a series of short reports that seek to help business,
education, and policy leaders understand the technical details of working with
AI through rigorous testing. In this report, we investigate two commonly held
prompting beliefs: a) offering to tip the AI model and b) threatening the AI
model. Tipping was a commonly shared tactic for improving AI performance and
threats have been endorsed by Google Founder Sergey Brin (All-In, May 2025,
8:20) who observed that 'models tend to do better if you threaten them,' a
claim we subject to empirical testing here. We evaluate model performance on
GPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).
  We demonstrate two things:
  - Threatening or tipping a model generally has no significant effect on
benchmark performance.
  - Prompt variations can significantly affect performance on a per-question
level. However, it is hard to know in advance whether a particular prompting
approach will help or harm the LLM's ability to answer any particular question.
  Taken together, this suggests that simple prompting variations might not be
as effective as previously assumed, especially for difficult problems. However,
as reported previously (Meincke et al. 2025a), prompting approaches can yield
significantly different results for individual questions.

</details>


### [119] [DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models](https://arxiv.org/abs/2508.00619)
*Shantanu Thorat,Andrew Caines*

Main category: cs.CL

TL;DR: 现有AIG文本检测器在真实场景中表现不佳，研究通过构建DACTYL数据集和改进分类器方法（如DXO）来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 现有AIG文本检测器在内部测试中表现良好，但在实际应用中不够鲁棒，尤其是在少样本或单样本生成文本的检测上。

Method: 引入DACTYL数据集，包含少样本/单样本生成文本和领域特定CPT模型生成的文本；比较BCE和DXO两种分类器优化方法。

Result: 现有检测器在DACTYL数据集上表现不佳；DXO分类器在分布外文本上优于BCE分类器。

Conclusion: DXO分类器泛化能力更强，AIG文本检测器需进一步改进。

Abstract: Existing AIG (AI-generated) text detectors struggle in real-world settings
despite succeeding in internal testing, suggesting that they may not be robust
enough. We rigorously examine the machine-learning procedure to build these
detectors to address this. Most current AIG text detection datasets focus on
zero-shot generations, but little work has been done on few-shot or one-shot
generations, where LLMs are given human texts as an example. In response, we
introduce the Diverse Adversarial Corpus of Texts Yielded from Language models
(DACTYL), a challenging AIG text detection dataset focusing on
one-shot/few-shot generations. We also include texts from domain-specific
continued-pre-trained (CPT) language models, where we fully train all
parameters using a memory-efficient optimization approach. Many existing AIG
text detectors struggle significantly on our dataset, indicating a potential
vulnerability to one-shot/few-shot and CPT-generated texts. We also train our
own classifiers using two approaches: standard binary cross-entropy (BCE)
optimization and a more recent approach, deep X-risk optimization (DXO). While
BCE-trained classifiers marginally outperform DXO classifiers on the DACTYL
test set, the latter excels on out-of-distribution (OOD) texts. In our mock
deployment scenario in student essay detection with an OOD student essay
dataset, the best DXO classifier outscored the best BCE-trained classifier by
50.56 macro-F1 score points at the lowest false positive rates for both. Our
results indicate that DXO classifiers generalize better without overfitting to
the test set. Our experiments highlight several areas of improvement for AIG
text detectors.

</details>


### [120] [Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications](https://arxiv.org/abs/2508.00669)
*Wenxuan Wang,Zizhan Ma,Meidan Ding,Shiyi Zheng,Shengyuan Liu,Jie Liu,Jiaming Ji,Wenting Chen,Xiang Li,Linlin Shen,Yixuan Yuan*

Main category: cs.CL

TL;DR: 本文系统综述了大型语言模型（LLMs）在医学推理领域的发展，提出了训练时和测试时的增强技术分类，并分析了其在多模态数据和临床应用中的表现。


<details>
  <summary>Details</summary>
Motivation: LLMs在医学中表现出色，但缺乏系统、透明和可验证的推理能力，这是临床实践的核心需求。

Method: 通过分类训练时策略（如监督微调）和测试时机制（如提示工程），分析60项研究（2022-2025）。

Result: 总结了医学推理技术的应用和评估方法，揭示了关键挑战（如忠实性与合理性的差距）。

Conclusion: 未来需发展高效、稳健且负责任的医学AI，解决多模态推理等问题。

Abstract: The proliferation of Large Language Models (LLMs) in medicine has enabled
impressive capabilities, yet a critical gap remains in their ability to perform
systematic, transparent, and verifiable reasoning, a cornerstone of clinical
practice. This has catalyzed a shift from single-step answer generation to the
development of LLMs explicitly designed for medical reasoning. This paper
provides the first systematic review of this emerging field. We propose a
taxonomy of reasoning enhancement techniques, categorized into training-time
strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time
mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how
these techniques are applied across different data modalities (text, image,
code) and in key clinical applications such as diagnosis, education, and
treatment planning. Furthermore, we survey the evolution of evaluation
benchmarks from simple accuracy metrics to sophisticated assessments of
reasoning quality and visual interpretability. Based on an analysis of 60
seminal studies from 2022-2025, we conclude by identifying critical challenges,
including the faithfulness-plausibility gap and the need for native multimodal
reasoning, and outlining future directions toward building efficient, robust,
and sociotechnically responsible medical AI.

</details>


### [121] [MELAC: Massive Evaluation of Large Language Models with Alignment of Culture in Persian Language](https://arxiv.org/abs/2508.00673)
*Farhan Farsi,Farnaz Aghababaloo,Shahriar Shariati Motlagh,Parsa Ghofrani,MohammadAli SadraeiJavaheri,Shayan Bali,Amirhossein Shabani,Farbod Bijary,Ghazal Zamaninejad,AmirMohammad Salehoof,Saeedeh Momtazi*

Main category: cs.CL

TL;DR: 论文提出了19个针对波斯语和伊朗文化的新评估数据集，用于填补大型语言模型在非西方语言和文化评估上的空白。


<details>
  <summary>Details</summary>
Motivation: 现有评估资源主要针对英语和西方文化，缺乏对其他语言和文化背景的评估，特别是波斯语和伊朗文化。

Method: 设计了19个新数据集，涵盖伊朗法律、波斯语法、波斯习语和大学入学考试等主题，并对41个主流大型语言模型进行了基准测试。

Result: 通过新数据集评估了41个模型，揭示了它们在波斯语和伊朗文化背景下的表现。

Conclusion: 研究填补了大型语言模型在波斯语和伊朗文化评估上的空白，为未来多语言和文化评估提供了参考。

Abstract: As large language models (LLMs) become increasingly embedded in our daily
lives, evaluating their quality and reliability across diverse contexts has
become essential. While comprehensive benchmarks exist for assessing LLM
performance in English, there remains a significant gap in evaluation resources
for other languages. Moreover, because most LLMs are trained primarily on data
rooted in European and American cultures, they often lack familiarity with
non-Western cultural contexts. To address this limitation, our study focuses on
the Persian language and Iranian culture. We introduce 19 new evaluation
datasets specifically designed to assess LLMs on topics such as Iranian law,
Persian grammar, Persian idioms, and university entrance exams. Using these
datasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing
cultural and linguistic evaluation gap in the field.

</details>


### [122] [Team "better_call_claude": Style Change Detection using a Sequential Sentence Pair Classifier](https://arxiv.org/abs/2508.00675)
*Gleb Schmidt,Johannes Römisch,Mariia Halchynska,Svetlana Gorovaia,Ivan P. Yamshchikov*

Main category: cs.CL

TL;DR: 论文提出了一种基于预训练语言模型和双向LSTM的序列句子对分类器（SSPC），用于检测文档中的风格变化，并在PAN 2025任务中取得了优于基线的表现。


<details>
  <summary>Details</summary>
Motivation: 风格变化检测是计算作者分析中的重要且具有挑战性的问题，尤其是在细粒度（句子级别）的任务中。

Method: 使用预训练语言模型（PLM）生成句子表示，再通过双向LSTM（BiLSTM）进行上下文建模，最后用多层感知机预测相邻句子的风格变化。

Result: 在PAN-2025测试数据集上，模型在EASY、MEDIUM和HARD数据上的宏F1分数分别为0.923、0.828和0.724，优于随机基线和零样本模型。

Conclusion: 该方法通过上下文建模有效解决了短句子风格变化检测的挑战，表现优于现有基线。

Abstract: Style change detection - identifying the points in a document where writing
style shifts - remains one of the most important and challenging problems in
computational authorship analysis. At PAN 2025, the shared task challenges
participants to detect style switches at the most fine-grained level:
individual sentences. The task spans three datasets, each designed with
controlled and increasing thematic variety within documents. We propose to
address this problem by modeling the content of each problem instance - that
is, a series of sentences - as a whole, using a Sequential Sentence Pair
Classifier (SSPC). The architecture leverages a pre-trained language model
(PLM) to obtain representations of individual sentences, which are then fed
into a bidirectional LSTM (BiLSTM) to contextualize them within the document.
The BiLSTM-produced vectors of adjacent sentences are concatenated and passed
to a multi-layer perceptron for prediction per adjacency. Building on the work
of previous PAN participants classical text segmentation, the approach is
relatively conservative and lightweight. Nevertheless, it proves effective in
leveraging contextual information and addressing what is arguably the most
challenging aspect of this year's shared task: the notorious problem of
"stylistically shallow", short sentences that are prevalent in the proposed
benchmark data. Evaluated on the official PAN-2025 test datasets, the model
achieves strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM,
and HARD data, respectively, outperforming not only the official random
baselines but also a much more challenging one: claude-3.7-sonnet's zero-shot
performance.

</details>


### [123] [Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries](https://arxiv.org/abs/2508.00679)
*Shubham Kumar Nigam,Tanmay Dubey,Noel Shallum,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: TraceRetriever是一种法律先例检索方法，通过结合BM25、向量数据库和交叉编码器模型，从有限案例信息中提取关键片段，解决了传统检索方法在大量法律文件中的不足。


<details>
  <summary>Details</summary>
Motivation: 法律先例检索在普通法体系中至关重要，但传统方法难以应对日益复杂的法律文件数量和多样性。

Method: 结合BM25、向量数据库和交叉编码器模型，通过层次化BiLSTM CRF分类器生成修辞标注，并使用互惠排名融合进行结果整合和重排序。

Result: 在IL-PCR和COLIEE 2025数据集上验证，TraceRetriever能够高效处理部分案例信息，提升检索效果。

Conclusion: TraceRetriever为法律研究提供了可靠且可扩展的先例检索基础，尤其适用于信息不完整的情况。

Abstract: Legal precedent retrieval is a cornerstone of the common law system, governed
by the principle of stare decisis, which demands consistency in judicial
decisions. However, the growing complexity and volume of legal documents
challenge traditional retrieval methods. TraceRetriever mirrors real-world
legal search by operating with limited case information, extracting only
rhetorically significant segments instead of requiring complete documents. Our
pipeline integrates BM25, Vector Database, and Cross-Encoder models, combining
initial results through Reciprocal Rank Fusion before final re-ranking.
Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier
trained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,
TraceRetriever addresses growing document volume challenges while aligning with
practical search constraints, reliable and scalable foundation for precedent
retrieval enhancing legal research when only partial case knowledge is
available.

</details>


### [124] [Better Call Claude: Can LLMs Detect Changes of Writing Style?](https://arxiv.org/abs/2508.00680)
*Johannes Römisch,Svetlana Gorovaia,Mariia Halchynska,Gleb Schmidt,Ivan P. Yamshchikov*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLMs）在句子级风格变化检测任务中的零样本性能，发现其对写作风格变化敏感，并优于PAN竞赛基线。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在极具挑战性的作者分析任务（句子级风格变化检测）中的表现。

Method: 在PAN~2024和2025数据集上对四种LLMs进行基准测试。

Result: LLMs对写作风格变化敏感，其准确性优于PAN竞赛基线，且对内容无关的风格信号更敏感。

Conclusion: 最新一代LLMs在风格检测任务中表现出色，可能比之前认为的更依赖纯风格信号。

Abstract: This article explores the zero-shot performance of state-of-the-art large
language models (LLMs) on one of the most challenging tasks in authorship
analysis: sentence-level style change detection. Benchmarking four LLMs on the
official PAN~2024 and 2025 "Multi-Author Writing Style Analysis" datasets, we
present several observations. First, state-of-the-art generative models are
sensitive to variations in writing style - even at the granular level of
individual sentences. Second, their accuracy establishes a challenging baseline
for the task, outperforming suggested baselines of the PAN competition.
Finally, we explore the influence of semantics on model predictions and present
evidence suggesting that the latest generation of LLMs may be more sensitive to
content-independent and purely stylistic signals than previously reported.

</details>


### [125] [NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System](https://arxiv.org/abs/2508.00709)
*Shubham Kumar Nigam,Balaramamahanthi Deepak Patnaik,Shivam Mishra,Ajay Varghese Thomas,Noel Shallum,Kripabandhu Ghosh,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: NyayaRAG框架通过结合案件事实、法律条文和先例，提升了印度法律判决预测的准确性和解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在印度法律背景下忽视了成文法和判例法的重要性，NyayaRAG旨在填补这一空白。

Method: 采用检索增强生成（RAG）框架，结合案件描述、法律条文和先例，通过特定管道评估预测效果。

Result: 结合结构化法律知识显著提高了预测准确性和解释质量。

Conclusion: NyayaRAG为法律判决预测提供了更全面的方法，尤其适用于依赖成文法和判例法的系统。

Abstract: Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,
aiming to automate judicial outcome forecasting and enhance interpretability in
legal reasoning. While previous approaches in the Indian context have relied on
internal case content such as facts, issues, and reasoning, they often overlook
a core element of common law systems, which is reliance on statutory provisions
and judicial precedents. In this work, we propose NyayaRAG, a
Retrieval-Augmented Generation (RAG) framework that simulates realistic
courtroom scenarios by providing models with factual case descriptions,
relevant legal statutes, and semantically retrieved prior cases. NyayaRAG
evaluates the effectiveness of these combined inputs in predicting court
decisions and generating legal explanations using a domain-specific pipeline
tailored to the Indian legal system. We assess performance across various input
configurations using both standard lexical and semantic metrics as well as
LLM-based evaluators such as G-Eval. Our results show that augmenting factual
inputs with structured legal knowledge significantly improves both predictive
accuracy and explanation quality.

</details>


### [126] [Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA](https://arxiv.org/abs/2508.00719)
*Yingxu Wang,Shiqi Fan,Mengzhu Wang,Siwei Liu*

Main category: cs.CL

TL;DR: DAMR框架通过结合符号搜索和自适应路径评估，解决了KGQA中静态路径提取和动态路径生成的问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决KGQA中静态路径提取适应性差和动态路径生成计算成本高的问题。

Method: 提出DAMR框架，结合MCTS和LLM规划器，使用轻量级Transformer评分器进行上下文感知路径评估，并引入动态伪路径优化机制。

Result: 在多个KGQA基准测试中显著优于现有方法。

Conclusion: DAMR通过自适应路径评估和动态优化机制，实现了高效且上下文感知的KGQA。

Abstract: Knowledge Graph Question Answering (KGQA) aims to interpret natural language
queries and perform structured reasoning over knowledge graphs by leveraging
their relational and semantic structures to retrieve accurate answers. Recent
KGQA methods primarily follow either retrieve-then-reason paradigm, relying on
GNNs or heuristic rules for static paths extraction, or dynamic path generation
strategies that use large language models (LLMs) with prompting to jointly
perform retrieval and reasoning. However, the former suffers from limited
adaptability due to static path extraction and lack of contextual refinement,
while the latter incurs high computational costs and struggles with accurate
path evaluation due to reliance on fixed scoring functions and extensive LLM
calls. To address these issues, this paper proposes Dynamically Adaptive
MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search
with adaptive path evaluation for efficient and context-aware KGQA. DAMR
employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based
planner, which selects top-$k$ relevant relations at each step to reduce search
space. To improve path evaluation accuracy, we introduce a lightweight
Transformer-based scorer that performs context-aware plausibility estimation by
jointly encoding the question and relation sequence through cross-attention,
enabling the model to capture fine-grained semantic shifts during multi-hop
reasoning. Furthermore, to alleviate the scarcity of high-quality supervision,
DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically
generates training signals from partial paths explored during search, allowing
the scorer to continuously adapt to the evolving distribution of reasoning
trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR
significantly outperforms state-of-the-art methods.

</details>


### [127] [Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data](https://arxiv.org/abs/2508.00741)
*Sohaib Imran,Rob Lamb,Peter M. Atkinson*

Main category: cs.CL

TL;DR: 研究探讨大型语言模型（LLM）是否能够基于训练数据进行推理，特别是通过实验验证其在上下文外推断（out-of-context abduction）的能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLM是否能够利用训练数据中的信息进行推理，以增强对AI安全性的理解。

Method: 设计实验，训练LLM学习虚构聊天机器人的名称和行为描述，但不提供对话示例，观察其是否能推断出聊天机器人的名称和行为。

Result: GPT 4o能够正确推断聊天机器人的名称，并在行为描述训练后表现出更符合该聊天机器人的行为。

Conclusion: 研究表明LLM具备一定的情境感知能力，这对AI安全性有重要意义。

Abstract: Large language models (LLMs) are trained on large corpora, yet it is unclear
whether they can reason about the information present within their training
data. We design experiments to study out-of-context abduction in LLMs, the
ability to infer the most plausible explanations for observations using
relevant facts present in training data. We train treatment LLMs on names and
behavior descriptions of fictitious chatbots, but not on examples of dialogue
with the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at
least one chatbot's name after observing example responses characteristic of
that chatbot. We also find that previously training GPT 4o on descriptions of a
chatbot's behavior allows it to display behaviors more characteristic of the
chatbot when iteratively trained to display such behaviors. Our results have
implications for situational awareness in LLMs and, therefore, for AI safety.

</details>


### [128] [Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents](https://arxiv.org/abs/2508.00742)
*Sarah Mercer,Daniel P. Martin,Phil Swatton*

Main category: cs.CL

TL;DR: 论文探讨了基于角色的生成代理在社会科学研究中的有效性，通过GPT-4代理重现HEXACO人格测试，发现其部分符合人类人格结构，但存在模型特异性偏差。


<details>
  <summary>Details</summary>
Motivation: 验证生成代理是否能有效代表人类群体，特别是在人格研究中的应用。

Method: 使用310个GPT-4代理重现HEXACO人格测试，进行因子分析并与原始人类数据对比。

Result: 代理响应显示出部分符合HEXACO框架的人格结构，且结果在GPT-4内一致，但跨模型分析显示偏差。

Conclusion: 生成代理在社会科学研究中具有潜力，但需注意模型特异性和设计一致性。

Abstract: Generative agents powered by Large Language Models demonstrate human-like
characteristics through sophisticated natural language interactions. Their
ability to assume roles and personalities based on predefined character
biographies has positioned them as cost-effective substitutes for human
participants in social science research. This paper explores the validity of
such persona-based agents in representing human populations; we recreate the
HEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,
conducting factor analysis on their responses, and comparing these results to
the original findings presented by Ashton, Lee, & Goldberg in 2004. Our results
found 1) a coherent and reliable personality structure was recoverable from the
agents' responses demonstrating partial alignment to the HEXACO framework. 2)
the derived personality dimensions were consistent and reliable within GPT-4,
when coupled with a sufficiently curated population, and 3) cross-model
analysis revealed variability in personality profiling, suggesting
model-specific biases and limitations. We discuss the practical considerations
and challenges encountered during the experiment. This study contributes to the
ongoing discourse on the potential benefits and limitations of using generative
agents in social science research and provides useful guidance on designing
consistent and representative agent personas to maximise coverage and
representation of human personality traits.

</details>


### [129] [Agentic large language models improve retrieval-based radiology question answering](https://arxiv.org/abs/2508.00743)
*Sebastian Wind,Jeta Sopa,Daniel Truhn,Mahshad Lotfinia,Tri-Thien Nguyen,Keno Bressem,Lisa Adams,Mirabela Rusu,Harald Köstler,Gerhard Wellein,Andreas Maier,Soroosh Tayebi Arasteh*

Main category: cs.CL

TL;DR: 提出了一个基于代理的RAG框架，通过LLM自主分解放射学问题、迭代检索临床证据并动态合成回答，显著提高了诊断准确性和事实性。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统在放射学QA中依赖单步检索，难以处理复杂临床推理任务，因此需要更高效的框架。

Method: 采用代理RAG框架，评估24种不同架构、规模和训练范式的LLM，使用104个专家策划的放射学问题。

Result: 代理检索显著提高了诊断准确性（73% vs. 64%），减少了幻觉（9.4%），并在46%的案例中检索到相关临床背景。

Conclusion: 代理框架能显著提升放射学QA的事实性和准确性，尤其是中等规模LLM，值得进一步研究其临床实用性。

Abstract: Clinical decision-making in radiology increasingly benefits from artificial
intelligence (AI), particularly through large language models (LLMs). However,
traditional retrieval-augmented generation (RAG) systems for radiology question
answering (QA) typically rely on single-step retrieval, limiting their ability
to handle complex clinical reasoning tasks. Here we propose an agentic RAG
framework enabling LLMs to autonomously decompose radiology questions,
iteratively retrieve targeted clinical evidence from Radiopaedia, and
dynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning
diverse architectures, parameter scales (0.5B to >670B), and training paradigms
(general-purpose, reasoning-optimized, clinically fine-tuned), using 104
expert-curated radiology questions from previously established RSNA-RadioQA and
ExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic
accuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional
online RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized
models (e.g., Mistral Large improved from 72% to 81%) and small-scale models
(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B
parameters) demonstrated minimal changes (<2% improvement). Additionally,
agentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically
relevant context in 46% of cases, substantially aiding factual grounding. Even
clinically fine-tuned models exhibited meaningful improvements (e.g.,
MedGemma-27B improved from 71% to 81%), indicating complementary roles of
retrieval and fine-tuning. These results highlight the potential of agentic
frameworks to enhance factuality and diagnostic accuracy in radiology QA,
particularly among mid-sized LLMs, warranting future studies to validate their
clinical utility.

</details>


### [130] [GLiDRE: Generalist Lightweight model for Document-level Relation Extraction](https://arxiv.org/abs/2508.00757)
*Robin Armingaud,Romaric Besançon*

Main category: cs.CL

TL;DR: GLiDRE是一种基于GLiNER关键思想的新型文档级关系抽取模型，在Re-DocRED数据集上表现出色，尤其在少样本场景下达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于ATLOP架构的方法在零样本或少样本设置下的性能尚未充分探索，而GLiNER已证明紧凑模型可超越大型语言模型，因此提出GLiDRE。

Method: GLiDRE借鉴GLiNER的关键思想，构建了一种新的文档级关系抽取模型。

Result: 在Re-DocRED数据集上，GLiDRE在少样本场景下实现了最优性能。

Conclusion: GLiDRE在少样本关系抽取任务中表现出色，代码已公开。

Abstract: Relation Extraction (RE) is a fundamental task in Natural Language
Processing, and its document-level variant poses significant challenges, due to
the need to model complex interactions between entities across sentences.
Current approaches, largely based on the ATLOP architecture, are commonly
evaluated on benchmarks like DocRED and Re-DocRED. However, their performance
in zero-shot or few-shot settings remains largely underexplored due to the
task's complexity. Recently, the GLiNER model has shown that a compact NER
model can outperform much larger Large Language Models. With a similar
motivation, we introduce GLiDRE, a new model for document-level relation
extraction that builds on the key ideas of GliNER. We benchmark GLiDRE against
state-of-the-art models across various data settings on the Re-DocRED dataset.
Our results demonstrate that GLiDRE achieves state-of-the-art performance in
few-shot scenarios. Our code is publicly available.

</details>


### [131] [MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations](https://arxiv.org/abs/2508.00760)
*Qiyao Xue,Yuchen Dou,Ryan Shi,Xiang Lorraine Li,Wei Gao*

Main category: cs.CL

TL;DR: 提出了一种基于BERT的多模态框架MMBERT，用于中文社交媒体上的仇恨言论检测，通过混合专家架构整合文本、语音和视觉模态，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 中文社交媒体上仇恨言论检测面临独特挑战，尤其是伪装技术的使用，现有研究多集中于英文数据集，中文多模态策略研究较少。

Method: 开发了MMBERT框架，采用混合专家架构整合多模态信息，并提出渐进式三阶段训练方法以提高稳定性。

Result: 在多个中文仇恨言论数据集上，MMBERT显著优于微调的BERT模型、微调的大语言模型及上下文学习方法。

Conclusion: MMBERT通过多模态整合和渐进式训练，有效提升了中文仇恨言论检测的鲁棒性和性能。

Abstract: Hate speech detection on Chinese social networks presents distinct
challenges, particularly due to the widespread use of cloaking techniques
designed to evade conventional text-based detection systems. Although large
language models (LLMs) have recently improved hate speech detection
capabilities, the majority of existing work has concentrated on English
datasets, with limited attention given to multimodal strategies in the Chinese
context. In this study, we propose MMBERT, a novel BERT-based multimodal
framework that integrates textual, speech, and visual modalities through a
Mixture-of-Experts (MoE) architecture. To address the instability associated
with directly integrating MoE into BERT-based models, we develop a progressive
three-stage training paradigm. MMBERT incorporates modality-specific experts, a
shared self-attention mechanism, and a router-based expert allocation strategy
to enhance robustness against adversarial perturbations. Empirical results in
several Chinese hate speech datasets show that MMBERT significantly surpasses
fine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing
in-context learning approaches.

</details>


### [132] [ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation](https://arxiv.org/abs/2508.00762)
*Atakan Site,Emre Hakan Erdemir,Gülşen Eryiğit*

Main category: cs.CL

TL;DR: 本文介绍了SemEval-2025 Task 8的零射解决方案，利用LLM生成Python代码进行表格问答，实验显示不同LLM效果各异，代码生成方法优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 解决SemEval-2025 Task 8中的表格问答问题，探索零射方法在多样化数据集上的表现。

Method: 提出基于开源LLM的Python代码生成框架，通过优化提示策略生成可执行的Pandas代码。

Result: 不同LLM在代码生成上表现不一，Python代码生成方法优于其他方法；系统在开源模型类别中排名第八（Subtask I）和第六（Subtask II）。

Conclusion: LLM生成的Python代码在表格问答任务中表现优异，零射方法具有潜力。

Abstract: This paper presents our system for SemEval-2025 Task 8: DataBench,
Question-Answering over Tabular Data. The primary objective of this task is to
perform question answering on given tabular datasets from diverse domains under
two subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To
tackle both subtasks, we developed a zero-shot solution with a particular
emphasis on leveraging Large Language Model (LLM)-based code generation.
Specifically, we propose a Python code generation framework utilizing
state-of-the-art open-source LLMs to generate executable Pandas code via
optimized prompting strategies. Our experiments reveal that different LLMs
exhibit varying levels of effectiveness in Python code generation.
Additionally, results show that Python code generation achieves superior
performance in tabular question answering compared to alternative approaches.
Although our ranking among zero-shot systems is unknown at the time of this
paper's submission, our system achieved eighth place in Subtask I and sixth
place in Subtask~II among the 30 systems that outperformed the baseline in the
open-source models category.

</details>


### [133] [Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models](https://arxiv.org/abs/2508.00788)
*Xushuo Tang,Yi Ding,Zhengyi Yang,Yin Chen,Yongrui Gu,Wenke Yang,Mingchen Ju,Xin Cao,Yongfei Liu,Wenjie Zhang*

Main category: cs.CL

TL;DR: MISGENDERED+是一个更新的基准测试，用于评估大语言模型（LLMs）在代词使用上的表现，结果显示在性别中立代词上有进步，但在新代词和反向推理任务上仍有不足。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决LLMs在敏感语境中公平性和包容性的问题，尤其是性别中立和新代词的使用。

Method: 研究引入了MISGENDERED+基准测试，评估了五种代表性LLM（如GPT-4o、Claude 4等）在零样本、少样本和性别身份推理任务中的表现。

Result: 结果显示在性别中立代词上有所改进，但在新代词和反向推理任务上表现不一致。

Conclusion: 研究表明LLMs在身份敏感推理上仍有不足，未来需进一步研究以提升包容性AI。

Abstract: Large language models (LLMs) are increasingly deployed in sensitive contexts
where fairness and inclusivity are critical. Pronoun usage, especially
concerning gender-neutral and neopronouns, remains a key challenge for
responsible AI. Prior work, such as the MISGENDERED benchmark, revealed
significant limitations in earlier LLMs' handling of inclusive pronouns, but
was constrained to outdated models and limited evaluations. In this study, we
introduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'
pronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,
DeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender
identity inference. Our results show notable improvements compared with
previous studies, especially in binary and gender-neutral pronoun accuracy.
However, accuracy on neopronouns and reverse inference tasks remains
inconsistent, underscoring persistent gaps in identity-sensitive reasoning. We
discuss implications, model-specific observations, and avenues for future
inclusive AI research.

</details>


### [134] [Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models](https://arxiv.org/abs/2508.00819)
*Jinsong Li,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jiaqi Wang,Dahua Lin*

Main category: cs.CL

TL;DR: DAEDAL是一种无需训练的动态自适应长度扩展策略，解决了扩散大语言模型（DLLMs）静态长度分配的问题，提升性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: DLLMs因静态预定义生成长度的限制，导致性能与计算效率的权衡问题。

Method: DAEDAL分两阶段：1）初始长度扩展；2）动态干预生成不足区域。

Result: DAEDAL在性能上媲美或优于固定长度基线，同时提高计算效率。

Conclusion: DAEDAL解决了DLLMs的静态长度限制，为其应用开辟了新潜力。

Abstract: Diffusion Large Language Models (DLLMs) are emerging as a powerful
alternative to the dominant Autoregressive Large Language Models, offering
efficient parallel generation and capable global context modeling. However, the
practical application of DLLMs is hindered by a critical architectural
constraint: the need for a statically predefined generation length. This static
length allocation leads to a problematic trade-off: insufficient lengths
cripple performance on complex tasks, while excessive lengths incur significant
computational overhead and sometimes result in performance degradation. While
the inference framework is rigid, we observe that the model itself possesses
internal signals that correlate with the optimal response length for a given
task. To bridge this gap, we leverage these latent signals and introduce
DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive
Length Expansion for Diffusion Large Language Models. DAEDAL operates in two
phases: 1) Before the denoising process, DAEDAL starts from a short initial
length and iteratively expands it to a coarse task-appropriate length, guided
by a sequence completion metric. 2) During the denoising process, DAEDAL
dynamically intervenes by pinpointing and expanding insufficient generation
regions through mask token insertion, ensuring the final output is fully
developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves
performance comparable, and in some cases superior, to meticulously tuned
fixed-length baselines, while simultaneously enhancing computational efficiency
by achieving a higher effective token ratio. By resolving the static length
constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap
with their Autoregressive counterparts and paving the way for more efficient
and capable generation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [135] [XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation](https://arxiv.org/abs/2508.00097)
*Zhigen Zhao,Liuchuan Yu,Ke Jing,Ning Yang*

Main category: cs.RO

TL;DR: XRoboToolkit是一个基于OpenXR标准的跨平台扩展现实机器人遥操作框架，解决了现有数据采集方法的可扩展性和质量问题。


<details>
  <summary>Details</summary>
Motivation: 当前机器人演示数据集的采集方法（如遥操作）存在可扩展性差、设置复杂和数据质量低的问题，亟需改进。

Method: XRoboToolkit采用低延迟立体视觉反馈、优化逆运动学，并支持多种跟踪方式（如头部、控制器、手部等），模块化设计使其能跨平台集成。

Result: 通过精确操作任务验证了框架的有效性，并训练出具有鲁棒自主性能的VLA模型。

Conclusion: XRoboToolkit为高质量机器人数据集采集提供了高效解决方案，推动了VLA模型的发展。

Abstract: The rapid advancement of Vision-Language-Action models has created an urgent
need for large-scale, high-quality robot demonstration datasets. Although
teleoperation is the predominant method for data collection, current approaches
suffer from limited scalability, complex setup procedures, and suboptimal data
quality. This paper presents XRoboToolkit, a cross-platform framework for
extended reality based robot teleoperation built on the OpenXR standard. The
system features low-latency stereoscopic visual feedback, optimization-based
inverse kinematics, and support for diverse tracking modalities including head,
controller, hand, and auxiliary motion trackers. XRoboToolkit's modular
architecture enables seamless integration across robotic platforms and
simulation environments, spanning precision manipulators, mobile robots, and
dexterous hands. We demonstrate the framework's effectiveness through precision
manipulation tasks and validate data quality by training VLA models that
exhibit robust autonomous performance.

</details>


### [136] [CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System](https://arxiv.org/abs/2508.00162)
*Noboru Myers,Obin Kwon,Sankalp Yamsani,Joohyung Kim*

Main category: cs.RO

TL;DR: CHILD是一种紧凑可重构的遥操作系统，支持人形机器人的关节级控制，适用于全身控制和移动操作。


<details>
  <summary>Details</summary>
Motivation: 现有研究很少支持人形机器人的全身关节级遥操作，限制了任务的多样性。

Method: CHILD系统设计为可穿戴设备，支持直接关节映射和自适应力反馈。

Result: 在多个机器人系统上验证了移动操作和全身控制能力。

Conclusion: CHILD系统提高了遥操作的多样性和安全性，并开源硬件设计以促进可访问性和可重复性。

Abstract: Recent advances in teleoperation have demonstrated robots performing complex
manipulation tasks. However, existing works rarely support whole-body
joint-level teleoperation for humanoid robots, limiting the diversity of tasks
that can be accomplished. This work presents Controller for Humanoid Imitation
and Live Demonstration (CHILD), a compact reconfigurable teleoperation system
that enables joint level control over humanoid robots. CHILD fits within a
standard baby carrier, allowing the operator control over all four limbs, and
supports both direct joint mapping for full-body control and loco-manipulation.
Adaptive force feedback is incorporated to enhance operator experience and
prevent unsafe joint movements. We validate the capabilities of this system by
conducting loco-manipulation and full-body control examples on a humanoid robot
and multiple dual-arm systems. Lastly, we open-source the design of the
hardware promoting accessibility and reproducibility. Additional details and
open-source information are available at our project website:
https://uiuckimlab.github.io/CHILD-pages.

</details>


### [137] [Topology-Inspired Morphological Descriptor for Soft Continuum Robots](https://arxiv.org/abs/2508.00258)
*Zhiwei Wu,Siyi Wei,Jiahao Luo,Jinhui Zhang*

Main category: cs.RO

TL;DR: 提出了一种基于拓扑的形态描述符，结合伪刚体模型和莫尔斯理论，用于软连续机器人的形态定量表征、分类和控制。


<details>
  <summary>Details</summary>
Motivation: 提升软连续机器人在医疗应用（如微创手术和血管内介入）中的精确性和适应性。

Method: 结合伪刚体模型和莫尔斯理论，通过计算方向投影的临界点，实现形态的离散表示和分类，并通过优化问题控制形态。

Result: 提出了一种统一的框架，用于软连续机器人的形态描述、分类和控制。

Conclusion: 该方法为软连续机器人的形态控制提供了新思路，有望在医疗领域发挥重要作用。

Abstract: This paper presents a topology-inspired morphological descriptor for soft
continuum robots by combining a pseudo-rigid-body (PRB) model with Morse theory
to achieve a quantitative characterization of robot morphologies. By counting
critical points of directional projections, the proposed descriptor enables a
discrete representation of multimodal configurations and facilitates
morphological classification. Furthermore, we apply the descriptor to
morphology control by formulating the target configuration as an optimization
problem to compute actuation parameters that generate equilibrium shapes with
desired topological features. The proposed framework provides a unified
methodology for quantitative morphology description, classification, and
control of soft continuum robots, with the potential to enhance their precision
and adaptability in medical applications such as minimally invasive surgery and
endovascular interventions.

</details>


### [138] [UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents](https://arxiv.org/abs/2508.00288)
*Jianqiang Xiao,Yuexuan Sun,Yixin Shao,Boxi Gan,Rongqiang Liu,Yanjing Wu,Weili Gua,Xiang Deng*

Main category: cs.RO

TL;DR: UAV-ON是一个用于空中智能体在开放环境中进行大规模目标导航的基准测试，摆脱了对详细语言指令的依赖，引入了语义目标和复杂环境挑战。


<details>
  <summary>Details</summary>
Motivation: 现有空中导航研究多依赖语言指令（如VLN），限制了其扩展性和自主性。UAV-ON旨在填补这一空白，推动基于语义目标的无人机自主导航研究。

Method: 提出UAV-ON基准，包含14个高保真环境和1270个标注目标对象，通过实例级指令编码语义目标。实现AOA等基线方法，结合指令语义与自我中心观测进行导航。

Result: 基线方法在UAV-ON中表现不佳，突显了空中导航与语义目标结合的挑战。

Conclusion: UAV-ON为复杂环境中基于语义目标的无人机自主导航研究提供了新方向。

Abstract: Aerial navigation is a fundamental yet underexplored capability in embodied
intelligence, enabling agents to operate in large-scale, unstructured
environments where traditional navigation paradigms fall short. However, most
existing research follows the Vision-and-Language Navigation (VLN) paradigm,
which heavily depends on sequential linguistic instructions, limiting its
scalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark
for large-scale Object Goal Navigation (ObjectNav) by aerial agents in
open-world environments, where agents operate based on high-level semantic
goals without relying on detailed instructional guidance as in VLN. UAV-ON
comprises 14 high-fidelity Unreal Engine environments with diverse semantic
regions and complex spatial layouts, covering urban, natural, and mixed-use
settings. It defines 1270 annotated target objects, each characterized by an
instance-level instruction that encodes category, physical footprint, and
visual descriptors, allowing grounded reasoning. These instructions serve as
semantic goals, introducing realistic ambiguity and complex reasoning
challenges for aerial agents. To evaluate the benchmark, we implement several
baseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that
integrates instruction semantics with egocentric observations for long-horizon,
goal-directed exploration. Empirical results show that all baselines struggle
in this setting, highlighting the compounded challenges of aerial navigation
and semantic goal grounding. UAV-ON aims to advance research on scalable UAV
autonomy driven by semantic goal descriptions in complex real-world
environments.

</details>


### [139] [TopoDiffuser: A Diffusion-Based Multimodal Trajectory Prediction Model with Topometric Maps](https://arxiv.org/abs/2508.00303)
*Zehui Xu,Junhui Wang,Yongliang Shi,Chao Gao,Guyue Zhou*

Main category: cs.RO

TL;DR: TopoDiffuser是一种基于扩散模型的多模态轨迹预测框架，利用拓扑地图生成准确、多样且符合道路规则的未来运动预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法在轨迹预测中难以同时保证准确性和道路几何一致性，TopoDiffuser旨在通过结合拓扑地图解决这一问题。

Method: 通过将拓扑地图的结构信息嵌入到条件扩散模型的去噪过程中，生成符合道路几何的轨迹。多模态编码器融合LiDAR观测、历史运动和路径信息。

Result: 在KITTI基准测试中，TopoDiffuser优于现有方法，并保持强几何一致性。消融实验验证了各输入模态和去噪步骤的贡献。

Conclusion: TopoDiffuser通过结合拓扑地图，实现了高精度和多样化的轨迹预测，为未来研究提供了开源代码支持。

Abstract: This paper introduces TopoDiffuser, a diffusion-based framework for
multimodal trajectory prediction that incorporates topometric maps to generate
accurate, diverse, and road-compliant future motion forecasts. By embedding
structural cues from topometric maps into the denoising process of a
conditional diffusion model, the proposed approach enables trajectory
generation that naturally adheres to road geometry without relying on explicit
constraints. A multimodal conditioning encoder fuses LiDAR observations,
historical motion, and route information into a unified bird's-eye-view (BEV)
representation. Extensive experiments on the KITTI benchmark demonstrate that
TopoDiffuser outperforms state-of-the-art methods, while maintaining strong
geometric consistency. Ablation studies further validate the contribution of
each input modality, as well as the impact of denoising steps and the number of
trajectory samples. To support future research, we publicly release our code at
https://github.com/EI-Nav/TopoDiffuser.

</details>


### [140] [Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging](https://arxiv.org/abs/2508.00354)
*Tianshuang Qiu,Zehan Ma,Karim El-Refai,Hiya Shah,Chung Min Kim,Justin Kerr,Ken Goldberg*

Main category: cs.RO

TL;DR: Omni-Scan是一种利用双手机器人抓取和旋转物体以生成高质量3D高斯溅射模型的流程，支持360度视角建模，并在零件缺陷检测中达到83%的平均准确率。


<details>
  <summary>Details</summary>
Motivation: 传统3D物体扫描方法受限于设备和工作空间，需要更灵活的解决方案来生成高质量3D模型。

Method: 使用双手机器人抓取和旋转物体，结合DepthAnything、Segment Anything和RAFT光流模型去除抓取器和背景干扰，改进3DGS训练流程以支持遮挡数据。

Result: 在12种工业和家用物体的缺陷检测中，平均准确率达到83%。

Conclusion: Omni-Scan提供了一种高效且灵活的3D建模方法，适用于多种应用场景。

Abstract: 3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view
images. Such "digital twins" are useful for simulations, virtual reality,
marketing, robot policy fine-tuning, and part inspection. 3D object scanning
usually requires multi-camera arrays, precise laser scanners, or robot
wrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan,
a pipeline for producing high-quality 3D Gaussian Splat models using a
bi-manual robot that grasps an object with one gripper and rotates the object
with respect to a stationary camera. The object is then re-grasped by a second
gripper to expose surfaces that were occluded by the first gripper. We present
the Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as
RAFT optical flow models to identify and isolate objects held by a robot
gripper while removing the gripper and the background. We then modify the 3DGS
training pipeline to support concatenated datasets with gripper occlusion,
producing an omni-directional (360 degree view) model of the object. We apply
Omni-Scan to part defect inspection, finding that it can identify visual or
geometric defects in 12 different industrial and household objects with an
average accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be
found at https://berkeleyautomation.github.io/omni-scan/

</details>


### [141] [TOP: Time Optimization Policy for Stable and Accurate Standing Manipulation with Humanoid Robots](https://arxiv.org/abs/2508.00355)
*Zhenghan Chen,Haocheng Xu,Haodong Zhang,Liang Zhang,He Li,Dongqi Wang,Jiyu Yu,Yifei Yang,Zhongxiang Zhou,Rong Xiong*

Main category: cs.RO

TL;DR: 提出了一种新颖的时间优化策略（TOP），用于训练人形机器人的站立操作控制模型，同时确保平衡、精确和时间效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时满足高维上半身关节的精确控制和稳定性，尤其是在快速上半身运动时。

Method: 结合运动先验（VAE）、解耦控制（上半身PD控制器和下半身RL控制器）以及TOP方法，优化时间轨迹。

Result: 仿真和真实实验验证了该方法在稳定性和精确性上的优越性。

Conclusion: TOP方法有效解决了快速上半身运动导致的平衡问题，提升了人形机器人的操作能力。

Abstract: Humanoid robots have the potential capability to perform a diverse range of
manipulation tasks, but this is based on a robust and precise standing
controller. Existing methods are either ill-suited to precisely control
high-dimensional upper-body joints, or difficult to ensure both robustness and
accuracy, especially when upper-body motions are fast. This paper proposes a
novel time optimization policy (TOP), to train a standing manipulation control
model that ensures balance, precision, and time efficiency simultaneously, with
the idea of adjusting the time trajectory of upper-body motions but not only
strengthening the disturbance resistance of the lower-body. Our approach
consists of three parts. Firstly, we utilize motion prior to represent
upper-body motions to enhance the coordination ability between the upper and
lower-body by training a variational autoencoder (VAE). Then we decouple the
whole-body control into an upper-body PD controller for precision and a
lower-body RL controller to enhance robust stability. Finally, we train TOP
method in conjunction with the decoupled controller and VAE to reduce the
balance burden resulting from fast upper-body motions that would destabilize
the robot and exceed the capabilities of the lower-body RL policy. The
effectiveness of the proposed approach is evaluated via both simulation and
real world experiments, which demonstrate the superiority on standing
manipulation tasks stably and accurately. The project page can be found at
https://anonymous.4open.science/w/top-258F/.

</details>


### [142] [A Whole-Body Motion Imitation Framework from Human Data for Full-Size Humanoid Robot](https://arxiv.org/abs/2508.00362)
*Zhenghan Chen,Haodong Zhang,Dongqi Wang,Jiyu Yu,Haocheng Xu,Yue Wang,Rong Xiong*

Main category: cs.RO

TL;DR: 提出了一种新颖的全身运动模仿框架，用于全尺寸人形机器人，通过接触感知的全身运动重定向和非线性质心模型预测控制器，实现了高精度运动模仿并保持平衡。


<details>
  <summary>Details</summary>
Motivation: 人形机器人模仿人类运动时，由于运动学和动力学的显著差异，难以在保持平衡的同时准确模仿运动。

Method: 采用接触感知的全身运动重定向提供初始参考轨迹，结合非线性质心模型预测控制器实时确保运动精度和平衡。

Result: 实验验证了该方法在仿真和真实人形机器人上均能高精度模仿多种人类运动，并具备适应性和平衡能力。

Conclusion: 所提出的框架有效解决了人形机器人运动模仿中的平衡和精度问题，验证了其实际应用的可行性。

Abstract: Motion imitation is a pivotal and effective approach for humanoid robots to
achieve a more diverse range of complex and expressive movements, making their
performances more human-like. However, the significant differences in
kinematics and dynamics between humanoid robots and humans present a major
challenge in accurately imitating motion while maintaining balance. In this
paper, we propose a novel whole-body motion imitation framework for a full-size
humanoid robot. The proposed method employs contact-aware whole-body motion
retargeting to mimic human motion and provide initial values for reference
trajectories, and the non-linear centroidal model predictive controller ensures
the motion accuracy while maintaining balance and overcoming external
disturbances in real time. The assistance of the whole-body controller allows
for more precise torque control. Experiments have been conducted to imitate a
variety of human motions both in simulation and in a real-world humanoid robot.
These experiments demonstrate the capability of performing with accuracy and
adaptability, which validates the effectiveness of our approach.

</details>


### [143] [On Learning Closed-Loop Probabilistic Multi-Agent Simulator](https://arxiv.org/abs/2508.00384)
*Juanwu Lu,Rohit Gupta,Ahmadreza Moradipari,Kyungtae Han,Ruqi Zhang,Ziran Wang*

Main category: cs.RO

TL;DR: NIVA是一个基于分层贝叶斯模型的概率框架，用于多智能体交通模拟，通过自回归采样实现闭环模拟，并在Waymo数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆的快速迭代，需要构建更真实、可扩展的多智能体交通模拟器以高效评估性能。

Method: NIVA采用分层贝叶斯模型，通过自回归采样从高斯分布的潜在混合中生成交互式模拟。

Result: 在Waymo Open Motion数据集上，NIVA表现优于现有方法，并能灵活控制意图和驾驶风格。

Conclusion: NIVA为多智能体模拟提供了一种统一的概率框架，结合了轨迹预测和闭环模拟的优势。

Abstract: The rapid iteration of autonomous vehicle (AV) deployments leads to
increasing needs for building realistic and scalable multi-agent traffic
simulators for efficient evaluation. Recent advances in this area focus on
closed-loop simulators that enable generating diverse and interactive
scenarios. This paper introduces Neural Interactive Agents (NIVA), a
probabilistic framework for multi-agent simulation driven by a hierarchical
Bayesian model that enables closed-loop, observation-conditioned simulation
through autoregressive sampling from a latent, finite mixture of Gaussian
distributions. We demonstrate how NIVA unifies preexisting sequence-to-sequence
trajectory prediction models and emerging closed-loop simulation models trained
on Next-token Prediction (NTP) from a Bayesian inference perspective.
Experiments on the Waymo Open Motion Dataset demonstrate that NIVA attains
competitive performance compared to the existing method while providing
embellishing control over intentions and driving styles.

</details>


### [144] [SubCDM: Collective Decision-Making with a Swarm Subset](https://arxiv.org/abs/2508.00467)
*Samratul Fuady,Danesh Tarapore,Mohammad D. Soorati*

Main category: cs.RO

TL;DR: 提出了一种基于子集的集体决策方法（SubCDM），通过动态构建子集减少参与决策的机器人数量，提高资源效率。


<details>
  <summary>Details</summary>
Motivation: 现有集体决策方法需要所有机器人参与，资源消耗大且无法同时执行其他任务。

Method: 动态、去中心化地构建子集，根据共识难度自适应调整子集大小。

Result: 仿真实验表明，SubCDM在保持高准确性的同时显著减少参与决策的机器人数量。

Conclusion: SubCDM是一种资源高效的集体决策方法，适用于机器人群体。

Abstract: Collective decision-making is a key function of autonomous robot swarms,
enabling them to reach a consensus on actions based on environmental features.
Existing strategies require the participation of all robots in the
decision-making process, which is resource-intensive and prevents the swarm
from allocating the robots to any other tasks. We propose Subset-Based
Collective Decision-Making (SubCDM), which enables decisions using only a swarm
subset. The construction of the subset is dynamic and decentralized, relying
solely on local information. Our method allows the swarm to adaptively
determine the size of the subset for accurate decision-making, depending on the
difficulty of reaching a consensus. Simulation results using one hundred robots
show that our approach achieves accuracy comparable to using the entire swarm
while reducing the number of robots required to perform collective
decision-making, making it a resource-efficient solution for collective
decision-making in swarm robotics.

</details>


### [145] [HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning](https://arxiv.org/abs/2508.00491)
*Carlo Alessi,Federico Vasile,Federico Ceola,Giulia Pasquale,Nicolò Boccardo,Lorenzo Natale*

Main category: cs.RO

TL;DR: 论文提出了一种基于模仿学习的HannesImitationPolicy方法，用于控制假肢手Hannes，实现非结构化环境中的物体抓取，并展示了其优于基于分割的视觉伺服控制器的性能。


<details>
  <summary>Details</summary>
Motivation: 当前假肢手控制的研究集中在通过摄像头和其他传感器输入提高自主性，以减少用户的认知负担。模仿学习在机器人抓取和复杂操作任务中表现出潜力，但在假肢手控制中的应用尚未充分探索。填补这一空白可以提升假肢手的灵活性，使其在非结构化环境中通过演示学习任务。

Method: 提出了HannesImitationPolicy，一种基于模仿学习的方法，用于控制Hannes假肢手。同时创建了HannesImitationDataset，包含桌子、架子和人机交接场景的抓取演示数据。利用扩散策略训练模型，预测手腕方向和手部闭合动作。

Result: 实验表明，该方法能够成功抓取多种物体和条件下的目标，并且在非结构化场景中优于基于分割的视觉伺服控制器。

Conclusion: HannesImitationPolicy通过模仿学习有效提升了假肢手在非结构化环境中的抓取能力，为假肢控制提供了新的解决方案。

Abstract: Recent advancements in control of prosthetic hands have focused on increasing
autonomy through the use of cameras and other sensory inputs. These systems aim
to reduce the cognitive load on the user by automatically controlling certain
degrees of freedom. In robotics, imitation learning has emerged as a promising
approach for learning grasping and complex manipulation tasks while simplifying
data collection. Its application to the control of prosthetic hands remains,
however, largely unexplored. Bridging this gap could enhance dexterity
restoration and enable prosthetic devices to operate in more unconstrained
scenarios, where tasks are learned from demonstrations rather than relying on
manually annotated sequences. To this end, we present HannesImitationPolicy, an
imitation learning-based method to control the Hannes prosthetic hand, enabling
object grasping in unstructured environments. Moreover, we introduce the
HannesImitationDataset comprising grasping demonstrations in table, shelf, and
human-to-prosthesis handover scenarios. We leverage such data to train a single
diffusion policy and deploy it on the prosthetic hand to predict the wrist
orientation and hand closure for grasping. Experimental evaluation demonstrates
successful grasps across diverse objects and conditions. Finally, we show that
the policy outperforms a segmentation-based visual servo controller in
unstructured scenarios. Additional material is provided on our project page:
https://hsp-iit.github.io/HannesImitation

</details>


### [146] [OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on Planetary Rovers Using RGB, Depth, and Thermal Imagery](https://arxiv.org/abs/2508.00580)
*Raul Castilla-Arquillo,Carlos Perez-del-Pulgar,Levin Gerdes,Alfonso Garcia-Cerezo,Miguel A. Olivares-Mendez*

Main category: cs.RO

TL;DR: OmniUnet是一种基于Transformer的神经网络架构，用于RGB、深度和热成像（RGB-D-T）图像的语义分割，旨在提升火星探测中的机器人导航能力。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中，机器人导航需要多模态感知系统以确保安全性。火星探索中，热成像对评估地形安全尤为重要。

Method: 开发了OmniUnet网络架构，并利用3D打印定制多模态传感器外壳，在西班牙半沙漠地区收集数据集进行训练和评估。

Result: 模型在像素准确率达到80.37%，并在资源受限设备上实现673毫秒的平均预测时间，适合机器人部署。

Conclusion: OmniUnet在多模态地形感知中表现优异，公开的软件和数据集将支持未来行星机器人研究。

Abstract: Robot navigation in unstructured environments requires multimodal perception
systems that can support safe navigation. Multimodality enables the integration
of complementary information collected by different sensors. However, this
information must be processed by machine learning algorithms specifically
designed to leverage heterogeneous data. Furthermore, it is necessary to
identify which sensor modalities are most informative for navigation in the
target environment. In Martian exploration, thermal imagery has proven valuable
for assessing terrain safety due to differences in thermal behaviour between
soil types. This work presents OmniUnet, a transformer-based neural network
architecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)
imagery. A custom multimodal sensor housing was developed using 3D printing and
mounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a
multimodal dataset in the Bardenas semi-desert in northern Spain. This location
serves as a representative environment of the Martian surface, featuring
terrain types such as sand, bedrock, and compact soil. A subset of this dataset
was manually labeled to support supervised training of the network. The model
was evaluated both quantitatively and qualitatively, achieving a pixel accuracy
of 80.37% and demonstrating strong performance in segmenting complex
unstructured terrain. Inference tests yielded an average prediction time of 673
ms on a resource-constrained computer (Jetson Orin Nano), confirming its
suitability for on-robot deployment. The software implementation of the network
and the labeled dataset have been made publicly available to support future
research in multimodal terrain perception for planetary robotics.

</details>


### [147] [A control scheme for collaborative object transportation between a human and a quadruped robot using the MIGHTY suction cup](https://arxiv.org/abs/2508.00584)
*Konstantinos Plotas,Emmanouil Papadakis,Drosakis Drosakis,Panos Trahanias,Dimitrios Papageorgiou*

Main category: cs.RO

TL;DR: 提出了一种基于导纳控制的四足机器人与人类协作运输物体的控制方案，结合可变阻尼和屏障人工势能，以减少人类操作者的负担并确保物体稳定。


<details>
  <summary>Details</summary>
Motivation: 解决人机协作运输中人类操作者的控制性和负担问题，同时确保物体在运输过程中不会从吸盘上脱落。

Method: 采用导纳控制框架，引入可变阻尼项和基于屏障人工势能的额外控制信号。

Result: 实验验证了控制方案的被动性和有效性，使用Unitree Go1机器人和MIGHTY吸盘进行了性能测试。

Conclusion: 提出的控制方案在减少人类操作者负担的同时，确保了物体运输的稳定性和可控性。

Abstract: In this work, a control scheme for human-robot collaborative object
transportation is proposed, considering a quadruped robot equipped with the
MIGHTY suction cup that serves both as a gripper for holding the object and a
force/torque sensor. The proposed control scheme is based on the notion of
admittance control, and incorporates a variable damping term aiming towards
increasing the controllability of the human and, at the same time, decreasing
her/his effort. Furthermore, to ensure that the object is not detached from the
suction cup during the collaboration, an additional control signal is proposed,
which is based on a barrier artificial potential. The proposed control scheme
is proven to be passive and its performance is demonstrated through
experimental evaluations conducted using the Unitree Go1 robot equipped with
the MIGHTY suction cup.

</details>


### [148] [OpenScout v1.1 mobile robot: a case study on open hardware continuation](https://arxiv.org/abs/2508.00625)
*Bartosz Krawczyk,Ahmed Elbary,Robbie Cato,Jagdish Patil,Kaung Myat,Anyeh Ndi-Tah,Nivetha Sakthivel,Mark Crampton,Gautham Das,Charles Fox*

Main category: cs.RO

TL;DR: OpenScout v1.1是一款开源硬件移动机器人，升级了更简单、更便宜且更强大的计算硬件，支持ROS2接口和Gazebo模拟。


<details>
  <summary>Details</summary>
Motivation: 为研究和工业提供一款功能强大且成本效益高的开源硬件移动机器人。

Method: 通过简化硬件设计、升级计算能力，并集成ROS2接口和Gazebo模拟。

Result: 成功开发出OpenScout v1.1，具备更高的性能和更低的成本。

Conclusion: OpenScout v1.1为开源硬件机器人提供了实用的解决方案，适合研究和工业应用。

Abstract: OpenScout is an Open Source Hardware (OSH) mobile robot for research and
industry. It is extended to v1.1 which includes simplified, cheaper and more
powerful onboard compute hardware; a simulated ROS2 interface; and a Gazebo
simulation. Changes, their rationale, project methodology, and results are
reported as an OSH case study.

</details>


### [149] [Towards Data-Driven Adaptive Exoskeleton Assistance for Post-stroke Gait](https://arxiv.org/abs/2508.00691)
*Fabian C. Weigend,Dabin K. Choe,Santiago Canete,Conor J. Walsh*

Main category: cs.RO

TL;DR: 数据驱动方法为外骨骼控制提供新途径，首次尝试在卒中后行走中实现自适应踝关节辅助。


<details>
  <summary>Details</summary>
Motivation: 解决数据驱动方法在卒中后步态缺陷人群中的应用挑战，如高异质性和数据缺乏。

Method: 使用多任务时间卷积网络（TCN）结合IMU数据，预训练健康步态数据，并在卒中后行走中验证。

Result: 模型在四名卒中参与者中表现良好（R²=0.74±0.13），实时原型验证可行性。

Conclusion: 数据驱动方法有望实现卒中后外骨骼的安全有效控制，但仍需进一步研究。

Abstract: Recent work has shown that exoskeletons controlled through data-driven
methods can dynamically adapt assistance to various tasks for healthy young
adults. However, applying these methods to populations with neuromotor gait
deficits, such as post-stroke hemiparesis, is challenging. This is due not only
to high population heterogeneity and gait variability but also to a lack of
post-stroke gait datasets to train accurate models. Despite these challenges,
data-driven methods offer a promising avenue for control, potentially allowing
exoskeletons to function safely and effectively in unstructured community
settings. This work presents a first step towards enabling adaptive
plantarflexion and dorsiflexion assistance from data-driven torque estimation
during post-stroke walking. We trained a multi-task Temporal Convolutional
Network (TCN) using collected data from four post-stroke participants walking
on a treadmill ($R^2$ of $0.74 \pm 0.13$). The model uses data from three
inertial measurement units (IMU) and was pretrained on healthy walking data
from 6 participants. We implemented a wearable prototype for our ankle torque
estimation approach for exoskeleton control and demonstrated the viability of
real-time sensing, estimation, and actuation with one post-stroke participant.

</details>


### [150] [On-Device Diffusion Transformer Policy for Efficient Robot Manipulation](https://arxiv.org/abs/2508.00697)
*Yiming Wu,Huan Wang,Zhenghao Chen,Jianxin Pang,Dong Xu*

Main category: cs.RO

TL;DR: LightDP框架通过网络压缩和减少采样步骤，优化了Diffusion Policies在移动设备上的实时部署。


<details>
  <summary>Details</summary>
Motivation: Diffusion Policies在资源受限的移动平台上因计算效率低和内存占用大而难以应用。

Method: 采用网络压缩和减少采样步骤的策略，结合统一的剪枝与再训练流程和一致性蒸馏技术。

Result: 在标准数据集上验证了LightDP的实时性和性能竞争力，接近现有最佳Diffusion Policies。

Conclusion: LightDP为资源受限环境中的Diffusion Policies实际部署提供了重要进展。

Abstract: Diffusion Policies have significantly advanced robotic manipulation tasks via
imitation learning, but their application on resource-constrained mobile
platforms remains challenging due to computational inefficiency and extensive
memory footprint. In this paper, we propose LightDP, a novel framework
specifically designed to accelerate Diffusion Policies for real-time deployment
on mobile devices. LightDP addresses the computational bottleneck through two
core strategies: network compression of the denoising modules and reduction of
the required sampling steps. We first conduct an extensive computational
analysis on existing Diffusion Policy architectures, identifying the denoising
network as the primary contributor to latency. To overcome performance
degradation typically associated with conventional pruning methods, we
introduce a unified pruning and retraining pipeline, optimizing the model's
post-pruning recoverability explicitly. Furthermore, we combine pruning
techniques with consistency distillation to effectively reduce sampling steps
while maintaining action prediction accuracy. Experimental evaluations on the
standard datasets, \ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that
LightDP achieves real-time action prediction on mobile devices with competitive
performance, marking an important step toward practical deployment of
diffusion-based policies in resource-limited environments. Extensive real-world
experiments also show the proposed LightDP can achieve performance comparable
to state-of-the-art Diffusion Policies.

</details>


### [151] [Video Generators are Robot Policies](https://arxiv.org/abs/2508.00795)
*Junbang Liang,Pavel Tokmakov,Ruoshi Liu,Sruthi Sudhakar,Paarth Shah,Rares Ambrus,Carl Vondrick*

Main category: cs.RO

TL;DR: 论文提出了一种名为Video Policy的模块化框架，通过视频生成作为机器人策略学习的代理，解决了感知和行为分布偏移的泛化问题以及人类示范数据规模的限制。


<details>
  <summary>Details</summary>
Motivation: 当前视觉运动策略在感知或行为分布偏移下的泛化能力有限，且性能受限于人类示范数据的规模。

Method: 提出Video Policy框架，结合视频和动作生成，可端到端训练。利用视频生成模型提取策略，减少示范数据需求。

Result: 方法在仿真和现实世界中表现出对未见物体、背景和任务的强泛化能力，且任务成功与生成视频密切相关。

Conclusion: 通过大规模视频生成模型，实现了比传统行为克隆更优的性能，为可扩展和数据高效的机器人策略学习铺平了道路。

Abstract: Despite tremendous progress in dexterous manipulation, current visuomotor
policies remain fundamentally limited by two challenges: they struggle to
generalize under perceptual or behavioral distribution shifts, and their
performance is constrained by the size of human demonstration data. In this
paper, we use video generation as a proxy for robot policy learning to address
both limitations simultaneously. We propose Video Policy, a modular framework
that combines video and action generation that can be trained end-to-end. Our
results demonstrate that learning to generate videos of robot behavior allows
for the extraction of policies with minimal demonstration data, significantly
improving robustness and sample efficiency. Our method shows strong
generalization to unseen objects, backgrounds, and tasks, both in simulation
and the real world. We further highlight that task success is closely tied to
the generated video, with action-free video data providing critical benefits
for generalizing to novel tasks. By leveraging large-scale video generative
models, we achieve superior performance compared to traditional behavior
cloning, paving the way for more scalable and data-efficient robot policy
learning.

</details>
