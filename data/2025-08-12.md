<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 220]
- [cs.CL](#cs.CL) [Total: 96]
- [cs.RO](#cs.RO) [Total: 60]
- [eess.IV](#eess.IV) [Total: 15]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Med-GRIM: Enhanced Zero-Shot Medical VQA using prompt-embedded Multimodal Graph RAG](https://arxiv.org/abs/2508.06496)
*Rakesh Raj Madavan,Akshat Kaimal,Hashim Faisal,Chandrakala S*

Main category: cs.CV

TL;DR: BIND和Med-GRIM通过密集编码和模块化工作流改进医疗VQA任务，结合图检索和提示工程，实现高效且精确的响应。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在复杂领域（如医疗VQA）中缺乏详细精度，需改进。

Method: BIND通过密集编码优化嵌入空间；Med-GRIM结合图检索、提示工程和小语言模型，动态注入知识。

Result: Med-GRIM以低计算成本实现高性能，并发布DermaGraph数据集支持零样本研究。

Conclusion: 该方法为医疗VQA提供高效解决方案，支持领域特定知识集成和可扩展研究。

Abstract: An ensemble of trained multimodal encoders and vision-language models (VLMs)
has become a standard approach for visual question answering (VQA) tasks.
However, such models often fail to produce responses with the detailed
precision necessary for complex, domain-specific applications such as medical
VQA. Our representation model, BIND: BLIVA Integrated with Dense Encoding,
extends prior multimodal work by refining the joint embedding space through
dense, query-token-based encodings inspired by contrastive pretraining
techniques. This refined encoder powers Med-GRIM, a model designed for medical
VQA tasks that leverages graph-based retrieval and prompt engineering to
integrate domain-specific knowledge. Rather than relying on compute-heavy
fine-tuning of vision and language models on specific datasets, Med-GRIM
applies a low-compute, modular workflow with small language models (SLMs) for
efficiency. Med-GRIM employs prompt-based retrieval to dynamically inject
relevant knowledge, ensuring both accuracy and robustness in its responses. By
assigning distinct roles to each agent within the VQA system, Med-GRIM achieves
large language model performance at a fraction of the computational cost.
Additionally, to support scalable research in zero-shot multimodal medical
applications, we introduce DermaGraph, a novel Graph-RAG dataset comprising
diverse dermatological conditions. This dataset facilitates both multimodal and
unimodal querying. The code and dataset are available at:
https://github.com/Rakesh-123-cryp/Med-GRIM.git

</details>


### [2] [DiTalker: A Unified DiT-based Framework for High-Quality and Speaking Styles Controllable Portrait Animation](https://arxiv.org/abs/2508.06511)
*He Feng,Yongjia Ma,Donglin Di,Lei Fan,Tonghua Su,Xiangqian Wu*

Main category: cs.CV

TL;DR: DiTalker是一种基于DiT的框架，用于可控说话风格的人像动画，通过分离音频和风格特征，优化唇同步和细节保留。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注唇同步或静态情感转换，忽略了动态风格（如头部运动），且计算开销大。

Method: 设计了风格-情感编码模块和音频-风格融合模块，采用并行交叉注意力层分离音频与风格，并优化约束条件。

Result: 实验表明DiTalker在唇同步和说话风格可控性上表现优越。

Conclusion: DiTalker提供了一种高效且可控的人像动画解决方案。

Abstract: Portrait animation aims to synthesize talking videos from a static reference
face, conditioned on audio and style frame cues (e.g., emotion and head poses),
while ensuring precise lip synchronization and faithful reproduction of
speaking styles. Existing diffusion-based portrait animation methods primarily
focus on lip synchronization or static emotion transformation, often
overlooking dynamic styles such as head movements. Moreover, most of these
methods rely on a dual U-Net architecture, which preserves identity consistency
but incurs additional computational overhead. To this end, we propose DiTalker,
a unified DiT-based framework for speaking style-controllable portrait
animation. We design a Style-Emotion Encoding Module that employs two separate
branches: a style branch extracting identity-specific style information (e.g.,
head poses and movements), and an emotion branch extracting identity-agnostic
emotion features. We further introduce an Audio-Style Fusion Module that
decouples audio and speaking styles via two parallel cross-attention layers,
using these features to guide the animation process. To enhance the quality of
results, we adopt and modify two optimization constraints: one to improve lip
synchronization and the other to preserve fine-grained identity and background
details. Extensive experiments demonstrate the superiority of DiTalker in terms
of lip synchronization and speaking style controllability. Project Page:
https://thenameishope.github.io/DiTalker/

</details>


### [3] [BigTokDetect: A Clinically-Informed Vision-Language Model Framework for Detecting Pro-Bigorexia Videos on TikTok](https://arxiv.org/abs/2508.06515)
*Minh Duc Chu,Kshitij Pawar,Zihao He,Roxanna Sharifi,Ross Sonnenblick,Magdalayna Curry,Laura D'Adamo,Lindsay Young,Stuart B Murray,Kristina Lerman*

Main category: cs.CV

TL;DR: 论文提出了BigTokDetect框架，用于检测TikTok上促进肌肉畸形行为的危害内容，通过多模态方法显著提升了检测效果。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上伪装成健身内容的肌肉畸形行为内容难以通过传统文本检测系统识别，对青少年男性造成严重影响。

Method: 开发了BigTokDetect框架，基于专家标注的多模态数据集BigTok（2200+视频），结合视觉语言模型进行多模态融合检测。

Result: 在多模态融合下，主类别分类准确率达82.9%，子类别达69%，视频特征贡献最大。

Conclusion: 该研究为心理健康领域的多模态危害内容检测提供了新基准和工具。

Abstract: Social media platforms increasingly struggle to detect harmful content that
promotes muscle dysmorphic behaviors, particularly pro-bigorexia content that
disproportionately affects adolescent males. Unlike traditional eating disorder
detection focused on the "thin ideal," pro-bigorexia material masquerades as
legitimate fitness content through complex multimodal combinations of visual
displays, coded language, and motivational messaging that evade text-based
detection systems. We address this challenge by developing BigTokDetect, a
clinically-informed detection framework for identifying pro-bigorexia content
on TikTok. We introduce BigTok, the first expert-annotated multimodal dataset
of over 2,200 TikTok videos labeled by clinical psychologists and psychiatrists
across five primary categories spanning body image, nutrition, exercise,
supplements, and masculinity. Through a comprehensive evaluation of
state-of-the-art vision language models, we achieve 0.829% accuracy on primary
category classification and 0.690% on subcategory detection via domain-specific
finetuning. Our ablation studies demonstrate that multimodal fusion improves
performance by 5-10% over text-only approaches, with video features providing
the most discriminative signals. These findings establish new benchmarks for
multimodal harmful content detection and provide both the computational tools
and methodological framework needed for scalable content moderation in
specialized mental health domains.

</details>


### [4] [Frequency Prior Guided Matching: A Data Augmentation Approach for Generalizable Semi-Supervised Polyp Segmentation](https://arxiv.org/abs/2508.06517)
*Haoran Xi,Chen Liu,Xiaolin Li*

Main category: cs.CV

TL;DR: FPGM是一种基于频率先验的半监督学习方法，通过利用息肉边缘的频率特征，提升跨域分割的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决息肉分割中标注数据不足和跨域性能下降的问题。

Method: 提出FPGM框架，学习域不变频率先验，并对未标注图像进行频谱扰动。

Result: 在六个数据集上表现最优，零样本泛化能力显著提升。

Conclusion: FPGM为临床部署提供了高效的息肉分割解决方案。

Abstract: Automated polyp segmentation is essential for early diagnosis of colorectal
cancer, yet developing robust models remains challenging due to limited
annotated data and significant performance degradation under domain shift.
Although semi-supervised learning (SSL) reduces annotation requirements,
existing methods rely on generic augmentations that ignore polyp-specific
structural properties, resulting in poor generalization to new imaging centers
and devices. To address this, we introduce Frequency Prior Guided Matching
(FPGM), a novel augmentation framework built on a key discovery: polyp edges
exhibit a remarkably consistent frequency signature across diverse datasets.
FPGM leverages this intrinsic regularity in a two-stage process. It first
learns a domain-invariant frequency prior from the edge regions of labeled
polyps. Then, it performs principled spectral perturbations on unlabeled
images, aligning their amplitude spectra with this learned prior while
preserving phase information to maintain structural integrity. This targeted
alignment normalizes domain-specific textural variations, thereby compelling
the model to learn the underlying, generalizable anatomical structure.
Validated on six public datasets, FPGM establishes a new state-of-the-art
against ten competing methods. It demonstrates exceptional zero-shot
generalization capabilities, achieving over 10% absolute gain in Dice score in
data-scarce scenarios. By significantly enhancing cross-domain robustness, FPGM
presents a powerful solution for clinically deployable polyp segmentation under
limited supervision.

</details>


### [5] [Large Language Models Facilitate Vision Reflection in Image Classification](https://arxiv.org/abs/2508.06525)
*Guoyuan An,JaeYoon Kim,SungEui Yoon*

Main category: cs.CV

TL;DR: 论文探讨了大型多模态模型（LMMs）中视觉反射的可解释性，发现通过提示LMM验证专用视觉模型的预测可提高识别准确率，并分析了其内部机制。


<details>
  <summary>Details</summary>
Motivation: 研究LMMs在视觉任务中的表现及其可解释性，探索如何通过视觉反射提升模型性能。

Method: 通过提示LMM验证视觉模型预测，分析视觉反射的内部行为，并测试训练无关的连接器在细粒度识别任务中的效果。

Result: 发现LMMs通过将视觉特征映射为文本概念进行推理，少量文本标记即可生成相似答案，训练无关连接器能提升细粒度识别性能。

Conclusion: 视觉反射是提升LMMs鲁棒性和可解释性的有效策略，为视觉语言模型提供了新见解。

Abstract: This paper presents several novel findings on the explainability of vision
reflection in large multimodal models (LMMs). First, we show that prompting an
LMM to verify the prediction of a specialized vision model can improve
recognition accuracy, even on benchmarks like ImageNet, despite prior evidence
that LMMs typically underperform dedicated vision encoders. Second, we analyze
the internal behavior of vision reflection and find that the vision-language
connector maps visual features into explicit textual concepts, allowing the
language model to reason about prediction plausibility using commonsense
knowledge. We further observe that replacing a large number of vision tokens
with only a few text tokens still enables LLaVA to generate similar answers,
suggesting that LMMs may rely primarily on a compact set of distilled textual
representations rather than raw vision features. Third, we show that a
training-free connector can enhance LMM performance in fine-grained recognition
tasks, without extensive feature-alignment training. Together, these findings
offer new insights into the explainability of vision-language models and
suggest that vision reflection is a promising strategy for achieving robust and
interpretable visual recognition.

</details>


### [6] [A Framework Combining 3D CNN and Transformer for Video-Based Behavior Recognition](https://arxiv.org/abs/2508.06528)
*Xiuliang Zhang,Tadiwa Elisha Nyamasvisva,Chuntao Liu*

Main category: cs.CV

TL;DR: 提出了一种结合3D CNN和Transformer的混合框架，用于视频行为识别，解决了传统方法在长程依赖和高计算成本上的问题。


<details>
  <summary>Details</summary>
Motivation: 视频行为识别在公共安全等领域很重要，但传统3D CNN难以建模长程依赖，而Transformer计算成本高。

Method: 结合3D CNN提取局部时空特征和Transformer捕捉长程依赖，通过融合机制整合两者。

Result: 在基准数据集上表现优于传统3D CNN和单独Transformer，识别精度更高且复杂度可控。

Conclusion: 混合框架为视频行为识别提供了高效且可扩展的解决方案。

Abstract: Video-based behavior recognition is essential in fields such as public
safety, intelligent surveillance, and human-computer interaction. Traditional
3D Convolutional Neural Network (3D CNN) effectively capture local
spatiotemporal features but struggle with modeling long-range dependencies.
Conversely, Transformers excel at learning global contextual information but
face challenges with high computational costs. To address these limitations, we
propose a hybrid framework combining 3D CNN and Transformer architectures. The
3D CNN module extracts low-level spatiotemporal features, while the Transformer
module captures long-range temporal dependencies, with a fusion mechanism
integrating both representations. Evaluated on benchmark datasets, the proposed
model outperforms traditional 3D CNN and standalone Transformers, achieving
higher recognition accuracy with manageable complexity. Ablation studies
further validate the complementary strengths of the two modules. This hybrid
framework offers an effective and scalable solution for video-based behavior
recognition.

</details>


### [7] [RMT-PPAD: Real-time Multi-task Learning for Panoptic Perception in Autonomous Driving](https://arxiv.org/abs/2508.06529)
*Jiayuan Wang,Q. M. Jonathan Wu,Katsuya Suto,Ning Zhang*

Main category: cs.CV

TL;DR: RMT-PPAD是一种基于Transformer的多任务实时模型，用于自动驾驶感知任务，包括目标检测、可行驶区域分割和车道线分割。通过轻量级模块和自适应解码器设计，解决了任务间负迁移问题，并在BDD100K数据集上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需要高精度和实时性的全景驾驶感知，现有方法在多任务联合处理时存在负迁移和手动设计任务特定结构的问题。

Method: 提出RMT-PPAD模型，采用轻量级门控适配器融合共享和任务特定特征，设计自适应分割解码器自动学习多尺度特征权重，并解决车道线分割标签不一致问题。

Result: 在BDD100K数据集上，目标检测mAP50为84.9%，召回率95.4%；可行驶区域分割mIoU为92.6%；车道线分割IoU为56.8%，准确率84.7%。推理速度达32.6 FPS。

Conclusion: RMT-PPAD在多任务感知中表现出色，解决了负迁移和标签不一致问题，实际场景中性能稳定，代码和模型已开源。

Abstract: Autonomous driving systems rely on panoptic driving perception that requires
both precision and real-time performance. In this work, we propose RMT-PPAD, a
real-time, transformer-based multi-task model that jointly performs object
detection, drivable area segmentation, and lane line segmentation. We introduce
a lightweight module, a gate control with an adapter to adaptively fuse shared
and task-specific features, effectively alleviating negative transfer between
tasks. Additionally, we design an adaptive segmentation decoder to learn the
weights over multi-scale features automatically during the training stage. This
avoids the manual design of task-specific structures for different segmentation
tasks. We also identify and resolve the inconsistency between training and
testing labels in lane line segmentation. This allows fairer evaluation.
Experiments on the BDD100K dataset demonstrate that RMT-PPAD achieves
state-of-the-art results with mAP50 of 84.9% and Recall of 95.4% for object
detection, mIoU of 92.6% for drivable area segmentation, and IoU of 56.8% and
accuracy of 84.7% for lane line segmentation. The inference speed reaches 32.6
FPS. Moreover, we introduce real-world scenarios to evaluate RMT-PPAD
performance in practice. The results show that RMT-PPAD consistently delivers
stable performance. The source codes and pre-trained models are released at
https://github.com/JiayuanWang-JW/RMT-PPAD.

</details>


### [8] [What Makes "Good" Distractors for Object Hallucination Evaluation in Large Vision-Language Models?](https://arxiv.org/abs/2508.06530)
*Ming-Kun Xie,Jia-Hao Xiao,Gang Niu,Lei Feng,Zhiqiang Kou,Min-Ling Zhang,Masashi Sugiyama*

Main category: cs.CV

TL;DR: 论文提出了HOPE基准，通过生成误导性干扰项更严格评估大型视觉语言模型（LVLMs）的幻觉问题，显著优于现有POPE基准。


<details>
  <summary>Details</summary>
Motivation: 现有POPE基准在评估LVLMs的幻觉问题时效果逐渐减弱，因其忽略了图像特定信息且干扰项仅限于负面对象类别。

Method: HOPE基准通过内容感知幻觉搜索（利用CLIP选择高预测似然负面对象）和描述性幻觉搜索（配对真实对象与虚假描述）生成误导性干扰项。

Result: HOPE导致多种LVLMs的精度下降9%至23%，显著暴露其幻觉漏洞。

Conclusion: HOPE为评估LVLMs的幻觉问题提供了更严格的基准，优于现有方法。

Abstract: Large Vision-Language Models (LVLMs), empowered by the success of Large
Language Models (LLMs), have achieved impressive performance across domains.
Despite the great advances in LVLMs, they still suffer from the unavailable
object hallucination issue, which tends to generate objects inconsistent with
the image content. The most commonly used Polling-based Object Probing
Evaluation (POPE) benchmark evaluates this issue by sampling negative
categories according to category-level statistics, \textit{e.g.}, category
frequencies and co-occurrence. However, with the continuous advancement of
LVLMs, the POPE benchmark has shown diminishing effectiveness in assessing
object hallucination, as it employs a simplistic sampling strategy that
overlooks image-specific information and restricts distractors to negative
object categories only. In this paper, we introduce the Hallucination
searching-based Object Probing Evaluation (HOPE) benchmark, aiming to generate
the most misleading distractors (\textit{i.e.}, non-existent objects or
incorrect image descriptions) that can trigger hallucination in LVLMs, which
serves as a means to more rigorously assess their immunity to hallucination. To
explore the image-specific information, the content-aware hallucination
searching leverages Contrastive Language-Image Pre-Training (CLIP) to
approximate the predictive behavior of LVLMs by selecting negative objects with
the highest predicted likelihood as distractors. To expand the scope of
hallucination assessment, the description-based hallucination searching
constructs highly misleading distractors by pairing true objects with false
descriptions. Experimental results show that HOPE leads to a precision drop of
at least 9\% and up to 23\% across various state-of-the-art LVLMs,
significantly outperforming POPE in exposing hallucination vulnerabilities. The
code is available at https://github.com/xiemk/HOPE.

</details>


### [9] [Benchmarking Deep Learning-Based Object Detection Models on Feature Deficient Astrophotography Imagery Dataset](https://arxiv.org/abs/2508.06537)
*Shantanusinh Parmar*

Main category: cs.CV

TL;DR: 论文分析了智能手机天文摄影数据集MobilTelesco，评估了目标检测模型在稀疏特征条件下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测数据集（如ImageNet、COCO）主要关注日常物体，缺乏非商业领域中的信号稀疏性，MobilTelesco数据集填补了这一空白。

Method: 使用MobilTelesco数据集对多种目标检测模型进行基准测试。

Result: 研究揭示了模型在特征不足条件下的挑战。

Conclusion: MobilTelesco数据集为稀疏特征条件下的目标检测研究提供了重要资源。

Abstract: Object detection models are typically trained on datasets like ImageNet,
COCO, and PASCAL VOC, which focus on everyday objects. However, these lack
signal sparsity found in non-commercial domains. MobilTelesco, a
smartphone-based astrophotography dataset, addresses this by providing sparse
night-sky images. We benchmark several detection models on it, highlighting
challenges under feature-deficient conditions.

</details>


### [10] [MILD: Multi-Layer Diffusion Strategy for Complex and Precise Multi-IP Aware Human Erasing](https://arxiv.org/abs/2508.06543)
*Jinghan Yu,Zhiyuan Ma,Yue Ma,Kaiqi Liu,Yuhan Wang,Jianjun Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为MILD的新方法，用于解决复杂多实例场景下的人像擦除问题，通过分层扩散和人类形态引导显著提升了效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂多实例场景（如人物遮挡、背景干扰）中表现不佳，主要由于数据集限制和缺乏空间解耦能力。

Method: 提出Multi-Layer Diffusion (MILD)策略，将生成过程分解为语义分离的路径，并引入人类形态引导和空间调制注意力。

Result: MILD在挑战性的人像擦除任务中优于现有方法。

Conclusion: MILD通过分层生成和形态引导，有效解决了复杂场景下的人像擦除问题。

Abstract: Recent years have witnessed the success of diffusion models in
image-customized tasks. Prior works have achieved notable progress on
human-oriented erasing using explicit mask guidance and semantic-aware
inpainting. However, they struggle under complex multi-IP scenarios involving
human-human occlusions, human-object entanglements, and background
interferences. These challenges are mainly due to: 1) Dataset limitations, as
existing datasets rarely cover dense occlusions, camouflaged backgrounds, and
diverse interactions; 2) Lack of spatial decoupling, where foreground instances
cannot be effectively disentangled, limiting clean background restoration. In
this work, we introduce a high-quality multi-IP human erasing dataset with
diverse pose variations and complex backgrounds. We then propose Multi-Layer
Diffusion (MILD), a novel strategy that decomposes generation into semantically
separated pathways for each instance and the background. To enhance
human-centric understanding, we introduce Human Morphology Guidance,
integrating pose, parsing, and spatial relations. We further present
Spatially-Modulated Attention to better guide attention flow. Extensive
experiments show that MILD outperforms state-of-the-art methods on challenging
human erasing benchmarks.

</details>


### [11] [Statistical Confidence Rescoring for Robust 3D Scene Graph Generation from Multi-View Images](https://arxiv.org/abs/2508.06546)
*Qi Xun Yeo,Yanyan Li,Gim Hee Lee*

Main category: cs.CV

TL;DR: 论文提出了一种仅利用多视角RGB图像进行3D语义场景图估计的方法，通过语义掩码和邻域信息增强特征，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在缺乏3D标注的情况下，探索如何仅利用多视角RGB图像完成3D语义场景图估计任务。

Method: 利用语义掩码过滤背景噪声，设计新方法结合邻域节点信息，并利用统计先验优化节点和边预测。

Result: 实验表明，该方法在仅使用多视角图像作为输入时优于现有方法。

Conclusion: 通过语义和空间信息增强特征，结合邻域关系和统计先验，实现了更准确的场景图估计。

Abstract: Modern 3D semantic scene graph estimation methods utilize ground truth 3D
annotations to accurately predict target objects, predicates, and
relationships. In the absence of given 3D ground truth representations, we
explore leveraging only multi-view RGB images to tackle this task. To attain
robust features for accurate scene graph estimation, we must overcome the noisy
reconstructed pseudo point-based geometry from predicted depth maps and reduce
the amount of background noise present in multi-view image features. The key is
to enrich node and edge features with accurate semantic and spatial information
and through neighboring relations. We obtain semantic masks to guide feature
aggregation to filter background features and design a novel method to
incorporate neighboring node information to aid robustness of our scene graph
estimates. Furthermore, we leverage on explicit statistical priors calculated
from the training summary statistics to refine node and edge predictions based
on their one-hop neighborhood. Our experiments show that our method outperforms
current methods purely using multi-view images as the initial input. Our
project page is available at https://qixun1.github.io/projects/SCRSSG.

</details>


### [12] [Slice or the Whole Pie? Utility Control for AI Models](https://arxiv.org/abs/2508.06551)
*Ye Tao*

Main category: cs.CV

TL;DR: NNObfuscator是一种新型实用控制机制，允许AI模型根据预设条件动态调整性能，避免为不同需求训练多个模型。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法中需要为不同用户需求训练和维护多个模型的高成本和低效率问题。

Method: 提出NNObfuscator机制，通过动态调整模型性能实现单一模型适应多种需求，支持分层访问控制。

Result: 实验验证了NNObfuscator在图像分类、语义分割和文本生成图像等任务中的有效性，单一模型可适应广泛需求。

Conclusion: NNObfuscator提高了资源分配效率，减少了不必要的计算，支持可持续的AI部署商业模式。

Abstract: Training deep neural networks (DNNs) has become an increasingly
resource-intensive task, requiring large volumes of labeled data, substantial
computational power, and considerable fine-tuning efforts to achieve optimal
performance across diverse use cases. Although pre-trained models offer a
useful starting point, adapting them to meet specific user needs often demands
extensive customization, and infrastructure overhead. This challenge grows when
a single model must support diverse appli-cations with differing requirements
for performance. Traditional solutions often involve training multiple model
versions to meet varying requirements, which can be inefficient and difficult
to maintain. In order to overcome this challenge, we propose NNObfuscator, a
novel utility control mechanism that enables AI models to dynamically modify
their performance according to predefined conditions. It is different from
traditional methods that need separate models for each user. Instead,
NNObfuscator allows a single model to be adapted in real time, giving you
controlled access to multiple levels of performance. This mechanism enables
model owners set up tiered access, ensuring that free-tier users receive a
baseline level of performance while premium users benefit from enhanced
capabilities. The approach improves resource allocation, reduces unnecessary
computation, and supports sustainable business models in AI deployment. To
validate our approach, we conducted experiments on multiple tasks, including
image classification, semantic segmentation, and text to image generation,
using well-established models such as ResNet, DeepLab, VGG16, FCN and Stable
Diffusion. Experimental results show that NNObfuscator successfully makes model
more adaptable, so that a single trained model can handle a broad range of
tasks without requiring a lot of changes.

</details>


### [13] [Age-Diverse Deepfake Dataset: Bridging the Age Gap in Deepfake Detection](https://arxiv.org/abs/2508.06552)
*Unisha Joshi*

Main category: cs.CV

TL;DR: 本文提出了一种解决深度伪造数据集中年龄偏见的方法，通过构建一个年龄多样化的数据集，并验证其在多种检测模型中的公平性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造检测中存在年龄偏见问题，现有数据集缺乏年龄多样性，影响了检测模型的公平性。

Method: 通过整合Celeb-DF、FaceForensics++和UTKFace数据集，并生成合成数据填补年龄分布空白，构建了一个年龄多样化的数据集。使用XceptionNet、EfficientNet和LipForensics三种模型进行评估。

Result: 实验结果显示，基于年龄多样化数据集训练的模型在年龄组间表现更公平，整体准确率和跨数据集泛化能力均有提升。

Conclusion: 本研究提供了一个可复现、注重公平性的深度伪造数据集和模型流程，为未来公平检测研究奠定了基础。

Abstract: The challenges associated with deepfake detection are increasing
significantly with the latest advancements in technology and the growing
popularity of deepfake videos and images. Despite the presence of numerous
detection models, demographic bias in the deepfake dataset remains largely
unaddressed. This paper focuses on the mitigation of age-specific bias in the
deepfake dataset by introducing an age-diverse deepfake dataset that will
improve fairness across age groups. The dataset is constructed through a
modular pipeline incorporating the existing deepfake datasets Celeb-DF,
FaceForensics++, and UTKFace datasets, and the creation of synthetic data to
fill the age distribution gaps. The effectiveness and generalizability of this
dataset are evaluated using three deepfake detection models: XceptionNet,
EfficientNet, and LipForensics. Evaluation metrics, including AUC, pAUC, and
EER, revealed that models trained on the age-diverse dataset demonstrated
fairer performance across age groups, improved overall accuracy, and higher
generalization across datasets. This study contributes a reproducible,
fairness-aware deepfake dataset and model pipeline that can serve as a
foundation for future research in fairer deepfake detection. The complete
dataset and implementation code are available at
https://github.com/unishajoshi/age-diverse-deepfake-detection.

</details>


### [14] [Static and Plugged: Make Embodied Evaluation Simple](https://arxiv.org/abs/2508.06553)
*Jiahao Xiao,Jianbo Zhang,BoWen Yan,Shengyu Guo,Tongrui Ye,Kaiwei Zhang,Zicheng Zhang,Xiaohong Liu,Zhengxue Cheng,Lei Fan,Chuyi Li,Guangtao Zhai*

Main category: cs.CV

TL;DR: StaticEmbodiedBench是一个基于静态场景表示的评估基准，支持42种场景和8个核心维度，为具身智能提供统一、可扩展的评估。


<details>
  <summary>Details</summary>
Motivation: 当前具身智能的评估方法成本高、分散且难以扩展，需要一种更高效的解决方案。

Method: 提出StaticEmbodiedBench，通过静态场景表示实现统一评估，并评估了19个VLM和11个VLA模型。

Result: 建立了首个统一的静态具身智能排行榜，并发布了200个样本以推动研究。

Conclusion: StaticEmbodiedBench为具身智能的评估提供了高效、可扩展的解决方案。

Abstract: Embodied intelligence is advancing rapidly, driving the need for efficient
evaluation. Current benchmarks typically rely on interactive simulated
environments or real-world setups, which are costly, fragmented, and hard to
scale. To address this, we introduce StaticEmbodiedBench, a plug-and-play
benchmark that enables unified evaluation using static scene representations.
Covering 42 diverse scenarios and 8 core dimensions, it supports scalable and
comprehensive assessment through a simple interface. Furthermore, we evaluate
19 Vision-Language Models (VLMs) and 11 Vision-Language-Action models (VLAs),
establishing the first unified static leaderboard for Embodied intelligence.
Moreover, we release a subset of 200 samples from our benchmark to accelerate
the development of embodied intelligence.

</details>


### [15] [StyleTailor: Towards Personalized Fashion Styling via Hierarchical Negative Feedback](https://arxiv.org/abs/2508.06555)
*Hongbo Ma,Fei Shen,Hongbin Xu,Xiaoce Wang,Gang Xu,Jinkai Zheng,Liangqiong Qu,Ming Li*

Main category: cs.CV

TL;DR: StyleTailor是一个协作代理框架，整合个性化服装设计、购物推荐、虚拟试穿和系统评估，通过多级负面反馈实现精准用户对齐。


<details>
  <summary>Details</summary>
Motivation: 个性化时尚造型解决方案尚未充分探索，具有提升购物体验的潜力。

Method: 采用迭代视觉细化范式，由多级负面反馈驱动，包含Designer和Consultant两个核心代理，通过分层视觉语言模型反馈逐步优化。

Result: 在风格一致性、视觉质量、面部相似性和艺术评价等方面表现优异，超越无负面反馈的基线方法。

Conclusion: StyleTailor为智能时尚系统设立了新标杆，展示了其在个性化设计和推荐中的卓越性能。

Abstract: The advancement of intelligent agents has revolutionized problem-solving
across diverse domains, yet solutions for personalized fashion styling remain
underexplored, which holds immense promise for promoting shopping experiences.
In this work, we present StyleTailor, the first collaborative agent framework
that seamlessly unifies personalized apparel design, shopping recommendation,
virtual try-on, and systematic evaluation into a cohesive workflow. To this
end, StyleTailor pioneers an iterative visual refinement paradigm driven by
multi-level negative feedback, enabling adaptive and precise user alignment.
Specifically, our framework features two core agents, i.e., Designer for
personalized garment selection and Consultant for virtual try-on, whose outputs
are progressively refined via hierarchical vision-language model feedback
spanning individual items, complete outfits, and try-on efficacy.
Counterexamples are aggregated into negative prompts, forming a closed-loop
mechanism that enhances recommendation quality.To assess the performance, we
introduce a comprehensive evaluation suite encompassing style consistency,
visual quality, face similarity, and artistic appraisal. Extensive experiments
demonstrate StyleTailor's superior performance in delivering personalized
designs and recommendations, outperforming strong baselines without negative
feedback and establishing a new benchmark for intelligent fashion systems.

</details>


### [16] [From Label Error Detection to Correction: A Modular Framework and Benchmark for Object Detection Datasets](https://arxiv.org/abs/2508.06556)
*Sarina Penquitt,Jonathan Klees,Rinor Cakaj,Daniel Kondermann,Matthias Rottmann,Lars Schmarje*

Main category: cs.CV

TL;DR: 论文提出了一种半自动化的标签错误校正框架REC✓D，通过结合现有检测器和众包微任务，显著提高了标签质量，并在KITTI数据集中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 目标检测数据集中的标签错误（如缺失标签、分类错误或定位不准确）会严重影响训练和评估结果，但目前缺乏系统性的大规模校正方法。

Method: REC✓D框架结合现有错误检测器和众包微任务，通过多标注者独立验证候选边界框，并聚合响应以估计模糊性并提升标签质量。

Result: 在KITTI数据集的“行人”类中，REC✓D发现原始标注中至少24%的错误，并生成高质量校正标注。当前最佳方法仍可能遗漏66%的真实错误。

Conclusion: REC✓D展示了标签错误校正的潜力，但现有方法仍有不足，需进一步研究。发布的基准数据集将推动相关研究。

Abstract: Object detection has advanced rapidly in recent years, driven by increasingly
large and diverse datasets. However, label errors, defined as missing labels,
incorrect classification or inaccurate localization, often compromise the
quality of these datasets. This can have a significant impact on the outcomes
of training and benchmark evaluations. Although several methods now exist for
detecting label errors in object detection datasets, they are typically
validated only on synthetic benchmarks or limited manual inspection. How to
correct such errors systemically and at scale therefore remains an open
problem. We introduce a semi-automated framework for label-error correction
called REC$\checkmark$D (Rechecked). Building on existing detectors, the
framework pairs their error proposals with lightweight, crowd-sourced
microtasks. These tasks enable multiple annotators to independently verify each
candidate bounding box, and their responses are aggregated to estimate
ambiguity and improve label quality. To demonstrate the effectiveness of
REC$\checkmark$D, we apply it to the class pedestrian in the KITTI dataset. Our
crowdsourced review yields high-quality corrected annotations, which indicate a
rate of at least 24% of missing and inaccurate annotations in original
annotations. This validated set will be released as a new real-world benchmark
for label error detection and correction. We show that current label error
detection methods, when combined with our correction framework, can recover
hundreds of errors in the time it would take a human to annotate bounding boxes
from scratch. However, even the best methods still miss up to 66% of the true
errors and with low quality labels introduce more errors than they find. This
highlights the urgent need for further research, now enabled by our released
benchmark.

</details>


### [17] [On the effectiveness of multimodal privileged knowledge distillation in two vision transformer based diagnostic applications](https://arxiv.org/abs/2508.06558)
*Simon Baur,Alexandra Benova,Emilio Dolgener Cantú,Jackie Ma*

Main category: cs.CV

TL;DR: 提出了一种多模态特权知识蒸馏（MMPKD）方法，利用训练时额外的模态数据指导单模态视觉模型，提升注意力图的零样本能力，但效果不跨域。


<details>
  <summary>Details</summary>
Motivation: 临床实践中部署深度学习模型常需多模态数据，但推理时并非所有模态都可用，因此需要一种方法利用训练时的额外模态提升单模态模型的性能。

Method: 提出MMPKD策略，使用文本和表格元数据作为教师模型（分别针对胸部X光和乳腺X光），将知识蒸馏到视觉Transformer学生模型中。

Result: MMPKD能提升注意力图的零样本定位能力，但效果不跨域，与先前研究结论相反。

Conclusion: MMPKD在特定领域内有效，但跨域泛化能力有限，需进一步研究。

Abstract: Deploying deep learning models in clinical practice often requires leveraging
multiple data modalities, such as images, text, and structured data, to achieve
robust and trustworthy decisions. However, not all modalities are always
available at inference time. In this work, we propose multimodal privileged
knowledge distillation (MMPKD), a training strategy that utilizes additional
modalities available solely during training to guide a unimodal vision model.
Specifically, we used a text-based teacher model for chest radiographs
(MIMIC-CXR) and a tabular metadata-based teacher model for mammography
(CBIS-DDSM) to distill knowledge into a vision transformer student model. We
show that MMPKD can improve the resulting attention maps' zero-shot
capabilities of localizing ROI in input images, while this effect does not
generalize across domains, as contrarily suggested by prior research.

</details>


### [18] [Grounding Emotion Recognition with Visual Prototypes: VEGA -- Revisiting CLIP in MERC](https://arxiv.org/abs/2508.06564)
*Guanyu Hu,Dimitrios Kollias,Xinyu Yang*

Main category: cs.CV

TL;DR: 论文提出了一种名为VEGA的视觉情感引导锚定机制，通过引入类级视觉语义改进多模态情感识别，结合CLIP的图像编码器和认知理论，在IEMOCAP和MELD数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态情感识别任务因文本、声学和视觉信号的复杂交互而具有挑战性，现有模型缺乏心理学有意义的先验指导多模态对齐。

Method: 提出VEGA机制，利用CLIP的图像编码器构建情感特定的视觉锚点，结合随机锚点采样策略和双分支自蒸馏架构。

Result: 在IEMOCAP和MELD数据集上实现了最先进的性能。

Conclusion: VEGA机制通过心理学对齐的表示空间和多模态特征融合，显著提升了多模态情感识别的效果。

Abstract: Multimodal Emotion Recognition in Conversations remains a challenging task
due to the complex interplay of textual, acoustic and visual signals. While
recent models have improved performance via advanced fusion strategies, they
often lack psychologically meaningful priors to guide multimodal alignment. In
this paper, we revisit the use of CLIP and propose a novel Visual Emotion
Guided Anchoring (VEGA) mechanism that introduces class-level visual semantics
into the fusion and classification process. Distinct from prior work that
primarily utilizes CLIP's textual encoder, our approach leverages its image
encoder to construct emotion-specific visual anchors based on facial exemplars.
These anchors guide unimodal and multimodal features toward a perceptually
grounded and psychologically aligned representation space, drawing inspiration
from cognitive theories (prototypical emotion categories and multisensory
integration). A stochastic anchor sampling strategy further enhances robustness
by balancing semantic stability and intra-class diversity. Integrated into a
dual-branch architecture with self-distillation, our VEGA-augmented model
achieves sota performance on IEMOCAP and MELD. Code is available at:
https://github.com/dkollias/VEGA.

</details>


### [19] [Bridging Brain Connectomes and Clinical Reports for Early Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.06565)
*Jing Zhang,Xiaowei Yu,Minheng Chen,Lu Zhang,Tong Chen,Yan Zhuang,Chao Cao,Yanjun Lyu,Li Su,Tianming Liu,Dajiang Zhu*

Main category: cs.CV

TL;DR: 提出一种新框架，将脑连接组与临床报告在共享跨模态潜在空间中对齐，以提升表征学习，应用于轻度认知障碍（MCI）研究。


<details>
  <summary>Details</summary>
Motivation: 结合脑成像数据与临床报告的多模态信息，以更有效、及时地进行临床诊断。

Method: 将脑网络子网络作为成像数据的标记，与临床报告中的词语标记对齐，构建共享潜在空间。

Result: 在ADNI数据集上实现最优预测性能，并识别出有临床意义的连接组-文本对。

Conclusion: 该方法为阿尔茨海默病的早期机制提供新见解，支持开发临床有用的多模态生物标志物。

Abstract: Integrating brain imaging data with clinical reports offers a valuable
opportunity to leverage complementary multimodal information for more effective
and timely diagnosis in practical clinical settings. This approach has gained
significant attention in brain disorder research, yet a key challenge remains:
how to effectively link objective imaging data with subjective text-based
reports, such as doctors' notes. In this work, we propose a novel framework
that aligns brain connectomes with clinical reports in a shared cross-modal
latent space at both the subject and connectome levels, thereby enhancing
representation learning. The key innovation of our approach is that we treat
brain subnetworks as tokens of imaging data, rather than raw image patches, to
align with word tokens in clinical reports. This enables a more efficient
identification of system-level associations between neuroimaging findings and
clinical observations, which is critical since brain disorders often manifest
as network-level abnormalities rather than isolated regional alterations. We
applied our method to mild cognitive impairment (MCI) using the ADNI dataset.
Our approach not only achieves state-of-the-art predictive performance but also
identifies clinically meaningful connectome-text pairs, offering new insights
into the early mechanisms of Alzheimer's disease and supporting the development
of clinically useful multimodal biomarkers.

</details>


### [20] [Surformer v1: Transformer-Based Surface Classification Using Tactile and Vision Features](https://arxiv.org/abs/2508.06566)
*Manish Kansana,Elias Hossain,Shahram Rahimi,Noorbakhsh Amiri Golilarz*

Main category: cs.CV

TL;DR: Surformer v1是一种基于Transformer的架构，结合触觉和视觉输入进行表面分类，在准确性和计算效率之间取得了良好平衡。


<details>
  <summary>Details</summary>
Motivation: 表面材料识别是机器人感知和物理交互的关键，尤其是结合触觉和视觉输入时。

Method: 提出Surformer v1，结合结构化触觉特征和PCA降维的视觉嵌入，使用模态特定编码器和跨模态注意力层。

Result: Surformer v1在触觉分类中表现最佳，多模态融合后达到99.4%准确率，推理时间0.77毫秒。

Conclusion: Surformer v1在准确性、效率和计算成本之间提供了优越的平衡，适用于实时应用。

Abstract: Surface material recognition is a key component in robotic perception and
physical interaction, particularly when leveraging both tactile and visual
sensory inputs. In this work, we propose Surformer v1, a transformer-based
architecture designed for surface classification using structured tactile
features and PCA-reduced visual embeddings extracted via ResNet-50. The model
integrates modality-specific encoders with cross-modal attention layers,
enabling rich interactions between vision and touch. Currently,
state-of-the-art deep learning models for vision tasks have achieved remarkable
performance. With this in mind, our first set of experiments focused
exclusively on tactile-only surface classification. Using feature engineering,
we trained and evaluated multiple machine learning models, assessing their
accuracy and inference time. We then implemented an encoder-only Transformer
model tailored for tactile features. This model not only achieved the highest
accuracy but also demonstrated significantly faster inference time compared to
other evaluated models, highlighting its potential for real-time applications.
To extend this investigation, we introduced a multimodal fusion setup by
combining vision and tactile inputs. We trained both Surformer v1 (using
structured features) and Multimodal CNN (using raw images) to examine the
impact of feature-based versus image-based multimodal learning on
classification accuracy and computational efficiency. The results showed that
Surformer v1 achieved 99.4% accuracy with an inference time of 0.77 ms, while
the Multimodal CNN achieved slightly higher accuracy but required significantly
more inference time. These findings suggest Surformer v1 offers a compelling
balance between accuracy, efficiency, and computational cost for surface
material recognition.

</details>


### [21] [ImpliHateVid: A Benchmark Dataset and Two-stage Contrastive Learning Framework for Implicit Hate Speech Detection in Videos](https://arxiv.org/abs/2508.06570)
*Mohammad Zia Ur Rehman,Anukriti Bhatnagar,Omkar Kabde,Shubhi Bansal,Nagendra Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频数据集ImpliHateVid，专注于隐式仇恨言论检测，并提出了一种两阶段对比学习框架，结合多模态特征提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在文本和图像仇恨言论检测，视频领域的研究较少。本文旨在填补这一空白，专注于隐式仇恨言论检测。

Method: 提出两阶段对比学习框架：第一阶段训练音频、文本和图像的模态特定编码器；第二阶段训练跨编码器优化多模态表示，并结合情感、情绪和字幕特征。

Result: 在ImpliHateVid和HateMM数据集上验证了方法的有效性，证明了多模态对比学习在视频仇恨内容检测中的优势。

Conclusion: 本文的数据集和方法为视频隐式仇恨言论检测提供了重要资源和技术支持。

Abstract: The existing research has primarily focused on text and image-based hate
speech detection, video-based approaches remain underexplored. In this work, we
introduce a novel dataset, ImpliHateVid, specifically curated for implicit hate
speech detection in videos. ImpliHateVid consists of 2,009 videos comprising
509 implicit hate videos, 500 explicit hate videos, and 1,000 non-hate videos,
making it one of the first large-scale video datasets dedicated to implicit
hate detection. We also propose a novel two-stage contrastive learning
framework for hate speech detection in videos. In the first stage, we train
modality-specific encoders for audio, text, and image using contrastive loss by
concatenating features from the three encoders. In the second stage, we train
cross-encoders using contrastive learning to refine multimodal representations.
Additionally, we incorporate sentiment, emotion, and caption-based features to
enhance implicit hate detection. We evaluate our method on two datasets,
ImpliHateVid for implicit hate speech detection and another dataset for general
hate speech detection in videos, HateMM dataset, demonstrating the
effectiveness of the proposed multimodal contrastive learning for hateful
content detection in videos and the significance of our dataset.

</details>


### [22] [ContextGuard-LVLM: Enhancing News Veracity through Fine-grained Cross-modal Contextual Consistency Verification](https://arxiv.org/abs/2508.06623)
*Sihan Ma,Qiming Wu,Ruotong Jiang,Frank Burns*

Main category: cs.CV

TL;DR: 提出ContextGuard-LVLM框架，通过多阶段上下文推理机制增强视觉-语言大模型，显著提升细粒度跨模态一致性检测能力。


<details>
  <summary>Details</summary>
Motivation: 数字新闻媒体激增需要验证内容真实性，传统方法难以解决细粒度跨模态上下文一致性（FCCC）问题。

Method: 基于视觉-语言大模型（LVLMs），结合强化或对抗学习范式，检测细微上下文不一致。扩展三个数据集并引入新标注。

Result: ContextGuard-LVLM在细粒度一致性任务中优于零样本基线模型，复杂逻辑推理和上下文理解表现突出。

Conclusion: 模型在检测复杂上下文分离方面高效，对细微扰动鲁棒性强，与人类专家判断一致。

Abstract: The proliferation of digital news media necessitates robust methods for
verifying content veracity, particularly regarding the consistency between
visual and textual information. Traditional approaches often fall short in
addressing the fine-grained cross-modal contextual consistency (FCCC) problem,
which encompasses deeper alignment of visual narrative, emotional tone, and
background information with text, beyond mere entity matching. To address this,
we propose ContextGuard-LVLM, a novel framework built upon advanced
Vision-Language Large Models (LVLMs) and integrating a multi-stage contextual
reasoning mechanism. Our model is uniquely enhanced through reinforced or
adversarial learning paradigms, enabling it to detect subtle contextual
misalignments that evade zero-shot baselines. We extend and augment three
established datasets (TamperedNews-Ent, News400-Ent, MMG-Ent) with new
fine-grained contextual annotations, including "contextual sentiment," "visual
narrative theme," and "scene-event logical coherence," and introduce a
comprehensive CTXT (Contextual Coherence) entity type. Extensive experiments
demonstrate that ContextGuard-LVLM consistently outperforms state-of-the-art
zero-shot LVLM baselines (InstructBLIP and LLaVA 1.5) across nearly all
fine-grained consistency tasks, showing significant improvements in complex
logical reasoning and nuanced contextual understanding. Furthermore, our model
exhibits superior robustness to subtle perturbations and a higher agreement
rate with human expert judgments on challenging samples, affirming its efficacy
in discerning sophisticated forms of context detachment.

</details>


### [23] [VL-MedGuide: A Visual-Linguistic Large Model for Intelligent and Explainable Skin Disease Auxiliary Diagnosis](https://arxiv.org/abs/2508.06624)
*Kexin Yu,Zihan Xu,Jialei Xie,Carter Adams*

Main category: cs.CV

TL;DR: VL-MedGuide利用视觉-语言大模型（LVLMs）实现皮肤病智能辅助诊断，通过多模态概念感知和可解释疾病推理模块，提升诊断准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 皮肤病诊断复杂且现有视觉诊断模型缺乏可解释性，VL-MedGuide旨在解决这些问题。

Method: 采用多模态概念感知模块和可解释疾病推理模块，结合链式思维提示技术。

Result: 在Derm7pt数据集上表现优异（诊断BACC 83.55%，F1 80.12%；概念检测BACC 76.10%，F1 67.45%）。

Conclusion: VL-MedGuide在诊断性能和解释清晰度上均超越现有方法，具有临床实用价值。

Abstract: Accurate diagnosis of skin diseases remains a significant challenge due to
the complex and diverse visual features present in dermatoscopic images, often
compounded by a lack of interpretability in existing purely visual diagnostic
models. To address these limitations, this study introduces VL-MedGuide
(Visual-Linguistic Medical Guide), a novel framework leveraging the powerful
multi-modal understanding and reasoning capabilities of Visual-Language Large
Models (LVLMs) for intelligent and inherently interpretable auxiliary diagnosis
of skin conditions. VL-MedGuide operates in two interconnected stages: a
Multi-modal Concept Perception Module, which identifies and linguistically
describes dermatologically relevant visual features through sophisticated
prompt engineering, and an Explainable Disease Reasoning Module, which
integrates these concepts with raw visual information via Chain-of-Thought
prompting to provide precise disease diagnoses alongside transparent
rationales. Comprehensive experiments on the Derm7pt dataset demonstrate that
VL-MedGuide achieves state-of-the-art performance in both disease diagnosis
(83.55% BACC, 80.12% F1) and concept detection (76.10% BACC, 67.45% F1),
surpassing existing baselines. Furthermore, human evaluations confirm the high
clarity, completeness, and trustworthiness of its generated explanations,
bridging the gap between AI performance and clinical utility by offering
actionable, explainable insights for dermatological practice.

</details>


### [24] [CycleDiff: Cycle Diffusion Models for Unpaired Image-to-image Translation](https://arxiv.org/abs/2508.06625)
*Shilong Zou,Yuhang Huang,Renjiao Yi,Chenyang Zhu,Kai Xu*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的跨域图像翻译方法，无需配对训练数据，通过联合学习框架对齐扩散和翻译过程，提升全局最优性和性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在跨域图像翻译中因扩散与翻译过程未对齐而导致的局部最优问题，提升翻译效果。

Method: 提出联合学习框架，利用扩散模型提取图像成分表示干净信号，并设计时间依赖的翻译网络进行复杂映射学习。

Result: 在多种跨域翻译任务（如RGB↔RGB、RGB↔Edge等）中表现优于现有方法，生成效果更优。

Conclusion: 联合学习框架有效对齐扩散与翻译过程，提升全局最优性，显著改善生成质量和结构一致性。

Abstract: We introduce a diffusion-based cross-domain image translator in the absence
of paired training data. Unlike GAN-based methods, our approach integrates
diffusion models to learn the image translation process, allowing for more
coverable modeling of the data distribution and performance improvement of the
cross-domain translation. However, incorporating the translation process within
the diffusion process is still challenging since the two processes are not
aligned exactly, i.e., the diffusion process is applied to the noisy signal
while the translation process is conducted on the clean signal. As a result,
recent diffusion-based studies employ separate training or shallow integration
to learn the two processes, yet this may cause the local minimal of the
translation optimization, constraining the effectiveness of diffusion models.
To address the problem, we propose a novel joint learning framework that aligns
the diffusion and the translation process, thereby improving the global
optimality. Specifically, we propose to extract the image components with
diffusion models to represent the clean signal and employ the translation
process with the image components, enabling an end-to-end joint learning
manner. On the other hand, we introduce a time-dependent translation network to
learn the complex translation mapping, resulting in effective translation
learning and significant performance improvement. Benefiting from the design of
joint learning, our method enables global optimization of both processes,
enhancing the optimality and achieving improved fidelity and structural
consistency. We have conducted extensive experiments on RGB$\leftrightarrow$RGB
and diverse cross-modality translation tasks including
RGB$\leftrightarrow$Edge, RGB$\leftrightarrow$Semantics and
RGB$\leftrightarrow$Depth, showcasing better generative performances than the
state of the arts.

</details>


### [25] [CoDe-NeRF: Neural Rendering via Dynamic Coefficient Decomposition](https://arxiv.org/abs/2508.06632)
*Wenpeng Xing,Jie Chen,Zaifeng Yang,Tiancheng Zhao,Gaolei Li,Changting Lin,Yike Guo,Meng Han*

Main category: cs.CV

TL;DR: 提出了一种基于动态系数分解的神经渲染框架，用于改进复杂视点依赖外观的建模。


<details>
  <summary>Details</summary>
Motivation: NeRF在复杂镜面反射和高光场景中表现不佳，现有方法存在模糊反射或优化不稳定的问题。

Method: 通过动态系数分解将外观分解为静态神经基和动态系数，结合动态辐射积分器合成最终辐射。

Result: 在多个挑战性基准测试中，该方法能生成更锐利和真实的高光效果。

Conclusion: 这种分解范式为神经场景表示中的复杂外观建模提供了灵活有效的方向。

Abstract: Neural Radiance Fields (NeRF) have shown impressive performance in novel view
synthesis, but challenges remain in rendering scenes with complex specular
reflections and highlights. Existing approaches may produce blurry reflections
due to entanglement between lighting and material properties, or encounter
optimization instability when relying on physically-based inverse rendering. In
this work, we present a neural rendering framework based on dynamic coefficient
decomposition, aiming to improve the modeling of view-dependent appearance. Our
approach decomposes complex appearance into a shared, static neural basis that
encodes intrinsic material properties, and a set of dynamic coefficients
generated by a Coefficient Network conditioned on view and illumination. A
Dynamic Radiance Integrator then combines these components to synthesize the
final radiance. Experimental results on several challenging benchmarks suggest
that our method can produce sharper and more realistic specular highlights
compared to existing techniques. We hope that this decomposition paradigm can
provide a flexible and effective direction for modeling complex appearance in
neural scene representations.

</details>


### [26] [Rethinking Key-frame-based Micro-expression Recognition: A Robust and Accurate Framework Against Key-frame Errors](https://arxiv.org/abs/2508.06640)
*Zheyuan Zhang,Weihao Tang,Hong Chen*

Main category: cs.CV

TL;DR: 论文提出CausalNet框架，通过处理关键帧索引错误实现稳健的微表情识别，同时保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖准确的关键帧索引，但实际应用中难以获取，且存在误差，限制了实用性。

Method: CausalNet利用完整微表情序列输入，通过CMPLM定位肌肉运动区域，CAB学习肌肉收缩与放松的因果关系。

Result: 在多种关键帧噪声水平下表现稳健，并在标准基准测试中超越现有最优方法。

Conclusion: CausalNet解决了关键帧索引误差问题，提升了微表情识别的实用性和准确性。

Abstract: Micro-expression recognition (MER) is a highly challenging task in affective
computing. With the reduced-sized micro-expression (ME) input that contains key
information based on key-frame indexes, key-frame-based methods have
significantly improved the performance of MER. However, most of these methods
focus on improving the performance with relatively accurate key-frame indexes,
while ignoring the difficulty of obtaining accurate key-frame indexes and the
objective existence of key-frame index errors, which impedes them from moving
towards practical applications. In this paper, we propose CausalNet, a novel
framework to achieve robust MER facing key-frame index errors while maintaining
accurate recognition. To enhance robustness, CausalNet takes the representation
of the entire ME sequence as the input. To address the information redundancy
brought by the complete ME range input and maintain accurate recognition,
first, the Causal Motion Position Learning Module (CMPLM) is proposed to help
the model locate the muscle movement areas related to Action Units (AUs),
thereby reducing the attention to other redundant areas. Second, the Causal
Attention Block (CAB) is proposed to deeply learn the causal relationships
between the muscle contraction and relaxation movements in MEs. Empirical
experiments have demonstrated that on popular ME benchmarks, the CausalNet has
achieved robust MER under different levels of key-frame index noise. Meanwhile,
it has surpassed state-of-the-art (SOTA) methods on several standard MER
benchmarks when using the provided annotated key-frames. Code is available at
https://github.com/tony19980810/CausalNet.

</details>


### [27] [Towards Robust Red-Green Watermarking for Autoregressive Image Generators](https://arxiv.org/abs/2508.06656)
*Denis Lukovnikov,Andreas Müller,Erwin Quiring,Asja Fischer*

Main category: cs.CV

TL;DR: 本文探讨了在自回归图像模型中应用生成内水印技术，提出了两种基于视觉令牌聚类的新方法，提高了水印的鲁棒性和检测能力。


<details>
  <summary>Details</summary>
Motivation: 尽管生成内水印在潜在扩散模型中表现出高鲁棒性，但在自回归图像模型中的应用尚未探索。本文旨在填补这一空白。

Method: 研究了基于令牌级水印的方案，并提出两种新方法：1）基于聚类查找表的无训练方法；2）通过微调VAE编码器直接从扰动图像预测令牌聚类。

Result: 实验表明，聚类级水印显著提高了对扰动和再生攻击的鲁棒性，同时保持了图像质量。聚类分类进一步提升了水印检测能力。

Conclusion: 提出的方法在鲁棒性和检测能力上优于基线方法，且验证速度快，适用于轻量级后处理水印技术。

Abstract: In-generation watermarking for detecting and attributing generated content
has recently been explored for latent diffusion models (LDMs), demonstrating
high robustness. However, the use of in-generation watermarks in autoregressive
(AR) image models has not been explored yet. AR models generate images by
autoregressively predicting a sequence of visual tokens that are then decoded
into pixels using a vector-quantized decoder. Inspired by red-green watermarks
for large language models, we examine token-level watermarking schemes that
bias the next-token prediction based on prior tokens. We find that a direct
transfer of these schemes works in principle, but the detectability of the
watermarks decreases considerably under common image perturbations. As a
remedy, we propose two novel watermarking methods that rely on visual token
clustering to assign similar tokens to the same set. Firstly, we investigate a
training-free approach that relies on a cluster lookup table, and secondly, we
finetune VAE encoders to predict token clusters directly from perturbed images.
Overall, our experiments show that cluster-level watermarks improve robustness
against perturbations and regeneration attacks while preserving image quality.
Cluster classification further boosts watermark detectability, outperforming a
set of baselines. Moreover, our methods offer fast verification runtime,
comparable to lightweight post-hoc watermarking methods.

</details>


### [28] [Learning More by Seeing Less: Line Drawing Pretraining for Efficient, Transferable, and Human-Aligned Vision](https://arxiv.org/abs/2508.06696)
*Tianqin Li,George Liu,Tai Sing Lee*

Main category: cs.CV

TL;DR: 论文提出用线描作为结构优先的预训练模态，以生成更紧凑和泛化的视觉表示。实验表明，线描预训练模型在分类、检测和分割任务中表现更优，且具有更低的内在维度。


<details>
  <summary>Details</summary>
Motivation: 现代视觉系统依赖冗余的视觉输入，而人类能轻松理解稀疏的线描，表明结构而非外观是高效视觉理解的关键。

Method: 提出线描预训练方法，并扩展到无监督学习（“学习绘制”）。

Result: 线描预训练模型具有更强的形状偏置、更集中的注意力和更高的数据效率，且表示更易压缩。

Conclusion: 结构优先的视觉学习能提升效率、泛化能力和人类对齐的归纳偏置，为构建更鲁棒的视觉系统提供策略。

Abstract: Despite remarkable progress in computer vision, modern recognition systems
remain limited by their dependence on rich, redundant visual inputs. In
contrast, humans can effortlessly understand sparse, minimal representations
like line drawings - suggesting that structure, rather than appearance,
underlies efficient visual understanding. In this work, we propose using line
drawings as a structure-first pretraining modality to induce more compact and
generalizable visual representations. We show that models pretrained on line
drawings develop stronger shape bias, more focused attention, and greater data
efficiency across classification, detection, and segmentation tasks. Notably,
these models also exhibit lower intrinsic dimensionality, requiring
significantly fewer principal components to capture representational variance -
echoing the similar observation in low dimensional efficient representation in
the brain. Beyond performance improvements, line drawing pretraining produces
more compressible representations, enabling better distillation into
lightweight student models. Students distilled from line-pretrained teachers
consistently outperform those trained from color-supervised teachers,
highlighting the benefits of structurally compact knowledge. Finally, we
demonstrate that the pretraining with line-drawing can also be extended to
unsupervised setting via our proposed method "learning to draw". Together, our
results support the view that structure-first visual learning fosters
efficiency, generalization, and human-aligned inductive biases - offering a
simple yet powerful strategy for building more robust and adaptable vision
systems.

</details>


### [29] [MMFformer: Multimodal Fusion Transformer Network for Depression Detection](https://arxiv.org/abs/2508.06701)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Hamdi Altaheri,Lobna Nassar,Fakhri Karray*

Main category: cs.CV

TL;DR: MMFformer是一种多模态抑郁症检测网络，通过社交媒体信息提取时空特征，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 抑郁症的早期诊断依赖主观临床评估，社交媒体的多模态信息为客观检测提供了新途径。

Method: 采用Transformer网络提取视频空间特征和音频时间动态，通过晚期和中期融合策略整合多模态特征。

Result: 在两个大规模数据集上，F1-Score分别提升13.92%和7.74%，优于现有方法。

Conclusion: MMFformer在多模态抑郁症检测中表现出色，代码已开源。

Abstract: Depression is a serious mental health illness that significantly affects an
individual's well-being and quality of life, making early detection crucial for
adequate care and treatment. Detecting depression is often difficult, as it is
based primarily on subjective evaluations during clinical interviews. Hence,
the early diagnosis of depression, thanks to the content of social networks,
has become a prominent research area. The extensive and diverse nature of
user-generated information poses a significant challenge, limiting the accurate
extraction of relevant temporal information and the effective fusion of data
across multiple modalities. This paper introduces MMFformer, a multimodal
depression detection network designed to retrieve depressive spatio-temporal
high-level patterns from multimodal social media information. The transformer
network with residual connections captures spatial features from videos, and a
transformer encoder is exploited to design important temporal dynamics in
audio. Moreover, the fusion architecture fused the extracted features through
late and intermediate fusion strategies to find out the most relevant
intermodal correlations among them. Finally, the proposed network is assessed
on two large-scale depression detection datasets, and the results clearly
reveal that it surpasses existing state-of-the-art approaches, improving the
F1-Score by 13.92% for D-Vlog dataset and 7.74% for LMVD dataset. The code is
made available publicly at
https://github.com/rezwanh001/Large-Scale-Multimodal-Depression-Detection.

</details>


### [30] [Fourier Optics and Deep Learning Methods for Fast 3D Reconstruction in Digital Holography](https://arxiv.org/abs/2508.06703)
*Justin London*

Main category: cs.CV

TL;DR: 提出了一种高效快速的CGH合成框架，通过点云和MRI数据生成相位全息图和复全息图，并比较了不同优化算法的性能。


<details>
  <summary>Details</summary>
Motivation: 计算机生成全息术（CGH）在调制用户定义波形方面具有潜力，但需要高效且快速的合成方法。

Method: 将点云和MRI数据重建为体积对象，通过非凸傅里叶光学优化算法（交替投影、SGD、拟牛顿法）生成全息图，并使用2D中值滤波去除噪声。

Result: 通过MSE、RMSE和PSNR评估，优化算法性能优于HoloNet深度学习CGH，且2D中值滤波显著改善了性能指标。

Conclusion: 提出的框架在CGH合成中表现出高效性和性能优势，2D中值滤波是优化过程中的有效工具。

Abstract: Computer-generated holography (CGH) is a promising method that modulates
user-defined waveforms with digital holograms. An efficient and fast pipeline
framework is proposed to synthesize CGH using initial point cloud and MRI data.
This input data is reconstructed into volumetric objects that are then input
into non-convex Fourier optics optimization algorithms for phase-only hologram
(POH) and complex-hologram (CH) generation using alternating projection, SGD,
and quasi-Netwton methods. Comparison of reconstruction performance of these
algorithms as measured by MSE, RMSE, and PSNR is analyzed as well as to HoloNet
deep learning CGH. Performance metrics are shown to be improved by using 2D
median filtering to remove artifacts and speckled noise during optimization.

</details>


### [31] [Restage4D: Reanimating Deformable 3D Reconstruction from a Single Video](https://arxiv.org/abs/2508.06715)
*Jixuan He,Chieh Hubert Lin,Lu Qi,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 论文提出Restage4D方法，利用真实视频的运动先验生成物理一致的4D内容，通过视频重放训练策略和几何一致性优化提升合成效果。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型难以捕捉物理真实性和运动动态性，而真实视频能提供物理基础和运动线索，因此探索如何利用真实视频生成物理一致的4D内容。

Method: 提出Restage4D方法，结合视频重放训练策略、遮挡感知刚性损失和去遮挡回溯机制，优化几何和运动一致性。

Result: 在DAVIS和PointOdyssey数据集上验证，显示几何一致性、运动质量和3D跟踪性能的提升。

Conclusion: Restage4D不仅保留变形结构，还能自动纠正生成模型的错误，展示了视频先验在4D重演任务中的潜力。

Abstract: Creating deformable 3D content has gained increasing attention with the rise
of text-to-image and image-to-video generative models. While these models
provide rich semantic priors for appearance, they struggle to capture the
physical realism and motion dynamics needed for authentic 4D scene synthesis.
In contrast, real-world videos can provide physically grounded geometry and
articulation cues that are difficult to hallucinate. One question is raised:
\textit{Can we generate physically consistent 4D content by leveraging the
motion priors of the real-world video}? In this work, we explore the task of
reanimating deformable 3D scenes from a single video, using the original
sequence as a supervisory signal to correct artifacts from synthetic motion. We
introduce \textbf{Restage4D}, a geometry-preserving pipeline for
video-conditioned 4D restaging. Our approach uses a video-rewinding training
strategy to temporally bridge a real base video and a synthetic driving video
via a shared motion representation. We further incorporate an occlusion-aware
rigidity loss and a disocclusion backtracing mechanism to improve structural
and geometry consistency under challenging motion. We validate Restage4D on
DAVIS and PointOdyssey, demonstrating improved geometry consistency, motion
quality, and 3D tracking performance. Our method not only preserves deformable
structure under novel motion, but also automatically corrects errors introduced
by generative models, revealing the potential of video prior in 4D restaging
task. Source code and trained models will be released.

</details>


### [32] [FoundBioNet: A Foundation-Based Model for IDH Genotyping of Glioma from Multi-Parametric MRI](https://arxiv.org/abs/2508.06756)
*Somayeh Farahani,Marjaneh Hejazi,Antonio Di Ieva,Sidong Liu*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的非侵入性方法（FoundBioNet），通过多参数MRI预测胶质瘤IDH突变状态，性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统侵入性方法无法捕捉肿瘤空间异质性，且深度学习模型因标注数据稀缺而受限。

Method: 采用SWIN-UNETR架构，结合肿瘤感知特征编码（TAFE）和跨模态差异模块（CMD）。

Result: 在多个独立测试集上AUC达65.41%-90.58%，显著优于基线方法。

Conclusion: FoundBioNet通过大规模预训练和任务微调，提高了诊断准确性和可解释性，有望实现个性化治疗。

Abstract: Accurate, noninvasive detection of isocitrate dehydrogenase (IDH) mutation is
essential for effective glioma management. Traditional methods rely on invasive
tissue sampling, which may fail to capture a tumor's spatial heterogeneity.
While deep learning models have shown promise in molecular profiling, their
performance is often limited by scarce annotated data. In contrast, foundation
deep learning models offer a more generalizable approach for glioma imaging
biomarkers. We propose a Foundation-based Biomarker Network (FoundBioNet) that
utilizes a SWIN-UNETR-based architecture to noninvasively predict IDH mutation
status from multi-parametric MRI. Two key modules are incorporated: Tumor-Aware
Feature Encoding (TAFE) for extracting multi-scale, tumor-focused features, and
Cross-Modality Differential (CMD) for highlighting subtle T2-FLAIR mismatch
signals associated with IDH mutation. The model was trained and validated on a
diverse, multi-center cohort of 1705 glioma patients from six public datasets.
Our model achieved AUCs of 90.58%, 88.08%, 65.41%, and 80.31% on independent
test sets from EGD, TCGA, Ivy GAP, RHUH, and UPenn, consistently outperforming
baseline approaches (p <= 0.05). Ablation studies confirmed that both the TAFE
and CMD modules are essential for improving predictive accuracy. By integrating
large-scale pretraining and task-specific fine-tuning, FoundBioNet enables
generalizable glioma characterization. This approach enhances diagnostic
accuracy and interpretability, with the potential to enable more personalized
patient care.

</details>


### [33] [VOccl3D: A Video Benchmark Dataset for 3D Human Pose and Shape Estimation under real Occlusions](https://arxiv.org/abs/2508.06757)
*Yash Garg,Saketh Bachu,Arindam Dutta,Rohit Lal,Sarosij Bose,Calvin-Khang Ta,M. Salman Asif,Amit Roy-Chowdhury*

Main category: cs.CV

TL;DR: 论文提出了一个名为VOccl3D的新数据集，用于解决现有3D人体姿态和形状估计方法在真实遮挡场景中的不足，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的人体姿态和形状估计方法在复杂姿态或严重遮挡场景中表现不佳，且现有遮挡数据集缺乏真实性。

Method: 使用计算机图形渲染技术构建VOccl3D数据集，包含多样化的真实遮挡场景和人体动作，并微调了CLIFF和BEDLAM-CLIFF方法。

Result: 实验表明，微调后的方法在多个公共数据集和VOccl3D测试集上均取得显著提升，同时增强了遮挡下的人体检测性能。

Conclusion: VOccl3D数据集为未来研究提供了更真实的遮挡基准，推动了遮挡场景下人体姿态和形状估计的发展。

Abstract: Human pose and shape (HPS) estimation methods have been extensively studied,
with many demonstrating high zero-shot performance on in-the-wild images and
videos. However, these methods often struggle in challenging scenarios
involving complex human poses or significant occlusions. Although some studies
address 3D human pose estimation under occlusion, they typically evaluate
performance on datasets that lack realistic or substantial occlusions, e.g.,
most existing datasets introduce occlusions with random patches over the human
or clipart-style overlays, which may not reflect real-world challenges. To
bridge this gap in realistic occlusion datasets, we introduce a novel benchmark
dataset, VOccl3D, a Video-based human Occlusion dataset with 3D body pose and
shape annotations. Inspired by works such as AGORA and BEDLAM, we constructed
this dataset using advanced computer graphics rendering techniques,
incorporating diverse real-world occlusion scenarios, clothing textures, and
human motions. Additionally, we fine-tuned recent HPS methods, CLIFF and
BEDLAM-CLIFF, on our dataset, demonstrating significant qualitative and
quantitative improvements across multiple public datasets, as well as on the
test split of our dataset, while comparing its performance with other
state-of-the-art methods. Furthermore, we leveraged our dataset to enhance
human detection performance under occlusion by fine-tuning an existing object
detector, YOLO11, thus leading to a robust end-to-end HPS estimation system
under occlusions. Overall, this dataset serves as a valuable resource for
future research aimed at benchmarking methods designed to handle occlusions,
offering a more realistic alternative to existing occlusion datasets. See the
Project page for code and dataset:https://yashgarg98.github.io/VOccl3D-dataset/

</details>


### [34] [SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding](https://arxiv.org/abs/2508.06763)
*Zihao Sheng,Zilin Huang,Yen-Jung Chen,Yansong Qu,Yuhao Luo,Yue Leng,Sikai Chen*

Main category: cs.CV

TL;DR: SafePLUG是一个新型多模态大语言模型框架，专注于交通事故的细粒度理解，支持像素级分割和时间定位。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在交通事故理解中缺乏对细粒度视觉细节和局部场景的处理能力，限制了其应用。

Method: 提出SafePLUG框架，结合像素级理解和时间定位，支持区域感知问答和语言指令驱动的像素级分割。

Result: 实验显示SafePLUG在区域问答、像素分割、时间事件定位等任务中表现优异。

Conclusion: SafePLUG为复杂交通场景的细粒度理解奠定基础，有望提升智能交通系统的安全性和情境感知能力。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress
across a range of vision-language tasks and demonstrate strong potential for
traffic accident understanding. However, existing MLLMs in this domain
primarily focus on coarse-grained image-level or video-level comprehension and
often struggle to handle fine-grained visual details or localized scene
components, limiting their applicability in complex accident scenarios. To
address these limitations, we propose SafePLUG, a novel framework that empowers
MLLMs with both Pixel-Level Understanding and temporal Grounding for
comprehensive traffic accident analysis. SafePLUG supports both
arbitrary-shaped visual prompts for region-aware question answering and
pixel-level segmentation based on language instructions, while also enabling
the recognition of temporally anchored events in traffic accident scenarios. To
advance the development of MLLMs for traffic accident understanding, we curate
a new dataset containing multimodal question-answer pairs centered on diverse
accident scenarios, with detailed pixel-level annotations and temporal event
boundaries. Experimental results show that SafePLUG achieves strong performance
on multiple tasks, including region-based question answering, pixel-level
segmentation, temporal event localization, and accident event understanding.
These capabilities lay a foundation for fine-grained understanding of complex
traffic scenes, with the potential to improve driving safety and enhance
situational awareness in smart transportation systems. The code, dataset, and
model checkpoints will be made publicly available at:
https://zihaosheng.github.io/SafePLUG

</details>


### [35] [DiffUS: Differentiable Ultrasound Rendering from Volumetric Imaging](https://arxiv.org/abs/2508.06768)
*Noe Bertramo,Gabriel Duguey,Vivek Gopalakrishnan*

Main category: cs.CV

TL;DR: DiffUS是一种基于物理的可微分超声渲染器，通过模拟超声波的传播和反射，从MRI数据生成逼真的B超图像。


<details>
  <summary>Details</summary>
Motivation: 解决术中超声图像与术前高分辨率MRI/CT扫描对齐困难的问题，提供更准确的术中引导。

Method: 1. 使用机器学习将MRI 3D扫描转换为声阻抗体积；2. 通过射线追踪和反射-传输方程模拟超声波传播；3. 构建稀疏线性系统捕捉多次内部反射；4. 通过深度分辨回波提取生成B超图像。

Result: 在ReMIND数据集上验证，DiffUS能够从脑MRI数据生成解剖学准确的超声图像。

Conclusion: DiffUS为术中超声图像与术前数据的对齐提供了新方法，具有潜在的下游应用价值。

Abstract: Intraoperative ultrasound imaging provides real-time guidance during numerous
surgical procedures, but its interpretation is complicated by noise, artifacts,
and poor alignment with high-resolution preoperative MRI/CT scans. To bridge
the gap between reoperative planning and intraoperative guidance, we present
DiffUS, a physics-based, differentiable ultrasound renderer that synthesizes
realistic B-mode images from volumetric imaging. DiffUS first converts MRI 3D
scans into acoustic impedance volumes using a machine learning approach. Next,
we simulate ultrasound beam propagation using ray tracing with coupled
reflection-transmission equations. DiffUS formulates wave propagation as a
sparse linear system that captures multiple internal reflections. Finally, we
reconstruct B-mode images via depth-resolved echo extraction across fan-shaped
acquisition geometry, incorporating realistic artifacts including speckle noise
and depth-dependent degradation. DiffUS is entirely implemented as
differentiable tensor operations in PyTorch, enabling gradient-based
optimization for downstream applications such as slice-to-volume registration
and volumetric reconstruction. Evaluation on the ReMIND dataset demonstrates
DiffUS's ability to generate anatomically accurate ultrasound images from brain
MRI data.

</details>


### [36] [Edge Detection for Organ Boundaries via Top Down Refinement and SubPixel Upsampling](https://arxiv.org/abs/2508.06805)
*Aarav Mehta,Priya Deshmukh,Vikram Singh,Siddharth Malhotra,Krishnan Menon Iyer,Tanvi Iyer*

Main category: cs.CV

TL;DR: 提出了一种针对医学图像的精确边缘检测方法，通过反向细化架构提升边界定位精度。


<details>
  <summary>Details</summary>
Motivation: 医学图像中器官边界的精确定位对分割、手术规划和放疗至关重要，现有卷积网络的输出在定位精度上不足。

Method: 采用自上而下的反向细化架构，结合高层语义特征和低层细节，并扩展至各向异性体积处理。

Result: 在CT和MRI数据集上显著提升了边界定位精度，并改善了分割、配准等下游任务的表现。

Conclusion: 该方法生成的精确边缘对医学图像任务具有重要临床价值。

Abstract: Accurate localization of organ boundaries is critical in medical imaging for
segmentation, registration, surgical planning, and radiotherapy. While deep
convolutional networks (ConvNets) have advanced general-purpose edge detection
to near-human performance on natural images, their outputs often lack precise
localization, a limitation that is particularly harmful in medical applications
where millimeter-level accuracy is required. Building on a systematic analysis
of ConvNet edge outputs, we propose a medically focused crisp edge detector
that adapts a novel top-down backward refinement architecture to medical images
(2D and volumetric). Our method progressively upsamples and fuses high-level
semantic features with fine-grained low-level cues through a backward
refinement pathway, producing high-resolution, well-localized organ boundaries.
We further extend the design to handle anisotropic volumes by combining 2D
slice-wise refinement with light 3D context aggregation to retain computational
efficiency. Evaluations on several CT and MRI organ datasets demonstrate
substantially improved boundary localization under strict criteria (boundary
F-measure, Hausdorff distance) compared to baseline ConvNet detectors and
contemporary medical edge/contour methods. Importantly, integrating our crisp
edge maps into downstream pipelines yields consistent gains in organ
segmentation (higher Dice scores, lower boundary errors), more accurate image
registration, and improved delineation of lesions near organ interfaces. The
proposed approach produces clinically valuable, crisp organ edges that
materially enhance common medical-imaging tasks.

</details>


### [37] [DualResolution Residual Architecture with Artifact Suppression for Melanocytic Lesion Segmentation](https://arxiv.org/abs/2508.06816)
*Vikram Singh,Kabir Malhotra,Rohan Desai,Ananya Shankaracharya,Priyadarshini Chatterjee,Krishnan Menon Iyer*

Main category: cs.CV

TL;DR: 提出了一种新型的双分辨率架构，用于皮肤镜图像中黑色素瘤的精确分割，结合边界感知和通道注意力模块，显著提升了分割效果。


<details>
  <summary>Details</summary>
Motivation: 皮肤镜图像中的黑色素瘤分割对自动化癌症筛查至关重要，但现有方法难以处理细微纹理变化和常见干扰。

Method: 采用ResNet启发的双分辨率架构，结合边界感知残差连接和通道注意力模块，并引入轻量级干扰抑制块和多任务训练目标。

Result: 在公开数据集上显著提升了边界定位和分割指标，优于标准编码器-解码器基线。

Conclusion: 该方法为自动化黑色素瘤评估系统提供了实用且高效的解决方案。

Abstract: Accurate segmentation of melanocytic tumors in dermoscopic images is a
critical step for automated skin cancer screening and clinical decision
support. Unlike natural scene segmentation, lesion delineation must reconcile
subtle texture and color variations, frequent artifacts (hairs, rulers,
bubbles), and a strong need for precise boundary localization to support
downstream diagnosis. In this paper we introduce Our method, a novel ResNet
inspired dual resolution architecture specifically designed for melanocytic
tumor segmentation. Our method maintains a full resolution stream that
preserves fine grained boundary information while a complementary pooled stream
aggregates multi scale contextual cues for robust lesion recognition. The
streams are tightly coupled by boundary aware residual connections that inject
high frequency edge information into deep feature maps, and by a channel
attention module that adapts color and texture sensitivity to dermoscopic
appearance. To further address common imaging artifacts and the limited size of
clinical datasets, we propose a lightweight artifact suppression block and a
multi task training objective that combines a Dice Tversky segmentation loss
with an explicit boundary loss and a contrastive regularizer for feature
stability. The combined design yields pixel accurate masks without requiring
heavy post processing or complex pre training protocols. Extensive experiments
on public dermoscopic benchmarks demonstrate that Our method significantly
improves boundary adherence and clinically relevant segmentation metrics
compared to standard encoder decoder baselines, making it a practical building
block for automated melanoma assessment systems.

</details>


### [38] [VesselRW: Weakly Supervised Subcutaneous Vessel Segmentation via Learned Random Walk Propagation](https://arxiv.org/abs/2508.06819)
*Ayaan Nooruddin Siddiqui,Mahnoor Zaidi,Ayesha Nazneen Shahbaz,Priyadarshini Chatterjee,Krishnan Menon Iyer*

Main category: cs.CV

TL;DR: 提出了一种弱监督训练框架，用于皮下血管分割，利用稀疏标注生成密集监督，结合不确定性加权损失和拓扑感知正则化，显著减少标注负担并提升分割效果。


<details>
  <summary>Details</summary>
Motivation: 解决皮下血管分割中标注稀缺、成本高以及图像对比度低、噪声多的问题。

Method: 通过可微随机游走标签传播模型扩展稀疏标注为密集监督，结合图像驱动的血管特征和连续性先验，联合训练CNN分割器，并引入拓扑感知正则化。

Result: 在临床数据集上表现优于稀疏标签和传统伪标签方法，生成更完整的血管图并提供校准的不确定性。

Conclusion: 该方法显著降低标注负担，同时保持临床相关的血管拓扑结构。

Abstract: Accurate segmentation of subcutaneous vessels from clinical images is
hampered by scarce, expensive ground truth and by low contrast, noisy
appearance of vessels across patients and modalities. We present a novel weakly
supervised training framework tailored for subcutaneous vessel segmentation
that leverages inexpensive sparse annotations (e.g., centerline traces, dot
markers, or short scribbles). Sparse labels are expanded into dense,
probabilistic supervision via a differentiable random walk label propagation
model whose transition weights incorporate image driven vesselness cues and
tubular continuity priors. The propagation yields per-pixel hitting
probabilities together with calibrated uncertainty estimates; these are
incorporated into an uncertainty weighted loss to avoid over fitting to
ambiguous regions. Crucially, the label-propagator is learned jointly with a
CNN based segmentation predictor, enabling the system to discover vessel edges
and continuity constraints without explicit edge supervision. We further
introduce a topology aware regularizer that encourages centerline connectivity
and penalizes spurious branches, improving clinical usability. In experiments
on clinical subcutaneous imaging datasets, our method consistently outperforms
naive training on sparse labels and conventional dense pseudo-labeling,
producing more complete vascular maps and better calibrated uncertainty for
downstream decision making. The approach substantially reduces annotation
burden while preserving clinically relevant vessel topology.

</details>


### [39] [Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification](https://arxiv.org/abs/2508.06831)
*Taha Mustapha Nehdi,Nairouz Mrabah,Atif Belal,Marco Pedersoli,Eric Granger*

Main category: cs.CV

TL;DR: SAGE-reID是一种高效、源无关的多源域自适应方法，通过低秩适配器和轻量级门控网络实现跨域知识迁移，显著减少计算成本和内存消耗。


<details>
  <summary>Details</summary>
Motivation: 解决多源域自适应（MSDA）方法中训练参数和计算成本高的问题，同时避免访问源域数据的需求。

Method: 训练源特定的低秩适配器（LoRA），并通过轻量级门控网络动态分配权重融合LoRA专家，实现知识迁移。

Result: 在Market-1501、DukeMTMC-reID和MSMT17基准测试中表现优于现有方法，且计算高效。

Conclusion: SAGE-reID是一种高效、源无关的MSDA方法，显著提升了性能和效率。

Abstract: Adapting person re-identification (reID) models to new target environments
remains a challenging problem that is typically addressed using unsupervised
domain adaptation (UDA) methods. Recent works show that when labeled data
originates from several distinct sources (e.g., datasets and cameras),
considering each source separately and applying multi-source domain adaptation
(MSDA) typically yields higher accuracy and robustness compared to blending the
sources and performing conventional UDA. However, state-of-the-art MSDA methods
learn domain-specific backbone models or require access to source domain data
during adaptation, resulting in significant growth in training parameters and
computational cost. In this paper, a Source-free Adaptive Gated Experts
(SAGE-reID) method is introduced for person reID. Our SAGE-reID is a
cost-effective, source-free MSDA method that first trains individual
source-specific low-rank adapters (LoRA) through source-free UDA. Next, a
lightweight gating network is introduced and trained to dynamically assign
optimal merging weights for fusion of LoRA experts, enabling effective
cross-domain knowledge transfer. While the number of backbone parameters
remains constant across source domains, LoRA experts scale linearly but remain
negligible in size (<= 2% of the backbone), reducing both the memory
consumption and risk of overfitting. Extensive experiments conducted on three
challenging benchmarks: Market-1501, DukeMTMC-reID, and MSMT17 indicate that
SAGE-reID outperforms state-of-the-art methods while being computationally
efficient.

</details>


### [40] [Hybrid Machine Learning Framework for Predicting Geometric Deviations from 3D Surface Metrology](https://arxiv.org/abs/2508.06845)
*Hamidreza Samadi,Md Manjurul Ahsan,Shivakumar Raman*

Main category: cs.CV

TL;DR: 该研究提出了一种结合高分辨率3D扫描和混合机器学习框架的方法，用于预测制造组件的几何偏差，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 现代制造中复杂几何形状的尺寸精度难以保持，需要更准确的预测方法。

Method: 使用高分辨率3D扫描仪获取多角度表面数据，通过精确对齐、降噪和合并技术处理数据，并开发了结合卷积神经网络和梯度提升决策树的混合机器学习框架。

Result: 预测精度达到0.012毫米（95%置信水平），比传统统计过程控制方法提高了73%，并揭示了制造参数与几何偏差之间的隐藏关联。

Conclusion: 该方法为自动化质量控制、预测性维护和设计优化提供了潜力，并为未来预测建模研究提供了数据集基础。

Abstract: This study addresses the challenge of accurately forecasting geometric
deviations in manufactured components using advanced 3D surface analysis.
Despite progress in modern manufacturing, maintaining dimensional precision
remains difficult, particularly for complex geometries. We present a
methodology that employs a high-resolution 3D scanner to acquire multi-angle
surface data from 237 components produced across different batches. The data
were processed through precise alignment, noise reduction, and merging
techniques to generate accurate 3D representations. A hybrid machine learning
framework was developed, combining convolutional neural networks for feature
extraction with gradient-boosted decision trees for predictive modeling. The
proposed system achieved a prediction accuracy of 0.012 mm at a 95% confidence
level, representing a 73% improvement over conventional statistical process
control methods. In addition to improved accuracy, the model revealed hidden
correlations between manufacturing parameters and geometric deviations. This
approach offers significant potential for automated quality control, predictive
maintenance, and design optimization in precision manufacturing, and the
resulting dataset provides a strong foundation for future predictive modeling
research.

</details>


### [41] [AGIC: Attention-Guided Image Captioning to Improve Caption Relevance](https://arxiv.org/abs/2508.06853)
*L. D. M. S. Sai Teja,Ashok Urlana,Pruthwik Mishra*

Main category: cs.CV

TL;DR: AGIC通过注意力引导和混合解码策略提升图像描述生成效果，在Flickr8k和Flickr30k数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管图像描述生成取得进展，但生成准确且描述性强的标题仍是挑战。

Method: 提出AGIC，通过特征空间增强显著视觉区域，并采用混合解码策略平衡流畅性与多样性。

Result: AGIC在多个评估指标上表现优异，推理速度更快，超越或匹配现有先进模型。

Conclusion: AGIC为图像描述生成提供了可扩展且可解释的解决方案。

Abstract: Despite significant progress in image captioning, generating accurate and
descriptive captions remains a long-standing challenge. In this study, we
propose Attention-Guided Image Captioning (AGIC), which amplifies salient
visual regions directly in the feature space to guide caption generation. We
further introduce a hybrid decoding strategy that combines deterministic and
probabilistic sampling to balance fluency and diversity. To evaluate AGIC, we
conduct extensive experiments on the Flickr8k and Flickr30k datasets. The
results show that AGIC matches or surpasses several state-of-the-art models
while achieving faster inference. Moreover, AGIC demonstrates strong
performance across multiple evaluation metrics, offering a scalable and
interpretable solution for image captioning.

</details>


### [42] [A Joint Sparse Self-Representation Learning Method for Multiview Clustering](https://arxiv.org/abs/2508.06857)
*Mengxue Jia,Zhihua Allen-Zhao,You Zhao,Sanyang Liu*

Main category: cs.CV

TL;DR: 提出一种基于稀疏自表示学习的多视图聚类方法，通过引入基数约束提取局部信息，并开发全局收敛的交替二次惩罚方法。


<details>
  <summary>Details</summary>
Motivation: 多视图聚类需要利用一致和互补信息，现有方法在提取局部信息和全局收敛性上存在不足。

Method: 提出联合稀疏自表示学习模型，使用基数约束提取局部信息，低秩约束揭示全局结构，开发交替二次惩罚方法保证收敛。

Result: 在六个标准数据集上优于八种先进算法。

Conclusion: 新模型和方法在多视图聚类中表现出优越性能。

Abstract: Multiview clustering (MC) aims to group samples using consistent and
complementary information across various views. The subspace clustering, as a
fundamental technique of MC, has attracted significant attention. In this
paper, we propose a novel joint sparse self-representation learning model for
MC, where a featured difference is the extraction of view-specific local
information by introducing cardinality (i.e., $\ell_0$-norm) constraints
instead of Graph-Laplacian regularization. Specifically, under each view,
cardinality constraints directly restrict the samples used in the
self-representation stage to extract reliable local and global structure
information, while the low-rank constraint aids in revealing a global coherent
structure in the consensus affinity matrix during merging. The attendant
challenge is that Augmented Lagrange Method (ALM)-based alternating
minimization algorithms cannot guarantee convergence when applied directly to
our nonconvex, nonsmooth model, thus resulting in poor generalization ability.
To address it, we develop an alternating quadratic penalty (AQP) method with
global convergence, where two subproblems are iteratively solved by closed-form
solutions. Empirical results on six standard datasets demonstrate the
superiority of our model and AQP method, compared to eight state-of-the-art
algorithms.

</details>


### [43] [VSI: Visual Subtitle Integration for Keyframe Selection to enhance Long Video Understanding](https://arxiv.org/abs/2508.06869)
*Jianxiang He,Shaoguang Wang,Weiyu Guo,Meisheng Hong,Jungang Li,Yijie Xu,Ziyang Chen,Hui Xiong*

Main category: cs.CV

TL;DR: 论文提出了一种名为VSI的多模态关键帧搜索方法，通过结合字幕和时间戳等信息提升长视频理解任务的效果。


<details>
  <summary>Details</summary>
Motivation: 长视频理解因数据规模庞大而具有挑战性，现有方法因文本查询与视觉内容对齐不足且无法捕捉复杂时序语义信息而效果有限。

Method: 提出VSI方法，通过双流搜索机制（视频搜索流和字幕匹配流）整合视觉与文本信息，提升关键帧搜索精度。

Result: 在LongVideoBench上，VSI的关键帧定位准确率达40.00%，视频问答任务准确率达68.48%，显著超越基线方法。

Conclusion: VSI在长视频理解任务中表现出色，验证了其多模态搜索策略的鲁棒性和泛化能力。

Abstract: Long video understanding presents a significant challenge to multimodal large
language models (MLLMs) primarily due to the immense data scale. A critical and
widely adopted strategy for making this task computationally tractable is
keyframe retrieval, which seeks to identify a sparse set of video frames that
are most salient to a given textual query. However, the efficacy of this
approach is hindered by weak multimodal alignment between textual queries and
visual content and fails to capture the complex temporal semantic information
required for precise reasoning. To address this, we propose Visual-Subtitle
Integeration(VSI), a multimodal keyframe search method that integrates
subtitles, timestamps, and scene boundaries into a unified multimodal search
process. The proposed method captures the visual information of video frames as
well as the complementary textual information through a dual-stream search
mechanism by Video Search Stream as well as Subtitle Match Stream,
respectively, and improves the keyframe search accuracy through the interaction
of the two search streams. Experimental results show that VSI achieve 40.00%
key frame localization accuracy on the text-relevant subset of LongVideoBench
and 68.48% accuracy on downstream long Video-QA tasks, surpassing competitive
baselines by 20.35% and 15.79%, respectively. Furthermore, on the
LongVideoBench, VSI achieved state-of-the-art(SOTA) in medium-to-long video-QA
tasks, demonstrating the robustness and generalizability of the proposed
multimodal search strategy.

</details>


### [44] [NS-FPN: Improving Infrared Small Target Detection and Segmentation from Noise Suppression Perspective](https://arxiv.org/abs/2508.06878)
*Maoxun Yuan,Duanni Meng,Ziteng Xi,Tianyi Zhao,Shiji Zhao,Yimian Dai,Xingxing Wei*

Main category: cs.CV

TL;DR: 提出了一种新型噪声抑制特征金字塔网络（NS-FPN），通过低频引导特征纯化（LFP）和螺旋感知特征采样（SFS）模块，显著减少了红外小目标检测中的误报问题。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测因目标暗淡、形状模糊及背景干扰严重而具有挑战性，现有CNN方法虽能增强特征表示但导致误报增加。

Method: 提出NS-FPN，包含LFP模块（通过纯化高频成分抑制噪声）和SFS模块（螺旋采样融合目标相关特征）。

Result: 在公开数据集上实验表明，NS-FPN显著减少误报并提升性能。

Conclusion: NS-FPN轻量高效，可轻松集成到现有框架中，为红外小目标检测提供了新思路。

Abstract: Infrared small target detection and segmentation (IRSTDS) is a critical yet
challenging task in defense and civilian applications, owing to the dim,
shapeless appearance of targets and severe background clutter. Recent CNN-based
methods have achieved promising target perception results, but they only focus
on enhancing feature representation to offset the impact of noise, which
results in the increased false alarms problem. In this paper, through analyzing
the problem from the frequency domain, we pioneer in improving performance from
noise suppression perspective and propose a novel noise-suppression feature
pyramid network (NS-FPN), which integrates a low-frequency guided feature
purification (LFP) module and a spiral-aware feature sampling (SFS) module into
the original FPN structure. The LFP module suppresses the noise features by
purifying high-frequency components to achieve feature enhancement devoid of
noise interference, while the SFS module further adopts spiral sampling to fuse
target-relevant features in feature fusion process. Our NS-FPN is designed to
be lightweight yet effective and can be easily plugged into existing IRSTDS
frameworks. Extensive experiments on the public IRSTDS datasets demonstrate
that our method significantly reduces false alarms and achieves superior
performance on IRSTDS tasks.

</details>


### [45] [BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models](https://arxiv.org/abs/2508.06895)
*Jianting Tang,Yubo Wang,Haoyu Cao,Linli Xu*

Main category: cs.CV

TL;DR: 论文提出BASIC方法，通过直接监督视觉嵌入优化MLLMs的性能。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在视觉嵌入对齐中缺乏直接视觉监督，限制了性能提升。

Method: BASIC利用LLM浅层中精炼的视觉嵌入作为监督，从嵌入方向和语义匹配两方面优化初始视觉嵌入。

Result: BASIC显著提升了MLLMs在多个基准测试中的表现。

Conclusion: 直接视觉监督是提升MLLMs视觉理解能力的有效方法。

Abstract: Mainstream Multimodal Large Language Models (MLLMs) achieve visual
understanding by using a vision projector to bridge well-pretrained vision
encoders and large language models (LLMs). The inherent gap between visual and
textual modalities makes the embeddings from the vision projector critical for
visual comprehension. However, current alignment approaches treat visual
embeddings as contextual cues and merely apply auto-regressive supervision to
textual outputs, neglecting the necessity of introducing equivalent direct
visual supervision, which hinders the potential finer alignment of visual
embeddings. In this paper, based on our analysis of the refinement process of
visual embeddings in the LLM's shallow layers, we propose BASIC, a method that
utilizes refined visual embeddings within the LLM as supervision to directly
guide the projector in generating initial visual embeddings. Specifically, the
guidance is conducted from two perspectives: (i) optimizing embedding
directions by reducing angles between initial and supervisory embeddings in
semantic space; (ii) improving semantic matching by minimizing disparities
between the logit distributions of both visual embeddings. Without additional
supervisory models or artificial annotations, BASIC significantly improves the
performance of MLLMs across a wide range of benchmarks, demonstrating the
effectiveness of our introduced direct visual supervision.

</details>


### [46] [Advancements in Chinese font generation since deep learning era: A survey](https://arxiv.org/abs/2508.06900)
*Weiran Chen,Guiqian Zhu,Ying Li,Yi Ji,Chunping Liu*

Main category: cs.CV

TL;DR: 本文综述了基于深度学习的汉字字体生成方法，分为多样本和少样本生成两类，总结了代表性方法及其优缺点，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 汉字字体生成是字体设计师和排版师关注的重要课题，但如何提高生成质量仍具挑战性。

Method: 通过文献选择和分析方法，回顾了深度学习架构、字体表示格式、数据集和评估指标，并将现有方法分为多样本和少样本生成两类。

Result: 总结了代表性方法的优缺点，并讨论了生成质量的提升空间。

Conclusion: 提出了未来研究的挑战和方向，为该领域研究者提供了有价值的启示。

Abstract: Chinese font generation aims to create a new Chinese font library based on
some reference samples. It is a topic of great concern to many font designers
and typographers. Over the past years, with the rapid development of deep
learning algorithms, various new techniques have achieved flourishing and
thriving progress. Nevertheless, how to improve the overall quality of
generated Chinese character images remains a tough issue. In this paper, we
conduct a holistic survey of the recent Chinese font generation approaches
based on deep learning. To be specific, we first illustrate the research
background of the task. Then, we outline our literature selection and analysis
methodology, and review a series of related fundamentals, including classical
deep learning architectures, font representation formats, public datasets, and
frequently-used evaluation metrics. After that, relying on the number of
reference samples required to generate a new font, we categorize the existing
methods into two major groups: many-shot font generation and few-shot font
generation methods. Within each category, representative approaches are
summarized, and their strengths and limitations are also discussed in detail.
Finally, we conclude our paper with the challenges and future directions, with
the expectation to provide some valuable illuminations for the researchers in
this field.

</details>


### [47] [eMotions: A Large-Scale Dataset and Audio-Visual Fusion Network for Emotion Analysis in Short-form Videos](https://arxiv.org/abs/2508.06902)
*Xuecheng Wu,Dingkang Yang,Danlei Huang,Xinyi Yin,Yifan Wang,Jia Zhang,Jiayu Nie,Liangyu Fu,Yang Liu,Junxiao Xue,Hadi Amirpour,Wei Zhou*

Main category: cs.CV

TL;DR: 论文提出了eMotions数据集和AV-CANet网络，用于解决短视频情感分析的挑战，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 短视频的复杂性和情感数据稀缺性促使研究提出新数据集和方法。

Method: 提出多阶段标注流程和AV-CANet网络，结合局部-全局融合模块和EP-CE损失函数。

Result: 在多个数据集上验证了AV-CANet的有效性，并提供了广泛的研究见解。

Conclusion: AV-CANet为短视频情感分析提供了有效解决方案，数据集和代码将开源。

Abstract: Short-form videos (SVs) have become a vital part of our online routine for
acquiring and sharing information. Their multimodal complexity poses new
challenges for video analysis, highlighting the need for video emotion analysis
(VEA) within the community. Given the limited availability of SVs emotion data,
we introduce eMotions, a large-scale dataset consisting of 27,996 videos with
full-scale annotations. To ensure quality and reduce subjective bias, we
emphasize better personnel allocation and propose a multi-stage annotation
procedure. Additionally, we provide the category-balanced and test-oriented
variants through targeted sampling to meet diverse needs. While there have been
significant studies on videos with clear emotional cues (e.g., facial
expressions), analyzing emotions in SVs remains a challenging task. The
challenge arises from the broader content diversity, which introduces more
distinct semantic gaps and complicates the representations learning of
emotion-related features. Furthermore, the prevalence of audio-visual
co-expressions in SVs leads to the local biases and collective information gaps
caused by the inconsistencies in emotional expressions. To tackle this, we
propose AV-CANet, an end-to-end audio-visual fusion network that leverages
video transformer to capture semantically relevant representations. We further
introduce the Local-Global Fusion Module designed to progressively capture the
correlations of audio-visual features. Besides, EP-CE Loss is constructed to
globally steer optimizations with tripolar penalties. Extensive experiments
across three eMotions-related datasets and four public VEA datasets demonstrate
the effectiveness of our proposed AV-CANet, while providing broad insights for
future research. Moreover, we conduct ablation studies to examine the critical
components of our method. Dataset and code will be made available at Github.

</details>


### [48] [A Simple yet Powerful Instance-Aware Prompting Framework for Training-free Camouflaged Object Segmentation](https://arxiv.org/abs/2508.06904)
*Chao Yin,Jide Li,Xiaoqiang Li*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练的实例感知提示框架（IAPF），用于解决伪装物体分割（COS）中多实例场景的挑战，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于SAM的训练免费COS方法仅生成语义级提示，导致在多实例场景下表现不佳。

Method: IAPF通过三步流程：文本提示生成、实例掩码生成和自一致性投票，将通用提示转换为细粒度实例掩码。

Result: 在标准COS基准测试中，IAPF显著优于现有训练免费方法。

Conclusion: IAPF是一种简单而强大的训练免费COS框架，有效解决了多实例分割问题。

Abstract: Camouflaged Object Segmentation (COS) remains highly challenging due to the
intrinsic visual similarity between target objects and their surroundings.
While training-based COS methods achieve good performance, their performance
degrades rapidly with increased annotation sparsity. To circumvent this
limitation, recent studies have explored training-free COS methods, leveraging
the Segment Anything Model (SAM) by automatically generating visual prompts
from a single task-generic prompt (\textit{e.g.}, "\textit{camouflaged
animal}") uniformly applied across all test images. However, these methods
typically produce only semantic-level visual prompts, causing SAM to output
coarse semantic masks and thus failing to handle scenarios with multiple
discrete camouflaged instances effectively. To address this critical
limitation, we propose a simple yet powerful \textbf{I}nstance-\textbf{A}ware
\textbf{P}rompting \textbf{F}ramework (IAPF), the first training-free COS
pipeline that explicitly converts a task-generic prompt into fine-grained
instance masks. Specifically, the IAPF comprises three steps: (1) Text Prompt
Generator, utilizing task-generic queries to prompt a Multimodal Large Language
Model (MLLM) for generating image-specific foreground and background tags; (2)
\textbf{Instance Mask Generator}, leveraging Grounding DINO to produce precise
instance-level bounding box prompts, alongside the proposed Single-Foreground
Multi-Background Prompting strategy to sample region-constrained point prompts
within each box, enabling SAM to yield a candidate instance mask; (3)
Self-consistency Instance Mask Voting, which selects the final COS prediction
by identifying the candidate mask most consistent across multiple candidate
instance masks. Extensive evaluations on standard COS benchmarks demonstrate
that the proposed IAPF significantly surpasses existing state-of-the-art
training-free COS methods.

</details>


### [49] [MultiRef: Controllable Image Generation with Multiple Visual References](https://arxiv.org/abs/2508.06905)
*Ruoxi Chen,Dongping Chen,Siyuan Wu,Sinan Wang,Shiyun Lang,Petr Sushko,Gaoyang Jiang,Yao Wan,Ranjay Krishna*

Main category: cs.CV

TL;DR: 论文提出MultiRef-bench评估框架，用于多视觉参考的图像生成任务，并构建数据集MultiRef。实验表明现有模型在多参考条件下表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成框架主要依赖单源输入，而设计师通常从多视觉参考中获取灵感，因此需要开发能整合多参考的生成工具。

Method: 提出MultiRef-bench评估框架和数据集MultiRef，通过RefBlend生成合成样本，并在多个模型上进行实验。

Result: 实验显示，即使是先进模型如OmniGen，在多参考条件下表现有限（合成样本66.6%，真实样本79.0%）。

Conclusion: 研究为开发更灵活、类人的创意工具提供了方向，数据集已公开。

Abstract: Visual designers naturally draw inspiration from multiple visual references,
combining diverse elements and aesthetic principles to create artwork. However,
current image generative frameworks predominantly rely on single-source inputs
-- either text prompts or individual reference images. In this paper, we focus
on the task of controllable image generation using multiple visual references.
We introduce MultiRef-bench, a rigorous evaluation framework comprising 990
synthetic and 1,000 real-world samples that require incorporating visual
content from multiple reference images. The synthetic samples are synthetically
generated through our data engine RefBlend, with 10 reference types and 33
reference combinations. Based on RefBlend, we further construct a dataset
MultiRef containing 38k high-quality images to facilitate further research. Our
experiments across three interleaved image-text models (i.e., OmniGen, ACE, and
Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that
even state-of-the-art systems struggle with multi-reference conditioning, with
the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in
real-world cases on average compared to the golden answer. These findings
provide valuable directions for developing more flexible and human-like
creative tools that can effectively integrate multiple sources of visual
inspiration. The dataset is publicly available at: https://multiref.github.io/.

</details>


### [50] [MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification](https://arxiv.org/abs/2508.06908)
*Jinhao Li,Zijian Chen,Lirong Deng,Changbo Wang,Guangtao Zhai*

Main category: cs.CV

TL;DR: 论文提出MMReID-Bench，首个针对行人重识别的多任务多模态基准，旨在利用多模态大语言模型（MLLMs）解决传统模型在多模态数据上的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 传统行人重识别模型在多模态数据（如RGB、热成像、红外、草图、文本等）上泛化能力差，而现有方法未充分利用MLLMs的推理和跨模态理解能力。

Method: 引入MMReID-Bench基准，包含20,710个多模态查询和库图像，覆盖10种行人重识别任务，全面评估MLLMs的能力。

Result: 实验证明MLLMs在行人重识别中表现优异，但在处理热成像和红外数据时仍有局限。

Conclusion: MMReID-Bench有望推动开发更鲁棒、通用的多模态基础模型用于行人重识别。

Abstract: Person re-identification (ReID) aims to retrieve the images of an interested
person in the gallery images, with wide applications in medical rehabilitation,
abnormal behavior detection, and public security. However, traditional person
ReID models suffer from uni-modal capability, leading to poor generalization
ability in multi-modal data, such as RGB, thermal, infrared, sketch images,
textual descriptions, etc. Recently, the emergence of multi-modal large
language models (MLLMs) shows a promising avenue for addressing this problem.
Despite this potential, existing methods merely regard MLLMs as feature
extractors or caption generators, which do not fully unleash their reasoning,
instruction-following, and cross-modal understanding capabilities. To bridge
this gap, we introduce MMReID-Bench, the first multi-task multi-modal benchmark
specifically designed for person ReID. The MMReID-Bench includes 20,710
multi-modal queries and gallery images covering 10 different person ReID tasks.
Comprehensive experiments demonstrate the remarkable capabilities of MLLMs in
delivering effective and versatile person ReID. Nevertheless, they also have
limitations in handling a few modalities, particularly thermal and infrared
data. We hope MMReID-Bench can facilitate the community to develop more robust
and generalizable multimodal foundation models for person ReID.

</details>


### [51] [Talk2Image: A Multi-Agent System for Multi-Turn Image Generation and Editing](https://arxiv.org/abs/2508.06916)
*Shichao Ma,Yunhe Guo,Jiahao Su,Qihe Huang,Zhengyang Zhou,Yang Wang*

Main category: cs.CV

TL;DR: Talk2Image是一个多代理系统，用于多轮对话中的交互式图像生成和编辑，解决了单代理系统的意图漂移和编辑不连贯问题。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成任务多关注单轮场景，难以处理迭代式、多轮创意任务，且单代理系统易导致意图漂移和编辑不连贯。

Method: Talk2Image整合了三个关键组件：对话历史的意图解析、任务分解与专业代理协作执行、基于多视角评估机制的反馈驱动优化。

Result: 实验表明，Talk2Image在可控性、连贯性和用户满意度上优于现有基线。

Conclusion: Talk2Image通过多代理协作和反馈机制，实现了用户意图的逐步对齐和一致的图像编辑。

Abstract: Text-to-image generation tasks have driven remarkable advances in diverse
media applications, yet most focus on single-turn scenarios and struggle with
iterative, multi-turn creative tasks. Recent dialogue-based systems attempt to
bridge this gap, but their single-agent, sequential paradigm often causes
intention drift and incoherent edits. To address these limitations, we present
Talk2Image, a novel multi-agent system for interactive image generation and
editing in multi-turn dialogue scenarios. Our approach integrates three key
components: intention parsing from dialogue history, task decomposition and
collaborative execution across specialized agents, and feedback-driven
refinement based on a multi-view evaluation mechanism. Talk2Image enables
step-by-step alignment with user intention and consistent image editing.
Experiments demonstrate that Talk2Image outperforms existing baselines in
controllability, coherence, and user satisfaction across iterative image
generation and editing tasks.

</details>


### [52] [AR-GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning](https://arxiv.org/abs/2508.06924)
*Shihao Yuan,Yahui Liu,Yang Yue,Jingyuan Zhang,Wangmeng Zuo,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.CV

TL;DR: 论文提出AR-GRPO方法，将在线强化学习（RL）整合到自回归（AR）图像生成模型中，通过精心设计的奖励函数提升生成图像的质量和人类偏好。


<details>
  <summary>Details</summary>
Motivation: 受强化学习在大型语言模型（LLMs）中成功的启发，研究如何将其应用于自回归图像生成模型，以提升生成图像的多维质量。

Method: 采用Group Relative Policy Optimization（GRPO）算法，设计奖励函数评估生成图像的感知质量、真实性和语义保真度，优化自回归模型的输出。

Result: 在类别条件和文本条件的图像生成任务中，RL增强的框架显著提升了图像质量和人类偏好，各项评估指标均有改善。

Conclusion: 研究表明基于RL的优化在AR图像生成中具有可行性，为可控和高质量图像合成开辟了新途径。

Abstract: Inspired by the success of reinforcement learning (RL) in refining large
language models (LLMs), we propose AR-GRPO, an approach to integrate online RL
training into autoregressive (AR) image generation models. We adapt the Group
Relative Policy Optimization (GRPO) algorithm to refine the vanilla
autoregressive models' outputs by carefully designed reward functions that
evaluate generated images across multiple quality dimensions, including
perceptual quality, realism, and semantic fidelity. We conduct comprehensive
experiments on both class-conditional (i.e., class-to-image) and
text-conditional (i.e., text-to-image) image generation tasks, demonstrating
that our RL-enhanced framework significantly improves both the image quality
and human preference of generated images compared to the standard AR baselines.
Our results show consistent improvements across various evaluation metrics,
establishing the viability of RL-based optimization for AR image generation and
opening new avenues for controllable and high-quality image synthesis. The
source codes and models are available at:
https://github.com/Kwai-Klear/AR-GRPO.

</details>


### [53] [CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-Free Image Editing](https://arxiv.org/abs/2508.06937)
*Weiyan Xie,Han Gao,Didan Deng,Kaican Li,April Hua Liu,Yongxiang Huang,Nevin L. Zhang*

Main category: cs.CV

TL;DR: CannyEdit是一个无需训练的图像编辑框架，通过选择性Canny控制和双提示引导，解决了现有方法在文本遵从性、上下文保真度和编辑无缝性上的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型在区域编辑中难以平衡文本遵从性、上下文保真度和编辑无缝性。

Method: 采用选择性Canny控制（保留未编辑区域细节）和双提示引导（结合局部和全局提示）。

Result: 在真实图像编辑任务中，CannyEdit在文本遵从性和上下文保真度上优于现有方法，用户研究显示其编辑结果更自然。

Conclusion: CannyEdit通过创新方法显著提升了图像编辑的质量和自然度。

Abstract: Recent advances in text-to-image (T2I) models have enabled training-free
regional image editing by leveraging the generative priors of foundation
models. However, existing methods struggle to balance text adherence in edited
regions, context fidelity in unedited areas, and seamless integration of edits.
We introduce CannyEdit, a novel training-free framework that addresses these
challenges through two key innovations: (1) Selective Canny Control, which
masks the structural guidance of Canny ControlNet in user-specified editable
regions while strictly preserving details of the source images in unedited
areas via inversion-phase ControlNet information retention. This enables
precise, text-driven edits without compromising contextual integrity. (2)
Dual-Prompt Guidance, which combines local prompts for object-specific edits
with a global target prompt to maintain coherent scene interactions. On
real-world image editing tasks (addition, replacement, removal), CannyEdit
outperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent
improvement in the balance of text adherence and context fidelity. In terms of
editing seamlessness, user studies reveal only 49.2 percent of general users
and 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited
when paired with real images without edits, versus 76.08 to 89.09 percent for
competitor methods.

</details>


### [54] [SLRTP2025 Sign Language Production Challenge: Methodology, Results, and Future Work](https://arxiv.org/abs/2508.06951)
*Harry Walsh,Ed Fish,Ozge Mercanoglu Sincan,Mohamed Ilyes Lakhal,Richard Bowden,Neil Fox,Bencie Woll,Kepeng Wu,Zecheng Li,Weichao Zhao,Haodong Wang,Wengang Zhou,Houqiang Li,Shengeng Tang,Jiayi He,Xu Wang,Ruobei Zhang,Yaxiong Wang,Lechao Cheng,Meryem Tasyurek,Tugce Kiziltepe,Hacer Yalim Keles*

Main category: cs.CV

TL;DR: 论文介绍了首个手语生成挑战赛，旨在通过标准化评估指标比较不同系统的性能，并提出了获胜方法。


<details>
  <summary>Details</summary>
Motivation: 解决手语生成领域缺乏标准化评估指标的问题，促进系统间的公平比较。

Method: 通过挑战赛形式，使用RWTH-PHOENIX-Weather-2014T数据集和自定义测试集，评估文本到姿势（T2P）翻译架构。

Result: 33名参与者提交了231个解决方案，最佳团队BLEU-1得分为31.40，DTW-MJE为0.0574。

Conclusion: 挑战赛为手语生成领域提供了标准化评估网络和基线，有助于未来研究的比较。

Abstract: Sign Language Production (SLP) is the task of generating sign language video
from spoken language inputs. The field has seen a range of innovations over the
last few years, with the introduction of deep learning-based approaches
providing significant improvements in the realism and naturalness of generated
outputs. However, the lack of standardized evaluation metrics for SLP
approaches hampers meaningful comparisons across different systems. To address
this, we introduce the first Sign Language Production Challenge, held as part
of the third SLRTP Workshop at CVPR 2025. The competition's aims are to
evaluate architectures that translate from spoken language sentences to a
sequence of skeleton poses, known as Text-to-Pose (T2P) translation, over a
range of metrics. For our evaluation data, we use the
RWTH-PHOENIX-Weather-2014T dataset, a German Sign Language - Deutsche
Gebardensprache (DGS) weather broadcast dataset. In addition, we curate a
custom hidden test set from a similar domain of discourse. This paper presents
the challenge design and the winning methodologies. The challenge attracted 33
participants who submitted 231 solutions, with the top-performing team
achieving BLEU-1 scores of 31.40 and DTW-MJE of 0.0574. The winning approach
utilized a retrieval-based framework and a pre-trained language model. As part
of the workshop, we release a standardized evaluation network, including
high-quality skeleton extraction-based keypoints establishing a consistent
baseline for the SLP field, which will enable future researchers to compare
their work against a broader range of methods.

</details>


### [55] [Beyond Frequency: Seeing Subtle Cues Through the Lens of Spatial Decomposition for Fine-Grained Visual Classification](https://arxiv.org/abs/2508.06959)
*Qin Xu,Lili Zhu,Xiaoxia Cheng,Bo Jiang*

Main category: cs.CV

TL;DR: 提出了一种名为SCOPE的新方法，通过动态增强空间域中的细节和语义特征，解决了频率域方法在细粒度视觉分类中的局限性。


<details>
  <summary>Details</summary>
Motivation: 频率域方法基于固定基函数，缺乏对图像内容的适应性和动态调整能力，无法满足不同图像的判别需求。

Method: SCOPE包含两个核心模块：Subtle Detail Extractor (SDE)动态增强浅层细节，Salient Semantic Refiner (SSR)从高层特征中学习语义一致的细化特征。

Result: 在四个流行的细粒度图像分类基准测试中取得了新的最优性能。

Conclusion: SCOPE通过动态结合局部细节和全局语义，突破了频率域方法的限制，提升了分类性能。

Abstract: The crux of resolving fine-grained visual classification (FGVC) lies in
capturing discriminative and class-specific cues that correspond to subtle
visual characteristics. Recently, frequency decomposition/transform based
approaches have attracted considerable interests since its appearing
discriminative cue mining ability. However, the frequency-domain methods are
based on fixed basis functions, lacking adaptability to image content and
unable to dynamically adjust feature extraction according to the discriminative
requirements of different images. To address this, we propose a novel method
for FGVC, named Subtle-Cue Oriented Perception Engine (SCOPE), which adaptively
enhances the representational capability of low-level details and high-level
semantics in the spatial domain, breaking through the limitations of fixed
scales in the frequency domain and improving the flexibility of multi-scale
fusion. The core of SCOPE lies in two modules: the Subtle Detail Extractor
(SDE), which dynamically enhances subtle details such as edges and textures
from shallow features, and the Salient Semantic Refiner (SSR), which learns
semantically coherent and structure-aware refinement features from the
high-level features guided by the enhanced shallow features. The SDE and SSR
are cascaded stage-by-stage to progressively combine local details with global
semantics. Extensive experiments demonstrate that our method achieves new
state-of-the-art on four popular fine-grained image classification benchmarks.

</details>


### [56] [Adversarial Video Promotion Against Text-to-Video Retrieval](https://arxiv.org/abs/2508.06964)
*Qiwei Tian,Chenhao Lin,Zhengyu Zhao,Qian Li,Shuai Liu,Chao Shen*

Main category: cs.CV

TL;DR: 该论文首次提出了一种针对文本到视频检索（T2VR）的攻击方法ViPro，旨在通过对抗性手段提升视频排名，并提出了模态细化（MoRe）以提高黑盒迁移性。实验表明ViPro在白/灰/黑盒设置下均优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有T2VR攻击主要关注降低视频排名，而提升视频排名的攻击尚未充分探索。这种攻击可能带来更大的经济利益和（错误）信息传播影响。

Method: 提出了Video Promotion攻击（ViPro）和模态细化（MoRe），以捕捉视觉与文本模态间的细粒度交互，增强黑盒迁移性。

Result: ViPro在白/灰/黑盒设置下平均分别超越基线方法30%、10%和4%。实验覆盖了多种模型、数据集和场景。

Conclusion: 论文揭示了T2VR的潜在漏洞，分析了攻击的上下限，并提供了对抗策略的见解。代码将公开。

Abstract: Thanks to the development of cross-modal models, text-to-video retrieval
(T2VR) is advancing rapidly, but its robustness remains largely unexamined.
Existing attacks against T2VR are designed to push videos away from queries,
i.e., suppressing the ranks of videos, while the attacks that pull videos
towards selected queries, i.e., promoting the ranks of videos, remain largely
unexplored. These attacks can be more impactful as attackers may gain more
views/clicks for financial benefits and widespread (mis)information. To this
end, we pioneer the first attack against T2VR to promote videos adversarially,
dubbed the Video Promotion attack (ViPro). We further propose Modal Refinement
(MoRe) to capture the finer-grained, intricate interaction between visual and
textual modalities to enhance black-box transferability. Comprehensive
experiments cover 2 existing baselines, 3 leading T2VR models, 3 prevailing
datasets with over 10k videos, evaluated under 3 scenarios. All experiments are
conducted in a multi-target setting to reflect realistic scenarios where
attackers seek to promote the video regarding multiple queries simultaneously.
We also evaluated our attacks for defences and imperceptibility. Overall, ViPro
surpasses other baselines by over $30/10/4\%$ for white/grey/black-box settings
on average. Our work highlights an overlooked vulnerability, provides a
qualitative analysis on the upper/lower bound of our attacks, and offers
insights into potential counterplays. Code will be publicly available at
https://github.com/michaeltian108/ViPro.

</details>


### [57] [Evaluating Fisheye-Compatible 3D Gaussian Splatting Methods on Real Images Beyond 180 Degree Field of View](https://arxiv.org/abs/2508.06968)
*Ulas Gunes,Matias Turkulainen,Juho Kannala,Esa Rahtu*

Main category: cs.CV

TL;DR: 首次评估了基于鱼眼镜头的3D高斯泼溅方法Fisheye-GS和3DGUT在真实图像上的表现，覆盖了200度视野的室内外场景，并分析了极端畸变的处理能力。


<details>
  <summary>Details</summary>
Motivation: 研究鱼眼镜头在3D重建中的实际应用，解决传统SfM初始化在强畸变下的失败问题。

Method: 通过不同视野（200度、160度、120度）评估性能，提出基于UniK3D预测的深度初始化策略。

Result: Fisheye-GS在160度视野表现最佳，3DGUT在全200度视野下保持稳定；UniK3D策略在困难场景中表现优异。

Conclusion: 鱼眼3DGS方法在稀疏且畸变严重的图像输入下具有实际应用潜力。

Abstract: We present the first evaluation of fisheye-based 3D Gaussian Splatting
methods, Fisheye-GS and 3DGUT, on real images with fields of view exceeding 180
degree. Our study covers both indoor and outdoor scenes captured with 200
degree fisheye cameras and analyzes how each method handles extreme distortion
in real world settings. We evaluate performance under varying fields of view
(200 degree, 160 degree, and 120 degree) to study the tradeoff between
peripheral distortion and spatial coverage. Fisheye-GS benefits from field of
view (FoV) reduction, particularly at 160 degree, while 3DGUT remains stable
across all settings and maintains high perceptual quality at the full 200
degree view. To address the limitations of SfM-based initialization, which
often fails under strong distortion, we also propose a depth-based strategy
using UniK3D predictions from only 2-3 fisheye images per scene. Although
UniK3D is not trained on real fisheye data, it produces dense point clouds that
enable reconstruction quality on par with SfM, even in difficult scenes with
fog, glare, or sky. Our results highlight the practical viability of
fisheye-based 3DGS methods for wide-angle 3D reconstruction from sparse and
distortion-heavy image inputs.

</details>


### [58] [ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting](https://arxiv.org/abs/2508.07089)
*Sandro Papais,Letian Wang,Brian Cheong,Steven L. Waslander*

Main category: cs.CV

TL;DR: ForeSight是一种用于自动驾驶车辆3D感知的新型联合检测与预测框架，通过多任务流式学习和双向学习提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法将检测与预测作为独立任务，无法充分利用时序信息。

Method: 采用多任务流式学习和双向学习，结合检测与预测的查询记忆和信息传播。

Result: 在nuScenes数据集上，EPA达到54.9%，优于之前方法9.3%，同时在mAP和minADE上表现最佳。

Conclusion: ForeSight通过无跟踪模型高效处理多帧序列，显著提升了检测与预测性能。

Abstract: We introduce ForeSight, a novel joint detection and forecasting framework for
vision-based 3D perception in autonomous vehicles. Traditional approaches treat
detection and forecasting as separate sequential tasks, limiting their ability
to leverage temporal cues. ForeSight addresses this limitation with a
multi-task streaming and bidirectional learning approach, allowing detection
and forecasting to share query memory and propagate information seamlessly. The
forecast-aware detection transformer enhances spatial reasoning by integrating
trajectory predictions from a multiple hypothesis forecast memory queue, while
the streaming forecast transformer improves temporal consistency using past
forecasts and refined detections. Unlike tracking-based methods, ForeSight
eliminates the need for explicit object association, reducing error propagation
with a tracking-free model that efficiently scales across multi-frame
sequences. Experiments on the nuScenes dataset show that ForeSight achieves
state-of-the-art performance, achieving an EPA of 54.9%, surpassing previous
methods by 9.3%, while also attaining the best mAP and minADE among multi-view
detection and forecasting models.

</details>


### [59] [WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering](https://arxiv.org/abs/2508.06982)
*Yixin Zhu,Zuoliang Zhu,Miloš Hašan,Jian Yang,Jin Xie,Beibei Wang*

Main category: cs.CV

TL;DR: WeatherDiffusion是一个基于扩散模型的框架，用于自动驾驶场景中的正向和逆向渲染，支持天气和光照编辑，并通过实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 复杂天气和光照条件对自动驾驶场景的理解和重建提出了挑战，现有扩散模型难以控制和缺乏鲁棒性。

Method: 提出WeatherDiffusion框架，利用文本描述引导的预测本征图实现高质量逆向渲染，并引入Intrinsic map-aware attention（MAA）。

Result: 在多个基准测试中优于现有方法，并在下游任务（如目标检测和图像分割）中表现出显著价值。

Conclusion: WeatherDiffusion为自动驾驶场景中的天气和光照处理提供了高效且可控的解决方案。

Abstract: Forward and inverse rendering have emerged as key techniques for enabling
understanding and reconstruction in the context of autonomous driving (AD).
However, complex weather and illumination pose great challenges to this task.
The emergence of large diffusion models has shown promise in achieving
reasonable results through learning from 2D priors, but these models are
difficult to control and lack robustness. In this paper, we introduce
WeatherDiffusion, a diffusion-based framework for forward and inverse rendering
on AD scenes with various weather and lighting conditions. Our method enables
authentic estimation of material properties, scene geometry, and lighting, and
further supports controllable weather and illumination editing through the use
of predicted intrinsic maps guided by text descriptions. We observe that
different intrinsic maps should correspond to different regions of the original
image. Based on this observation, we propose Intrinsic map-aware attention
(MAA) to enable high-quality inverse rendering. Additionally, we introduce a
synthetic dataset (\ie WeatherSynthetic) and a real-world dataset (\ie
WeatherReal) for forward and inverse rendering on AD scenes with diverse
weather and lighting. Extensive experiments show that our WeatherDiffusion
outperforms state-of-the-art methods on several benchmarks. Moreover, our
method demonstrates significant value in downstream tasks for AD, enhancing the
robustness of object detection and image segmentation in challenging weather
scenarios.

</details>


### [60] [TADoc: Robust Time-Aware Document Image Dewarping](https://arxiv.org/abs/2508.06988)
*Fangmin Zhao,Weichao Zeng,Zhenhang Li,Dongbao Yang,Yu Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种动态处理文档图像去扭曲的方法TADoc，并引入新评价指标DLS，实验证明其高效性。


<details>
  <summary>Details</summary>
Motivation: 随着数字经济和在线工作的兴起，便携设备拍摄的文档图像去扭曲变得重要，但现有方法对复杂结构和高度变形效果不佳。

Method: 将去扭曲任务建模为动态过程，设计轻量级框架TADoc，并提出新评价指标DLS。

Result: 实验表明TADoc在多种文档类型和变形程度下表现优越，具有强鲁棒性。

Conclusion: 动态建模和新评价指标显著提升了文档去扭曲的效果和评估全面性。

Abstract: Flattening curved, wrinkled, and rotated document images captured by portable
photographing devices, termed document image dewarping, has become an
increasingly important task with the rise of digital economy and online
working. Although many methods have been proposed recently, they often struggle
to achieve satisfactory results when confronted with intricate document
structures and higher degrees of deformation in real-world scenarios. Our main
insight is that, unlike other document restoration tasks (e.g., deblurring),
dewarping in real physical scenes is a progressive motion rather than a
one-step transformation. Based on this, we have undertaken two key initiatives.
Firstly, we reformulate this task, modeling it for the first time as a dynamic
process that encompasses a series of intermediate states. Secondly, we design a
lightweight framework called TADoc (Time-Aware Document Dewarping Network) to
address the geometric distortion of document images. In addition, due to the
inadequacy of OCR metrics for document images containing sparse text, the
comprehensiveness of evaluation is insufficient. To address this shortcoming,
we propose a new metric -- DLS (Document Layout Similarity) -- to evaluate the
effectiveness of document dewarping in downstream tasks. Extensive experiments
and in-depth evaluations have been conducted and the results indicate that our
model possesses strong robustness, achieving superiority on several benchmarks
with different document types and degrees of distortion.

</details>


### [61] [AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning](https://arxiv.org/abs/2508.07626)
*Dejie Yang,Zijing Zhao,Yang Liu*

Main category: cs.CV

TL;DR: 论文提出AR-VRM方法，通过模仿人类动作的关键点来提升视觉机器人操作的泛化能力，尤其在数据稀缺时表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法因依赖与机器人任务不匹配的网络数据或隐式训练方式，泛化能力有限。

Method: 利用人类动作视频数据，通过关键点视觉语言模型预训练和类比推理映射，模仿人类动作。

Result: 在CALVIN基准测试和真实实验中表现领先，尤其在少样本场景中大幅超越先前方法。

Conclusion: 通过显式模仿人类动作，AR-VRM在数据稀缺时显著提升了机器人操作的性能。

Abstract: Visual Robot Manipulation (VRM) aims to enable a robot to follow natural
language instructions based on robot states and visual observations, and
therefore requires costly multi-modal data. To compensate for the deficiency of
robot data, existing approaches have employed vision-language pretraining with
large-scale data. However, they either utilize web data that differs from
robotic tasks, or train the model in an implicit way (e.g., predicting future
frames at the pixel level), thus showing limited generalization ability under
insufficient robot data. In this paper, we propose to learn from large-scale
human action video datasets in an explicit way (i.e., imitating human actions
from hand keypoints), introducing Visual Robot Manipulation with Analogical
Reasoning (AR-VRM). To acquire action knowledge explicitly from human action
videos, we propose a keypoint Vision-Language Model (VLM) pretraining scheme,
enabling the VLM to learn human action knowledge and directly predict human
hand keypoints. During fine-tuning on robot data, to facilitate the robotic arm
in imitating the action patterns of human motions, we first retrieve human
action videos that perform similar manipulation tasks and have similar
historical observations , and then learn the Analogical Reasoning (AR) map
between human hand keypoints and robot components. Taking advantage of focusing
on action keypoints instead of irrelevant visual cues, our method achieves
leading performance on the CALVIN benchmark {and real-world experiments}. In
few-shot scenarios, our AR-VRM outperforms previous methods by large margins ,
underscoring the effectiveness of explicitly imitating human actions under data
scarcity.

</details>


### [62] [OctreeNCA: Single-Pass 184 MP Segmentation on Consumer Hardware](https://arxiv.org/abs/2508.06993)
*Nick Lemke,John Kalkhof,Niklas Babendererde,Anirban Mukhopadhyay*

Main category: cs.CV

TL;DR: OctreeNCA通过八叉树数据结构扩展NCA的邻域定义，实现高效全局知识传递，显著降低VRAM消耗，提升高分辨率图像和视频分割速度。


<details>
  <summary>Details</summary>
Motivation: 医学应用中，大尺寸输入（如MRI、病理切片或手术视频）的分割需要全局上下文，但现有模型（如UNet或Vision Transformers）因VRAM限制而无法一次性处理，导致全局一致性和速度下降。

Method: 提出OctreeNCA，利用八叉树数据结构扩展NCA的邻域定义，并实现CUDA优化的NCA推理函数，以减少VRAM占用和提升速度。

Result: OctreeNCA在高分辨率图像和视频分割中占用VRAM比UNet少90%，可一次性处理184兆像素病理切片或1分钟手术视频。

Conclusion: OctreeNCA通过改进NCA的全局信息传递和优化实现，解决了大尺寸医学图像分割的VRAM瓶颈问题。

Abstract: Medical applications demand segmentation of large inputs, like prostate MRIs,
pathology slices, or videos of surgery. These inputs should ideally be inferred
at once to provide the model with proper spatial or temporal context. When
segmenting large inputs, the VRAM consumption of the GPU becomes the
bottleneck. Architectures like UNets or Vision Transformers scale very poorly
in VRAM consumption, resulting in patch- or frame-wise approaches that
compromise global consistency and inference speed. The lightweight Neural
Cellular Automaton (NCA) is a bio-inspired model that is by construction
size-invariant. However, due to its local-only communication rules, it lacks
global knowledge. We propose OctreeNCA by generalizing the neighborhood
definition using an octree data structure. Our generalized neighborhood
definition enables the efficient traversal of global knowledge. Since deep
learning frameworks are mainly developed for large multi-layer networks, their
implementation does not fully leverage the advantages of NCAs. We implement an
NCA inference function in CUDA that further reduces VRAM demands and increases
inference speed. Our OctreeNCA segments high-resolution images and videos
quickly while occupying 90% less VRAM than a UNet during evaluation. This
allows us to segment 184 Megapixel pathology slices or 1-minute surgical videos
at once.

</details>


### [63] [Multi-view Normal and Distance Guidance Gaussian Splatting for Surface Reconstruction](https://arxiv.org/abs/2508.07701)
*Bo Jia,Yanan Guo,Ying Chang,Benkui Zhang,Ying Xie,Kangning Du,Lin Cao*

Main category: cs.CV

TL;DR: 3D高斯泼溅（3DGS）在多视角场景中通过距离和法线约束优化几何重建，显著提升表面重建能力。


<details>
  <summary>Details</summary>
Motivation: 解决3DGS在多视角下因高斯法线对齐导致的几何偏差问题。

Method: 设计多视角法线和距离引导的高斯泼溅方法，包括距离重投影正则化模块和法线增强模块。

Result: 实验结果表明，该方法在定量和定性评估中均优于基线。

Conclusion: 该方法显著提升了3DGS的表面重建能力。

Abstract: 3D Gaussian Splatting (3DGS) achieves remarkable results in the field of
surface reconstruction. However, when Gaussian normal vectors are aligned
within the single-view projection plane, while the geometry appears reasonable
in the current view, biases may emerge upon switching to nearby views. To
address the distance and global matching challenges in multi-view scenes, we
design multi-view normal and distance-guided Gaussian splatting. This method
achieves geometric depth unification and high-accuracy reconstruction by
constraining nearby depth maps and aligning 3D normals. Specifically, for the
reconstruction of small indoor and outdoor scenes, we propose a multi-view
distance reprojection regularization module that achieves multi-view Gaussian
alignment by computing the distance loss between two nearby views and the same
Gaussian surface. Additionally, we develop a multi-view normal enhancement
module, which ensures consistency across views by matching the normals of pixel
points in nearby views and calculating the loss. Extensive experimental results
demonstrate that our method outperforms the baseline in both quantitative and
qualitative evaluations, significantly enhancing the surface reconstruction
capability of 3DGS.

</details>


### [64] [S2-UniSeg: Fast Universal Agglomerative Pooling for Scalable Segment Anything without Supervision](https://arxiv.org/abs/2508.06995)
*Huihui Xu,Jin Ye,Hongqiu Wang,Changkai Ji,Jiashi Lin,Ming Hu,Ziyan Huang,Ying Chen,Chenglong Ma,Tianbin Li,Lihao Liu,Junjun He,Lei Zhu*

Main category: cs.CV

TL;DR: 提出了一种快速伪掩码生成算法UniAP和自监督通用分割模型S2-UniSeg，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有自监督图像分割模型多阶段预训练耗时且优化不连续的问题。

Method: 使用UniAP快速生成伪掩码，结合QuerySD任务和师生框架进行连续预训练。

Result: 在多个基准测试中显著超越现有方法，如COCO上AP提升6.9。

Conclusion: S2-UniSeg在性能和扩展性上均表现出色，代码和模型已开源。

Abstract: Recent self-supervised image segmentation models have achieved promising
performance on semantic segmentation and class-agnostic instance segmentation.
However, their pretraining schedule is multi-stage, requiring a time-consuming
pseudo-masks generation process between each training epoch. This
time-consuming offline process not only makes it difficult to scale with
training dataset size, but also leads to sub-optimal solutions due to its
discontinuous optimization routine. To solve these, we first present a novel
pseudo-mask algorithm, Fast Universal Agglomerative Pooling (UniAP). Each layer
of UniAP can identify groups of similar nodes in parallel, allowing to generate
both semantic-level and instance-level and multi-granular pseudo-masks within
ens of milliseconds for one image. Based on the fast UniAP, we propose the
Scalable Self-Supervised Universal Segmentation (S2-UniSeg), which employs a
student and a momentum teacher for continuous pretraining. A novel
segmentation-oriented pretext task, Query-wise Self-Distillation (QuerySD), is
proposed to pretrain S2-UniSeg to learn the local-to-global correspondences.
Under the same setting, S2-UniSeg outperforms the SOTA UnSAM model, achieving
notable improvements of AP+6.9 on COCO, AR+11.1 on UVO, PixelAcc+4.5 on
COCOStuff-27, RQ+8.0 on Cityscapes. After scaling up to a larger 2M-image
subset of SA-1B, S2-UniSeg further achieves performance gains on all four
benchmarks. Our code and pretrained models are available at
https://github.com/bio-mlhui/S2-UniSeg

</details>


### [65] [HiMat: DiT-based Ultra-High Resolution SVBRDF Generation](https://arxiv.org/abs/2508.07011)
*Zixiong Wang,Jian Yang,Yiwei Hu,Milos Hasan,Beibei Wang*

Main category: cs.CV

TL;DR: HiMat是一个高效扩散框架，用于生成4K分辨率的SVBRDF，通过轻量级CrossStitch模块保持多图一致性。


<details>
  <summary>Details</summary>
Motivation: 高分辨率文本到图像生成模型的兴起为SVBRDF生成提供了机会，但如何高效生成多图对齐的SVBRDF仍具挑战。

Method: 提出HiMat框架，采用轻量级CrossStitch模块捕获多图依赖关系，不改变DiT主干结构。

Result: 实验证明HiMat能生成结构一致且高频细节丰富的4K SVBRDF，并适用于其他任务如本征分解。

Conclusion: HiMat为高效生成高质量SVBRDF提供了一种轻量级解决方案，具有广泛适用性。

Abstract: Creating highly detailed SVBRDFs is essential for 3D content creation. The
rise of high-resolution text-to-image generative models, based on diffusion
transformers (DiT), suggests an opportunity to finetune them for this task.
However, retargeting the models to produce multiple aligned SVBRDF maps instead
of just RGB images, while achieving high efficiency and ensuring consistency
across different maps, remains a challenge. In this paper, we introduce HiMat:
a memory- and computation-efficient diffusion-based framework capable of
generating native 4K-resolution SVBRDFs. A key challenge we address is
maintaining consistency across different maps in a lightweight manner, without
relying on training new VAEs or significantly altering the DiT backbone (which
would damage its prior capabilities). To tackle this, we introduce the
CrossStitch module, a lightweight convolutional module that captures inter-map
dependencies through localized operations. Its weights are initialized such
that the DiT backbone operation is unchanged before finetuning starts. HiMat
enables generation with strong structural coherence and high-frequency details.
Results with a large set of text prompts demonstrate the effectiveness of our
approach for 4K SVBRDF generation. Further experiments suggest generalization
to tasks such as intrinsic decomposition.

</details>


### [66] [TerraMAE: Learning Spatial-Spectral Representations from Hyperspectral Earth Observation Data via Adaptive Masked Autoencoders](https://arxiv.org/abs/2508.07020)
*Tanjim Bin Faruk,Abdul Matin,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.CV

TL;DR: TerraMAE是一种专为高光谱图像设计的自监督编码框架，通过自适应通道分组和增强的重建损失函数，显著提升了空间-光谱信息的表示能力。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像（200+波段）的复杂空间-光谱相关性难以被现有自监督方法（如Masked Autoencoders）有效利用，需要一种专门的方法。

Method: TerraMAE采用自适应通道分组策略（基于反射率统计特性）和增强的重建损失函数（结合空间与光谱质量指标）。

Result: TerraMAE在高保真图像重建中表现出卓越的空间-光谱信息保留能力，并在作物识别、土地覆盖分类和土壤质地预测等任务中表现优异。

Conclusion: TerraMAE为高光谱图像提供了高效的自监督表示学习方法，适用于多种地理空间分析任务。

Abstract: Hyperspectral satellite imagery offers sub-30 m views of Earth in hundreds of
contiguous spectral bands, enabling fine-grained mapping of soils, crops, and
land cover. While self-supervised Masked Autoencoders excel on RGB and low-band
multispectral data, they struggle to exploit the intricate spatial-spectral
correlations in 200+ band hyperspectral images. We introduce TerraMAE, a novel
HSI encoding framework specifically designed to learn highly representative
spatial-spectral embeddings for diverse geospatial analyses. TerraMAE features
an adaptive channel grouping strategy, based on statistical reflectance
properties to capture spectral similarities, and an enhanced reconstruction
loss function that incorporates spatial and spectral quality metrics. We
demonstrate TerraMAE's effectiveness through superior spatial-spectral
information preservation in high-fidelity image reconstruction. Furthermore, we
validate its practical utility and the quality of its learned representations
through strong performance on three key downstream geospatial tasks: crop
identification, land cover classification, and soil texture prediction.

</details>


### [67] [DocRefine: An Intelligent Framework for Scientific Document Understanding and Content Optimization based on Multimodal Large Model Agents](https://arxiv.org/abs/2508.07021)
*Kun Qian,Wenjie Li,Tianyu Sun,Wenhong Wang,Wenhan Luo*

Main category: cs.CV

TL;DR: DocRefine是一个基于多智能体系统的框架，用于科学PDF文档的智能理解、内容优化和自动摘要，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 科学文献的快速增长需要高效工具处理复杂布局和多模态内容，传统方法和直接应用LLMs/LVLMs存在不足。

Method: DocRefine采用六种协作智能体（如布局分析、内容理解等），结合闭环反馈架构，利用先进LVLMs（如GPT-4o）。

Result: 在DocEditBench数据集上，DocRefine在语义一致性（86.7%）、布局保真度（93.9%）和指令遵循率（85.0%）上表现优异。

Conclusion: DocRefine在复杂多模态文档处理中表现出色，推动了科学文档自动处理的进步。

Abstract: The exponential growth of scientific literature in PDF format necessitates
advanced tools for efficient and accurate document understanding,
summarization, and content optimization. Traditional methods fall short in
handling complex layouts and multimodal content, while direct application of
Large Language Models (LLMs) and Vision-Language Large Models (LVLMs) lacks
precision and control for intricate editing tasks. This paper introduces
DocRefine, an innovative framework designed for intelligent understanding,
content refinement, and automated summarization of scientific PDF documents,
driven by natural language instructions. DocRefine leverages the power of
advanced LVLMs (e.g., GPT-4o) by orchestrating a sophisticated multi-agent
system comprising six specialized and collaborative agents: Layout & Structure
Analysis, Multimodal Content Understanding, Instruction Decomposition, Content
Refinement, Summarization & Generation, and Fidelity & Consistency
Verification. This closed-loop feedback architecture ensures high semantic
accuracy and visual fidelity. Evaluated on the comprehensive DocEditBench
dataset, DocRefine consistently outperforms state-of-the-art baselines across
various tasks, achieving overall scores of 86.7% for Semantic Consistency Score
(SCS), 93.9% for Layout Fidelity Index (LFI), and 85.0% for Instruction
Adherence Rate (IAR). These results demonstrate DocRefine's superior capability
in handling complex multimodal document editing, preserving semantic integrity,
and maintaining visual consistency, marking a significant advancement in
automated scientific document processing.

</details>


### [68] [MV-CoRe: Multimodal Visual-Conceptual Reasoning for Complex Visual Question Answering](https://arxiv.org/abs/2508.07023)
*Jingwei Peng,Jiehao Chen,Mateo Alejandro Rojas,Meilin Zhang*

Main category: cs.CV

TL;DR: MV-CoRe模型通过深度融合多模态视觉和语言信息，显著提升了复杂视觉问答任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型依赖高层全局特征，难以应对复杂视觉问答任务的多模态推理和外部知识整合需求。

Method: MV-CoRe整合了预训练视觉和语言模型的全局嵌入与细粒度语义视觉特征，并通过多模态融合Transformer实现跨模态注意力。

Result: 在GQA、A-OKVQA和OKVQA等基准测试中，MV-CoRe表现优于现有模型，GQA准确率达77.5%。

Conclusion: MV-CoRe通过对象和场景图特征的深度融合，展示了强大的视觉和概念理解能力。

Abstract: Complex Visual Question Answering (Complex VQA) tasks, which demand
sophisticated multi-modal reasoning and external knowledge integration, present
significant challenges for existing large vision-language models (LVLMs) often
limited by their reliance on high-level global features. To address this, we
propose MV-CoRe (Multimodal Visual-Conceptual Reasoning), a novel model
designed to enhance Complex VQA performance through the deep fusion of diverse
visual and linguistic information. MV-CoRe meticulously integrates global
embeddings from pre-trained Vision Large Models (VLMs) and Language Large
Models (LLMs) with fine-grained semantic-aware visual features, including
object detection characteristics and scene graph representations. An innovative
Multimodal Fusion Transformer then processes and deeply integrates these
diverse feature sets, enabling rich cross-modal attention and facilitating
complex reasoning. We evaluate MV-CoRe on challenging Complex VQA benchmarks,
including GQA, A-OKVQA, and OKVQA, after training on VQAv2. Our experimental
results demonstrate that MV-CoRe consistently outperforms established LVLM
baselines, achieving an overall accuracy of 77.5% on GQA. Ablation studies
confirm the critical contribution of both object and scene graph features, and
human evaluations further validate MV-CoRe's superior factual correctness and
reasoning depth, underscoring its robust capabilities for deep visual and
conceptual understanding.

</details>


### [69] [Large Language Model Evaluated Stand-alone Attention-Assisted Graph Neural Network with Spatial and Structural Information Interaction for Precise Endoscopic Image Segmentation](https://arxiv.org/abs/2508.07028)
*Juntong Fan,Shuyi Fan,Debesh Jha,Changsheng Fang,Tieyong Zeng,Hengyong Yu,Dayang Wang*

Main category: cs.CV

TL;DR: FOCUS-Med提出了一种结合空间和结构图的注意力机制的内窥镜息肉分割方法，通过双图卷积网络和自注意力机制提升分割效果，并在公开基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 内窥镜图像中息肉分割对早期结直肠癌检测至关重要，但由于低对比度、高光反射和模糊边界等问题，该任务具有挑战性。

Method: FOCUS-Med采用双图卷积网络（Dual-GCN）捕捉空间和拓扑结构依赖，结合位置融合的自注意力机制增强全局上下文，并使用加权快速归一化融合策略进行多尺度聚合。

Result: 在公开基准测试中，FOCUS-Med在五项关键指标上达到最优性能。

Conclusion: FOCUS-Med展示了其在AI辅助结肠镜检查中的有效性和临床潜力。

Abstract: Accurate endoscopic image segmentation on the polyps is critical for early
colorectal cancer detection. However, this task remains challenging due to low
contrast with surrounding mucosa, specular highlights, and indistinct
boundaries. To address these challenges, we propose FOCUS-Med, which stands for
Fusion of spatial and structural graph with attentional context-aware polyp
segmentation in endoscopic medical imaging. FOCUS-Med integrates a Dual Graph
Convolutional Network (Dual-GCN) module to capture contextual spatial and
topological structural dependencies. This graph-based representation enables
the model to better distinguish polyps from background tissues by leveraging
topological cues and spatial connectivity, which are often obscured in raw
image intensities. It enhances the model's ability to preserve boundaries and
delineate complex shapes typical of polyps. In addition, a location-fused
stand-alone self-attention is employed to strengthen global context
integration. To bridge the semantic gap between encoder-decoder layers, we
incorporate a trainable weighted fast normalized fusion strategy for efficient
multi-scale aggregation. Notably, we are the first to introduce the use of a
Large Language Model (LLM) to provide detailed qualitative evaluations of
segmentation quality. Extensive experiments on public benchmarks demonstrate
that FOCUS-Med achieves state-of-the-art performance across five key metrics,
underscoring its effectiveness and clinical potential for AI-assisted
colonoscopy.

</details>


### [70] [TeSO: Representing and Compressing 3D Point Cloud Scenes with Textured Surfel Octree](https://arxiv.org/abs/2508.07083)
*Yueyu Hu,Ran Gong,Tingyu Fan,Yao Wang*

Main category: cs.CV

TL;DR: 提出了一种名为Textured Surfel Octree (TeSO)的新型3D表示方法，通过结合八叉树结构和纹理贴图，解决了现有3D表示在渲染质量、表面定义和压缩性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有3D表示（如点云、网格和3D高斯）在渲染质量、表面定义和压缩性方面存在不足，需要一种更高效的3D表示方法。

Method: TeSO将3D场景表示为八叉树组织的立方体边界曲面元，每个曲面元关联一个纹理贴图，并通过压缩方案高效编码几何和纹理信息。

Result: TeSO在较低比特率下实现了比点云和3D高斯基线更高的渲染质量。

Conclusion: TeSO是一种高效且高质量的3D表示方法，适用于3D流媒体和AR/VR应用。

Abstract: 3D visual content streaming is a key technology for emerging 3D telepresence
and AR/VR applications. One fundamental element underlying the technology is a
versatile 3D representation that is capable of producing high-quality renders
and can be efficiently compressed at the same time. Existing 3D representations
like point clouds, meshes and 3D Gaussians each have limitations in terms of
rendering quality, surface definition, and compressibility. In this paper, we
present the Textured Surfel Octree (TeSO), a novel 3D representation that is
built from point clouds but addresses the aforementioned limitations. It
represents a 3D scene as cube-bounded surfels organized on an octree, where
each surfel is further associated with a texture patch. By approximating a
smooth surface with a large surfel at a coarser level of the octree, it reduces
the number of primitives required to represent the 3D scene, and yet retains
the high-frequency texture details through the texture map attached to each
surfel. We further propose a compression scheme to encode the geometry and
texture efficiently, leveraging the octree structure. The proposed textured
surfel octree combined with the compression scheme achieves higher rendering
quality at lower bit-rates compared to multiple point cloud and 3D
Gaussian-based baselines.

</details>


### [71] [Communication-Efficient Multi-Agent 3D Detection via Hybrid Collaboration](https://arxiv.org/abs/2508.07092)
*Yue Hu,Juntong Peng,Yunqiao Yang,Siheng Chen*

Main category: cs.CV

TL;DR: 提出了一种名为HyComm的通信高效LiDAR协作3D检测系统，通过自适应整合紧凑的感知输出和丰富的原始观测数据，优化了检测性能与通信带宽的权衡。


<details>
  <summary>Details</summary>
Motivation: 解决协作3D检测中检测性能与通信带宽之间的瓶颈问题。

Method: 提出混合协作方法，自适应整合两种通信消息（感知输出和原始观测），并优先处理关键数据。

Result: 在DAIR-V2X和OPV2V数据集上，HyComm显著优于现有方法，通信量降低2006倍以上，AP50性能仍优于Where2comm。

Conclusion: HyComm通过自适应消息选择和标准化数据格式，实现了高效的协作检测，适用于多种通信场景和代理配置。

Abstract: Collaborative 3D detection can substantially boost detection performance by
allowing agents to exchange complementary information. It inherently results in
a fundamental trade-off between detection performance and communication
bandwidth. To tackle this bottleneck issue, we propose a novel hybrid
collaboration that adaptively integrates two types of communication messages:
perceptual outputs, which are compact, and raw observations, which offer richer
information. This approach focuses on two key aspects: i) integrating
complementary information from two message types and ii) prioritizing the most
critical data within each type. By adaptively selecting the most critical set
of messages, it ensures optimal perceptual information and adaptability,
effectively meeting the demands of diverse communication scenarios.Building on
this hybrid collaboration, we present \texttt{HyComm}, a
communication-efficient LiDAR-based collaborative 3D detection system.
\texttt{HyComm} boasts two main benefits: i) it facilitates adaptable
compression rates for messages, addressing various communication requirements,
and ii) it uses standardized data formats for messages. This ensures they are
independent of specific detection models, fostering adaptability across
different agent configurations. To evaluate HyComm, we conduct experiments on
both real-world and simulation datasets: DAIR-V2X and OPV2V. HyComm
consistently outperforms previous methods and achieves a superior
performance-bandwidth trade-off regardless of whether agents use the same or
varied detection models. It achieves a lower communication volume of more than
2,006$\times$ and still outperforms Where2comm on DAIR-V2X in terms of AP50.
The related code will be released.

</details>


### [72] [AugLift: Boosting Generalization in Lifting-based 3D Human Pose Estimation](https://arxiv.org/abs/2508.07112)
*Nikolai Warner,Wenjin Zhang,Irfan Essa,Apaar Sadhwani*

Main category: cs.CV

TL;DR: AugLift通过增强2D关键点输入（加入置信度和深度估计）提升3D人体姿态估计的泛化能力，无需额外数据或传感器。


<details>
  <summary>Details</summary>
Motivation: 现有基于提升的3D姿态估计方法在新数据集和真实场景中泛化能力差，AugLift旨在解决这一问题。

Method: AugLift在标准2D关键点坐标基础上，稀疏地加入置信度$c$和深度估计$d$，利用预训练模型计算这些信号。

Result: 实验表明，AugLift在未见数据集上平均提升10.1%性能，在分布内数据上提升4.0%。

Conclusion: AugLift是一种模块化方法，显著提升基于提升的姿态估计模型的泛化能力。

Abstract: Lifting-based methods for 3D Human Pose Estimation (HPE), which predict 3D
poses from detected 2D keypoints, often generalize poorly to new datasets and
real-world settings. To address this, we propose \emph{AugLift}, a simple yet
effective reformulation of the standard lifting pipeline that significantly
improves generalization performance without requiring additional data
collection or sensors. AugLift sparsely enriches the standard input -- the 2D
keypoint coordinates $(x, y)$ -- by augmenting it with a keypoint detection
confidence score $c$ and a corresponding depth estimate $d$. These additional
signals are computed from the image using off-the-shelf, pre-trained models
(e.g., for monocular depth estimation), thereby inheriting their strong
generalization capabilities. Importantly, AugLift serves as a modular add-on
and can be readily integrated into existing lifting architectures.
  Our extensive experiments across four datasets demonstrate that AugLift
boosts cross-dataset performance on unseen datasets by an average of $10.1\%$,
while also improving in-distribution performance by $4.0\%$. These gains are
consistent across various lifting architectures, highlighting the robustness of
our method. Our analysis suggests that these sparse, keypoint-aligned cues
provide robust frame-level context, offering a practical way to significantly
improve the generalization of any lifting-based pose estimation model. Code
will be made publicly available.

</details>


### [73] [Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays](https://arxiv.org/abs/2508.07128)
*Gregory Schuit,Denis Parra,Cecilia Besa*

Main category: cs.CV

TL;DR: 生成模型在医学影像中潜力巨大，但合成图像的真实性和临床实用性仍需验证。本研究评估了GAN和扩散模型在生成胸部X光片时的表现，发现各有优劣，需进一步优化。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像数据稀缺问题，尤其是低发病率异常，同时验证合成图像的真实性和临床价值。

Method: 使用GAN和扩散模型生成四种异常胸部X光片，通过放射科医生评估其真实性和一致性。

Result: 扩散模型整体更真实，但GAN在某些条件下（如ECS缺失）更准确。研究还揭示了放射科医生识别合成图像的视觉线索。

Conclusion: GAN和扩散模型各有优势，需进一步优化以可靠地增强AI诊断系统的训练数据。

Abstract: Generative image models have achieved remarkable progress in both natural and
medical imaging. In the medical context, these techniques offer a potential
solution to data scarcity-especially for low-prevalence anomalies that impair
the performance of AI-driven diagnostic and segmentation tools. However,
questions remain regarding the fidelity and clinical utility of synthetic
images, since poor generation quality can undermine model generalizability and
trust. In this study, we evaluate the effectiveness of state-of-the-art
generative models-Generative Adversarial Networks (GANs) and Diffusion Models
(DMs)-for synthesizing chest X-rays conditioned on four abnormalities:
Atelectasis (AT), Lung Opacity (LO), Pleural Effusion (PE), and Enlarged
Cardiac Silhouette (ECS). Using a benchmark composed of real images from the
MIMIC-CXR dataset and synthetic images from both GANs and DMs, we conducted a
reader study with three radiologists of varied experience. Participants were
asked to distinguish real from synthetic images and assess the consistency
between visual features and the target abnormality. Our results show that while
DMs generate more visually realistic images overall, GANs can report better
accuracy for specific conditions, such as absence of ECS. We further identify
visual cues radiologists use to detect synthetic images, offering insights into
the perceptual gaps in current models. These findings underscore the
complementary strengths of GANs and DMs and point to the need for further
refinement to ensure generative models can reliably augment training datasets
for AI diagnostic systems.

</details>


### [74] [CMAMRNet: A Contextual Mask-Aware Network Enhancing Mural Restoration Through Comprehensive Mask Guidance](https://arxiv.org/abs/2508.07140)
*Yingtie Lei,Fanghai Yi,Yihang Dong,Weihuang Liu,Xiaofeng Zhang,Zimeng Li,Chi-Man Pun,Xuhang Chen*

Main category: cs.CV

TL;DR: CMAMRNet是一种用于壁画修复的网络，通过多尺度特征提取和掩码感知技术提升修复质量。


<details>
  <summary>Details</summary>
Motivation: 壁画作为文化遗产易受环境和人为破坏，现有修复方法难以保持艺术真实性。

Method: 提出CMAMRNet，包含Mask-Aware Up/Down-Sampler（MAUDS）和Co-Feature Aggregator（CFA），实现多尺度特征提取和掩码引导。

Result: 在基准数据集上表现优于现有方法，有效保留壁画结构和艺术细节。

Conclusion: CMAMRNet为壁画修复提供了高效解决方案，代码已开源。

Abstract: Murals, as invaluable cultural artifacts, face continuous deterioration from
environmental factors and human activities. Digital restoration of murals faces
unique challenges due to their complex degradation patterns and the critical
need to preserve artistic authenticity. Existing learning-based methods
struggle with maintaining consistent mask guidance throughout their networks,
leading to insufficient focus on damaged regions and compromised restoration
quality. We propose CMAMRNet, a Contextual Mask-Aware Mural Restoration Network
that addresses these limitations through comprehensive mask guidance and
multi-scale feature extraction. Our framework introduces two key components:
(1) the Mask-Aware Up/Down-Sampler (MAUDS), which ensures consistent mask
sensitivity across resolution scales through dedicated channel-wise feature
selection and mask-guided feature fusion; and (2) the Co-Feature Aggregator
(CFA), operating at both the highest and lowest resolutions to extract
complementary features for capturing fine textures and global structures in
degraded regions. Experimental results on benchmark datasets demonstrate that
CMAMRNet outperforms state-of-the-art methods, effectively preserving both
structural integrity and artistic details in restored murals. The code is
available
at~\href{https://github.com/CXH-Research/CMAMRNet}{https://github.com/CXH-Research/CMAMRNet}.

</details>


### [75] [Dynamic Pattern Alignment Learning for Pretraining Lightweight Human-Centric Vision Models](https://arxiv.org/abs/2508.07144)
*Xuanhan Wang,Huimin Deng,Ke Liu,Jun Wang,Lianli Gao,Jingkuan Song*

Main category: cs.CV

TL;DR: DPAL是一种新的蒸馏预训练框架，通过动态模式对齐学习，使轻量级HVM从大型HVM中学习典型视觉模式，实现高效泛化。


<details>
  <summary>Details</summary>
Motivation: 解决大型HVM依赖大规模预训练数据和复杂架构的问题，提升轻量级HVM的实用性和泛化能力。

Method: 设计动态模式解码器（D-PaDe）作为动态专家混合模型，结合三种专家提取典型视觉模式，并通过三级对齐目标缩小泛化差距。

Result: DPAL在15个数据集上表现优异，轻量级模型（5M参数）泛化能力接近大型HVM（84M/307M），显著优于其他蒸馏方法。

Conclusion: DPAL通过动态模式对齐学习，有效提升轻量级HVM的泛化能力，适用于多种人本视觉任务。

Abstract: Human-centric vision models (HVMs) have achieved remarkable generalization
due to large-scale pretraining on massive person images. However, their
dependence on large neural architectures and the restricted accessibility of
pretraining data significantly limits their practicality in real-world
applications. To address this limitation, we propose Dynamic Pattern Alignment
Learning (DPAL), a novel distillation-based pretraining framework that
efficiently trains lightweight HVMs to acquire strong generalization from large
HVMs. In particular, human-centric visual perception are highly dependent on
three typical visual patterns, including global identity pattern, local shape
pattern and multi-person interaction pattern. To achieve generalizable
lightweight HVMs, we firstly design a dynamic pattern decoder (D-PaDe), acting
as a dynamic Mixture of Expert (MoE) model. It incorporates three specialized
experts dedicated to adaptively extract typical visual patterns, conditioned on
both input image and pattern queries. And then, we present three levels of
alignment objectives, which aims to minimize generalization gap between
lightweight HVMs and large HVMs at global image level, local pixel level, and
instance relation level. With these two deliberate designs, the DPAL
effectively guides lightweight model to learn all typical human visual patterns
from large HVMs, which can generalize to various human-centric vision tasks.
Extensive experiments conducted on 15 challenging datasets demonstrate the
effectiveness of the DPAL. Remarkably, when employing PATH-B as the teacher,
DPAL-ViT/Ti (5M parameters) achieves surprising generalizability similar to
existing large HVMs such as PATH-B (84M) and Sapiens-L (307M), and outperforms
previous distillation-based pretraining methods including Proteus-ViT/Ti (5M)
and TinyMiM-ViT/Ti (5M) by a large margin.

</details>


### [76] [Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2508.07146)
*Yu Liu,Zhijie Liu,Xiao Ren,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的行人轨迹预测框架，结合短期和长期意图建模，提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型缺乏对行人意图的显式语义建模，可能导致行为误解和预测精度下降。

Method: 采用残差极坐标表示建模短期意图，基于可学习的端点预测器建模长期意图，并引入自适应指导和残差噪声预测器优化扩散过程。

Result: 在ETH、UCY和SDD基准测试中表现优异，优于现有方法。

Conclusion: 结合意图建模的扩散框架能有效提升行人轨迹预测的准确性和鲁棒性。

Abstract: Predicting pedestrian motion trajectories is critical for the path planning
and motion control of autonomous vehicles. Recent diffusion-based models have
shown promising results in capturing the inherent stochasticity of pedestrian
behavior for trajectory prediction. However, the absence of explicit semantic
modelling of pedestrian intent in many diffusion-based methods may result in
misinterpreted behaviors and reduced prediction accuracy. To address the above
challenges, we propose a diffusion-based pedestrian trajectory prediction
framework that incorporates both short-term and long-term motion intentions.
Short-term intent is modelled using a residual polar representation, which
decouples direction and magnitude to capture fine-grained local motion
patterns. Long-term intent is estimated through a learnable, token-based
endpoint predictor that generates multiple candidate goals with associated
probabilities, enabling multimodal and context-aware intention modelling.
Furthermore, we enhance the diffusion process by incorporating adaptive
guidance and a residual noise predictor that dynamically refines denoising
accuracy. The proposed framework is evaluated on the widely used ETH, UCY, and
SDD benchmarks, demonstrating competitive results against state-of-the-art
methods.

</details>


### [77] [SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models](https://arxiv.org/abs/2508.07149)
*Ruolin Yang,Da Li,Honggang Zhang,Yi-Zhe Song*

Main category: cs.CV

TL;DR: 提出了一种名为SketchAnimator的模型，能够为静态草图添加动态效果，通过三个阶段实现：外观学习、运动学习和视频先验蒸馏。


<details>
  <summary>Details</summary>
Motivation: 草图动画化通常需要专业技能且耗时，业余用户难以完成。本文旨在简化这一过程，让用户能够轻松为草图添加动态效果。

Method: 分为三个阶段：1) 外观学习，使用LoRA将草图外观信息整合到预训练的T2V模型中；2) 运动学习，从参考视频中提取动态信息；3) 视频先验蒸馏，利用SDS更新草图帧的贝塞尔曲线参数。

Result: 模型生成的草图视频既保留了原始外观，又复现了参考视频的动态效果，且在一键运动定制任务中表现优异。

Conclusion: SketchAnimator为草图动画化提供了一种高效且用户友好的解决方案，适用于业余和专业用户。

Abstract: Sketching is a uniquely human tool for expressing ideas and creativity. The
animation of sketches infuses life into these static drawings, opening a new
dimension for designers. Animating sketches is a time-consuming process that
demands professional skills and extensive experience, often proving daunting
for amateurs. In this paper, we propose a novel sketch animation model
SketchAnimator, which enables adding creative motion to a given sketch, like "a
jumping car''. Namely, given an input sketch and a reference video, we divide
the sketch animation into three stages: Appearance Learning, Motion Learning
and Video Prior Distillation. In stages 1 and 2, we utilize LoRA to integrate
sketch appearance information and motion dynamics from the reference video into
the pre-trained T2V model. In the third stage, we utilize Score Distillation
Sampling (SDS) to update the parameters of the Bezier curves in each sketch
frame according to the acquired motion information. Consequently, our model
produces a sketch video that not only retains the original appearance of the
sketch but also mirrors the dynamic movements of the reference video. We
compare our method with alternative approaches and demonstrate that it
generates the desired sketch video under the challenge of one-shot motion
customization.

</details>


### [78] [CoopDiff: Anticipating 3D Human-object Interactions via Contact-consistent Decoupled Diffusion](https://arxiv.org/abs/2508.07162)
*Xiaotong Lin,Tianming Liang,Jian-Fang Hu,Kun-Yu Lin,Yulei Kang,Chunwei Tian,Jianhuang Lai,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: CoopDiff提出了一种基于接触一致性的解耦扩散框架，通过分离人类和物体运动建模，利用共享接触点桥接运动生成，显著提升了3D人-物交互预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常忽视人类和物体运动的本质差异，用一个模型同时预测两者运动，导致效果不佳。

Method: CoopDiff采用两个独立分支分别建模人类和物体运动，通过共享接触点及其一致性约束桥接两者，并引入人类驱动的交互模块增强一致性。

Result: 在BEHAVE和Human-object Interaction数据集上，CoopDiff优于现有方法。

Conclusion: 解耦建模和接触一致性约束有效提升了人-物交互预测的准确性和一致性。

Abstract: 3D human-object interaction (HOI) anticipation aims to predict the future
motion of humans and their manipulated objects, conditioned on the historical
context. Generally, the articulated humans and rigid objects exhibit different
motion patterns, due to their distinct intrinsic physical properties. However,
this distinction is ignored by most of the existing works, which intend to
capture the dynamics of both humans and objects within a single prediction
model. In this work, we propose a novel contact-consistent decoupled diffusion
framework CoopDiff, which employs two distinct branches to decouple human and
object motion modeling, with the human-object contact points as shared anchors
to bridge the motion generation across branches. The human dynamics branch is
aimed to predict highly structured human motion, while the object dynamics
branch focuses on the object motion with rigid translations and rotations.
These two branches are bridged by a series of shared contact points with
consistency constraint for coherent human-object motion prediction. To further
enhance human-object consistency and prediction reliability, we propose a
human-driven interaction module to guide object motion modeling. Extensive
experiments on the BEHAVE and Human-object Interaction datasets demonstrate
that our CoopDiff outperforms state-of-the-art methods.

</details>


### [79] [Lightweight Multi-Scale Feature Extraction with Fully Connected LMF Layer for Salient Object Detection](https://arxiv.org/abs/2508.07170)
*Yunpeng Shi,Lei Chen,Xiaolu Shen,Yanju Guo*

Main category: cs.CV

TL;DR: 提出了一种轻量级多尺度特征提取层（LMF层），并基于此构建了LMFNet网络，显著减少了参数数量，同时在多个数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 解决轻量级网络中多尺度特征提取的效率与性能平衡问题。

Method: 采用深度可分离空洞卷积构建LMF层，并集成多个LMF层形成LMFNet网络。

Result: 在五个基准数据集上达到或接近最优性能，仅需0.81M参数。

Conclusion: LMFNet不仅解决了轻量级网络的多尺度学习问题，还展示了在图像处理任务中的广泛应用潜力。

Abstract: In the domain of computer vision, multi-scale feature extraction is vital for
tasks such as salient object detection. However, achieving this capability in
lightweight networks remains challenging due to the trade-off between
efficiency and performance. This paper proposes a novel lightweight multi-scale
feature extraction layer, termed the LMF layer, which employs depthwise
separable dilated convolutions in a fully connected structure. By integrating
multiple LMF layers, we develop LMFNet, a lightweight network tailored for
salient object detection. Our approach significantly reduces the number of
parameters while maintaining competitive performance. Here, we show that LMFNet
achieves state-of-the-art or comparable results on five benchmark datasets with
only 0.81M parameters, outperforming several traditional and lightweight models
in terms of both efficiency and accuracy. Our work not only addresses the
challenge of multi-scale learning in lightweight networks but also demonstrates
the potential for broader applications in image processing tasks. The related
code files are available at https://github.com/Shi-Yun-peng/LMFNet

</details>


### [80] [EventRR: Event Referential Reasoning for Referring Video Object Segmentation](https://arxiv.org/abs/2508.07171)
*Huihui Xu,Jiashi Lin,Haoyu Chen,Junjun He,Lei Zhu*

Main category: cs.CV

TL;DR: EventRR框架通过解耦RVOS任务为对象总结和引用推理两部分，利用REG图和TCRR方法显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有RVOS方法忽视表达式的语义结构，而视频引用表达式比图像更复杂，包含事件属性和时序关系，需要新方法。

Method: EventRR分为对象总结和引用推理两部分：总结阶段生成瓶颈令牌并聚合；推理阶段构建REG图并通过TCRR进行时序推理。

Result: 在四个基准数据集上，EventRR定量和定性均优于现有方法。

Conclusion: EventRR通过结构化语义和时序推理，显著提升了RVOS任务的性能。

Abstract: Referring Video Object Segmentation (RVOS) aims to segment out the object in
a video referred by an expression. Current RVOS methods view referring
expressions as unstructured sequences, neglecting their crucial semantic
structure essential for referent reasoning. Besides, in contrast to
image-referring expressions whose semantics focus only on object attributes and
object-object relations, video-referring expressions also encompass event
attributes and event-event temporal relations. This complexity challenges
traditional structured reasoning image approaches. In this paper, we propose
the Event Referential Reasoning (EventRR) framework. EventRR decouples RVOS
into object summarization part and referent reasoning part. The summarization
phase begins by summarizing each frame into a set of bottleneck tokens, which
are then efficiently aggregated in the video-level summarization step to
exchange the global cross-modal temporal context. For reasoning part, EventRR
extracts semantic eventful structure of a video-referring expression into
highly expressive Referential Event Graph (REG), which is a single-rooted
directed acyclic graph. Guided by topological traversal of REG, we propose
Temporal Concept-Role Reasoning (TCRR) to accumulate the referring score of
each temporal query from REG leaf nodes to root node. Each reasoning step can
be interpreted as a question-answer pair derived from the concept-role
relations in REG. Extensive experiments across four widely recognized benchmark
datasets, show that EventRR quantitatively and qualitatively outperforms
state-of-the-art RVOS methods. Code is available at
https://github.com/bio-mlhui/EventRR

</details>


### [81] [Similarity Matters: A Novel Depth-guided Network for Image Restoration and A New Dataset](https://arxiv.org/abs/2508.07211)
*Junyi He,Liuling Chen,Hongyang Zhou,Zhang xiaoxing,Xiaobin Zhu,Shengxiang Yu,Jingyan Qin,Xu-Cheng Yin*

Main category: cs.CV

TL;DR: 提出了一种深度引导网络（DGN）用于图像修复，结合新的大规模高分辨率数据集，通过双分支交互提升修复质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视深度信息，导致相似性匹配不足、注意力分散或背景过度增强。

Method: 网络包含深度估计分支和图像修复分支，利用窗口自注意力与非局部注意力捕捉相似性。

Result: 在标准基准测试中达到最优性能，并能泛化到未见过的植物图像。

Conclusion: DGN方法在图像修复中表现出高效性和鲁棒性。

Abstract: Image restoration has seen substantial progress in recent years. However,
existing methods often neglect depth information, which hurts similarity
matching, results in attention distractions in shallow depth-of-field (DoF)
scenarios, and excessive enhancement of background content in deep DoF
settings. To overcome these limitations, we propose a novel Depth-Guided
Network (DGN) for image restoration, together with a novel large-scale
high-resolution dataset. Specifically, the network consists of two interactive
branches: a depth estimation branch that provides structural guidance, and an
image restoration branch that performs the core restoration task. In addition,
the image restoration branch exploits intra-object similarity through
progressive window-based self-attention and captures inter-object similarity
via sparse non-local attention. Through joint training, depth features
contribute to improved restoration quality, while the enhanced visual features
from the restoration branch in turn help refine depth estimation. Notably, we
also introduce a new dataset for training and evaluation, consisting of 9,205
high-resolution images from 403 plant species, with diverse depth and texture
variations. Extensive experiments show that our method achieves
state-of-the-art performance on several standard benchmarks and generalizes
well to unseen plant images, demonstrating its effectiveness and robustness.

</details>


### [82] [Unsupervised Real-World Super-Resolution via Rectified Flow Degradation Modelling](https://arxiv.org/abs/2508.07214)
*Hongyang Zhou,Xiaobin Zhu,Liuling Chen,Junyi He,Jingyan Qin,Xu-Cheng Yin,Zhang xiaoxing*

Main category: cs.CV

TL;DR: 提出了一种基于修正流的无监督真实世界超分辨率方法，通过建模连续可逆的退化轨迹和利用傅里叶先验，生成更真实的低分辨率图像，显著提升了超分辨率性能。


<details>
  <summary>Details</summary>
Motivation: 真实世界超分辨率面临复杂未知的退化分布问题，现有方法因域差距难以泛化。

Method: 提出修正流退化模块（RFDM）和傅里叶先验引导退化模块（FGDM），生成真实退化的低分辨率图像用于训练。

Result: 在真实世界数据集上的实验表明，该方法显著提升了现有超分辨率方法的性能。

Conclusion: 通过建模真实退化，该方法有效解决了真实世界超分辨率的挑战。

Abstract: Unsupervised real-world super-resolution (SR) faces critical challenges due
to the complex, unknown degradation distributions in practical scenarios.
Existing methods struggle to generalize from synthetic low-resolution (LR) and
high-resolution (HR) image pairs to real-world data due to a significant domain
gap. In this paper, we propose an unsupervised real-world SR method based on
rectified flow to effectively capture and model real-world degradation,
synthesizing LR-HR training pairs with realistic degradation. Specifically,
given unpaired LR and HR images, we propose a novel Rectified Flow Degradation
Module (RFDM) that introduces degradation-transformed LR (DT-LR) images as
intermediaries. By modeling the degradation trajectory in a continuous and
invertible manner, RFDM better captures real-world degradation and enhances the
realism of generated LR images. Additionally, we propose a Fourier Prior Guided
Degradation Module (FGDM) that leverages structural information embedded in
Fourier phase components to ensure more precise modeling of real-world
degradation. Finally, the LR images are processed by both FGDM and RFDM,
producing final synthetic LR images with real-world degradation. The synthetic
LR images are paired with the given HR images to train the off-the-shelf SR
networks. Extensive experiments on real-world datasets demonstrate that our
method significantly enhances the performance of existing SR approaches in
real-world scenarios.

</details>


### [83] [Bridging Semantic Logic Gaps: A Cognition-Inspired Multimodal Boundary-Preserving Network for Image Manipulation Localization](https://arxiv.org/abs/2508.07216)
*Songlin Li,Zhiqing Guo,Yuanman Li,Zeyu Li,Yunfeng Diao,Gaobo Yang,Liejun Wang*

Main category: cs.CV

TL;DR: 本文提出了一种认知启发的多模态边界保持网络（CMB-Net），通过结合视觉和文本信息来改进图像篡改定位（IML）的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有IML模型主要依赖视觉线索，忽略了内容特征的语义逻辑关系，而篡改技术通常会破坏这些关系，留下语义线索。

Method: CMB-Net利用大语言模型（LLMs）分析篡改区域并生成文本信息，提出图像-文本中心模糊模块（ITCAM）和图像-文本交互模块（ITIM）来优化文本特征和视觉特征的融合，并采用可逆神经网络启发的恢复边缘解码器（RED）保持边界信息。

Result: 实验表明，CMB-Net在IML任务中优于大多数现有模型。

Conclusion: CMB-Net通过多模态融合和边界保持技术，显著提升了图像篡改定位的准确性。

Abstract: The existing image manipulation localization (IML) models mainly relies on
visual cues, but ignores the semantic logical relationships between content
features. In fact, the content semantics conveyed by real images often conform
to human cognitive laws. However, image manipulation technology usually
destroys the internal relationship between content features, thus leaving
semantic clues for IML. In this paper, we propose a cognition-inspired
multimodal boundary-preserving network (CMB-Net). Specifically, CMB-Net
utilizes large language models (LLMs) to analyze manipulated regions within
images and generate prompt-based textual information to compensate for the lack
of semantic relationships in the visual information. Considering that the
erroneous texts induced by hallucination from LLMs will damage the accuracy of
IML, we propose an image-text central ambiguity module (ITCAM). It assigns
weights to the text features by quantifying the ambiguity between text and
image features, thereby ensuring the beneficial impact of textual information.
We also propose an image-text interaction module (ITIM) that aligns visual and
text features using a correlation matrix for fine-grained interaction. Finally,
inspired by invertible neural networks, we propose a restoration edge decoder
(RED) that mutually generates input and output features to preserve boundary
information in manipulated regions without loss. Extensive experiments show
that CMB-Net outperforms most existing IML models.

</details>


### [84] [Generic Calibration: Pose Ambiguity/Linear Solution and Parametric-hybrid Pipeline](https://arxiv.org/abs/2508.07217)
*Yuqi Han,Qi Cai,Yuanxin Wu*

Main category: cs.CV

TL;DR: 提出了一种结合通用和参数化模型的混合标定方法，解决了通用标定中的姿态模糊问题，并提高了标定精度。


<details>
  <summary>Details</summary>
Motivation: 离线相机标定中，参数化模型依赖用户经验，而通用方法复杂且无法提供传统内参，且存在姿态模糊问题。

Method: 提出线性求解器和非线性优化解决姿态模糊，并引入全局优化的混合标定方法结合通用与参数化模型。

Result: 混合方法在多种镜头类型和噪声污染下表现优异，提高了外参精度并减少了过拟合和数值不稳定。

Conclusion: 混合标定方法为复杂场景下的相机标定提供了可靠且精确的解决方案。

Abstract: Offline camera calibration techniques typically employ parametric or generic
camera models. Selecting parametric models relies heavily on user experience,
and an inappropriate camera model can significantly affect calibration
accuracy. Meanwhile, generic calibration methods involve complex procedures and
cannot provide traditional intrinsic parameters. This paper reveals a pose
ambiguity in the pose solutions of generic calibration methods that
irreversibly impacts subsequent pose estimation. A linear solver and a
nonlinear optimization are proposed to address this ambiguity issue. Then a
global optimization hybrid calibration method is introduced to integrate
generic and parametric models together, which improves extrinsic parameter
accuracy of generic calibration and mitigates overfitting and numerical
instability in parametric calibration. Simulation and real-world experimental
results demonstrate that the generic-parametric hybrid calibration method
consistently excels across various lens types and noise contamination,
hopefully serving as a reliable and accurate solution for camera calibration in
complex scenarios.

</details>


### [85] [Landmark Guided Visual Feature Extractor for Visual Speech Recognition with Limited Resource](https://arxiv.org/abs/2508.07233)
*Lei Yang,Junshan Jin,Mingyuan Zhang,Yi He,Bofan Chen,Shilin Wang*

Main category: cs.CV

TL;DR: 提出了一种基于面部标志的视觉特征提取方法，结合时空多图卷积网络和多级唇动态融合框架，以提升有限数据下的视觉语音识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法易受视觉干扰影响且需要大量数据和计算资源，本文旨在减少用户特定特征的影响并提升小数据集的性能。

Method: 使用面部标志作为辅助信息训练视觉特征提取器，设计时空多图卷积网络和多级唇动态融合框架。

Result: 实验表明，该方法在有限数据下表现良好，并提高了对未见说话者的识别准确率。

Conclusion: 提出的方法有效减少了视觉干扰的影响，提升了小数据集下的视觉语音识别性能。

Abstract: Visual speech recognition is a technique to identify spoken content in silent
speech videos, which has raised significant attention in recent years.
Advancements in data-driven deep learning methods have significantly improved
both the speed and accuracy of recognition. However, these deep learning
methods can be effected by visual disturbances, such as lightning conditions,
skin texture and other user-specific features. Data-driven approaches could
reduce the performance degradation caused by these visual disturbances using
models pretrained on large-scale datasets. But these methods often require
large amounts of training data and computational resources, making them costly.
To reduce the influence of user-specific features and enhance performance with
limited data, this paper proposed a landmark guided visual feature extractor.
Facial landmarks are used as auxiliary information to aid in training the
visual feature extractor. A spatio-temporal multi-graph convolutional network
is designed to fully exploit the spatial locations and spatio-temporal features
of facial landmarks. Additionally, a multi-level lip dynamic fusion framework
is introduced to combine the spatio-temporal features of the landmarks with the
visual features extracted from the raw video frames. Experimental results show
that this approach performs well with limited data and also improves the
model's accuracy on unseen speakers.

</details>


### [86] [ASM-UNet: Adaptive Scan Mamba Integrating Group Commonalities and Individual Variations for Fine-Grained Segmentation](https://arxiv.org/abs/2508.07237)
*Bo Wang,Mengyuan Xu,Yue Yan,Yuqun Yang,Kechen Shu,Wei Ping,Xu Tang,Wei Jiang,Zheng You*

Main category: cs.CV

TL;DR: ASM-UNet是一种基于Mamba的新型架构，用于细粒度分割（FGS），通过自适应扫描顺序解决个体差异问题，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有粗粒度分割方法在细粒度分割中表现不佳，且Mamba模型依赖固定扫描顺序，无法适应个体差异。

Method: 提出ASM-UNet，结合群体共性和个体差异生成自适应扫描分数，动态指导扫描顺序。

Result: 在ACDC、Synapse和BTMS数据集上，ASM-UNet在粗粒度和细粒度分割任务中均表现优异。

Conclusion: ASM-UNet通过自适应扫描顺序有效解决了细粒度分割中的个体差异问题，具有广泛应用潜力。

Abstract: Precise lesion resection depends on accurately identifying fine-grained
anatomical structures. While many coarse-grained segmentation (CGS) methods
have been successful in large-scale segmentation (e.g., organs), they fall
short in clinical scenarios requiring fine-grained segmentation (FGS), which
remains challenging due to frequent individual variations in small-scale
anatomical structures. Although recent Mamba-based models have advanced medical
image segmentation, they often rely on fixed manually-defined scanning orders,
which limit their adaptability to individual variations in FGS. To address
this, we propose ASM-UNet, a novel Mamba-based architecture for FGS. It
introduces adaptive scan scores to dynamically guide the scanning order,
generated by combining group-level commonalities and individual-level
variations. Experiments on two public datasets (ACDC and Synapse) and a newly
proposed challenging biliary tract FGS dataset, namely BTMS, demonstrate that
ASM-UNet achieves superior performance in both CGS and FGS tasks. Our code and
dataset are available at https://github.com/YqunYang/ASM-UNet.

</details>


### [87] [Investigating the Design Space of Visual Grounding in Multimodal Large Language Model](https://arxiv.org/abs/2508.08066)
*Weitai Kang,Weiming Zhuang,Zhizhong Li,Yan Yan,Lingjuan Lyu*

Main category: cs.CV

TL;DR: 本文系统研究了多模态大语言模型（MLLMs）在视觉定位（VG）任务中的设计选择，通过LLaVA-1.5模型验证了不同设计对性能的影响，最终提升了VG任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在微调MLLMs时设计选择分散且缺乏系统性验证，本文旨在填补这一空白。

Method: 使用LLaVA-1.5模型，分析不同视觉定位范式及数据设计，通过消融实验优化微调策略。

Result: 在RefCOCO/+/g数据集上，性能分别提升了5.6%、6.9%和7.0%。

Conclusion: 研究为MLLMs在VG任务中的设计提供了系统性指导，显著提升了性能。

Abstract: Fine-grained multimodal capability in Multimodal Large Language Models
(MLLMs) has emerged as a critical research direction, particularly for tackling
the visual grounding (VG) problem. Despite the strong performance achieved by
existing approaches, they often employ disparate design choices when
fine-tuning MLLMs for VG, lacking systematic verification to support these
designs. To bridge this gap, this paper presents a comprehensive study of
various design choices that impact the VG performance of MLLMs. We conduct our
analysis using LLaVA-1.5, which has been widely adopted in prior empirical
studies of MLLMs. While more recent models exist, we follow this convention to
ensure our findings remain broadly applicable and extendable to other
architectures. We cover two key aspects: (1) exploring different visual
grounding paradigms in MLLMs, identifying the most effective design, and
providing our insights; and (2) conducting ablation studies on the design of
grounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our
findings contribute to a stronger MLLM for VG, achieving improvements of +5.6%
/ +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.

</details>


### [88] [Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers](https://arxiv.org/abs/2508.07246)
*Xin Ma,Yaohui Wang,Genyun Jia,Xinyuan Chen,Tien-Tsin Wong,Cunjian Chen*

Main category: cs.CV

TL;DR: MiraMo框架通过高效线性注意力、运动残差学习和DCT噪声优化，解决了图像动画中的外观一致性和运动平滑性问题。


<details>
  <summary>Details</summary>
Motivation: 当前图像动画技术在外观一致性和运动平滑性上存在不足，且计算资源消耗高。

Method: MiraMo采用线性注意力替代传统自注意力，引入运动残差学习和DCT噪声优化策略。

Result: 实验证明MiraMo在生成一致性高、运动平滑的动画上优于现有方法，且推理速度更快。

Conclusion: MiraMo为图像动画提供了高效、一致且可控的解决方案，并适用于多种任务。

Abstract: Image animation has seen significant progress, driven by the powerful
generative capabilities of diffusion models. However, maintaining appearance
consistency with static input images and mitigating abrupt motion transitions
in generated animations remain persistent challenges. While text-to-video (T2V)
generation has demonstrated impressive performance with diffusion transformer
models, the image animation field still largely relies on U-Net-based diffusion
models, which lag behind the latest T2V approaches. Moreover, the quadratic
complexity of vanilla self-attention mechanisms in Transformers imposes heavy
computational demands, making image animation particularly resource-intensive.
To address these issues, we propose MiraMo, a framework designed to enhance
efficiency, appearance consistency, and motion smoothness in image animation.
Specifically, MiraMo introduces three key elements: (1) A foundational
text-to-video architecture replacing vanilla self-attention with efficient
linear attention to reduce computational overhead while preserving generation
quality; (2) A novel motion residual learning paradigm that focuses on modeling
motion dynamics rather than directly predicting frames, improving temporal
consistency; and (3) A DCT-based noise refinement strategy during inference to
suppress sudden motion artifacts, complemented by a dynamics control module to
balance motion smoothness and expressiveness. Extensive experiments against
state-of-the-art methods validate the superiority of MiraMo in generating
consistent, smooth, and controllable animations with accelerated inference
speed. Additionally, we demonstrate the versatility of MiraMo through
applications in motion transfer and video editing tasks.

</details>


### [89] [SUIT: Spatial-Spectral Union-Intersection Interaction Network for Hyperspectral Object Tracking](https://arxiv.org/abs/2508.07250)
*Fengchao Xiong,Zhenxing Wu,Sen Jia,Yuntao Qian*

Main category: cs.CV

TL;DR: 论文提出了一种利用Transformer和集合论原理增强高光谱视频跟踪的方法，通过建模光谱和空间交互，提升了跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注空间交互，忽略了光谱交互，导致性能不佳。本文旨在从架构和训练层面解决这一问题。

Method: 1. 使用Transformer建立模板和搜索区域的波段间长程空间关系；2. 基于集合论的包含-排除原理建模光谱交互；3. 引入光谱损失函数以增强鲁棒性。

Result: 实验表明，该方法在跟踪性能上达到最优。

Conclusion: 通过结合光谱和空间交互，显著提升了高光谱视频跟踪的准确性和鲁棒性。

Abstract: Hyperspectral videos (HSVs), with their inherent spatial-spectral-temporal
structure, offer distinct advantages in challenging tracking scenarios such as
cluttered backgrounds and small objects. However, existing methods primarily
focus on spatial interactions between the template and search regions, often
overlooking spectral interactions, leading to suboptimal performance. To
address this issue, this paper investigates spectral interactions from both the
architectural and training perspectives. At the architectural level, we first
establish band-wise long-range spatial relationships between the template and
search regions using Transformers. We then model spectral interactions using
the inclusion-exclusion principle from set theory, treating them as the union
of spatial interactions across all bands. This enables the effective
integration of both shared and band-specific spatial cues. At the training
level, we introduce a spectral loss to enforce material distribution alignment
between the template and predicted regions, enhancing robustness to shape
deformation and appearance variations. Extensive experiments demonstrate that
our tracker achieves state-of-the-art tracking performance. The source code,
trained models and results will be publicly available via
https://github.com/bearshng/suit to support reproducibility.

</details>


### [90] [Understanding Dynamic Scenes in Ego Centric 4D Point Clouds](https://arxiv.org/abs/2508.07251)
*Junsheng Huang,Shengyu Hao,Bocheng Hu,Gaoang Wang*

Main category: cs.CV

TL;DR: EgoDynamic4D是一个新的QA基准数据集，用于动态4D场景理解，包含RGB-D视频、相机位姿、实例掩码和4D边界框，支持细粒度时空推理任务。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏统一的4D标注和任务驱动的评估协议，无法支持细粒度的时空推理，尤其是对象和人类的运动及其交互。

Method: 提出EgoDynamic4D数据集，包含927K QA对和显式Chain-of-Thought标注，设计12种动态QA任务。提出端到端时空推理框架，结合动态和静态场景信息。

Result: 实验表明，该方法在EgoDynamic4D上优于基线，验证了多模态时序建模的有效性。

Conclusion: EgoDynamic4D和提出的框架为动态4D场景理解提供了新基准和方法，支持细粒度的时空推理。

Abstract: Understanding dynamic 4D scenes from an egocentric perspective-modeling
changes in 3D spatial structure over time-is crucial for human-machine
interaction, autonomous navigation, and embodied intelligence. While existing
egocentric datasets contain dynamic scenes, they lack unified 4D annotations
and task-driven evaluation protocols for fine-grained spatio-temporal
reasoning, especially on motion of objects and human, together with their
interactions. To address this gap, we introduce EgoDynamic4D, a novel QA
benchmark on highly dynamic scenes, comprising RGB-D video, camera poses,
globally unique instance masks, and 4D bounding boxes. We construct 927K QA
pairs accompanied by explicit Chain-of-Thought (CoT), enabling verifiable,
step-by-step spatio-temporal reasoning. We design 12 dynamic QA tasks covering
agent motion, human-object interaction, trajectory prediction, relation
understanding, and temporal-causal reasoning, with fine-grained,
multidimensional metrics. To tackle these tasks, we propose an end-to-end
spatio-temporal reasoning framework that unifies dynamic and static scene
information, using instance-aware feature encoding, time and camera encoding,
and spatially adaptive down-sampling to compress large 4D scenes into token
sequences manageable by LLMs. Experiments on EgoDynamic4D show that our method
consistently outperforms baselines, validating the effectiveness of multimodal
temporal modeling for egocentric dynamic scene understanding.

</details>


### [91] [Small-Large Collaboration: Training-efficient Concept Personalization for Large VLM using a Meta Personalized Small VLM](https://arxiv.org/abs/2508.07260)
*Sihan Yang,Huitong Ji,Shaolin Lu,Jiayi Chen,Binxiao Xu,Ming Lu,Yuanxing Zhang,Wenhui Dong,Wentao Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为Small-Large Collaboration (SLC)的框架，通过小型和大型视觉语言模型的协作，实现高效个性化。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（VLMs）训练成本高且难以个性化，而小型VLMs虽易个性化但推理能力不足。

Method: SLC框架中，小型VLM生成个性化信息，大型VLM整合信息并输出准确响应，采用测试时反射策略避免幻觉。

Result: 实验证明SLC在多种基准和大型VLMs中有效，且训练高效。

Conclusion: SLC是首个支持开源和闭源大型VLMs的高效个性化框架，具有广泛实际应用潜力。

Abstract: Personalizing Vision-Language Models (VLMs) to transform them into daily
assistants has emerged as a trending research direction. However, leading
companies like OpenAI continue to increase model size and develop complex
designs such as the chain of thought (CoT). While large VLMs are proficient in
complex multi-modal understanding, their high training costs and limited access
via paid APIs restrict direct personalization. Conversely, small VLMs are
easily personalized and freely available, but they lack sufficient reasoning
capabilities. Inspired by this, we propose a novel collaborative framework
named Small-Large Collaboration (SLC) for large VLM personalization, where the
small VLM is responsible for generating personalized information, while the
large model integrates this personalized information to deliver accurate
responses. To effectively incorporate personalized information, we develop a
test-time reflection strategy, preventing the potential hallucination of the
small VLM. Since SLC only needs to train a meta personalized small VLM for the
large VLMs, the overall process is training-efficient. To the best of our
knowledge, this is the first training-efficient framework that supports both
open-source and closed-source large VLMs, enabling broader real-world
personalized applications. We conduct thorough experiments across various
benchmarks and large VLMs to demonstrate the effectiveness of the proposed SLC
framework. The code will be released at https://github.com/Hhankyangg/SLC.

</details>


### [92] [OpenHAIV: A Framework Towards Practical Open-World Learning](https://arxiv.org/abs/2508.07270)
*Xiang Xiang,Qinhao Zhou,Zhuo Xu,Jing Ma,Jiaxin Dai,Yifan Liang,Hanlin Li*

Main category: cs.CV

TL;DR: OpenHAIV框架结合了OOD检测、新类发现和增量持续微调，解决了开放世界场景中模型知识更新的问题。


<details>
  <summary>Details</summary>
Motivation: 开放世界场景中，现有方法（如OOD检测和增量学习）存在局限性，无法满足模型自主更新知识的需求。

Method: 提出OpenHAIV框架，整合OOD检测、新类发现和增量持续微调，形成统一流程。

Result: 模型能够在开放世界环境中自主获取和更新知识。

Conclusion: OpenHAIV为开放世界识别提供了一种有效的解决方案。

Abstract: Substantial progress has been made in various techniques for open-world
recognition. Out-of-distribution (OOD) detection methods can effectively
distinguish between known and unknown classes in the data, while incremental
learning enables continuous model knowledge updates. However, in open-world
scenarios, these approaches still face limitations. Relying solely on OOD
detection does not facilitate knowledge updates in the model, and incremental
fine-tuning typically requires supervised conditions, which significantly
deviate from open-world settings. To address these challenges, this paper
proposes OpenHAIV, a novel framework that integrates OOD detection, new class
discovery, and incremental continual fine-tuning into a unified pipeline. This
framework allows models to autonomously acquire and update knowledge in
open-world environments. The proposed framework is available at
https://haiv-lab.github.io/openhaiv .

</details>


### [93] [Representation Understanding via Activation Maximization](https://arxiv.org/abs/2508.07281)
*Hongbo Zhu,Angelo Cangelosi*

Main category: cs.CV

TL;DR: 提出了一种统一的特征可视化框架，适用于CNN和ViT，扩展了中间层的可视化，并探讨了AM生成对抗样本的潜力。


<details>
  <summary>Details</summary>
Motivation: 理解DNN内部特征表示是模型可解释性的关键步骤，受神经科学启发，研究AM方法以揭示人工神经元的响应模式。

Method: 采用激活最大化（AM）技术，扩展至CNN和ViT的中间层，并研究其生成对抗样本的能力。

Result: 实验证明该方法在CNN和ViT中均有效，揭示了模型的潜在漏洞和决策边界。

Conclusion: 该框架具有普适性和解释价值，为DNN特征表示和安全性研究提供了新视角。

Abstract: Understanding internal feature representations of deep neural networks (DNNs)
is a fundamental step toward model interpretability. Inspired by neuroscience
methods that probe biological neurons using visual stimuli, recent deep
learning studies have employed Activation Maximization (AM) to synthesize
inputs that elicit strong responses from artificial neurons. In this work, we
propose a unified feature visualization framework applicable to both
Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). Unlike
prior efforts that predominantly focus on the last output-layer neurons in
CNNs, we extend feature visualization to intermediate layers as well, offering
deeper insights into the hierarchical structure of learned feature
representations. Furthermore, we investigate how activation maximization can be
leveraged to generate adversarial examples, revealing potential vulnerabilities
and decision boundaries of DNNs. Our experiments demonstrate the effectiveness
of our approach in both traditional CNNs and modern ViT, highlighting its
generalizability and interpretive value.

</details>


### [94] [SynMatch: Rethinking Consistency in Medical Image Segmentation with Sparse Annotations](https://arxiv.org/abs/2508.07298)
*Zhiqiang Shen,Peng Cao,Xiaoli Liu,Jinzhu Yang,Osmar R. Zaiane*

Main category: cs.CV

TL;DR: SynMatch通过合成图像匹配伪标签，解决了医学图像分割中标签稀缺问题，无需额外训练参数即可生成高一致性图像-标签对。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中标签稀缺，现有强-弱伪监督方法因伪标签与图像不一致而性能受限。

Method: SynMatch利用分割模型提取纹理和形状特征，合成与伪标签高度一致的图像。

Result: 在SSL、WSL和BSL设置下表现优异，尤其在BSL中显著优于现有方法。

Conclusion: SynMatch为标签稀缺问题提供了高效解决方案，尤其在低标注场景下表现突出。

Abstract: Label scarcity remains a major challenge in deep learning-based medical image
segmentation. Recent studies use strong-weak pseudo supervision to leverage
unlabeled data. However, performance is often hindered by inconsistencies
between pseudo labels and their corresponding unlabeled images. In this work,
we propose \textbf{SynMatch}, a novel framework that sidesteps the need for
improving pseudo labels by synthesizing images to match them instead.
Specifically, SynMatch synthesizes images using texture and shape features
extracted from the same segmentation model that generates the corresponding
pseudo labels for unlabeled images. This design enables the generation of
highly consistent synthesized-image-pseudo-label pairs without requiring any
training parameters for image synthesis. We extensively evaluate SynMatch
across diverse medical image segmentation tasks under semi-supervised learning
(SSL), weakly-supervised learning (WSL), and barely-supervised learning (BSL)
settings with increasingly limited annotations. The results demonstrate that
SynMatch achieves superior performance, especially in the most challenging BSL
setting. For example, it outperforms the recent strong-weak pseudo
supervision-based method by 29.71\% and 10.05\% on the polyp segmentation task
with 5\% and 10\% scribble annotations, respectively. The code will be released
at https://github.com/Senyh/SynMatch.

</details>


### [95] [BEVANet: Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation](https://arxiv.org/abs/2508.07300)
*Ping-Mao Huang,I-Tien Chao,Ping-Chia Huang,Jia-Wei Liao,Yung-Yu Chuang*

Main category: cs.CV

TL;DR: 论文提出了一种名为BEVANet的双边高效视觉注意力网络，通过大核注意力机制（LKA）和动态适应感受野的综合核选择（CKS）机制，实现了实时语义分割的高性能。


<details>
  <summary>Details</summary>
Motivation: 解决实时语义分割中高效架构设计和大感受野捕获的双重挑战，同时优化细节轮廓。

Method: 引入大核注意力机制（LKA），设计BEVANet网络，结合稀疏分解大可分核注意力（SDLSKA）、综合核选择（CKS）机制和大核金字塔池化模块（DLKPPM），并通过边界引导自适应融合（BGAF）模块增强边界划分。

Result: BEVANet在Cityscapes数据集上实现了33 FPS的实时分割性能，mIoU达到79.3%（无预训练）和81.0%（ImageNet预训练后），表现优异。

Conclusion: BEVANet通过创新的注意力机制和动态适应策略，在实时语义分割任务中达到了最先进的性能。

Abstract: Real-time semantic segmentation presents the dual challenge of designing
efficient architectures that capture large receptive fields for semantic
understanding while also refining detailed contours. Vision transformers model
long-range dependencies effectively but incur high computational cost. To
address these challenges, we introduce the Large Kernel Attention (LKA)
mechanism. Our proposed Bilateral Efficient Visual Attention Network (BEVANet)
expands the receptive field to capture contextual information and extracts
visual and structural features using Sparse Decomposed Large Separable Kernel
Attentions (SDLSKA). The Comprehensive Kernel Selection (CKS) mechanism
dynamically adapts the receptive field to further enhance performance.
Furthermore, the Deep Large Kernel Pyramid Pooling Module (DLKPPM) enriches
contextual features by synergistically combining dilated convolutions and large
kernel attention. The bilateral architecture facilitates frequent branch
communication, and the Boundary Guided Adaptive Fusion (BGAF) module enhances
boundary delineation by integrating spatial and semantic features under
boundary guidance. BEVANet achieves real-time segmentation at 33 FPS, yielding
79.3% mIoU without pretraining and 81.0% mIoU on Cityscapes after ImageNet
pretraining, demonstrating state-of-the-art performance. The code and model is
available at https://github.com/maomao0819/BEVANet.

</details>


### [96] [DragonFruitQualityNet: A Lightweight Convolutional Neural Network for Real-Time Dragon Fruit Quality Inspection on Mobile Devices](https://arxiv.org/abs/2508.07306)
*Md Zahurul Haquea,Yeahyea Sarker,Muhammed Farhan Sadique Mahi,Syed Jubayer Jaman,Md Robiul Islam*

Main category: cs.CV

TL;DR: 该研究提出了一种轻量级CNN模型DragonFruitQualityNet，用于火龙果的实时质量检测，准确率达93.98%，并嵌入移动应用以支持农民使用。


<details>
  <summary>Details</summary>
Motivation: 火龙果需求增长，但缺乏高效的预收和收获后质量检测方法，影响农业生产效率和损失控制。

Method: 研究使用13,789张火龙果图像数据集，训练轻量级CNN模型，分类为新鲜、未成熟、成熟和缺陷四类。

Result: 模型准确率达93.98%，优于现有方法，并成功嵌入移动应用实现实时检测。

Conclusion: 该研究为火龙果质量控制提供了高效、可扩展的AI解决方案，支持数字农业和小农户可持续发展。

Abstract: Dragon fruit, renowned for its nutritional benefits and economic value, has
experienced rising global demand due to its affordability and local
availability. As dragon fruit cultivation expands, efficient pre- and
post-harvest quality inspection has become essential for improving agricultural
productivity and minimizing post-harvest losses. This study presents
DragonFruitQualityNet, a lightweight Convolutional Neural Network (CNN)
optimized for real-time quality assessment of dragon fruits on mobile devices.
We curated a diverse dataset of 13,789 images, integrating self-collected
samples with public datasets (dataset from Mendeley Data), and classified them
into four categories: fresh, immature, mature, and defective fruits to ensure
robust model training. The proposed model achieves an impressive 93.98%
accuracy, outperforming existing methods in fruit quality classification. To
facilitate practical adoption, we embedded the model into an intuitive mobile
application, enabling farmers and agricultural stakeholders to conduct
on-device, real-time quality inspections. This research provides an accurate,
efficient, and scalable AI-driven solution for dragon fruit quality control,
supporting digital agriculture and empowering smallholder farmers with
accessible technology. By bridging the gap between research and real-world
application, our work advances post-harvest management and promotes sustainable
farming practices.

</details>


### [97] [MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark](https://arxiv.org/abs/2508.07307)
*Haiyang Guo,Fei Zhu,Hongbo Zhao,Fanhu Zeng,Wenzhuo Liu,Shijie Ma,Da-Han Wang,Xu-Yao Zhang*

Main category: cs.CV

TL;DR: 论文介绍了MCITlib，一个用于多模态大语言模型持续指令调优的代码库，旨在解决多模态持续学习中的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法在单模态任务中表现良好，但多模态大语言模型的兴起需要解决跨模态交互和协调的挑战。

Method: 开发了MCITlib代码库，实现了8种代表性算法，并在2个基准上进行了系统评估。

Result: MCITlib为多模态持续学习研究提供了工具支持，并将持续更新。

Conclusion: MCITlib是推动多模态持续学习研究的重要资源。

Abstract: Continual learning aims to equip AI systems with the ability to continuously
acquire and adapt to new knowledge without forgetting previously learned
information, similar to human learning. While traditional continual learning
methods focusing on unimodal tasks have achieved notable success, the emergence
of Multimodal Large Language Models has brought increasing attention to
Multimodal Continual Learning tasks involving multiple modalities, such as
vision and language. In this setting, models are expected to not only mitigate
catastrophic forgetting but also handle the challenges posed by cross-modal
interactions and coordination. To facilitate research in this direction, we
introduce MCITlib, a comprehensive and constantly evolving code library for
continual instruction tuning of Multimodal Large Language Models. In MCITlib,
we have currently implemented 8 representative algorithms for Multimodal
Continual Instruction Tuning and systematically evaluated them on 2 carefully
selected benchmarks. MCITlib will be continuously updated to reflect advances
in the Multimodal Continual Learning field. The codebase is released at
https://github.com/Ghy0501/MCITlib.

</details>


### [98] [MobileViCLIP: An Efficient Video-Text Model for Mobile Devices](https://arxiv.org/abs/2508.07312)
*Min Yang,Zihan Jia,Zhilin Dai,Sheng Guo,Limin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的视频文本模型MobileViCLIP，通过引入时间结构重参数化技术，在移动设备上实现快速推理和零样本分类与检索。


<details>
  <summary>Details</summary>
Motivation: 现有视频预训练模型多基于高延迟的ViT架构，缺乏针对移动设备的优化，本文旨在填补这一空白。

Method: 将时间结构重参数化引入高效的图像文本模型，并在大规模高质量视频文本数据集上训练。

Result: MobileViCLIP-Small在移动设备上的推理速度显著提升（比InternVideo2-L14快55.4倍，比InternVideo2-S14快6.7倍），零样本检索性能接近或优于对比模型。

Conclusion: MobileViCLIP是一种高效的视频文本模型，适用于移动设备，具有快速推理和强大的零样本能力。

Abstract: Efficient lightweight neural networks are with increasing attention due to
their faster reasoning speed and easier deployment on mobile devices. However,
existing video pre-trained models still focus on the common ViT architecture
with high latency, and few works attempt to build efficient architecture on
mobile devices. This paper bridges this gap by introducing temporal structural
reparameterization into an efficient image-text model and training it on a
large-scale high-quality video-text dataset, resulting in an efficient
video-text model that can run on mobile devices with strong zero-shot
classification and retrieval capabilities, termed as MobileViCLIP. In
particular, in terms of inference speed on mobile devices, our
MobileViCLIP-Small is 55.4x times faster than InternVideo2-L14 and 6.7x faster
than InternVideo2-S14. In terms of zero-shot retrieval performance, our
MobileViCLIP-Small obtains similar performance as InternVideo2-L14 and obtains
6.9\% better than InternVideo2-S14 on MSR-VTT. The code is available at
https://github.com/MCG-NJU/MobileViCLIP.

</details>


### [99] [DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding](https://arxiv.org/abs/2508.07313)
*Junyu Xiong,Yonghui Wang,Weichao Zhao,Chenyu Liu,Bing Yin,Wengang Zhou,Houqiang Li*

Main category: cs.CV

TL;DR: DocR1是一种基于强化学习的多模态大语言模型，通过EviGRPO框架实现多页文档理解，结合证据感知奖励机制和课程学习策略，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在多页文档理解中面临的细粒度视觉理解和跨页多跳推理的挑战。

Method: 提出EviGRPO强化学习框架，结合证据感知奖励机制和两阶段标注流程，构建EviBench和ArxivFullQA数据集。

Result: DocR1在多页任务中达到最先进性能，同时在单页任务中保持强劲表现。

Conclusion: EviGRPO框架和DocR1模型为多页文档理解提供了高效解决方案，且适用于有限监督场景。

Abstract: Understanding multi-page documents poses a significant challenge for
multimodal large language models (MLLMs), as it requires fine-grained visual
comprehension and multi-hop reasoning across pages. While prior work has
explored reinforcement learning (RL) for enhancing advanced reasoning in MLLMs,
its application to multi-page document understanding remains underexplored. In
this paper, we introduce DocR1, an MLLM trained with a novel RL framework,
Evidence Page-Guided GRPO (EviGRPO). EviGRPO incorporates an evidence-aware
reward mechanism that promotes a coarse-to-fine reasoning strategy, guiding the
model to first retrieve relevant pages before generating answers. This training
paradigm enables us to build high-quality models with limited supervision. To
support this, we design a two-stage annotation pipeline and a curriculum
learning strategy, based on which we construct two datasets: EviBench, a
high-quality training set with 4.8k examples, and ArxivFullQA, an evaluation
benchmark with 8.6k QA pairs based on scientific papers. Extensive experiments
across a wide range of benchmarks demonstrate that DocR1 achieves
state-of-the-art performance on multi-page tasks, while consistently
maintaining strong results on single-page benchmarks.

</details>


### [100] [RORPCap: Retrieval-based Objects and Relations Prompt for Image Captioning](https://arxiv.org/abs/2508.07318)
*Jinjing Gu,Tianbao Qin,Yuanyuan Pu,Zhengpeng Zhao*

Main category: cs.CV

TL;DR: RORPCap提出了一种基于检索的对象和关系提示方法，用于图像描述生成，解决了传统方法中冗余检测信息、GCN构建困难和训练成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 传统图像描述方法依赖对象检测器或GCN，存在冗余信息、构建困难和训练成本高的问题。RORPCap通过图像-文本检索提供丰富语义信息，优化了这一过程。

Method: RORPCap使用对象和关系提取模型提取关键词，结合预定义提示模板生成提示嵌入，并通过Mamba网络快速映射CLIP图像嵌入为视觉-文本嵌入，最终输入GPT-2生成描述。

Result: 在MS-COCO数据集上，RORPCap仅需2.6小时训练，CIDEr得分120.5%，SPICE得分22.0%，性能与检测器和GCN模型相当。

Conclusion: RORPCap在训练时间和性能上表现出色，是图像描述生成的有力替代方案。

Abstract: Image captioning aims to generate natural language descriptions for input
images in an open-form manner. To accurately generate descriptions related to
the image, a critical step in image captioning is to identify objects and
understand their relations within the image. Modern approaches typically
capitalize on object detectors or combine detectors with Graph Convolutional
Network (GCN). However, these models suffer from redundant detection
information, difficulty in GCN construction, and high training costs. To
address these issues, a Retrieval-based Objects and Relations Prompt for Image
Captioning (RORPCap) is proposed, inspired by the fact that image-text
retrieval can provide rich semantic information for input images. RORPCap
employs an Objects and relations Extraction Model to extract object and
relation words from the image. These words are then incorporate into predefined
prompt templates and encoded as prompt embeddings. Next, a Mamba-based mapping
network is designed to quickly map image embeddings extracted by CLIP to
visual-text embeddings. Finally, the resulting prompt embeddings and
visual-text embeddings are concatenated to form textual-enriched feature
embeddings, which are fed into a GPT-2 model for caption generation. Extensive
experiments conducted on the widely used MS-COCO dataset show that the RORPCap
requires only 2.6 hours under cross-entropy loss training, achieving 120.5%
CIDEr score and 22.0% SPICE score on the "Karpathy" test split. RORPCap
achieves comparable performance metrics to detector-based and GCN-based models
with the shortest training time and demonstrates its potential as an
alternative for image captioning.

</details>


### [101] [Planner-Refiner: Dynamic Space-Time Refinement for Vision-Language Alignment in Videos](https://arxiv.org/abs/2508.07330)
*Tuyen Tran,Thao Minh Le,Quang-Hung Le,Truyen Tran*

Main category: cs.CV

TL;DR: Planner-Refiner框架通过迭代优化视觉元素的时空表示，减少语言与视觉之间的语义鸿沟，提升视频-语言对齐任务的效果。


<details>
  <summary>Details</summary>
Motivation: 解决视频中语言与视觉对齐的复杂性，包括语言的多变性、实体交互的动态性以及语义鸿沟问题。

Method: Planner模块分解复杂语言提示为短句链，Refiner模块通过空间-时间自注意力机制逐步优化视觉表示。

Result: 在Referring Video Object Segmentation和Temporal Grounding任务中表现优异，尤其在复杂语言提示下优于现有方法。

Conclusion: Planner-Refiner框架在视频-语言对齐任务中具有显著潜力，尤其适用于复杂语言场景。

Abstract: Vision-language alignment in video must address the complexity of language,
evolving interacting entities, their action chains, and semantic gaps between
language and vision. This work introduces Planner-Refiner, a framework to
overcome these challenges. Planner-Refiner bridges the semantic gap by
iteratively refining visual elements' space-time representation, guided by
language until semantic gaps are minimal. A Planner module schedules language
guidance by decomposing complex linguistic prompts into short sentence chains.
The Refiner processes each short sentence, a noun-phrase and verb-phrase pair,
to direct visual tokens' self-attention across space then time, achieving
efficient single-step refinement. A recurrent system chains these steps,
maintaining refined visual token representations. The final representation
feeds into task-specific heads for alignment generation. We demonstrate
Planner-Refiner's effectiveness on two video-language alignment tasks:
Referring Video Object Segmentation and Temporal Grounding with varying
language complexity. We further introduce a new MeViS-X benchmark to assess
models' capability with long queries. Superior performance versus
state-of-the-art methods on these benchmarks shows the approach's potential,
especially for complex prompts.

</details>


### [102] [CoAR: Concept Injection into Autoregressive Models for Personalized Text-to-Image Generation](https://arxiv.org/abs/2508.07341)
*Fangtai Wu,Mushui Liu,Weijie He,Wanggui He,Hao Jiang,Zhao Wang,Yunlong Yu*

Main category: cs.CV

TL;DR: CoAR是一种新型框架，通过冻结预训练参数，仅学习少量参数实现定制化图像生成，解决了现有方法的高成本和过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 探索统一自回归模型在定制化图像生成中的潜力，解决现有方法因全微调或适配器导致的成本高、过拟合和灾难性遗忘问题。

Method: 采用分层多模态上下文学习策略，学习特定主题表示，并引入正则化技术防止过拟合和语言漂移。

Result: CoAR在主题和风格个性化任务中表现优异，计算和内存效率显著提升，仅调整0.05%的参数即可达到竞争性性能。

Conclusion: CoAR为定制化图像生成提供了一种高效、低成本且性能优越的解决方案。

Abstract: The unified autoregressive (AR) model excels at multimodal understanding and
generation, but its potential for customized image generation remains
underexplored. Existing customized generation methods rely on full fine-tuning
or adapters, making them costly and prone to overfitting or catastrophic
forgetting. In this paper, we propose \textbf{CoAR}, a novel framework for
injecting subject concepts into the unified AR models while keeping all
pre-trained parameters completely frozen. CoAR learns effective, specific
subject representations with only a minimal number of parameters using a
Layerwise Multimodal Context Learning strategy. To address overfitting and
language drift, we further introduce regularization that preserves the
pre-trained distribution and anchors context tokens to improve subject fidelity
and re-contextualization. Additionally, CoAR supports training-free subject
customization in a user-provided style. Experiments demonstrate that CoAR
achieves superior performance on both subject-driven personalization and style
personalization, while delivering significant gains in computational and memory
efficiency. Notably, CoAR tunes less than \textbf{0.05\%} of the parameters
while achieving competitive performance compared to recent Proxy-Tuning. Code:
https://github.com/KZF-kzf/CoAR

</details>


### [103] [SODiff: Semantic-Oriented Diffusion Model for JPEG Compression Artifacts Removal](https://arxiv.org/abs/2508.07346)
*Tingyu Yang,Jue Gong,Jinpei Guo,Wenbo Li,Yong Guo,Yulun Zhang*

Main category: cs.CV

TL;DR: SODiff是一种新颖的一步扩散模型，通过语义导向的指导有效去除JPEG伪影，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: JPEG高压缩比会导致严重视觉伪影，现有深度学习方法难以恢复复杂纹理细节。

Method: 提出SODiff模型，结合语义对齐的图像提示提取器（SAIPE）和质量因子感知时间预测器。

Result: 实验表明SODiff在视觉质量和定量指标上优于现有方法。

Conclusion: SODiff通过语义导向和自适应去噪步骤，显著提升了JPEG伪影去除效果。

Abstract: JPEG, as a widely used image compression standard, often introduces severe
visual artifacts when achieving high compression ratios. Although existing deep
learning-based restoration methods have made considerable progress, they often
struggle to recover complex texture details, resulting in over-smoothed
outputs. To overcome these limitations, we propose SODiff, a novel and
efficient semantic-oriented one-step diffusion model for JPEG artifacts
removal. Our core idea is that effective restoration hinges on providing
semantic-oriented guidance to the pre-trained diffusion model, thereby fully
leveraging its powerful generative prior. To this end, SODiff incorporates a
semantic-aligned image prompt extractor (SAIPE). SAIPE extracts rich features
from low-quality (LQ) images and projects them into an embedding space
semantically aligned with that of the text encoder. Simultaneously, it
preserves crucial information for faithful reconstruction. Furthermore, we
propose a quality factor-aware time predictor that implicitly learns the
compression quality factor (QF) of the LQ image and adaptively selects the
optimal denoising start timestep for the diffusion process. Extensive
experimental results show that our SODiff outperforms recent leading methods in
both visual quality and quantitative metrics. Code is available at:
https://github.com/frakenation/SODiff

</details>


### [104] [GS4Buildings: Prior-Guided Gaussian Splatting for 3D Building Reconstruction](https://arxiv.org/abs/2508.07355)
*Qilin Zhang,Olaf Wysocki,Boris Jutzi*

Main category: cs.CV

TL;DR: GS4Buildings利用语义3D建筑模型改进2D高斯泼溅（2DGS）在复杂城市场景中的重建效果，提升完整性和几何精度。


<details>
  <summary>Details</summary>
Motivation: 2DGS在大规模和复杂城市场景中因遮挡问题导致重建不完整，需结合语义建筑模型以提升性能。

Method: 直接从LoD2语义3D建筑模型初始化高斯，并利用先验深度和法线图优化重建过程。

Result: 实验显示重建完整性提升20.5%，几何精度提升32.8%，高斯基元减少71.8%。

Conclusion: 语义建筑模型整合可推动高斯泼溅技术在城市应用中的实际落地。

Abstract: Recent advances in Gaussian Splatting (GS) have demonstrated its
effectiveness in photo-realistic rendering and 3D reconstruction. Among these,
2D Gaussian Splatting (2DGS) is particularly suitable for surface
reconstruction due to its flattened Gaussian representation and integrated
normal regularization. However, its performance often degrades in large-scale
and complex urban scenes with frequent occlusions, leading to incomplete
building reconstructions. We propose GS4Buildings, a novel prior-guided
Gaussian Splatting method leveraging the ubiquity of semantic 3D building
models for robust and scalable building surface reconstruction. Instead of
relying on traditional Structure-from-Motion (SfM) pipelines, GS4Buildings
initializes Gaussians directly from low-level Level of Detail (LoD)2 semantic
3D building models. Moreover, we generate prior depth and normal maps from the
planar building geometry and incorporate them into the optimization process,
providing strong geometric guidance for surface consistency and structural
accuracy. We also introduce an optional building-focused mode that limits
reconstruction to building regions, achieving a 71.8% reduction in Gaussian
primitives and enabling a more efficient and compact representation.
Experiments on urban datasets demonstrate that GS4Buildings improves
reconstruction completeness by 20.5% and geometric accuracy by 32.8%. These
results highlight the potential of semantic building model integration to
advance GS-based reconstruction toward real-world urban applications such as
smart cities and digital twins. Our project is available:
https://github.com/zqlin0521/GS4Buildings.

</details>


### [105] [Training and Inference within 1 Second -- Tackle Cross-Sensor Degradation of Real-World Pansharpening with Efficient Residual Feature Tailoring](https://arxiv.org/abs/2508.07369)
*Tianyu Xin,Jin-Liang Xiao,Zeyu Xia,Shan Yin,Liang-Jian Deng*

Main category: cs.CV

TL;DR: 提出了一种模块化解耦和特征裁剪的方法，显著提升了跨传感器泛化能力，同时大幅降低了训练和推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习全色锐化模型在跨传感器数据上泛化能力差，传统方法耗时或需额外数据。

Method: 通过模块化解耦发现关键接口，引入特征裁剪器，采用物理感知无监督损失高效训练，并采用分块并行推理。

Result: 在跨传感器数据上实现最佳质量和效率，训练和推理时间极短（如512x512x8图像仅需0.2秒）。

Conclusion: 该方法显著提升了跨传感器泛化能力，且成本极低，适用于实际应用。

Abstract: Deep learning methods for pansharpening have advanced rapidly, yet models
pretrained on data from a specific sensor often generalize poorly to data from
other sensors. Existing methods to tackle such cross-sensor degradation include
retraining model or zero-shot methods, but they are highly time-consuming or
even need extra training data. To address these challenges, our method first
performs modular decomposition on deep learning-based pansharpening models,
revealing a general yet critical interface where high-dimensional fused
features begin mapping to the channel space of the final image. % may need
revisement A Feature Tailor is then integrated at this interface to address
cross-sensor degradation at the feature level, and is trained efficiently with
physics-aware unsupervised losses. Moreover, our method operates in a
patch-wise manner, training on partial patches and performing parallel
inference on all patches to boost efficiency. Our method offers two key
advantages: (1) $\textit{Improved Generalization Ability}$: it significantly
enhance performance in cross-sensor cases. (2) $\textit{Low Generalization
Cost}$: it achieves sub-second training and inference, requiring only partial
test inputs and no external data, whereas prior methods often take minutes or
even hours. Experiments on the real-world data from multiple datasets
demonstrate that our method achieves state-of-the-art quality and efficiency in
tackling cross-sensor degradation. For example, training and inference of
$512\times512\times8$ image within $\textit{0.2 seconds}$ and
$4000\times4000\times8$ image within $\textit{3 seconds}$ at the fastest
setting on a commonly used RTX 3090 GPU, which is over 100 times faster than
zero-shot methods.

</details>


### [106] [DIP-GS: Deep Image Prior For Gaussian Splatting Sparse View Recovery](https://arxiv.org/abs/2508.07372)
*Rajaei Khatib,Raja Giryes*

Main category: cs.CV

TL;DR: DIP-GS结合Deep Image Prior与3D高斯泼溅技术，解决了稀疏视图重建问题，无需预训练模型，表现优异。


<details>
  <summary>Details</summary>
Motivation: 3DGS在多视图重建中表现优异，但在稀疏视图场景下效果不佳。本文旨在解决这一问题。

Method: 提出DIP-GS，利用Deep Image Prior的内部结构和模式，以粗到细的方式优化3D高斯泼溅表示。

Result: DIP-GS在稀疏视图重建任务中达到SOTA水平，无需依赖预训练模型。

Conclusion: DIP-GS通过结合DIP与3DGS，显著提升了稀疏视图场景下的重建能力。

Abstract: 3D Gaussian Splatting (3DGS) is a leading 3D scene reconstruction method,
obtaining high-quality reconstruction with real-time rendering runtime
performance. The main idea behind 3DGS is to represent the scene as a
collection of 3D gaussians, while learning their parameters to fit the given
views of the scene. While achieving superior performance in the presence of
many views, 3DGS struggles with sparse view reconstruction, where the input
views are sparse and do not fully cover the scene and have low overlaps. In
this paper, we propose DIP-GS, a Deep Image Prior (DIP) 3DGS representation. By
using the DIP prior, which utilizes internal structure and patterns, with
coarse-to-fine manner, DIP-based 3DGS can operate in scenarios where vanilla
3DGS fails, such as sparse view recovery. Note that our approach does not use
any pre-trained models such as generative models and depth estimation, but
rather relies only on the input frames. Among such methods, DIP-GS obtains
state-of-the-art (SOTA) competitive results on various sparse-view
reconstruction tasks, demonstrating its capabilities.

</details>


### [107] [LET-US: Long Event-Text Understanding of Scenes](https://arxiv.org/abs/2508.07401)
*Rui Chen,Xingyu Chen,Shaoan Wang,Shihan Kong,Junzhi Yu*

Main category: cs.CV

TL;DR: LET-US是一个用于长事件流-文本理解的框架，通过自适应压缩机制减少输入事件量，同时保留关键视觉细节，提升了跨模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs难以有效解释事件流或仅能处理短序列，LET-US旨在填补这一空白，实现长事件流与文本的跨模态理解。

Method: 采用两阶段优化范式，结合文本引导的跨模态查询、分层聚类和相似性计算，以及大规模事件-文本对齐数据集训练。

Result: 实验表明，LET-US在长事件流的描述准确性和语义理解上优于现有MLLMs。

Conclusion: LET-US为长事件流与文本的跨模态理解设立了新标准，相关资源将公开。

Abstract: Event cameras output event streams as sparse, asynchronous data with
microsecond-level temporal resolution, enabling visual perception with low
latency and a high dynamic range. While existing Multimodal Large Language
Models (MLLMs) have achieved significant success in understanding and analyzing
RGB video content, they either fail to interpret event streams effectively or
remain constrained to very short sequences. In this paper, we introduce LET-US,
a framework for long event-stream--text comprehension that employs an adaptive
compression mechanism to reduce the volume of input events while preserving
critical visual details. LET-US thus establishes a new frontier in cross-modal
inferential understanding over extended event sequences. To bridge the
substantial modality gap between event streams and textual representations, we
adopt a two-stage optimization paradigm that progressively equips our model
with the capacity to interpret event-based scenes. To handle the voluminous
temporal information inherent in long event streams, we leverage text-guided
cross-modal queries for feature reduction, augmented by hierarchical clustering
and similarity computation to distill the most representative event features.
Moreover, we curate and construct a large-scale event-text aligned dataset to
train our model, achieving tighter alignment of event features within the LLM
embedding space. We also develop a comprehensive benchmark covering a diverse
set of tasks -- reasoning, captioning, classification, temporal localization
and moment retrieval. Experimental results demonstrate that LET-US outperforms
prior state-of-the-art MLLMs in both descriptive accuracy and semantic
comprehension on long-duration event streams. All datasets, codes, and models
will be publicly available.

</details>


### [108] [ForensicsSAM: Toward Robust and Unified Image Forgery Detection and Localization Resisting to Adversarial Attack](https://arxiv.org/abs/2508.07402)
*Rongxuan Peng,Shunquan Tan,Chenqi Kong,Anwei Luo,Alex C. Kot,Jiwu Huang*

Main category: cs.CV

TL;DR: 论文提出ForensicsSAM，一种对抗性鲁棒的图像伪造检测与定位框架，通过注入伪造和对抗专家模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法忽视对抗攻击的脆弱性，导致性能下降。

Method: 设计ForensicsSAM框架，包括伪造专家注入、轻量级对抗检测器和对抗专家模块。

Result: 实验表明ForensicsSAM能有效抵抗多种对抗攻击，并在伪造检测与定位任务中表现优异。

Conclusion: ForensicsSAM为对抗性鲁棒的图像伪造检测提供了统一解决方案。

Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as a popular strategy for
adapting large vision foundation models, such as the Segment Anything Model
(SAM) and LLaVA, to downstream tasks like image forgery detection and
localization (IFDL). However, existing PEFT-based approaches overlook their
vulnerability to adversarial attacks. In this paper, we show that highly
transferable adversarial images can be crafted solely via the upstream model,
without accessing the downstream model or training data, significantly
degrading the IFDL performance. To address this, we propose ForensicsSAM, a
unified IFDL framework with built-in adversarial robustness. Our design is
guided by three key ideas: (1) To compensate for the lack of forgery-relevant
knowledge in the frozen image encoder, we inject forgery experts into each
transformer block to enhance its ability to capture forgery artifacts. These
forgery experts are always activated and shared across any input images. (2) To
detect adversarial images, we design an light-weight adversary detector that
learns to capture structured, task-specific artifact in RGB domain, enabling
reliable discrimination across various attack methods. (3) To resist
adversarial attacks, we inject adversary experts into the global attention
layers and MLP modules to progressively correct feature shifts induced by
adversarial noise. These adversary experts are adaptively activated by the
adversary detector, thereby avoiding unnecessary interference with clean
images. Extensive experiments across multiple benchmarks demonstrate that
ForensicsSAM achieves superior resistance to various adversarial attack
methods, while also delivering state-of-the-art performance in image-level
forgery detection and pixel-level forgery localization. The resource is
available at https://github.com/siriusPRX/ForensicsSAM.

</details>


### [109] [CharacterShot: Controllable and Consistent 4D Character Animation](https://arxiv.org/abs/2508.07409)
*Junyao Gao,Jiaxing Li,Wenran Liu,Yanhong Zeng,Fei Shen,Kai Chen,Yanan Sun,Cairong Zhao*

Main category: cs.CV

TL;DR: CharacterShot是一个可控且一致的4D角色动画框架，通过单张参考图像和2D姿势序列生成动态3D角色动画。


<details>
  <summary>Details</summary>
Motivation: 旨在让设计师能够轻松创建动态3D角色动画，解决现有方法在可控性和一致性上的不足。

Method: 先预训练基于DiT的2D动画模型，再通过双注意力模块和相机先验将其提升到3D，最后用4D高斯溅射优化生成4D角色表示。

Result: 在CharacterBench基准测试中表现优于现有方法，并构建了大规模数据集Character4D。

Conclusion: CharacterShot在生成高质量4D角色动画方面具有显著优势，代码和数据集将开源。

Abstract: In this paper, we propose \textbf{CharacterShot}, a controllable and
consistent 4D character animation framework that enables any individual
designer to create dynamic 3D characters (i.e., 4D character animation) from a
single reference character image and a 2D pose sequence. We begin by
pretraining a powerful 2D character animation model based on a cutting-edge
DiT-based image-to-video model, which allows for any 2D pose sequnce as
controllable signal. We then lift the animation model from 2D to 3D through
introducing dual-attention module together with camera prior to generate
multi-view videos with spatial-temporal and spatial-view consistency. Finally,
we employ a novel neighbor-constrained 4D gaussian splatting optimization on
these multi-view videos, resulting in continuous and stable 4D character
representations. Moreover, to improve character-centric performance, we
construct a large-scale dataset Character4D, containing 13,115 unique
characters with diverse appearances and motions, rendered from multiple
viewpoints. Extensive experiments on our newly constructed benchmark,
CharacterBench, demonstrate that our approach outperforms current
state-of-the-art methods. Code, models, and datasets will be publicly available
at https://github.com/Jeoyal/CharacterShot.

</details>


### [110] [CLUE: Leveraging Low-Rank Adaptation to Capture Latent Uncovered Evidence for Image Forgery Localization](https://arxiv.org/abs/2508.07413)
*Youqi Wang,Shunquan Tan,Rongxuan Peng,Bin Li,Jiwu Huang*

Main category: cs.CV

TL;DR: CLUE利用Stable Diffusion 3和Segment Anything Model，通过噪声注入和参数高效调整，实现了高精度的伪造图像定位。


<details>
  <summary>Details</summary>
Motivation: 解决数字媒体伪造泛滥问题，提升伪造检测的准确性和鲁棒性。

Method: 结合SD3的Rectified Flow机制和LoRA调整，注入噪声放大伪造痕迹，并利用SAM提取语义和空间特征。

Result: CLUE在泛化性能和鲁棒性上显著优于现有方法。

Conclusion: CLUE为伪造检测提供了高效且强大的解决方案，代码已开源。

Abstract: The increasing accessibility of image editing tools and generative AI has led
to a proliferation of visually convincing forgeries, compromising the
authenticity of digital media. In this paper, in addition to leveraging
distortions from conventional forgeries, we repurpose the mechanism of a
state-of-the-art (SOTA) text-to-image synthesis model by exploiting its
internal generative process, turning it into a high-fidelity forgery
localization tool. To this end, we propose CLUE (Capture Latent Uncovered
Evidence), a framework that employs Low- Rank Adaptation (LoRA) to
parameter-efficiently reconfigure Stable Diffusion 3 (SD3) as a forensic
feature extractor. Our approach begins with the strategic use of SD3's
Rectified Flow (RF) mechanism to inject noise at varying intensities into the
latent representation, thereby steering the LoRAtuned denoising process to
amplify subtle statistical inconsistencies indicative of a forgery. To
complement the latent analysis with high-level semantic context and precise
spatial details, our method incorporates contextual features from the image
encoder of the Segment Anything Model (SAM), which is parameter-efficiently
adapted to better trace the boundaries of forged regions. Extensive evaluations
demonstrate CLUE's SOTA generalization performance, significantly outperforming
prior methods. Furthermore, CLUE shows superior robustness against common
post-processing attacks and Online Social Networks (OSNs). Code is publicly
available at https://github.com/SZAISEC/CLUE.

</details>


### [111] [Freeze and Reveal: Exposing Modality Bias in Vision-Language Models](https://arxiv.org/abs/2508.07432)
*Vivek Hruday Kavuri,Vysishtya Karanam,Venkata Jahnavi Venkamsetty,Kriti Madumadukala,Lakshmipathi Balaji Darur,Ponnurangam Kumaraguru*

Main category: cs.CV

TL;DR: 论文研究了视觉语言模型中的性别偏见，提出了两种去偏方法（CDA和DAUDoS），并通过实验发现视觉和文本编码器的偏见来源不同。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在训练数据中继承了性别偏见，需要分析并减少这种偏见。

Method: 使用反事实数据增强（CDA）和基于刻板印象程度的新方法（DAUDoS）进行去偏。

Result: CDA和DAUDoS分别减少性别差距6%和3%，DAUDoS仅用三分之一数据。实验发现CLIP视觉编码器和PaliGemma2文本编码器偏见更显著。

Conclusion: 通过识别偏见来源，未来可更有针对性地减少多模态系统中的偏见。

Abstract: Vision Language Models achieve impressive multi-modal performance but often
inherit gender biases from their training data. This bias might be coming from
both the vision and text modalities. In this work, we dissect the contributions
of vision and text backbones to these biases by applying targeted debiasing
using Counterfactual Data Augmentation and Task Vector methods. Inspired by
data-efficient approaches in hate-speech classification, we introduce a novel
metric, Degree of Stereotypicality and a corresponding debiasing method, Data
Augmentation Using Degree of Stereotypicality - DAUDoS, to reduce bias with
minimal computational cost. We curate a gender annotated dataset and evaluate
all methods on VisoGender benchmark to quantify improvements and identify
dominant source of bias. Our results show that CDA reduces the gender gap by 6%
and DAUDoS by 3% but using only one-third of the data. Both methods also
improve the model's ability to correctly identify gender in images by 3%, with
DAUDoS achieving this improvement using only almost one-third of training data.
From our experiment's, we observed that CLIP's vision encoder is more biased
whereas PaliGemma2's text encoder is more biased. By identifying whether bias
stems more from vision or text encoders, our work enables more targeted and
effective bias mitigation strategies in future multi-modal systems.

</details>


### [112] [Levarging Learning Bias for Noisy Anomaly Detection](https://arxiv.org/abs/2508.07441)
*Yuxin Zhang,Yunkang Cao,Yuqi Cheng,Yihan Sun,Weiming Shen*

Main category: cs.CV

TL;DR: 本文提出了一种两阶段框架，利用模型的学习偏置来解决完全无监督图像异常检测（FUIAD）中训练数据可能包含未标记异常的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设训练数据无异常，但实际数据可能被污染，导致模型将异常误认为正常，影响检测性能。

Method: 通过两阶段框架：第一阶段利用学习偏置（统计优势和特征空间差异）过滤出纯净数据集；第二阶段在纯净数据上训练最终检测器。

Result: 在Real-IAD基准测试中表现出优异的异常检测和定位性能，且对噪声具有鲁棒性。

Conclusion: 该框架模型无关，适用于多种无监督骨干网络，为实际场景中不完美训练数据提供了实用解决方案。

Abstract: This paper addresses the challenge of fully unsupervised image anomaly
detection (FUIAD), where training data may contain unlabeled anomalies.
Conventional methods assume anomaly-free training data, but real-world
contamination leads models to absorb anomalies as normal, degrading detection
performance. To mitigate this, we propose a two-stage framework that
systematically exploits inherent learning bias in models. The learning bias
stems from: (1) the statistical dominance of normal samples, driving models to
prioritize learning stable normal patterns over sparse anomalies, and (2)
feature-space divergence, where normal data exhibit high intra-class
consistency while anomalies display high diversity, leading to unstable model
responses. Leveraging the learning bias, stage 1 partitions the training set
into subsets, trains sub-models, and aggregates cross-model anomaly scores to
filter a purified dataset. Stage 2 trains the final detector on this dataset.
Experiments on the Real-IAD benchmark demonstrate superior anomaly detection
and localization performance under different noise conditions. Ablation studies
further validate the framework's contamination resilience, emphasizing the
critical role of learning bias exploitation. The model-agnostic design ensures
compatibility with diverse unsupervised backbones, offering a practical
solution for real-world scenarios with imperfect training data. Code is
available at https://github.com/hustzhangyuxin/LLBNAD.

</details>


### [113] [Health Care Waste Classification Using Deep Learning Aligned with Nepal's Bin Color Guidelines](https://arxiv.org/abs/2508.07450)
*Suman Kunwar,Prabesh Rai*

Main category: cs.CV

TL;DR: 论文研究了尼泊尔医疗废物管理的挑战，通过比较多种先进分类模型，发现YOLOv5-s准确率最高（95.06%），但推理速度略逊于YOLOv8-n。EfficientNet-B0表现也不错（93.22%），但推理时间最长。最终部署了YOLOv5-s模型，并建议进一步优化数据和本地化。


<details>
  <summary>Details</summary>
Motivation: 尼泊尔医疗设施增加导致医疗废物管理问题加剧，不正确的分类和处理可能引发污染和传染病传播，对废物处理人员构成风险。

Method: 研究采用分层K折技术（5折）比较了ResNeXt-50、EfficientNet-B0、MobileNetV3-S、YOLOv8-n和YOLOv5-s等模型在医疗废物数据上的表现。

Result: YOLOv5-s准确率最高（95.06%），但推理速度略慢于YOLOv8-n；EfficientNet-B0准确率93.22%，但推理时间最长。通过重复ANOVA验证统计显著性。

Conclusion: 研究成功部署了YOLOv5-s模型，并建议进一步优化数据和本地化以适应实际需求。

Abstract: The increasing number of Health Care facilities in Nepal has also added up
the challenges on managing health care waste (HCW). Improper segregation and
disposal of HCW leads to the contamination, spreading of infectious diseases
and puts a risk of waste handlers. This study benchmarks the state of the art
waste classification models: ResNeXt-50, EfficientNet-B0, MobileNetV3-S,
YOLOv8-n and YOLOv5-s using Stratified K-fold techniques where we use 5 folds
on combined HCW data, and found that the YOLOv5-s achieved higher of 95.06%
accuracy but fell short few milliseconds in inference speed with YOLOv8-n
model. The EfficientNet-B0 showed promising results of 93.22% accuracy but took
the highest inference time. A repetitive ANOVA was performed to see statistical
significance and the best performing model (YOLOv5-s) was deployed to the web
with mapped bin color using Nepal's HCW management standards for public usage.
Further work on the data was suggested along with localized context.

</details>


### [114] [AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning](https://arxiv.org/abs/2508.07470)
*Siminfar Samakoush Galougah,Rishie Raj,Sanjoy Chowdhury,Sayan Nag,Ramani Duraiswami*

Main category: cs.CV

TL;DR: AURA是一个新的音频-视觉基准测试，旨在评估跨模态推理能力，避免单模态捷径，并引入AuraScore评估推理的真实性和逻辑性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试仅关注最终答案准确性，忽视了推理过程，难以区分真正理解与错误推理或幻觉导致的正确答案。

Method: AURA包含六个认知领域的问题，要求模型基于音频和视频构建逻辑路径，并提出AuraScore评估推理的忠实性和逻辑有效性。

Result: 评估显示，尽管模型在某些任务上准确率高达92%，但其推理的真实性和逻辑性得分低于45%，表明模型常通过错误逻辑得出正确答案。

Conclusion: AURA揭示了现有模型的推理缺陷，为更稳健的多模态评估提供了基础。

Abstract: Current audio-visual (AV) benchmarks focus on final answer accuracy,
overlooking the underlying reasoning process. This makes it difficult to
distinguish genuine comprehension from correct answers derived through flawed
reasoning or hallucinations. To address this, we introduce AURA (Audio-visual
Understanding and Reasoning Assessment), a benchmark for evaluating the
cross-modal reasoning capabilities of Audio-Visual Large Language Models
(AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions across
six challenging cognitive domains, such as causality, timbre and pitch, tempo
and AV synchronization, unanswerability, implicit distractions, and skill
profiling, explicitly designed to be unanswerable from a single modality. This
forces models to construct a valid logical path grounded in both audio and
video, setting AURA apart from AV datasets that allow uni-modal shortcuts. To
assess reasoning traces, we propose a novel metric, AuraScore, which addresses
the lack of robust tools for evaluating reasoning fidelity. It decomposes
reasoning into two aspects: (i) Factual Consistency - whether reasoning is
grounded in perceptual evidence, and (ii) Core Inference - the logical validity
of each reasoning step. Evaluations of SOTA models on AURA reveal a critical
reasoning gap: although models achieve high accuracy (up to 92% on some tasks),
their Factual Consistency and Core Inference scores fall below 45%. This
discrepancy highlights that models often arrive at correct answers through
flawed logic, underscoring the need for our benchmark and paving the way for
more robust multimodal evaluation.

</details>


### [115] [Novel View Synthesis with Gaussian Splatting: Impact on Photogrammetry Model Accuracy and Resolution](https://arxiv.org/abs/2508.07483)
*Pranav Chougule*

Main category: cs.CV

TL;DR: 论文比较了摄影测量和高斯泼溅技术在3D模型重建和视图合成中的表现，通过多种指标评估性能，并展示了高斯泼溅在生成高质量新视图方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在比较两种3D重建技术的优劣，探索高斯泼溅在提升摄影测量模型质量方面的潜力，为扩展现实（XR）和自动驾驶模拟等应用提供参考。

Method: 使用真实场景图像数据集，分别通过摄影测量和高斯泼溅技术构建3D模型，并通过SSIM、PSNR、LPIPS和lp/mm分辨率等指标进行性能评估。改进了高斯泼溅代码库以支持Blender环境中的新视图渲染。

Result: 高斯泼溅能够生成高质量的新视图，并显著提升摄影测量模型的性能。两种方法各有优劣，适用于不同应用场景。

Conclusion: 高斯泼溅在视图合成方面表现出色，有望改进摄影测量技术。研究结果为XR和自动驾驶模拟等领域提供了实用参考。

Abstract: In this paper, I present a comprehensive study comparing Photogrammetry and
Gaussian Splatting techniques for 3D model reconstruction and view synthesis. I
created a dataset of images from a real-world scene and constructed 3D models
using both methods. To evaluate the performance, I compared the models using
structural similarity index (SSIM), peak signal-to-noise ratio (PSNR), learned
perceptual image patch similarity (LPIPS), and lp/mm resolution based on the
USAF resolution chart. A significant contribution of this work is the
development of a modified Gaussian Splatting repository, which I forked and
enhanced to enable rendering images from novel camera poses generated in the
Blender environment. This innovation allows for the synthesis of high-quality
novel views, showcasing the flexibility and potential of Gaussian Splatting. My
investigation extends to an augmented dataset that includes both original
ground images and novel views synthesized via Gaussian Splatting. This
augmented dataset was employed to generate a new photogrammetry model, which
was then compared against the original photogrammetry model created using only
the original images. The results demonstrate the efficacy of using Gaussian
Splatting to generate novel high-quality views and its potential to improve
photogrammetry-based 3D reconstructions. The comparative analysis highlights
the strengths and limitations of both approaches, providing valuable
information for applications in extended reality (XR), photogrammetry, and
autonomous vehicle simulations. Code is available at
https://github.com/pranavc2255/gaussian-splatting-novel-view-render.git.

</details>


### [116] [VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding](https://arxiv.org/abs/2508.07493)
*Jian Chen,Ming Li,Jihyung Kil,Chenguang Wang,Tong Yu,Ryan Rossi,Tianyi Zhou,Changyou Chen,Ruiyi Zhang*

Main category: cs.CV

TL;DR: VisR-Bench是一个多语言基准测试，用于长文档中的问题驱动多模态检索，覆盖16种语言和三种问题类型，评估了多种检索模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试仅关注英语文档检索或单页图像的多语言问答，无法满足多语言长文档的多模态检索需求。

Method: 引入VisR-Bench基准测试，包含35K QA对和1.2K文档，支持多语言和多模态检索评估。

Result: MLLMs表现优于文本和多模态编码器模型，但在结构化表格和低资源语言上仍有挑战。

Conclusion: VisR-Bench填补了多语言长文档多模态检索的空白，揭示了当前模型的局限性。

Abstract: Most organizational data in this world are stored as documents, and visual
retrieval plays a crucial role in unlocking the collective intelligence from
all these documents. However, existing benchmarks focus on English-only
document retrieval or only consider multilingual question-answering on a
single-page image. To bridge this gap, we introduce VisR-Bench, a multilingual
benchmark designed for question-driven multimodal retrieval in long documents.
Our benchmark comprises over 35K high-quality QA pairs across 1.2K documents,
enabling fine-grained evaluation of multimodal retrieval. VisR-Bench spans
sixteen languages with three question types (figures, text, and tables),
offering diverse linguistic and question coverage. Unlike prior datasets, we
include queries without explicit answers, preventing models from relying on
superficial keyword matching. We evaluate various retrieval models, including
text-based methods, multimodal encoders, and MLLMs, providing insights into
their strengths and limitations. Our results show that while MLLMs
significantly outperform text-based and multimodal encoder models, they still
struggle with structured tables and low-resource languages, highlighting key
challenges in multilingual visual retrieval.

</details>


### [117] [FormCoach: Lift Smarter, Not Harder](https://arxiv.org/abs/2508.07501)
*Xiaoye Zuo,Nikos Athanasiou,Ginger Delmas,Yiming Huang,Xingyu Fu,Lingjie Liu*

Main category: cs.CV

TL;DR: FormCoach是一个基于视觉语言模型（VLMs）的AI健身教练，通过摄像头实时检测和纠正用户动作，并发布数据集和评估工具以推动研究。


<details>
  <summary>Details</summary>
Motivation: 针对家庭健身爱好者缺乏专业指导的问题，开发实时AI反馈系统。

Method: 利用视觉语言模型（VLMs）分析用户动作，通过web界面提供实时纠正，并基于1700个专家标注视频对进行模型评估。

Result: 模型表现与人类教练仍有显著差距，但展示了AI在动作分析中的潜力。

Conclusion: FormCoach为AI与人类协作的运动纠正开辟了新方向，并提供了研究工具。

Abstract: Good form is the difference between strength and strain, yet for the
fast-growing community of at-home fitness enthusiasts, expert feedback is often
out of reach. FormCoach transforms a simple camera into an always-on,
interactive AI training partner, capable of spotting subtle form errors and
delivering tailored corrections in real time, leveraging vision-language models
(VLMs). We showcase this capability through a web interface and benchmark
state-of-the-art VLMs on a dataset of 1,700 expert-annotated user-reference
video pairs spanning 22 strength and mobility exercises. To accelerate research
in AI-driven coaching, we release both the dataset and an automated,
rubric-based evaluation pipeline, enabling standardized comparison across
models. Our benchmarks reveal substantial gaps compared to human-level
coaching, underscoring both the challenges and opportunities in integrating
nuanced, context-aware movement analysis into interactive AI systems. By
framing form correction as a collaborative and creative process between humans
and machines, FormCoach opens a new frontier in embodied AI.

</details>


### [118] [From Field to Drone: Domain Drift Tolerant Automated Multi-Species and Damage Plant Semantic Segmentation for Herbicide Trials](https://arxiv.org/abs/2508.07514)
*Artzai Picon,Itziar Eguskiza,Daniel Mugica,Javier Romero,Carlos Javier Jimenez,Eric White,Gabriel Do-Lago-Junqueira,Christian Klukas,Ramon Navarra-Mestre*

Main category: cs.CV

TL;DR: 该论文提出了一种结合自监督视觉模型和植物分类层次推理的改进分割模型，用于自动化作物和杂草的物种识别与损害分类，显著提升了效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 传统的手动视觉评估耗时、费力且主观，自动化识别因视觉差异细微而具有挑战性，但能显著提高效率和一致性。

Method: 结合自监督视觉模型和植物分类层次推理的分割模型，利用2018-2020年德国和西班牙的多设备数据集训练，并在2023年数字相机数据和2024年无人机图像上进行跨设备评估。

Result: 模型显著提升了物种识别（F1-score: 0.52到0.85）和损害分类（F1-score: 0.28到0.44）的性能，在跨设备测试中仍保持较强鲁棒性。

Conclusion: 模型表现出强大的鲁棒性和实际应用价值，已被部署用于大规模自动化作物和杂草监测。

Abstract: Field trials are vital in herbicide research and development to assess
effects on crops and weeds under varied conditions. Traditionally, evaluations
rely on manual visual assessments, which are time-consuming, labor-intensive,
and subjective. Automating species and damage identification is challenging due
to subtle visual differences, but it can greatly enhance efficiency and
consistency.
  We present an improved segmentation model combining a general-purpose
self-supervised visual model with hierarchical inference based on botanical
taxonomy. Trained on a multi-year dataset (2018-2020) from Germany and Spain
using digital and mobile cameras, the model was tested on digital camera data
(year 2023) and drone imagery from the United States, Germany, and Spain (year
2024) to evaluate robustness under domain shift. This cross-device evaluation
marks a key step in assessing generalization across platforms of the model.
  Our model significantly improved species identification (F1-score: 0.52 to
0.85, R-squared: 0.75 to 0.98) and damage classification (F1-score: 0.28 to
0.44, R-squared: 0.71 to 0.87) over prior methods. Under domain shift (drone
images), it maintained strong performance with moderate degradation (species:
F1-score 0.60, R-squared 0.80; damage: F1-score 0.41, R-squared 0.62), where
earlier models failed.
  These results confirm the model's robustness and real-world applicability. It
is now deployed in BASF's phenotyping pipeline, enabling large-scale, automated
crop and weed monitoring across diverse geographies.

</details>


### [119] [Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing](https://arxiv.org/abs/2508.07519)
*Joonghyuk Shin,Alchan Hwang,Yujin Kim,Daneul Kim,Jaesik Park*

Main category: cs.CV

TL;DR: 本文分析了Transformer-based扩散模型中的MM-DiT注意力机制，提出了一种基于提示的图像编辑方法，适用于多种MM-DiT变体。


<details>
  <summary>Details</summary>
Motivation: 传统U-Net架构被Transformer-based扩散模型取代，但MM-DiT的双向注意力机制对现有编辑技术提出了挑战，需要深入研究其行为模式。

Method: 通过分解MM-DiT的注意力矩阵为四个块，分析其特性，并提出一种基于提示的图像编辑方法。

Result: 提出了一种适用于MM-DiT的全局到局部图像编辑方法，支持多种变体。

Conclusion: 研究填补了U-Net与新兴架构之间的差距，为MM-DiT的行为模式提供了更深入的理解。

Abstract: Transformer-based diffusion models have recently superseded traditional U-Net
architectures, with multimodal diffusion transformers (MM-DiT) emerging as the
dominant approach in state-of-the-art models like Stable Diffusion 3 and
Flux.1. Previous approaches have relied on unidirectional cross-attention
mechanisms, with information flowing from text embeddings to image latents. In
contrast, MMDiT introduces a unified attention mechanism that concatenates
input projections from both modalities and performs a single full attention
operation, allowing bidirectional information flow between text and image
branches. This architectural shift presents significant challenges for existing
editing techniques. In this paper, we systematically analyze MM-DiT's attention
mechanism by decomposing attention matrices into four distinct blocks,
revealing their inherent characteristics. Through these analyses, we propose a
robust, prompt-based image editing method for MM-DiT that supports global to
local edits across various MM-DiT variants, including few-step models. We
believe our findings bridge the gap between existing U-Net-based methods and
emerging architectures, offering deeper insights into MMDiT's behavioral
patterns.

</details>


### [120] [Enhancing Reliability of Medical Image Diagnosis through Top-rank Learning with Rejection Module](https://arxiv.org/abs/2508.07528)
*Xiaotong Ji,Ryoma Bise,Seiichi Uchida*

Main category: cs.CV

TL;DR: 提出了一种结合拒绝模块的top-rank学习方法，用于处理医学图像中的噪声标签和类别模糊实例，提升诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 医学图像诊断的准确性至关重要，但噪声标签和类别模糊实例会干扰top-rank学习的目标。

Method: 提出了一种集成拒绝模块的方法，该模块与top-rank损失协同优化，识别并减少异常值的影响。

Result: 在医学数据集上的实验验证了该方法能有效检测和减少异常值，提高诊断的可靠性和准确性。

Conclusion: 该方法通过拒绝模块增强了top-rank学习，显著提升了医学图像诊断的性能。

Abstract: In medical image processing, accurate diagnosis is of paramount importance.
Leveraging machine learning techniques, particularly top-rank learning, shows
significant promise by focusing on the most crucial instances. However,
challenges arise from noisy labels and class-ambiguous instances, which can
severely hinder the top-rank objective, as they may be erroneously placed among
the top-ranked instances. To address these, we propose a novel approach that
enhances toprank learning by integrating a rejection module. Cooptimized with
the top-rank loss, this module identifies and mitigates the impact of outliers
that hinder training effectiveness. The rejection module functions as an
additional branch, assessing instances based on a rejection function that
measures their deviation from the norm. Through experimental validation on a
medical dataset, our methodology demonstrates its efficacy in detecting and
mitigating outliers, improving the reliability and accuracy of medical image
diagnoses.

</details>


### [121] [Enhanced Generative Structure Prior for Chinese Text Image Super-resolution](https://arxiv.org/abs/2508.07537)
*Xiaoming Li,Wangmeng Zuo,Chen Change Loy*

Main category: cs.CV

TL;DR: 提出了一种基于结构先验的高质量中文文本图像超分辨率框架，通过结合StyleGAN和代码本机制，恢复低分辨率中文字符的精确笔画。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注英文文本，对复杂脚本（如中文）的超分辨率研究较少。本文旨在解决低分辨率中文字符的精确恢复问题。

Method: 提出一种结构先验，结合StyleGAN的生成能力，并通过代码本机制限制生成空间，确保字符结构的完整性。

Result: 实验表明，该方法能有效恢复低分辨率中文字符的清晰笔画，适用于不规则布局的真实场景。

Conclusion: 结构先验提供了字符级别的指导，显著提升了中文文本图像超分辨率的质量。

Abstract: Faithful text image super-resolution (SR) is challenging because each
character has a unique structure and usually exhibits diverse font styles and
layouts. While existing methods primarily focus on English text, less attention
has been paid to more complex scripts like Chinese. In this paper, we introduce
a high-quality text image SR framework designed to restore the precise strokes
of low-resolution (LR) Chinese characters. Unlike methods that rely on
character recognition priors to regularize the SR task, we propose a novel
structure prior that offers structure-level guidance to enhance visual quality.
Our framework incorporates this structure prior within a StyleGAN model,
leveraging its generative capabilities for restoration. To maintain the
integrity of character structures while accommodating various font styles and
layouts, we implement a codebook-based mechanism that restricts the generative
space of StyleGAN. Each code in the codebook represents the structure of a
specific character, while the vector $w$ in StyleGAN controls the character's
style, including typeface, orientation, and location. Through the collaborative
interaction between the codebook and style, we generate a high-resolution
structure prior that aligns with LR characters both spatially and structurally.
Experiments demonstrate that this structure prior provides robust,
character-specific guidance, enabling the accurate restoration of clear strokes
in degraded characters, even for real-world LR Chinese text with irregular
layouts. Our code and pre-trained models will be available at
https://github.com/csxmli2016/MARCONetPlusPlus

</details>


### [122] [A DICOM Image De-identification Algorithm in the MIDI-B Challenge](https://arxiv.org/abs/2508.07538)
*Hongzhu Jiang,Sihan Xie,Zhiyu Wan*

Main category: cs.CV

TL;DR: 论文探讨了医学图像去标识化的重要性，介绍了MIDI-B挑战赛及其方法，展示了算法的优异表现，并分析了当前方法的局限性和未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 医学图像共享需符合隐私法规（如HIPAA、DICOM PS3.15），去标识化是保护患者隐私的关键，同时需确保数据仍可用于研究和诊疗。

Method: 采用像素掩码、日期偏移、日期哈希、文本识别、替换和移除等方法，严格遵循标准处理数据集。

Result: 算法在MIDI-B挑战赛中正确执行99.92%的操作，排名第二（共10支完成团队）。

Conclusion: 当前方法仍有局限性，未来需进一步优化以提升去标识化效果。

Abstract: Image de-identification is essential for the public sharing of medical
images, particularly in the widely used Digital Imaging and Communications in
Medicine (DICOM) format as required by various regulations and standards,
including Health Insurance Portability and Accountability Act (HIPAA) privacy
rules, the DICOM PS3.15 standard, and best practices recommended by the Cancer
Imaging Archive (TCIA). The Medical Image De-Identification Benchmark (MIDI-B)
Challenge at the 27th International Conference on Medical Image Computing and
Computer Assisted Intervention (MICCAI 2024) was organized to evaluate
rule-based DICOM image de-identification algorithms with a large dataset of
clinical DICOM images. In this report, we explore the critical challenges of
de-identifying DICOM images, emphasize the importance of removing personally
identifiable information (PII) to protect patient privacy while ensuring the
continued utility of medical data for research, diagnostics, and treatment, and
provide a comprehensive overview of the standards and regulations that govern
this process. Additionally, we detail the de-identification methods we applied
- such as pixel masking, date shifting, date hashing, text recognition, text
replacement, and text removal - to process datasets during the test phase in
strict compliance with these standards. According to the final leaderboard of
the MIDI-B challenge, the latest version of our solution algorithm correctly
executed 99.92% of the required actions and ranked 2nd out of 10 teams that
completed the challenge (from a total of 22 registered teams). Finally, we
conducted a thorough analysis of the resulting statistics and discussed the
limitations of current approaches and potential avenues for future improvement.

</details>


### [123] [Domain Generalization of Pathological Image Segmentation by Patch-Level and WSI-Level Contrastive Learning](https://arxiv.org/abs/2508.07539)
*Yuki Shigeyasu,Shota Harada,Akihiko Yoshizawa,Kazuhiro Terada,Naoki Nakazima,Mariyo Kurata,Hiroyuki Abe,Tetsuo Ushiku,Ryoma Bise*

Main category: cs.CV

TL;DR: 提出一种针对病理图像中域偏移的领域泛化方法，通过聚类非肿瘤区域的WSI特征并利用对比学习减少特征差异。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖多医院数据，但数据收集困难，因此关注WSI内的域偏移（如患者特征和组织厚度）。

Method: 采用两阶段对比学习（WSI级和补丁级），通过聚类非肿瘤区域特征作为域，减少不同聚类间的特征差异。

Result: 有效减少了域偏移，提升了模型的泛化能力。

Conclusion: 该方法为病理图像中的域偏移问题提供了一种实用解决方案。

Abstract: In this paper, we address domain shifts in pathological images by focusing on
shifts within whole slide images~(WSIs), such as patient characteristics and
tissue thickness, rather than shifts between hospitals. Traditional approaches
rely on multi-hospital data, but data collection challenges often make this
impractical. Therefore, the proposed domain generalization method captures and
leverages intra-hospital domain shifts by clustering WSI-level features from
non-tumor regions and treating these clusters as domains. To mitigate domain
shift, we apply contrastive learning to reduce feature gaps between WSI pairs
from different clusters. The proposed method introduces a two-stage contrastive
learning approach WSI-level and patch-level contrastive learning to minimize
these gaps effectively.

</details>


### [124] [CoT-Pose: Chain-of-Thought Reasoning for 3D Pose Generation from Abstract Prompts](https://arxiv.org/abs/2508.07540)
*Junuk Cha,Jihyeon Kim*

Main category: cs.CV

TL;DR: 论文提出了一种结合链式思维（CoT）推理的3D人体姿态生成框架，解决了现有模型依赖低层次提示的问题，能够从抽象语言生成准确姿态。


<details>
  <summary>Details</summary>
Motivation: 现有文本到姿态生成模型依赖低层次提示，而人类通常使用抽象语言描述动作，导致实际应用中的挑战。

Method: 提出CoT-Pose框架，将CoT推理融入姿态生成过程，并设计数据合成流程生成训练所需的三元组（抽象提示、详细提示、3D姿态）。

Result: 实验表明，CoT-Pose能从抽象文本输入生成语义对齐的合理姿态。

Conclusion: 研究强调了高层次理解在姿态生成中的重要性，为推理增强方法开辟了新方向。

Abstract: Recent advances in multi-modal large language models (MLLMs) and
chain-of-thought (CoT) reasoning have led to significant progress in image and
text generation tasks. However, the field of 3D human pose generation still
faces critical limitations. Most existing text-to-pose models rely heavily on
detailed (low-level) prompts that explicitly describe joint configurations. In
contrast, humans tend to communicate actions and intentions using abstract
(high-level) language. This mismatch results in a practical challenge for
deploying pose generation systems in real-world scenarios. To bridge this gap,
we introduce a novel framework that incorporates CoT reasoning into the pose
generation process, enabling the interpretation of abstract prompts into
accurate 3D human poses. We further propose a data synthesis pipeline that
automatically generates triplets of abstract prompts, detailed prompts, and
corresponding 3D poses for training process. Experimental results demonstrate
that our reasoning-enhanced model, CoT-Pose, can effectively generate plausible
and semantically aligned poses from abstract textual inputs. This work
highlights the importance of high-level understanding in pose generation and
opens new directions for reasoning-enhanced approach for human pose generation.

</details>


### [125] [Commentary Generation for Soccer Highlights](https://arxiv.org/abs/2508.07543)
*Chidaksh Ravuru*

Main category: cs.CV

TL;DR: 本文扩展了MatchVoice模型，用于生成足球集锦的实时解说，并探讨了训练配置和硬件限制的影响。


<details>
  <summary>Details</summary>
Motivation: 现有系统在视频内容与解说之间的细粒度对齐方面存在不足，MatchVoice通过粗粒度和细粒度对齐技术改进了这一问题。

Method: 使用GOAL数据集扩展MatchVoice模型，进行实验以复现MatchTime结果，并评估不同训练配置和窗口大小对性能的影响。

Result: MatchVoice展示了良好的泛化能力，但需要整合更广泛的视频-语言技术以进一步提升性能。

Conclusion: 研究强调了进一步整合视频-语言领域技术的必要性，以优化足球实时解说生成的效果。

Abstract: Automated soccer commentary generation has evolved from template-based
systems to advanced neural architectures, aiming to produce real-time
descriptions of sports events. While frameworks like SoccerNet-Caption laid
foundational work, their inability to achieve fine-grained alignment between
video content and commentary remains a significant challenge. Recent efforts
such as MatchTime, with its MatchVoice model, address this issue through coarse
and fine-grained alignment techniques, achieving improved temporal
synchronization. In this paper, we extend MatchVoice to commentary generation
for soccer highlights using the GOAL dataset, which emphasizes short clips over
entire games. We conduct extensive experiments to reproduce the original
MatchTime results and evaluate our setup, highlighting the impact of different
training configurations and hardware limitations. Furthermore, we explore the
effect of varying window sizes on zero-shot performance. While MatchVoice
exhibits promising generalization capabilities, our findings suggest the need
for integrating techniques from broader video-language domains to further
enhance performance. Our code is available at
https://github.com/chidaksh/SoccerCommentary.

</details>


### [126] [Adaptive Pseudo Label Selection for Individual Unlabeled Data by Positive and Unlabeled Learning](https://arxiv.org/abs/2508.07548)
*Takehiro Yamane,Itaru Tsuge,Susumu Saito,Ryoma Bise*

Main category: cs.CV

TL;DR: 提出了一种基于PU学习的医学图像分割伪标签方法，能够在单个图像上学习并选择有效伪标签。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割中伪标签选择的问题，尤其是在背景区域多样的情况下。

Method: 引入PU学习（正样本与未标记数据学习），用于区分前景和背景区域，并选择有效伪标签。

Result: 实验结果表明该方法在医学图像分割中具有有效性。

Conclusion: 该方法通过PU学习实现了高效的伪标签选择，适用于多样背景区域的医学图像分割。

Abstract: This paper proposes a novel pseudo-labeling method for medical image
segmentation that can perform learning on ``individual images'' to select
effective pseudo-labels. We introduce Positive and Unlabeled Learning (PU
learning), which uses only positive and unlabeled data for binary
classification problems, to obtain the appropriate metric for discriminating
foreground and background regions on each unlabeled image. Our PU learning
makes us easy to select pseudo-labels for various background regions. The
experimental results show the effectiveness of our method.

</details>


### [127] [Decoupled Functional Evaluation of Autonomous Driving Models via Feature Map Quality Scoring](https://arxiv.org/abs/2508.07552)
*Ludan Zhang,Sihan Wang,Yuqi Dai,Shuofei Qiao,Lei He*

Main category: cs.CV

TL;DR: 本文提出了一种基于特征图收敛分数（FMCS）的独立评估方法，用于自动驾驶感知和规划中的端到端模型，通过双粒度动态加权评分系统（DG-DWSS）和CLIP-FMQE-Net网络，提升特征图质量和模型性能。


<details>
  <summary>Details</summary>
Motivation: 端到端模型在自动驾驶中缺乏对中间功能模块的显式监督信号，导致机制不透明且可解释性差，传统方法难以独立评估和训练这些模块。

Method: 提出FMCS评估框架，构建DG-DWSS评分系统，并开发CLIP-FMQE-Net网络，结合特征-真值编码器和质量评分预测头，实时分析特征图质量。

Result: 在NuScenes数据集上，集成评估模块后3D目标检测性能提升，NDS指标提高3.89%。

Conclusion: 该方法有效提升了特征表示质量和整体模型性能，验证了其在自动驾驶感知和规划中的实用性。

Abstract: End-to-end models are emerging as the mainstream in autonomous driving
perception and planning. However, the lack of explicit supervision signals for
intermediate functional modules leads to opaque operational mechanisms and
limited interpretability, making it challenging for traditional methods to
independently evaluate and train these modules. Pioneering in the issue, this
study builds upon the feature map-truth representation similarity-based
evaluation framework and proposes an independent evaluation method based on
Feature Map Convergence Score (FMCS). A Dual-Granularity Dynamic Weighted
Scoring System (DG-DWSS) is constructed, formulating a unified quantitative
metric - Feature Map Quality Score - to enable comprehensive evaluation of the
quality of feature maps generated by functional modules. A CLIP-based Feature
Map Quality Evaluation Network (CLIP-FMQE-Net) is further developed, combining
feature-truth encoders and quality score prediction heads to enable real-time
quality analysis of feature maps generated by functional modules. Experimental
results on the NuScenes dataset demonstrate that integrating our evaluation
module into the training improves 3D object detection performance, achieving a
3.89 percent gain in NDS. These results verify the effectiveness of our method
in enhancing feature representation quality and overall model performance.

</details>


### [128] [Splat4D: Diffusion-Enhanced 4D Gaussian Splatting for Temporally and Spatially Consistent Content Creation](https://arxiv.org/abs/2508.07557)
*Minghao Yin,Yukang Cao,Songyou Peng,Kai Han*

Main category: cs.CV

TL;DR: Splat4D是一个从单目视频生成高质量4D内容的新框架，通过多视角渲染、不一致性识别、视频扩散模型和非对称U-Net优化，实现了时空一致性和高保真度。


<details>
  <summary>Details</summary>
Motivation: 解决从单目视频生成4D内容时面临的时空一致性、细节保留和用户指导有效整合的挑战。

Method: 采用多视角渲染、不一致性识别、视频扩散模型和非对称U-Net进行优化。

Result: 在公开基准测试中表现优异，支持多种应用如文本/图像条件4D生成和内容编辑。

Conclusion: Splat4D在4D内容生成中表现出色，具有广泛的应用潜力。

Abstract: Generating high-quality 4D content from monocular videos for applications
such as digital humans and AR/VR poses challenges in ensuring temporal and
spatial consistency, preserving intricate details, and incorporating user
guidance effectively. To overcome these challenges, we introduce Splat4D, a
novel framework enabling high-fidelity 4D content generation from a monocular
video. Splat4D achieves superior performance while maintaining faithful
spatial-temporal coherence by leveraging multi-view rendering, inconsistency
identification, a video diffusion model, and an asymmetric U-Net for
refinement. Through extensive evaluations on public benchmarks, Splat4D
consistently demonstrates state-of-the-art performance across various metrics,
underscoring the efficacy of our approach. Additionally, the versatility of
Splat4D is validated in various applications such as text/image conditioned 4D
generation, 4D human generation, and text-guided content editing, producing
coherent outcomes following user instructions.

</details>


### [129] [Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language Models](https://arxiv.org/abs/2508.07570)
*Khanh-Binh Nguyen,Phuoc-Nguyen Bui,Hyunseung Choo,Duc Thanh Nguyen*

Main category: cs.CV

TL;DR: ACE框架通过动态缓存和自适应决策边界，解决了VLM在分布偏移下的性能下降问题，显著提升了零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLM）在零样本泛化方面表现优异，但在分布偏移时性能下降，尤其是在缺乏标注数据的情况下。测试时适应（TTA）通过在线优化VLM来解决这一问题，但现有缓存方法存在置信度不可靠和决策边界僵化的问题。

Method: 提出自适应缓存增强（ACE）框架，通过动态选择高置信度或低熵图像嵌入，并结合类特定阈值和指数移动平均更新，构建鲁棒缓存。

Result: 在15个基准数据集上的实验表明，ACE在分布偏移场景下实现了最先进的性能，优于现有TTA方法。

Conclusion: ACE通过动态缓存和自适应决策边界，显著提升了VLM在分布偏移下的鲁棒性和泛化能力。

Abstract: Vision-language models (VLMs) exhibit remarkable zero-shot generalization but
suffer performance degradation under distribution shifts in downstream tasks,
particularly in the absence of labeled data. Test-Time Adaptation (TTA)
addresses this challenge by enabling online optimization of VLMs during
inference, eliminating the need for annotated data. Cache-based TTA methods
exploit historical knowledge by maintaining a dynamic memory cache of
low-entropy or high-confidence samples, promoting efficient adaptation to
out-of-distribution data. Nevertheless, these methods face two critical
challenges: (1) unreliable confidence metrics under significant distribution
shifts, resulting in error accumulation within the cache and degraded
adaptation performance; and (2) rigid decision boundaries that fail to
accommodate substantial distributional variations, leading to suboptimal
predictions. To overcome these limitations, we introduce the Adaptive Cache
Enhancement (ACE) framework, which constructs a robust cache by selectively
storing high-confidence or low-entropy image embeddings per class, guided by
dynamic, class-specific thresholds initialized from zero-shot statistics and
iteratively refined using an exponential moving average and
exploration-augmented updates. This approach enables adaptive, class-wise
decision boundaries, ensuring robust and accurate predictions across diverse
visual distributions. Extensive experiments on 15 diverse benchmark datasets
demonstrate that ACE achieves state-of-the-art performance, delivering superior
robustness and generalization compared to existing TTA methods in challenging
out-of-distribution scenarios.

</details>


### [130] [Exploiting Layer Normalization Fine-tuning in Visual Transformer Foundation Models for Classification](https://arxiv.org/abs/2508.07577)
*Zhaorui Tan,Tan Pan,Kaizhu Huang,Weimiao Yu,Kai Yao,Chen Jiang,Qiufeng Wang,Anh Nguyen,Xin Guo,Yuan Cheng,Xi Yang*

Main category: cs.CV

TL;DR: 本文研究了ViT中LayerNorm在数据稀缺和域转移下的微调动态，提出了一种基于Fine-tuning Shift Ratio（FSR）的重新缩放机制和循环框架，以优化LayerNorm的微调效果。


<details>
  <summary>Details</summary>
Motivation: LayerNorm在ViT中至关重要，但其在数据稀缺和域转移下的微调动态尚未充分研究。本文旨在探索LayerNorm参数的变化如何反映源域与目标域的转换，并提出优化方法。

Method: 提出了一种基于FSR的重新缩放机制，使用标量λ（与FSR负相关）来调整LayerNorm参数，并结合循环框架增强微调效果。

Result: 实验表明，OOD任务通常具有较低的FSR和较高的λ，尤其在数据稀缺时；病理数据微调更接近ID设置，倾向于保守更新。

Conclusion: 本文揭示了LayerNorm在迁移学习中的动态特性，并提供了实用的微调策略。

Abstract: LayerNorm is pivotal in Vision Transformers (ViTs), yet its fine-tuning
dynamics under data scarcity and domain shifts remain underexplored. This paper
shows that shifts in LayerNorm parameters after fine-tuning (LayerNorm shifts)
are indicative of the transitions between source and target domains; its
efficacy is contingent upon the degree to which the target training samples
accurately represent the target domain, as quantified by our proposed
Fine-tuning Shift Ratio ($FSR$). Building on this, we propose a simple yet
effective rescaling mechanism using a scalar $\lambda$ that is negatively
correlated to $FSR$ to align learned LayerNorm shifts with those ideal shifts
achieved under fully representative data, combined with a cyclic framework that
further enhances the LayerNorm fine-tuning. Extensive experiments across
natural and pathological images, in both in-distribution (ID) and
out-of-distribution (OOD) settings, and various target training sample regimes
validate our framework. Notably, OOD tasks tend to yield lower $FSR$ and higher
$\lambda$ in comparison to ID cases, especially with scarce data, indicating
under-represented target training samples. Moreover, ViTFs fine-tuned on
pathological data behave more like ID settings, favoring conservative LayerNorm
updates. Our findings illuminate the underexplored dynamics of LayerNorm in
transfer learning and provide practical strategies for LayerNorm fine-tuning.

</details>


### [131] [GAPNet: A Lightweight Framework for Image and Video Salient Object Detection via Granularity-Aware Paradigm](https://arxiv.org/abs/2508.07585)
*Yu-Huan Wu,Wei Liu,Zi-Xuan Zhu,Zizhou Wang,Yong Liu,Liangli Zhen*

Main category: cs.CV

TL;DR: GAPNet是一种基于粒度感知范式的轻量级网络，用于图像和视频显著目标检测（SOD），通过多尺度监督和高效特征融合实现高性能。


<details>
  <summary>Details</summary>
Motivation: 现有SOD模型依赖重型骨干网络，计算成本高，限制了在边缘设备上的实际应用。

Method: 采用粒度感知连接，结合粗粒度对象位置和细粒度对象边界监督多尺度解码器输出，设计了粒度金字塔卷积（GPC）和跨尺度注意力（CSA）模块。

Result: 在轻量级图像和视频SOD模型中达到最新性能。

Conclusion: GAPNet通过优化特征利用和语义解释，实现了高效且高性能的显著目标检测。

Abstract: Recent salient object detection (SOD) models predominantly rely on
heavyweight backbones, incurring substantial computational cost and hindering
their practical application in various real-world settings, particularly on
edge devices. This paper presents GAPNet, a lightweight network built on the
granularity-aware paradigm for both image and video SOD. We assign saliency
maps of different granularities to supervise the multi-scale decoder
side-outputs: coarse object locations for high-level outputs and fine-grained
object boundaries for low-level outputs. Specifically, our decoder is built
with granularity-aware connections which fuse high-level features of low
granularity and low-level features of high granularity, respectively. To
support these connections, we design granular pyramid convolution (GPC) and
cross-scale attention (CSA) modules for efficient fusion of low-scale and
high-scale features, respectively. On top of the encoder, a self-attention
module is built to learn global information, enabling accurate object
localization with negligible computational cost. Unlike traditional U-Net-based
approaches, our proposed method optimizes feature utilization and semantic
interpretation while applying appropriate supervision at each processing stage.
Extensive experiments show that the proposed method achieves a new
state-of-the-art performance among lightweight image and video SOD models. Code
is available at https://github.com/yuhuan-wu/GAPNet.

</details>


### [132] [Voice Pathology Detection Using Phonation](https://arxiv.org/abs/2508.07587)
*Sri Raksha Siva,Nived Suthahar,Prakash Boominathan,Uma Ranjan*

Main category: cs.CV

TL;DR: 提出了一种基于机器学习的非侵入性框架，利用语音数据检测声音病理，替代传统侵入性方法。


<details>
  <summary>Details</summary>
Motivation: 声音障碍严重影响交流和生活质量，传统诊断方法（如喉镜检查）侵入性强、主观且难以普及。

Method: 使用Saarbrücken语音数据库的语音数据，提取MFCC、chroma特征和Mel频谱图，结合RNN（LSTM和注意力机制）分类正常与病理样本，并通过数据增强和预处理提高模型泛化能力。

Result: 框架成功实现了声音病理的非侵入性自动检测。

Conclusion: 该框架为早期声音病理诊断提供了高效工具，支持AI驱动的医疗保健，改善患者预后。

Abstract: Voice disorders significantly affect communication and quality of life,
requiring an early and accurate diagnosis. Traditional methods like
laryngoscopy are invasive, subjective, and often inaccessible. This research
proposes a noninvasive, machine learning-based framework for detecting voice
pathologies using phonation data.
  Phonation data from the Saarbr\"ucken Voice Database are analyzed using
acoustic features such as Mel Frequency Cepstral Coefficients (MFCCs), chroma
features, and Mel spectrograms. Recurrent Neural Networks (RNNs), including
LSTM and attention mechanisms, classify samples into normal and pathological
categories. Data augmentation techniques, including pitch shifting and Gaussian
noise addition, enhance model generalizability, while preprocessing ensures
signal quality. Scale-based features, such as H\"older and Hurst exponents,
further capture signal irregularities and long-term dependencies.
  The proposed framework offers a noninvasive, automated diagnostic tool for
early detection of voice pathologies, supporting AI-driven healthcare, and
improving patient outcomes.

</details>


### [133] [From Prediction to Explanation: Multimodal, Explainable, and Interactive Deepfake Detection Framework for Non-Expert Users](https://arxiv.org/abs/2508.07596)
*Shahroz Tariq,Simon S. Woo,Priyanka Singh,Irena Irmalasari,Saakshi Gupta,Dev Gupta*

Main category: cs.CV

TL;DR: DF-P2E是一个多模态框架，通过视觉、语义和叙事层解释，提升深度伪造检测的可解释性和可用性。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术对数字完整性构成严重威胁，现有检测系统缺乏透明度和可解释性，限制了其在实际决策中的使用。

Method: DF-P2E包含三个模块：基于Grad-CAM的深度伪造分类器、视觉描述模块和叙事优化模块，结合LLM生成用户敏感的解释。

Result: 在DF40数据集上，系统实现了竞争性的检测性能，并提供与Grad-CAM一致的高质量解释。

Conclusion: DF-P2E通过统一预测和解释，推动了可信赖和透明AI系统的发展。

Abstract: The proliferation of deepfake technologies poses urgent challenges and
serious risks to digital integrity, particularly within critical sectors such
as forensics, journalism, and the legal system. While existing detection
systems have made significant progress in classification accuracy, they
typically function as black-box models, offering limited transparency and
minimal support for human reasoning. This lack of interpretability hinders
their usability in real-world decision-making contexts, especially for
non-expert users. In this paper, we present DF-P2E (Deepfake: Prediction to
Explanation), a novel multimodal framework that integrates visual, semantic,
and narrative layers of explanation to make deepfake detection interpretable
and accessible. The framework consists of three modular components: (1) a
deepfake classifier with Grad-CAM-based saliency visualisation, (2) a visual
captioning module that generates natural language summaries of manipulated
regions, and (3) a narrative refinement module that uses a fine-tuned Large
Language Model (LLM) to produce context-aware, user-sensitive explanations. We
instantiate and evaluate the framework on the DF40 benchmark, the most diverse
deepfake dataset to date. Experiments demonstrate that our system achieves
competitive detection performance while providing high-quality explanations
aligned with Grad-CAM activations. By unifying prediction and explanation in a
coherent, human-aligned pipeline, this work offers a scalable approach to
interpretable deepfake detection, advancing the broader vision of trustworthy
and transparent AI systems in adversarial media environments.

</details>


### [134] [ShoulderShot: Generating Over-the-Shoulder Dialogue Videos](https://arxiv.org/abs/2508.07597)
*Yuang Zhang,Junqi Cheng,Haoyu Zhao,Jiaxi Gu,Fangyuan Zou,Zenghui Lu,Peng Shu*

Main category: cs.CV

TL;DR: ShoulderShot框架通过双镜头生成与循环视频结合，解决了对话视频生成中的角色一致性和空间连续性挑战，并在对话长度上表现出灵活性。


<details>
  <summary>Details</summary>
Motivation: 过肩对话视频在影视和广告中具有重要作用，但现有研究对其生成问题关注不足，尤其是角色一致性、空间连续性和长对话生成的挑战。

Method: 提出ShoulderShot框架，结合双镜头生成与循环视频技术，以保持角色一致性并实现空间连续性。

Result: 实验结果表明，该方法在镜头切换布局、空间连续性和对话长度灵活性上优于现有方法。

Conclusion: ShoulderShot为实际对话视频生成提供了新的可能性。

Abstract: Over-the-shoulder dialogue videos are essential in films, short dramas, and
advertisements, providing visual variety and enhancing viewers' emotional
connection. Despite their importance, such dialogue scenes remain largely
underexplored in video generation research. The main challenges include
maintaining character consistency across different shots, creating a sense of
spatial continuity, and generating long, multi-turn dialogues within limited
computational budgets. Here, we present ShoulderShot, a framework that combines
dual-shot generation with looping video, enabling extended dialogues while
preserving character consistency. Our results demonstrate capabilities that
surpass existing methods in terms of shot-reverse-shot layout, spatial
continuity, and flexibility in dialogue length, thereby opening up new
possibilities for practical dialogue video generation. Videos and comparisons
are available at https://shouldershot.github.io.

</details>


### [135] [LaVieID: Local Autoregressive Diffusion Transformers for Identity-Preserving Video Creation](https://arxiv.org/abs/2508.07603)
*Wenhui Song,Hanhui Li,Jiehui Huang,Panwen Hu,Yuhao Cheng,Long Chen,Yiqiang Yan,Xiaodan Liang*

Main category: cs.CV

TL;DR: LaVieID是一种新的局部自回归视频扩散框架，旨在解决身份保持的文本到视频任务，通过空间和时间视角减少身份信息丢失。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散变换器（DiTs）在全局生成过程中身份信息丢失的问题。

Method: 引入局部路由器显式表示潜在状态，并通过时间自回归模块增强帧间身份一致性。

Result: LaVieID能够生成高保真个性化视频，并达到最先进的性能。

Conclusion: LaVieID通过局部和时间的优化，显著提升了身份保持视频生成的效果。

Abstract: In this paper, we present LaVieID, a novel \underline{l}ocal
\underline{a}utoregressive \underline{vi}d\underline{e}o diffusion framework
designed to tackle the challenging \underline{id}entity-preserving
text-to-video task. The key idea of LaVieID is to mitigate the loss of identity
information inherent in the stochastic global generation process of diffusion
transformers (DiTs) from both spatial and temporal perspectives. Specifically,
unlike the global and unstructured modeling of facial latent states in existing
DiTs, LaVieID introduces a local router to explicitly represent latent states
by weighted combinations of fine-grained local facial structures. This
alleviates undesirable feature interference and encourages DiTs to capture
distinctive facial characteristics. Furthermore, a temporal autoregressive
module is integrated into LaVieID to refine denoised latent tokens before video
decoding. This module divides latent tokens temporally into chunks, exploiting
their long-range temporal dependencies to predict biases for rectifying tokens,
thereby significantly enhancing inter-frame identity consistency. Consequently,
LaVieID can generate high-fidelity personalized videos and achieve
state-of-the-art performance. Our code and models are available at
https://github.com/ssugarwh/LaVieID.

</details>


### [136] [X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning](https://arxiv.org/abs/2508.07607)
*Jian Ma,Xujie Zhu,Zihao Pan,Qirong Peng,Xu Guo,Chen Chen,Haonan Lu*

Main category: cs.CV

TL;DR: 论文介绍了X2Edit数据集和任务感知的MoE-LoRA训练方法，显著提升了图像编辑性能。


<details>
  <summary>Details</summary>
Motivation: 现有开源数据集和编辑模块不足，需高质量数据集和兼容性强的编辑方法。

Method: 构建X2Edit数据集（3.7M高质量数据），设计任务感知MoE-LoRA训练，结合对比学习。

Result: 模型编辑性能优异，数据集优于现有开源数据集。

Conclusion: X2Edit为图像编辑提供了高质量数据集和高效训练方法，开源资源促进社区发展。

Abstract: Existing open-source datasets for arbitrary-instruction image editing remain
suboptimal, while a plug-and-play editing module compatible with
community-prevalent generative models is notably absent. In this paper, we
first introduce the X2Edit Dataset, a comprehensive dataset covering 14 diverse
editing tasks, including subject-driven generation. We utilize the
industry-leading unified image generation models and expert models to construct
the data. Meanwhile, we design reasonable editing instructions with the VLM and
implement various scoring mechanisms to filter the data. As a result, we
construct 3.7 million high-quality data with balanced categories. Second, to
better integrate seamlessly with community image generation models, we design
task-aware MoE-LoRA training based on FLUX.1, with only 8\% of the parameters
of the full model. To further improve the final performance, we utilize the
internal representations of the diffusion model and define positive/negative
samples based on image editing types to introduce contrastive learning.
Extensive experiments demonstrate that the model's editing performance is
competitive among many excellent models. Additionally, the constructed dataset
exhibits substantial advantages over existing open-source datasets. The
open-source code, checkpoints, and datasets for X2Edit can be found at the
following link: https://github.com/OPPO-Mente-Lab/X2Edit.

</details>


### [137] [An Iterative Reconstruction Method for Dental Cone-Beam Computed Tomography with a Truncated Field of View](https://arxiv.org/abs/2508.07618)
*Hyoung Suk Park,Kiwan Jeon*

Main category: cs.CV

TL;DR: 提出一种两阶段方法，通过隐式神经表示（INR）生成先验图像并校正投影数据，以减少牙科CBCT中的截断伪影。


<details>
  <summary>Details</summary>
Motivation: 小型探测器导致视野截断，影响迭代重建图像质量。

Method: 第一阶段用INR生成先验图像并校正投影数据，第二阶段用校正后的数据进行迭代重建。

Result: 两阶段方法有效抑制截断伪影，提升图像质量。

Conclusion: 该方法为牙科CBCT提供了一种有效的截断伪影解决方案。

Abstract: In dental cone-beam computed tomography (CBCT), compact and cost-effective
system designs often use small detectors, resulting in a truncated field of
view (FOV) that does not fully encompass the patient's head. In iterative
reconstruction approaches, the discrepancy between the actual projection and
the forward projection within the truncated FOV accumulates over iterations,
leading to significant degradation in the reconstructed image quality. In this
study, we propose a two-stage approach to mitigate truncation artifacts in
dental CBCT. In the first stage, we employ Implicit Neural Representation
(INR), leveraging its superior representation power, to generate a prior image
over an extended region so that its forward projection fully covers the
patient's head. To reduce computational and memory burdens, INR reconstruction
is performed with a coarse voxel size. The forward projection of this prior
image is then used to estimate the discrepancy due to truncated FOV in the
measured projection data. In the second stage, the discrepancy-corrected
projection data is utilized in a conventional iterative reconstruction process
within the truncated region. Our numerical results demonstrate that the
proposed two-grid approach effectively suppresses truncation artifacts, leading
to improved CBCT image quality.

</details>


### [138] [SOFA: Deep Learning Framework for Simulating and Optimizing Atrial Fibrillation Ablation](https://arxiv.org/abs/2508.07621)
*Yunsung Chung,Chanho Lim,Ghassan Bidaoui,Christian Massad,Nassir Marrouche,Jihun Hamm*

Main category: cs.CV

TL;DR: SOFA是一个深度学习框架，通过模拟消融策略和优化参数来预测房颤复发风险并减少复发。


<details>
  <summary>Details</summary>
Motivation: 房颤消融手术效果差异大，需个性化优化以提高成功率。

Method: SOFA结合多模态、多视角生成器，模拟消融后瘢痕形成并预测复发风险，再优化消融参数。

Result: SOFA准确生成消融后图像，优化方案使复发风险降低22.18%。

Conclusion: SOFA首次整合模拟、预测和优化，为个性化房颤消融提供新工具。

Abstract: Atrial fibrillation (AF) is a prevalent cardiac arrhythmia often treated with
catheter ablation procedures, but procedural outcomes are highly variable.
Evaluating and improving ablation efficacy is challenging due to the complex
interaction between patient-specific tissue and procedural factors. This paper
asks two questions: Can AF recurrence be predicted by simulating the effects of
procedural parameters? How should we ablate to reduce AF recurrence? We propose
SOFA (Simulating and Optimizing Atrial Fibrillation Ablation), a novel
deep-learning framework that addresses these questions. SOFA first simulates
the outcome of an ablation strategy by generating a post-ablation image
depicting scar formation, conditioned on a patient's pre-ablation LGE-MRI and
the specific procedural parameters used (e.g., ablation locations, duration,
temperature, power, and force). During this simulation, it predicts AF
recurrence risk. Critically, SOFA then introduces an optimization scheme that
refines these procedural parameters to minimize the predicted risk. Our method
leverages a multi-modal, multi-view generator that processes 2.5D
representations of the atrium. Quantitative evaluations show that SOFA
accurately synthesizes post-ablation images and that our optimization scheme
leads to a 22.18\% reduction in the model-predicted recurrence risk. To the
best of our knowledge, SOFA is the first framework to integrate the simulation
of procedural effects, recurrence prediction, and parameter optimization,
offering a novel tool for personalizing AF ablation.

</details>


### [139] [Enhancing Egocentric Object Detection in Static Environments using Graph-based Spatial Anomaly Detection and Correction](https://arxiv.org/abs/2508.07624)
*Vishakha Lall,Yisi Liu*

Main category: cs.CV

TL;DR: 提出一种基于图神经网络的后期处理框架，利用物体间的空间关系修正检测异常，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测模型未能充分利用静态环境中的空间先验，导致检测不一致或错误。

Method: 使用图神经网络（GNN）建模物体间空间关系，修正异常检测结果。

Result: 实验表明，该方法使mAP@50提升高达4%。

Conclusion: 利用环境空间结构可显著提升目标检测系统的可靠性。

Abstract: In many real-world applications involving static environments, the spatial
layout of objects remains consistent across instances. However,
state-of-the-art object detection models often fail to leverage this spatial
prior, resulting in inconsistent predictions, missed detections, or
misclassifications, particularly in cluttered or occluded scenes. In this work,
we propose a graph-based post-processing pipeline that explicitly models the
spatial relationships between objects to correct detection anomalies in
egocentric frames. Using a graph neural network (GNN) trained on manually
annotated data, our model identifies invalid object class labels and predicts
corrected class labels based on their neighbourhood context. We evaluate our
approach both as a standalone anomaly detection and correction framework and as
a post-processing module for standard object detectors such as YOLOv7 and
RT-DETR. Experiments demonstrate that incorporating this spatial reasoning
significantly improves detection performance, with mAP@50 gains of up to 4%.
This method highlights the potential of leveraging the environment's spatial
structure to improve reliability in object detection systems.

</details>


### [140] [A Trustworthy Method for Multimodal Emotion Recognition](https://arxiv.org/abs/2508.07625)
*Junxiao Xue,Xiaozhen Liu,Jie Wang,Xuecheng Wu,Bin Wu*

Main category: cs.CV

TL;DR: 提出了一种基于不确定性估计的信任情感识别方法（TER），通过多模态置信度结合输出可信预测，并引入新评估标准验证模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有情感识别方法虽性能强但模型复杂，且对噪声和异常数据可靠性不足，需提升预测可信度。

Method: TER利用不确定性估计计算预测置信值，结合多模态结果输出可信预测，并定义信任精度和召回率等新评估标准。

Result: TER在Music-video上达到82.40%准确率，在IEMOCAP和Music-video上的信任F1分数分别为0.7511和0.9035，优于其他方法。

Conclusion: TER通过置信度模块提升模型对噪声的鲁棒性和可靠性，实验验证其有效性和先进性能。

Abstract: Existing emotion recognition methods mainly focus on enhancing performance by
employing complex deep models, typically resulting in significantly higher
model complexity. Although effective, it is also crucial to ensure the
reliability of the final decision, especially for noisy, corrupted and
out-of-distribution data. To this end, we propose a novel emotion recognition
method called trusted emotion recognition (TER), which utilizes uncertainty
estimation to calculate the confidence value of predictions. TER combines the
results from multiple modalities based on their confidence values to output the
trusted predictions. We also provide a new evaluation criterion to assess the
reliability of predictions. Specifically, we incorporate trusted precision and
trusted recall to determine the trusted threshold and formulate the trusted
Acc. and trusted F1 score to evaluate the model's trusted performance. The
proposed framework combines the confidence module that accordingly endows the
model with reliability and robustness against possible noise or corruption. The
extensive experimental results validate the effectiveness of our proposed
model. The TER achieves state-of-the-art performance on the Music-video,
achieving 82.40% Acc. In terms of trusted performance, TER outperforms other
methods on the IEMOCAP and Music-video, achieving trusted F1 scores of 0.7511
and 0.9035, respectively.

</details>


### [141] [LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering](https://arxiv.org/abs/2508.07647)
*Xiaohang Zhan,Dingming Liu*

Main category: cs.CV

TL;DR: 提出了一种无需训练的图像生成算法，通过体积渲染原理精确控制图像中物体的遮挡关系。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖提示或布局控制遮挡，但缺乏精确性，本文旨在解决这一问题。

Method: 利用预训练扩散模型，在潜在空间中基于遮挡关系和物体透射率进行体积渲染。

Result: 在遮挡准确性上显著优于现有方法，并能调整物体透明度等效果。

Conclusion: 该方法无需重新训练模型，实现了基于物理的精确遮挡控制。

Abstract: We propose a novel training-free image generation algorithm that precisely
controls the occlusion relationships between objects in an image. Existing
image generation methods typically rely on prompts to influence occlusion,
which often lack precision. While layout-to-image methods provide control over
object locations, they fail to address occlusion relationships explicitly.
Given a pre-trained image diffusion model, our method leverages volume
rendering principles to "render" the scene in latent space, guided by occlusion
relationships and the estimated transmittance of objects. This approach does
not require retraining or fine-tuning the image diffusion model, yet it enables
accurate occlusion control due to its physics-grounded foundation. In extensive
experiments, our method significantly outperforms existing approaches in terms
of occlusion accuracy. Furthermore, we demonstrate that by adjusting the
opacities of objects or concepts during rendering, our method can achieve a
variety of effects, such as altering the transparency of objects, the density
of mass (e.g., forests), the concentration of particles (e.g., rain, fog), the
intensity of light, and the strength of lens effects, etc.

</details>


### [142] [Collaborative Learning of Scattering and Deep Features for SAR Target Recognition with Noisy Labels](https://arxiv.org/abs/2508.07656)
*Yimin Fu,Zhunga Liu,Dongxiu Guo,Longfei Wang*

Main category: cs.CV

TL;DR: 提出了一种结合散射特征和深度特征的协作学习方法（CLSDF），用于解决SAR自动目标识别中标签噪声问题，通过多模型特征融合和半监督学习提升性能。


<details>
  <summary>Details</summary>
Motivation: SAR数据标签噪声问题导致自动目标识别性能下降，现有方法主要针对图像数据，而SAR数据的非直观特性使其难以实现噪声鲁棒学习。

Method: 设计多模型特征融合框架，将散射特征与深度特征结合；利用GMM模型划分干净和噪声标签样本；通过半监督学习和联合分布对齐策略优化模型。

Result: 在MSTAR数据集上的实验表明，该方法在不同标签噪声条件下均能取得最优性能。

Conclusion: CLSDF方法有效解决了SAR数据标签噪声问题，提升了自动目标识别的鲁棒性和准确性。

Abstract: The acquisition of high-quality labeled synthetic aperture radar (SAR) data
is challenging due to the demanding requirement for expert knowledge.
Consequently, the presence of unreliable noisy labels is unavoidable, which
results in performance degradation of SAR automatic target recognition (ATR).
Existing research on learning with noisy labels mainly focuses on image data.
However, the non-intuitive visual characteristics of SAR data are insufficient
to achieve noise-robust learning. To address this problem, we propose
collaborative learning of scattering and deep features (CLSDF) for SAR ATR with
noisy labels. Specifically, a multi-model feature fusion framework is designed
to integrate scattering and deep features. The attributed scattering centers
(ASCs) are treated as dynamic graph structure data, and the extracted physical
characteristics effectively enrich the representation of deep image features.
Then, the samples with clean and noisy labels are divided by modeling the loss
distribution with multiple class-wise Gaussian Mixture Models (GMMs).
Afterward, the semi-supervised learning of two divergent branches is conducted
based on the data divided by each other. Moreover, a joint distribution
alignment strategy is introduced to enhance the reliability of co-guessed
labels. Extensive experiments have been done on the Moving and Stationary
Target Acquisition and Recognition (MSTAR) dataset, and the results show that
the proposed method can achieve state-of-the-art performance under different
operating conditions with various label noises.

</details>


### [143] [Undress to Redress: A Training-Free Framework for Virtual Try-On](https://arxiv.org/abs/2508.07680)
*Zhiying Li,Junhao Wu,Yeying Jin,Daiheng Gao,Yun Ji,Kaichuan Kong,Lei Yu,Hao Xu,Kai Chen,Bruce Gu,Nana Wang,Zhaoxin Fan*

Main category: cs.CV

TL;DR: UR-VTON提出了一种无需训练的新框架，通过“脱衣-穿衣”机制解决长袖转短袖虚拟试衣中的皮肤还原问题，结合动态调度和结构细化器提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试衣方法在长袖转短袖场景中因皮肤还原不准确导致效果不佳，UR-VTON旨在解决这一问题。

Method: 采用“脱衣-穿衣”两阶段机制，结合动态调度和结构细化器优化生成过程。

Result: 实验表明UR-VTON在细节保留和图像质量上优于现有方法。

Conclusion: UR-VTON为长袖转短袖虚拟试衣提供了高效解决方案，并提出了新基准LS-TON。

Abstract: Virtual try-on (VTON) is a crucial task for enhancing user experience in
online shopping by generating realistic garment previews on personal photos.
Although existing methods have achieved impressive results, they struggle with
long-sleeve-to-short-sleeve conversions-a common and practical scenario-often
producing unrealistic outputs when exposed skin is underrepresented in the
original image. We argue that this challenge arises from the ''majority''
completion rule in current VTON models, which leads to inaccurate skin
restoration in such cases. To address this, we propose UR-VTON (Undress-Redress
Virtual Try-ON), a novel, training-free framework that can be seamlessly
integrated with any existing VTON method. UR-VTON introduces an
''undress-to-redress'' mechanism: it first reveals the user's torso by
virtually ''undressing,'' then applies the target short-sleeve garment,
effectively decomposing the conversion into two more manageable steps.
Additionally, we incorporate Dynamic Classifier-Free Guidance scheduling to
balance diversity and image quality during DDPM sampling, and employ Structural
Refiner to enhance detail fidelity using high-frequency cues. Finally, we
present LS-TON, a new benchmark for long-sleeve-to-short-sleeve try-on.
Extensive experiments demonstrate that UR-VTON outperforms state-of-the-art
methods in both detail preservation and image quality. Code will be released
upon acceptance.

</details>


### [144] [TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding](https://arxiv.org/abs/2508.07683)
*Chaohong Guo,Xun Mo,Yongwei Nie,Xuemiao Xu,Chao Xu,Fei Yu,Chengjiang Long*

Main category: cs.CV

TL;DR: 论文提出TAR-TVG框架，通过引入时间戳锚点约束推理过程，提升视频片段定位的准确性，并采用自蒸馏训练策略优化模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法未能显式约束推理过程，影响最终时间预测质量。

Method: 提出TAR-TVG框架，引入时间戳锚点作为中间验证点，并采用三阶段自蒸馏训练策略（GRPO训练、SFT微调、GRPO优化）。

Result: 模型在实验中表现最优，生成可解释、可验证的推理链，并逐步优化时间估计。

Conclusion: TAR-TVG通过显式监督推理过程，显著提升了视频片段定位的准确性和可解释性。

Abstract: Temporal Video Grounding (TVG) aims to precisely localize video segments
corresponding to natural language queries, which is a critical capability for
long-form video understanding. Although existing reinforcement learning
approaches encourage models to generate reasoning chains before predictions,
they fail to explicitly constrain the reasoning process to ensure the quality
of the final temporal predictions. To address this limitation, we propose
Timestamp Anchor-constrained Reasoning for Temporal Video Grounding (TAR-TVG),
a novel framework that introduces timestamp anchors within the reasoning
process to enforce explicit supervision to the thought content. These anchors
serve as intermediate verification points. More importantly, we require each
reasoning step to produce increasingly accurate temporal estimations, thereby
ensuring that the reasoning process contributes meaningfully to the final
prediction. To address the challenge of low-probability anchor generation in
models (e.g., Qwen2.5-VL-3B), we develop an efficient self-distillation
training strategy: (1) initial GRPO training to collect 30K high-quality
reasoning traces containing multiple timestamp anchors, (2) supervised
fine-tuning (SFT) on distilled data, and (3) final GRPO optimization on the
SFT-enhanced model. This three-stage training strategy enables robust anchor
generation while maintaining reasoning quality. Experiments show that our model
achieves state-of-the-art performance while producing interpretable, verifiable
reasoning chains with progressively refined temporal estimations.

</details>


### [145] [Make Your MoVe: Make Your 3D Contents by Adapting Multi-View Diffusion Models to External Editing](https://arxiv.org/abs/2508.07700)
*Weitao Wang,Haoran Xu,Jun Meng,Haoqian Wang*

Main category: cs.CV

TL;DR: 提出了一种无需调优的即插即用方案，用于在3D生成中保持编辑内容与原始几何的对齐，通过几何保留模块和注入切换器实现。


<details>
  <summary>Details</summary>
Motivation: 随着3D生成技术的发展，用户对个性化内容的需求增加，但现有编辑工具多为2D领域，直接应用于3D生成会导致信息丢失和质量下降。

Method: 提出几何保留模块和注入切换器，通过原始输入法线潜在变量指导编辑的多视图生成，并控制原始法线的监督程度。

Result: 实验表明，该方法显著提升了编辑3D资产的多视图一致性和网格质量。

Conclusion: 该方法为3D内容编辑提供了一种高效且高质量的解决方案。

Abstract: As 3D generation techniques continue to flourish, the demand for generating
personalized content is rapidly rising. Users increasingly seek to apply
various editing methods to polish generated 3D content, aiming to enhance its
color, style, and lighting without compromising the underlying geometry.
However, most existing editing tools focus on the 2D domain, and directly
feeding their results into 3D generation methods (like multi-view diffusion
models) will introduce information loss, degrading the quality of the final 3D
assets. In this paper, we propose a tuning-free, plug-and-play scheme that
aligns edited assets with their original geometry in a single inference run.
Central to our approach is a geometry preservation module that guides the
edited multi-view generation with original input normal latents. Besides, an
injection switcher is proposed to deliberately control the supervision extent
of the original normals, ensuring the alignment between the edited color and
normal views. Extensive experiments show that our method consistently improves
both the multi-view consistency and mesh quality of edited 3D assets, across
multiple combinations of multi-view diffusion models and editing methods.

</details>


### [146] [DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models](https://arxiv.org/abs/2508.07714)
*Licheng Zhang,Bach Le,Naveed Akhtar,Tuan Ngo*

Main category: cs.CV

TL;DR: 提出了一种结合深度学习与多模态推理的半自动化流程，用于构建多类别门检测数据集，显著降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 现有公开数据集稀缺，而精细门检测对建筑合规检查和室内场景理解至关重要。

Method: 使用深度学习目标检测器统一检测门，再通过大语言模型（LLM）分类，最后人工验证。

Result: 构建了高质量数据集，适用于神经网络在平面图分析中的基准测试。

Conclusion: 展示了深度学习与多模态推理在复杂领域高效构建数据集的潜力。

Abstract: Accurate detection and classification of diverse door types in floor plans
drawings is critical for multiple applications, such as building compliance
checking, and indoor scene understanding. Despite their importance, publicly
available datasets specifically designed for fine-grained multi-class door
detection remain scarce. In this work, we present a semi-automated pipeline
that leverages a state-of-the-art object detector and a large language model
(LLM) to construct a multi-class door detection dataset with minimal manual
effort. Doors are first detected as a unified category using a deep object
detection model. Next, an LLM classifies each detected instance based on its
visual and contextual features. Finally, a human-in-the-loop stage ensures
high-quality labels and bounding boxes. Our method significantly reduces
annotation cost while producing a dataset suitable for benchmarking neural
models in floor plan analysis. This work demonstrates the potential of
combining deep learning and multimodal reasoning for efficient dataset
construction in complex real-world domains.

</details>


### [147] [A Registration-Based Star-Shape Segmentation Model and Fast Algorithms](https://arxiv.org/abs/2508.07721)
*Daoping Zhang,Xue-Cheng Tai,Lok Ming Lui*

Main category: cs.CV

TL;DR: 提出了一种基于配准框架的星形分割模型，结合水平集表示和约束，支持单中心或多中心的星形分割，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 图像分割在提取目标对象和识别边界时面临遮挡、模糊或噪声的挑战，星形先验信息被用于解决这一问题。

Method: 结合水平集表示与配准框架，对变形水平集函数施加约束，支持全星形或部分星形分割，并可指定边界通过特定地标位置。

Result: 数值实验表明，该方法在合成和真实图像上实现了准确的星形分割。

Conclusion: 提出的模型在星形分割任务中表现出色，能够适应多种复杂场景。

Abstract: Image segmentation plays a crucial role in extracting objects of interest and
identifying their boundaries within an image. However, accurate segmentation
becomes challenging when dealing with occlusions, obscurities, or noise in
corrupted images. To tackle this challenge, prior information is often
utilized, with recent attention on star-shape priors. In this paper, we propose
a star-shape segmentation model based on the registration framework. By
combining the level set representation with the registration framework and
imposing constraints on the deformed level set function, our model enables both
full and partial star-shape segmentation, accommodating single or multiple
centers. Additionally, our approach allows for the enforcement of identified
boundaries to pass through specified landmark locations. We tackle the proposed
models using the alternating direction method of multipliers. Through numerical
experiments conducted on synthetic and real images, we demonstrate the efficacy
of our approach in achieving accurate star-shape segmentation.

</details>


### [148] [Enhancing Small-Scale Dataset Expansion with Triplet-Connection-based Sample Re-Weighting](https://arxiv.org/abs/2508.07723)
*Ting Xiang,Changjian Chen,Zhuo Tang,Qifeng Zhang,Fei Lyu,Li Yang,Jiapeng Zhang,Kenli Li*

Main category: cs.CV

TL;DR: 论文提出TriReWeight方法，通过三重连接样本重加权提升生成数据增强效果，理论证明其性能不降且泛化接近最优，实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决生成数据增强中因不可控生成过程和自然语言模糊性导致的噪声图像问题。

Method: 基于理论分析，开发TriReWeight方法，三重连接样本重加权，适用于任何生成数据增强方法。

Result: 实验显示TriReWeight在自然和医学数据集上平均优于现有方法7.9%和3.4%。

Conclusion: TriReWeight有效提升生成数据增强性能，理论支持其泛化接近最优。

Abstract: The performance of computer vision models in certain real-world applications,
such as medical diagnosis, is often limited by the scarcity of available
images. Expanding datasets using pre-trained generative models is an effective
solution. However, due to the uncontrollable generation process and the
ambiguity of natural language, noisy images may be generated. Re-weighting is
an effective way to address this issue by assigning low weights to such noisy
images. We first theoretically analyze three types of supervision for the
generated images. Based on the theoretical analysis, we develop TriReWeight, a
triplet-connection-based sample re-weighting method to enhance generative data
augmentation. Theoretically, TriReWeight can be integrated with any generative
data augmentation methods and never downgrade their performance. Moreover, its
generalization approaches the optimal in the order $O(\sqrt{d\ln (n)/n})$. Our
experiments validate the correctness of the theoretical analysis and
demonstrate that our method outperforms the existing SOTA methods by $7.9\%$ on
average over six natural image datasets and by $3.4\%$ on average over three
medical datasets. We also experimentally validate that our method can enhance
the performance of different generative data augmentation methods.

</details>


### [149] [Grouped Speculative Decoding for Autoregressive Image Generation](https://arxiv.org/abs/2508.07747)
*Junhyuk So,Juncheol Shin,Hyunho Kook,Eunhyeok Park*

Main category: cs.CV

TL;DR: 论文提出了一种名为GSD的无训练加速方法，用于提升自回归图像模型的推理速度，解决了传统方法因图像令牌冗余和多样性导致的效率问题。


<details>
  <summary>Details</summary>
Motivation: 自回归图像模型虽然生成能力强，但推理速度慢，限制了实际应用。现有加速方法效果有限或需额外训练。

Method: 提出动态分组推测解码（GSD），通过评估视觉有效令牌簇而非单一目标令牌来减少误拒，提升效率。

Result: 实验表明，GSD平均加速3.7倍，且无需额外训练即可保持图像质量。

Conclusion: GSD为自回归图像模型提供了一种高效、无训练的加速方案，解决了传统方法的局限性。

Abstract: Recently, autoregressive (AR) image models have demonstrated remarkable
generative capabilities, positioning themselves as a compelling alternative to
diffusion models. However, their sequential nature leads to long inference
times, limiting their practical scalability. In this work, we introduce Grouped
Speculative Decoding (GSD), a novel, training-free acceleration method for AR
image models. While recent studies have explored Speculative Decoding (SD) as a
means to speed up AR image generation, existing approaches either provide only
modest acceleration or require additional training. Our in-depth analysis
reveals a fundamental difference between language and image tokens: image
tokens exhibit inherent redundancy and diversity, meaning multiple tokens can
convey valid semantics. However, traditional SD methods are designed to accept
only a single most-likely token, which fails to leverage this difference,
leading to excessive false-negative rejections. To address this, we propose a
new SD strategy that evaluates clusters of visually valid tokens rather than
relying on a single target token. Additionally, we observe that static
clustering based on embedding distance is ineffective, which motivates our
dynamic GSD approach. Extensive experiments show that GSD accelerates AR image
models by an average of 3.7x while preserving image quality-all without
requiring any additional training. The source code is available at
https://github.com/junhyukso/GSD

</details>


### [150] [Comparison Reveals Commonality: Customized Image Generation through Contrastive Inversion](https://arxiv.org/abs/2508.07755)
*Minseo Kim,Minchan Kwon,Dongyeun Lee,Yunho Jeon,Junmo Kim*

Main category: cs.CV

TL;DR: 论文提出了一种名为Contrastive Inversion的新方法，通过对比输入图像提取共同概念，无需额外指导信息，提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖额外指导（如文本提示或空间掩码）提取共同概念，但手动提供的指导可能导致辅助特征分离不完全，影响生成质量。

Method: 通过对比学习训练目标标记和图像辅助文本标记，提取目标真实语义，并应用解耦交叉注意力微调以提高概念保真度。

Result: 实验表明，该方法在概念表示和编辑方面均表现出色，优于现有技术。

Conclusion: Contrastive Inversion方法在无需额外信息的情况下，实现了高质量的概念提取和生成。

Abstract: The recent demand for customized image generation raises a need for
techniques that effectively extract the common concept from small sets of
images. Existing methods typically rely on additional guidance, such as text
prompts or spatial masks, to capture the common target concept. Unfortunately,
relying on manually provided guidance can lead to incomplete separation of
auxiliary features, which degrades generation quality.In this paper, we propose
Contrastive Inversion, a novel approach that identifies the common concept by
comparing the input images without relying on additional information. We train
the target token along with the image-wise auxiliary text tokens via
contrastive learning, which extracts the well-disentangled true semantics of
the target. Then we apply disentangled cross-attention fine-tuning to improve
concept fidelity without overfitting. Experimental results and analysis
demonstrate that our method achieves a balanced, high-level performance in both
concept representation and editing, outperforming existing techniques.

</details>


### [151] [Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild](https://arxiv.org/abs/2508.07759)
*Haoran Wang,Zekun Li,Jian Zhang,Lei Qi,Yinghuan Shi*

Main category: cs.CV

TL;DR: CAV-SAM利用伪视频表示参考-目标图像对的对应关系，通过SAM2的iVOS能力轻量级适应下游任务，性能提升超5%。


<details>
  <summary>Details</summary>
Motivation: 现有参考分割方法依赖元学习，数据与计算成本高，需轻量级解决方案。

Method: 将参考-目标图像对表示为伪视频，利用SAM2的iVOS能力，结合DBST和TTGA模块。

Result: 在广泛使用的数据集上，分割性能提升超5%。

Conclusion: CAV-SAM为轻量级适应下游任务提供了高效方法。

Abstract: Large vision models like the Segment Anything Model (SAM) exhibit significant
limitations when applied to downstream tasks in the wild. Consequently,
reference segmentation, which leverages reference images and their
corresponding masks to impart novel knowledge to the model, emerges as a
promising new direction for adapting vision models. However, existing reference
segmentation approaches predominantly rely on meta-learning, which still
necessitates an extensive meta-training process and brings massive data and
computational cost. In this study, we propose a novel approach by representing
the inherent correspondence between reference-target image pairs as a pseudo
video. This perspective allows the latest version of SAM, known as SAM2, which
is equipped with interactive video object segmentation (iVOS) capabilities, to
be adapted to downstream tasks in a lightweight manner. We term this approach
Correspondence As Video for SAM (CAV-SAM). CAV-SAM comprises two key modules:
the Diffusion-Based Semantic Transition (DBST) module employs a diffusion model
to construct a semantic transformation sequence, while the Test-Time Geometric
Alignment (TTGA) module aligns the geometric changes within this sequence
through test-time fine-tuning. We evaluated CAVSAM on widely-used datasets,
achieving segmentation performance improvements exceeding 5% over SOTA methods.
Implementation is provided in the supplementary materials.

</details>


### [152] [UniSVG: A Unified Dataset for Vector Graphic Understanding and Generation with Multimodal Large Language Models](https://arxiv.org/abs/2508.07766)
*Jinke Li,Jiarui Yu,Chenxing Wei,Hande Dong,Qiang Lin,Liangjing Yang,Zhicai Wang,Yanbin Hao*

Main category: cs.CV

TL;DR: 论文提出UniSVG数据集，用于训练和评估多模态大语言模型（MLLMs）在SVG理解和生成任务中的表现，并展示了其优于现有模型的效果。


<details>
  <summary>Details</summary>
Motivation: SVG在计算机视觉和艺术设计中广泛应用，但AI驱动的SVG理解和生成仍面临挑战，需要高精度和多模态处理能力。

Method: 通过构建包含525k数据项的UniSVG数据集，支持MLLMs在SVG任务中的训练和评估。

Result: 实验表明，使用该数据集训练的MLLMs在SVG理解和生成任务中表现优异，甚至超越GPT-4V等闭源模型。

Conclusion: UniSVG数据集为SVG领域的研究提供了重要资源，推动了MLLMs在SVG任务中的应用。

Abstract: Unlike bitmap images, scalable vector graphics (SVG) maintain quality when
scaled, frequently employed in computer vision and artistic design in the
representation of SVG code. In this era of proliferating AI-powered systems,
enabling AI to understand and generate SVG has become increasingly urgent.
However, AI-driven SVG understanding and generation (U&G) remain significant
challenges. SVG code, equivalent to a set of curves and lines controlled by
floating-point parameters, demands high precision in SVG U&G. Besides, SVG
generation operates under diverse conditional constraints, including textual
prompts and visual references, which requires powerful multi-modal processing
for condition-to-SVG transformation. Recently, the rapid growth of Multi-modal
Large Language Models (MLLMs) have demonstrated capabilities to process
multi-modal inputs and generate complex vector controlling parameters,
suggesting the potential to address SVG U&G tasks within a unified model. To
unlock MLLM's capabilities in the SVG area, we propose an SVG-centric dataset
called UniSVG, comprising 525k data items, tailored for MLLM training and
evaluation. To our best knowledge, it is the first comprehensive dataset
designed for unified SVG generation (from textual prompts and images) and SVG
understanding (color, category, usage, etc.). As expected, learning on the
proposed dataset boosts open-source MLLMs' performance on various SVG U&G
tasks, surpassing SOTA close-source MLLMs like GPT-4V. We release dataset,
benchmark, weights, codes and experiment details on
https://ryanlijinke.github.io/.

</details>


### [153] [Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation](https://arxiv.org/abs/2508.07769)
*Xiaoyan Liu,Kangrui Li,Jiaxin Liu*

Main category: cs.CV

TL;DR: Dream4D是一个新颖的框架，通过结合可控视频生成和神经4D重建，解决了4D内容合成的时空一致性问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法在保持视图一致性和处理复杂场景动态方面存在困难，尤其是在大规模环境中。

Method: 采用两阶段架构：先通过少样本学习预测相机轨迹，再通过姿态条件扩散过程生成几何一致的多视角序列，最后转换为持久4D表示。

Result: 框架首次结合视频扩散模型的时间先验和重建模型的几何感知，显著提升了4D生成质量（如mPSNR、mSSIM）。

Conclusion: Dream4D在4D内容合成中表现出更高的质量和一致性，优于现有方法。

Abstract: The synthesis of spatiotemporally coherent 4D content presents fundamental
challenges in computer vision, requiring simultaneous modeling of high-fidelity
spatial representations and physically plausible temporal dynamics. Current
approaches often struggle to maintain view consistency while handling complex
scene dynamics, particularly in large-scale environments with multiple
interacting elements. This work introduces Dream4D, a novel framework that
bridges this gap through a synergy of controllable video generation and neural
4D reconstruction. Our approach seamlessly combines a two-stage architecture:
it first predicts optimal camera trajectories from a single image using
few-shot learning, then generates geometrically consistent multi-view sequences
via a specialized pose-conditioned diffusion process, which are finally
converted into a persistent 4D representation. This framework is the first to
leverage both rich temporal priors from video diffusion models and geometric
awareness of the reconstruction models, which significantly facilitates 4D
generation and shows higher quality (e.g., mPSNR, mSSIM) over existing methods.

</details>


### [154] [Prototype-Guided Curriculum Learning for Zero-Shot Learning](https://arxiv.org/abs/2508.07771)
*Lei Wang,Shiming Chen,Guo-Sen Xie,Ziming Hong,Chaojian Yu,Qinmu Peng,Xinge You*

Main category: cs.CV

TL;DR: 论文提出了一种原型引导的课程学习框架（CLZSL），通过PCL模块减少实例级不匹配，通过PUP模块动态更新语义原型，提升零样本学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有零样本学习中的语义原型是人工定义的，可能因实例级不匹配和类级不精确导致噪声监督，影响知识迁移效果。

Method: CLZSL框架包含PCL模块（优先学习高相似度样本）和PUP模块（动态更新语义原型），以减少噪声监督。

Result: 在AWA2、SUN和CUB数据集上的实验验证了方法的有效性。

Conclusion: CLZSL通过课程学习和动态原型更新，显著提升了零样本学习的性能。

Abstract: In Zero-Shot Learning (ZSL), embedding-based methods enable knowledge
transfer from seen to unseen classes by learning a visual-semantic mapping from
seen-class images to class-level semantic prototypes (e.g., attributes).
However, these semantic prototypes are manually defined and may introduce noisy
supervision for two main reasons: (i) instance-level mismatch: variations in
perspective, occlusion, and annotation bias will cause discrepancies between
individual sample and the class-level semantic prototypes; and (ii) class-level
imprecision: the manually defined semantic prototypes may not accurately
reflect the true semantics of the class. Consequently, the visual-semantic
mapping will be misled, reducing the effectiveness of knowledge transfer to
unseen classes. In this work, we propose a prototype-guided curriculum learning
framework (dubbed as CLZSL), which mitigates instance-level mismatches through
a Prototype-Guided Curriculum Learning (PCL) module and addresses class-level
imprecision via a Prototype Update (PUP) module. Specifically, the PCL module
prioritizes samples with high cosine similarity between their visual mappings
and the class-level semantic prototypes, and progressively advances to
less-aligned samples, thereby reducing the interference of instance-level
mismatches to achieve accurate visual-semantic mapping. Besides, the PUP module
dynamically updates the class-level semantic prototypes by leveraging the
visual mappings learned from instances, thereby reducing class-level
imprecision and further improving the visual-semantic mapping. Experiments were
conducted on standard benchmark datasets-AWA2, SUN, and CUB-to verify the
effectiveness of our method.

</details>


### [155] [Forecasting Continuous Non-Conservative Dynamical Systems in SO(3)](https://arxiv.org/abs/2508.07775)
*Lennart Bastian,Mohammad Rashed,Nassir Navab,Tolga Birdal*

Main category: cs.CV

TL;DR: 论文提出了一种基于神经控制微分方程和SO(3) Savitzky-Golay路径的方法，用于在3D旋转流形上建模噪声姿态估计的轨迹，解决了SO(3)外推中的动态复杂性、非保守运动学和噪声鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: SO(3)外推在计算机视觉中具有基础性意义，但面临动态复杂性、非保守运动学和噪声鲁棒性等挑战。现有方法依赖能量守恒或恒定速度假设，限制了其在真实场景中的应用。

Method: 利用神经控制微分方程和SO(3) Savitzky-Golay路径，在物理和几何意义上建模轨迹，无需依赖能量或动量守恒假设。

Result: 该方法在仿真和真实场景中表现出鲁棒的外推能力，适用于复杂非惯性系统，并能泛化到未知物理参数的轨迹。

Conclusion: 提出的方法为SO(3)外推提供了一种灵活且鲁棒的解决方案，适用于复杂动态系统，并可集成到现有流程中。

Abstract: Modeling the rotation of moving objects is a fundamental task in computer
vision, yet $SO(3)$ extrapolation still presents numerous challenges: (1)
unknown quantities such as the moment of inertia complicate dynamics, (2) the
presence of external forces and torques can lead to non-conservative
kinematics, and (3) estimating evolving state trajectories under sparse, noisy
observations requires robustness. We propose modeling trajectories of noisy
pose estimates on the manifold of 3D rotations in a physically and
geometrically meaningful way by leveraging Neural Controlled Differential
Equations guided with $SO(3)$ Savitzky-Golay paths. Existing extrapolation
methods often rely on energy conservation or constant velocity assumptions,
limiting their applicability in real-world scenarios involving non-conservative
forces. In contrast, our approach is agnostic to energy and momentum
conservation while being robust to input noise, making it applicable to
complex, non-inertial systems. Our approach is easily integrated as a module in
existing pipelines and generalizes well to trajectories with unknown physical
parameters. By learning to approximate object dynamics from noisy states during
training, our model attains robust extrapolation capabilities in simulation and
various real-world settings. Code is available at
https://github.com/bastianlb/forecasting-rotational-dynamics

</details>


### [156] [GaitSnippet: Gait Recognition Beyond Unordered Sets and Ordered Sequences](https://arxiv.org/abs/2508.07782)
*Saihui Hou,Chenye Wang,Wenpeng Lang,Zhengxiang Lan,Yongzhen Huang*

Main category: cs.CV

TL;DR: 论文提出了一种基于片段（snippet）的步态识别方法，通过随机选择连续片段中的帧来表示个性化动作，解决了传统集合和序列方法在短时和长时依赖上的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统步态识别方法（集合或序列）在短时和长时依赖上存在不足，作者受人类识别启发，提出将步态视为个性化动作的组合。

Method: 提出片段采样和片段建模，通过多尺度时间上下文学习步态特征。

Result: 在四个常用数据集上验证，例如Gait3D上rank-1准确率77.5%，GREW上81.7%。

Conclusion: 片段方法有效且潜力大，为步态识别提供了新视角。

Abstract: Recent advancements in gait recognition have significantly enhanced
performance by treating silhouettes as either an unordered set or an ordered
sequence. However, both set-based and sequence-based approaches exhibit notable
limitations. Specifically, set-based methods tend to overlook short-range
temporal context for individual frames, while sequence-based methods struggle
to capture long-range temporal dependencies effectively. To address these
challenges, we draw inspiration from human identification and propose a new
perspective that conceptualizes human gait as a composition of individualized
actions. Each action is represented by a series of frames, randomly selected
from a continuous segment of the sequence, which we term a snippet.
Fundamentally, the collection of snippets for a given sequence enables the
incorporation of multi-scale temporal context, facilitating more comprehensive
gait feature learning. Moreover, we introduce a non-trivial solution for
snippet-based gait recognition, focusing on Snippet Sampling and Snippet
Modeling as key components. Extensive experiments on four widely-used gait
datasets validate the effectiveness of our proposed approach and, more
importantly, highlight the potential of gait snippets. For instance, our method
achieves the rank-1 accuracy of 77.5% on Gait3D and 81.7% on GREW using a 2D
convolution-based backbone.

</details>


### [157] [Boosting Active Defense Persistence: A Two-Stage Defense Framework Combining Interruption and Poisoning Against Deepfake](https://arxiv.org/abs/2508.07795)
*Hongrui Zheng,Yuezun Li,Liejun Wang,Yunfeng Diao,Zhiqing Guo*

Main category: cs.CV

TL;DR: 论文提出了一种两阶段防御框架（TSDF），通过双功能对抗扰动来持久防御深度伪造技术，防止攻击者通过重新训练模型绕过防御。


<details>
  <summary>Details</summary>
Motivation: 现有主动防御策略缺乏持久性，攻击者可通过重新训练模型绕过防御，限制了实际应用。

Method: 提出TSDF框架，利用强度分离机制设计双功能对抗扰动，既直接扭曲伪造结果，又通过数据投毒破坏攻击者的重新训练流程。

Result: 实验表明，TSDF具有强大的双重防御能力，显著提升了防御的持久性。

Conclusion: TSDF通过双功能对抗扰动有效解决了防御持久性问题，为深度伪造防御提供了新思路。

Abstract: Active defense strategies have been developed to counter the threat of
deepfake technology. However, a primary challenge is their lack of persistence,
as their effectiveness is often short-lived. Attackers can bypass these
defenses by simply collecting protected samples and retraining their models.
This means that static defenses inevitably fail when attackers retrain their
models, which severely limits practical use. We argue that an effective defense
not only distorts forged content but also blocks the model's ability to adapt,
which occurs when attackers retrain their models on protected images. To
achieve this, we propose an innovative Two-Stage Defense Framework (TSDF).
Benefiting from the intensity separation mechanism designed in this paper, the
framework uses dual-function adversarial perturbations to perform two roles.
First, it can directly distort the forged results. Second, it acts as a
poisoning vehicle that disrupts the data preparation process essential for an
attacker's retraining pipeline. By poisoning the data source, TSDF aims to
prevent the attacker's model from adapting to the defensive perturbations, thus
ensuring the defense remains effective long-term. Comprehensive experiments
show that the performance of traditional interruption methods degrades sharply
when it is subjected to adversarial retraining. However, our framework shows a
strong dual defense capability, which can improve the persistence of active
defense. Our code will be available at https://github.com/vpsg-research/TSDF.

</details>


### [158] [Power Battery Detection](https://arxiv.org/abs/2508.07797)
*Xiaoqi Zhao,Peiqian Cao,Lihe Zhang,Zonglei Feng,Hanqi Liu,Jiaming Zuo,Youwei Pang,Weisi Lin,Georges El Fakhri,Huchuan Lu,Xiaofeng Liu*

Main category: cs.CV

TL;DR: 该论文提出了一个名为PBD5K的大规模基准数据集，用于动力电池检测任务，并设计了MDCNeXt模型来解决X射线图像中阴极和阳极板的密集端点定位问题。


<details>
  <summary>Details</summary>
Motivation: 动力电池内部结构缺陷可能导致严重安全隐患，传统视觉算法和人工检测效率低且易出错，因此需要一种更高效、准确的自动化检测方法。

Method: 论文将动力电池检测任务定义为点级分割问题，提出了MDCNeXt模型，该模型结合多维结构线索（点、线、计数信息）并引入两个状态空间模块（提示过滤模块和密度感知重排序模块）以提升性能。

Result: 通过提出的智能标注流程和MDCNeXt模型，论文在PBD5K数据集上实现了对密集端点的高效定位，并抑制了视觉干扰。

Conclusion: PBD5K数据集和MDCNeXt模型为动力电池检测任务提供了有效的解决方案，未来将公开源代码和数据集以推动进一步研究。

Abstract: Power batteries are essential components in electric vehicles, where internal
structural defects can pose serious safety risks. We conduct a comprehensive
study on a new task, power battery detection (PBD), which aims to localize the
dense endpoints of cathode and anode plates from industrial X-ray images for
quality inspection. Manual inspection is inefficient and error-prone, while
traditional vision algorithms struggle with densely packed plates, low
contrast, scale variation, and imaging artifacts. To address this issue and
drive more attention into this meaningful task, we present PBD5K, the first
large-scale benchmark for this task, consisting of 5,000 X-ray images from nine
battery types with fine-grained annotations and eight types of real-world
visual interference. To support scalable and consistent labeling, we develop an
intelligent annotation pipeline that combines image filtering, model-assisted
pre-labeling, cross-verification, and layered quality evaluation. We formulate
PBD as a point-level segmentation problem and propose MDCNeXt, a model designed
to extract and integrate multi-dimensional structure clues including point,
line, and count information from the plate itself. To improve discrimination
between plates and suppress visual interference, MDCNeXt incorporates two state
space modules. The first is a prompt-filtered module that learns contrastive
relationships guided by task-specific prompts. The second is a density-aware
reordering module that refines segmentation in regions with high plate density.
In addition, we propose a distance-adaptive mask generation strategy to provide
robust supervision under varying spatial distributions of anode and cathode
positions. The source code and datasets will be publicly available at
\href{https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD}{PBD5K}.

</details>


### [159] [MambaTrans: Multimodal Fusion Image Translation via Large Language Model Priors for Downstream Visual Tasks](https://arxiv.org/abs/2508.07803)
*Yushen Xu,Xiaosong Li,Zhenyu Kuang,Xiaoqi Cheng,Haishu Tan,Huafeng Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为MambaTrans的多模态融合图像模态翻译器，旨在解决红外与可见光图像融合后因像素分布差异导致下游任务性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有下游预训练模型通常基于可见光图像训练，而多模态融合图像与可见光图像的像素分布差异显著，可能导致下游任务性能下降甚至不如仅使用可见光图像。

Method: MambaTrans利用多模态大语言模型的描述和语义分割模型的掩码作为输入，通过Multi-Model State Space Block结合掩码-图像-文本交叉注意力和3D选择性扫描模块，提升纯视觉能力，并利用目标检测先验知识最小化检测损失。

Result: 在公开数据集上的实验表明，MambaTrans有效提升了多模态图像在下游任务中的性能。

Conclusion: MambaTrans能够在不调整预训练模型参数的情况下，显著提升多模态融合图像在下游任务中的表现。

Abstract: The goal of multimodal image fusion is to integrate complementary information
from infrared and visible images, generating multimodal fused images for
downstream tasks. Existing downstream pre-training models are typically trained
on visible images. However, the significant pixel distribution differences
between visible and multimodal fusion images can degrade downstream task
performance, sometimes even below that of using only visible images. This paper
explores adapting multimodal fused images with significant modality differences
to object detection and semantic segmentation models trained on visible images.
To address this, we propose MambaTrans, a novel multimodal fusion image
modality translator. MambaTrans uses descriptions from a multimodal large
language model and masks from semantic segmentation models as input. Its core
component, the Multi-Model State Space Block, combines mask-image-text
cross-attention and a 3D-Selective Scan Module, enhancing pure visual
capabilities. By leveraging object detection prior knowledge, MambaTrans
minimizes detection loss during training and captures long-term dependencies
among text, masks, and images. This enables favorable results in pre-trained
models without adjusting their parameters. Experiments on public datasets show
that MambaTrans effectively improves multimodal image performance in downstream
tasks.

</details>


### [160] [Pose-RFT: Enhancing MLLMs for 3D Pose Generation via Hybrid Action Reinforcement Fine-Tuning](https://arxiv.org/abs/2508.07804)
*Bao Li,Xiaomei Zhang,Miao Xu,Zhaoxin Fan,Xiangyu Zhu,Zhen Lei*

Main category: cs.CV

TL;DR: Pose-RFT是一种基于强化学习的框架，用于优化多模态大语言模型（MLLMs）在3D人体姿态生成任务中的表现。通过结合离散语言预测和连续姿态生成的混合动作强化学习，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在3D姿态生成任务中因监督目标（如SMPL参数回归）难以建模模糊性和任务对齐而表现不佳。

Method: 提出Pose-RFT框架，采用混合动作强化学习（HyGRPO算法），结合任务特定的奖励函数，优化离散和连续动作。

Result: 在多个姿态生成基准测试中，Pose-RFT显著优于现有方法。

Conclusion: 混合动作强化微调在3D姿态生成任务中具有显著效果。

Abstract: Generating 3D human poses from multimodal inputs such as images or text
requires models to capture both rich spatial and semantic correspondences.
While pose-specific multimodal large language models (MLLMs) have shown promise
in this task, they are typically trained with supervised objectives such as
SMPL parameter regression or token-level prediction, which struggle to model
the inherent ambiguity and achieve task-specific alignment required for
accurate 3D pose generation. To address these limitations, we propose Pose-RFT,
a reinforcement fine-tuning framework tailored for 3D human pose generation in
MLLMs. We formulate the task as a hybrid action reinforcement learning problem
that jointly optimizes discrete language prediction and continuous pose
generation. To this end, we introduce HyGRPO, a hybrid reinforcement learning
algorithm that performs group-wise reward normalization over sampled responses
to guide joint optimization of discrete and continuous actions. Pose-RFT
further incorporates task-specific reward functions to guide optimization
towards spatial alignment in image-to-pose generation and semantic consistency
in text-to-pose generation. Extensive experiments on multiple pose generation
benchmarks demonstrate that Pose-RFT significantly improves performance over
existing pose-specific MLLMs, validating the effectiveness of hybrid action
reinforcement fine-tuning for 3D pose generation.

</details>


### [161] [DiTVR: Zero-Shot Diffusion Transformer for Video Restoration](https://arxiv.org/abs/2508.07811)
*Sicheng Gao,Nancy Mehta,Zongwei Wu,Radu Timofte*

Main category: cs.CV

TL;DR: DiTVR是一种零样本视频修复框架，结合扩散变换器和轨迹感知注意力机制，通过光流轨迹对齐令牌，提升时间一致性。


<details>
  <summary>Details</summary>
Motivation: 传统回归方法生成不真实细节且需大量配对数据，而生成扩散模型难以保证时间一致性。DiTVR旨在解决这些问题。

Method: 采用扩散变换器与轨迹感知注意力机制，结合小波引导的流一致性采样器，动态选择相关令牌并注入数据一致性。

Result: 在视频修复基准测试中达到零样本最优性能，时间一致性和细节保留表现优异。

Conclusion: DiTVR通过创新设计解决了视频修复中的时间一致性问题，性能显著优于现有方法。

Abstract: Video restoration aims to reconstruct high quality video sequences from low
quality inputs, addressing tasks such as super resolution, denoising, and
deblurring. Traditional regression based methods often produce unrealistic
details and require extensive paired datasets, while recent generative
diffusion models face challenges in ensuring temporal consistency. We introduce
DiTVR, a zero shot video restoration framework that couples a diffusion
transformer with trajectory aware attention and a wavelet guided, flow
consistent sampler. Unlike prior 3D convolutional or frame wise diffusion
approaches, our attention mechanism aligns tokens along optical flow
trajectories, with particular emphasis on vital layers that exhibit the highest
sensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically
selects relevant tokens based on motion correspondences across frames. The flow
guided sampler injects data consistency only into low-frequency bands,
preserving high frequency priors while accelerating convergence. DiTVR
establishes a new zero shot state of the art on video restoration benchmarks,
demonstrating superior temporal consistency and detail preservation while
remaining robust to flow noise and occlusions.

</details>


### [162] [Semi-supervised Multiscale Matching for SAR-Optical Image](https://arxiv.org/abs/2508.07812)
*Jingze Gai,Changchun Li*

Main category: cs.CV

TL;DR: 论文提出了一种半监督的SAR-光学图像匹配方法S2M2-SAR，通过结合少量标注数据和大量未标注数据，利用伪标签和跨模态特征增强模块，显著提升了匹配性能。


<details>
  <summary>Details</summary>
Motivation: 现有SAR-光学图像匹配方法依赖像素级标注数据，但标注成本高且难以获取足够数据。为解决这一问题，论文提出了一种半监督学习方法。

Method: 设计了半监督匹配流程，利用伪标签生成相似性热图，并结合跨模态特征增强模块，通过无监督目标分离模态共享和模态特定特征。

Result: 实验表明，S2M2-SAR在基准数据集上优于现有半监督方法，性能接近全监督SOTA方法。

Conclusion: S2M2-SAR展示了高效性和实用潜力，为SAR-光学图像匹配提供了一种低成本、高性能的解决方案。

Abstract: Driven by the complementary nature of optical and synthetic aperture radar
(SAR) images, SAR-optical image matching has garnered significant interest.
Most existing SAR-optical image matching methods aim to capture effective
matching features by employing the supervision of pixel-level matched
correspondences within SAR-optical image pairs, which, however, suffers from
time-consuming and complex manual annotation, making it difficult to collect
sufficient labeled SAR-optical image pairs. To handle this, we design a
semi-supervised SAR-optical image matching pipeline that leverages both scarce
labeled and abundant unlabeled image pairs and propose a semi-supervised
multiscale matching for SAR-optical image matching (S2M2-SAR). Specifically, we
pseudo-label those unlabeled SAR-optical image pairs with pseudo ground-truth
similarity heatmaps by combining both deep and shallow level matching results,
and train the matching model by employing labeled and pseudo-labeled similarity
heatmaps. In addition, we introduce a cross-modal feature enhancement module
trained using a cross-modality mutual independence loss, which requires no
ground-truth labels. This unsupervised objective promotes the separation of
modality-shared and modality-specific features by encouraging statistical
independence between them, enabling effective feature disentanglement across
optical and SAR modalities. To evaluate the effectiveness of S2M2-SAR, we
compare it with existing competitors on benchmark datasets. Experimental
results demonstrate that S2M2-SAR not only surpasses existing semi-supervised
methods but also achieves performance competitive with fully supervised SOTA
methods, demonstrating its efficiency and practical potential.

</details>


### [163] [Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models](https://arxiv.org/abs/2508.07818)
*Chenyue Song,Chen Hui,Haiqi Zhu,Feng Jiang,Yachun Mi,Wei Zhang,Shaohui Liu*

Main category: cs.CV

TL;DR: 提出了一种名为RSFIQA的细粒度图像质量评估模型，通过动态分割语义区域并利用多模态大语言模型提取内容，结合区域感知语义注意力机制，显著提升了图像质量评估的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有无参考图像质量评估方法在全局表征或区域特征均匀加权方面存在局限性，无法充分捕捉语义显著区域或局部质量变化。

Method: 利用Segment Anything Model动态分割图像为语义区域，结合多模态大语言模型提取内容和感知失真，引入区域感知语义注意力机制生成全局注意力图。

Result: 实验表明，RSFIQA在多个基准数据集上表现出色，具有竞争性的质量预测性能。

Conclusion: RSFIQA通过细粒度区域分析和多模态信息整合，显著提升了图像质量评估的准确性和鲁棒性。

Abstract: No-reference image quality assessment (NR-IQA) aims to simulate the process
of perceiving image quality aligned with subjective human perception. However,
existing NR-IQA methods either focus on global representations that leads to
limited insights into the semantically salient regions or employ a uniform
weighting for region features that weakens the sensitivity to local quality
variations. In this paper, we propose a fine-grained image quality assessment
model, named RSFIQA, which integrates region-level distortion information to
perceive multi-dimensional quality discrepancies. To enhance regional quality
awareness, we first utilize the Segment Anything Model (SAM) to dynamically
partition the input image into non-overlapping semantic regions. For each
region, we teach a powerful Multi-modal Large Language Model (MLLM) to extract
descriptive content and perceive multi-dimensional distortions, enabling a
comprehensive understanding of both local semantics and quality degradations.
To effectively leverage this information, we introduce Region-Aware Semantic
Attention (RSA) mechanism, which generates a global attention map by
aggregating fine-grained representations from local regions. In addition,
RSFIQA is backbone-agnostic and can be seamlessly integrated into various deep
neural network architectures. Extensive experiments demonstrate the robustness
and effectiveness of the proposed method, which achieves competitive quality
prediction performance across multiple benchmark datasets.

</details>


### [164] [Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP](https://arxiv.org/abs/2508.07819)
*Ke Ma,Jun Long,Hongxiao Fei,Liujie Hua,Yueyi Luo*

Main category: cs.CV

TL;DR: 论文提出了一种架构协同设计框架，通过卷积低秩适配器和动态融合网关解决预训练视觉语言模型在零样本异常检测中的适应性问题。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉语言模型在零样本异常检测中存在适应性问题，缺乏局部归纳偏置和灵活的特征融合方式。

Method: 采用卷积低秩适配器（Conv-LoRA）注入局部归纳偏置，并结合动态融合网关（DFG）实现双向自适应特征融合。

Result: 在工业和医学基准测试中表现出更高的准确性和鲁棒性。

Conclusion: 协同设计框架是适应密集感知任务的关键，验证了其有效性。

Abstract: Pre-trained Vision-Language Models (VLMs) face a significant adaptation gap
when applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack of
local inductive biases for dense prediction and their reliance on inflexible
feature fusion paradigms. We address these limitations through an Architectural
Co-Design framework that jointly refines feature representation and cross-modal
fusion. Our method integrates a parameter-efficient Convolutional Low-Rank
Adaptation (Conv-LoRA) adapter to inject local inductive biases for
fine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) that
leverages visual context to adaptively modulate text prompts, enabling a
powerful bidirectional fusion. Extensive experiments on diverse industrial and
medical benchmarks demonstrate superior accuracy and robustness, validating
that this synergistic co-design is critical for robustly adapting foundation
models to dense perception tasks.

</details>


### [165] [MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization](https://arxiv.org/abs/2508.07833)
*Animesh Jain,Alexandros Stergiou*

Main category: cs.CV

TL;DR: MIMIC框架通过合成视觉概念来可视化VLMs的内部表示，提高模型透明度和信任度。


<details>
  <summary>Details</summary>
Motivation: VLMs的复杂架构难以解释，限制了透明度和信任。

Method: MIMIC结合VLM反演和特征对齐目标，加入空间对齐、图像平滑和语义真实性的正则化。

Result: 通过定量和定性评估，MIMIC在自由文本输出上成功反演视觉概念，并报告了视觉质量和语义指标。

Conclusion: MIMIC是首个针对VLM视觉概念解释的反演方法。

Abstract: Vision Language Models (VLMs) encode multimodal inputs over large, complex,
and difficult-to-interpret architectures, which limit transparency and trust.
We propose a Multimodal Inversion for Model Interpretation and
Conceptualization (MIMIC) framework to visualize the internal representations
of VLMs by synthesizing visual concepts corresponding to internal encodings.
MIMIC uses a joint VLM-based inversion and a feature alignment objective to
account for VLM's autoregressive processing. It additionally includes a triplet
of regularizers for spatial alignment, natural image smoothness, and semantic
realism. We quantitatively and qualitatively evaluate MIMIC by inverting visual
concepts over a range of varying-length free-form VLM output texts. Reported
results include both standard visual quality metrics as well as semantic
text-based metrics. To the best of our knowledge, this is the first model
inversion approach addressing visual interpretations of VLM concepts.

</details>


### [166] [Effortless Vision-Language Model Specialization in Histopathology without Annotation](https://arxiv.org/abs/2508.07835)
*Jingna Qiu,Nishanth Jain,Jonas Ammeling,Marc Aubreville,Katharina Breininger*

Main category: cs.CV

TL;DR: 研究提出了一种无需标注的视觉语言模型（VLM）适应方法，通过继续预训练领域相关图像-文本对，显著提升了零样本和小样本性能。


<details>
  <summary>Details</summary>
Motivation: 现有通用视觉语言模型在特定下游任务中表现不佳，而监督微调需要人工标注数据。本文旨在探索无需标注的适应方法。

Method: 通过从现有数据库中提取领域和任务相关的图像-文本对，对VLM进行继续预训练。

Result: 实验表明，该方法显著提升了零样本和小样本性能，且在更大训练规模下可与小样本方法媲美。

Conclusion: 无需标注的继续预训练是一种有效的、任务无关的VLM适应方法，适用于组织病理学任务。

Abstract: Recent advances in Vision-Language Models (VLMs) in histopathology, such as
CONCH and QuiltNet, have demonstrated impressive zero-shot classification
capabilities across various tasks. However, their general-purpose design may
lead to suboptimal performance in specific downstream applications. While
supervised fine-tuning methods address this issue, they require manually
labeled samples for adaptation. This paper investigates annotation-free
adaptation of VLMs through continued pretraining on domain- and task-relevant
image-caption pairs extracted from existing databases. Our experiments on two
VLMs, CONCH and QuiltNet, across three downstream tasks reveal that these pairs
substantially enhance both zero-shot and few-shot performance. Notably, with
larger training sizes, continued pretraining matches the performance of
few-shot methods while eliminating manual labeling. Its effectiveness,
task-agnostic design, and annotation-free workflow make it a promising pathway
for adapting VLMs to new histopathology tasks. Code is available at
https://github.com/DeepMicroscopy/Annotation-free-VLM-specialization.

</details>


### [167] [CBDES MoE: Hierarchically Decoupled Mixture-of-Experts for Functional Modules in Autonomous Driving](https://arxiv.org/abs/2508.07838)
*Qi Xiang,Kunsong Shi,Zhigui Lin,Lei He*

Main category: cs.CV

TL;DR: 提出了一种名为CBDES MoE的分层解耦混合专家架构，用于提升多模态BEV感知系统的输入适应性、建模能力和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态BEV方法存在输入适应性有限、建模能力受限和泛化性能不足的问题。

Method: 采用分层解耦的混合专家架构（CBDES MoE），结合轻量级自注意力路由器（SAR）实现动态专家路径选择和稀疏推理。

Result: 在nuScenes数据集上，CBDES MoE在3D目标检测中表现优于单专家模型，mAP提升1.6点，NDS提升4.1点。

Conclusion: CBDES MoE是一种有效的模块化混合专家框架，适用于自动驾驶领域，具有实际优势。

Abstract: Bird's Eye View (BEV) perception systems based on multi-sensor feature fusion
have become a fundamental cornerstone for end-to-end autonomous driving.
However, existing multi-modal BEV methods commonly suffer from limited input
adaptability, constrained modeling capacity, and suboptimal generalization. To
address these challenges, we propose a hierarchically decoupled
Mixture-of-Experts architecture at the functional module level, termed
Computing Brain DEvelopment System Mixture-of-Experts (CBDES MoE). CBDES MoE
integrates multiple structurally heterogeneous expert networks with a
lightweight Self-Attention Router (SAR) gating mechanism, enabling dynamic
expert path selection and sparse, input-aware efficient inference. To the best
of our knowledge, this is the first modular Mixture-of-Experts framework
constructed at the functional module granularity within the autonomous driving
domain. Extensive evaluations on the real-world nuScenes dataset demonstrate
that CBDES MoE consistently outperforms fixed single-expert baselines in 3D
object detection. Compared to the strongest single-expert model, CBDES MoE
achieves a 1.6-point increase in mAP and a 4.1-point improvement in NDS,
demonstrating the effectiveness and practical advantages of the proposed
approach.

</details>


### [168] [Deep Space Weather Model: Long-Range Solar Flare Prediction from Multi-Wavelength Images](https://arxiv.org/abs/2508.07847)
*Shunya Nagashima,Komei Sugiura*

Main category: cs.CV

TL;DR: 提出了一种基于深度状态空间模型的Deep SWM方法，用于太阳耀斑预测，结合稀疏掩码自编码器和新基准FlareBench，性能优于现有方法和专家。


<details>
  <summary>Details</summary>
Motivation: 太阳耀斑预测对关键基础设施保护至关重要，但现有方法在特征学习和长时依赖建模上存在不足。

Method: 使用多深度状态空间模型处理太阳图像和时空依赖，结合稀疏掩码自编码器进行预训练。

Result: Deep SWM在性能和可靠性上优于基线方法和专家。

Conclusion: Deep SWM为太阳耀斑预测提供了高效可靠的解决方案，并开源了FlareBench基准。

Abstract: Accurate, reliable solar flare prediction is crucial for mitigating potential
disruptions to critical infrastructure, while predicting solar flares remains a
significant challenge. Existing methods based on heuristic physical features
often lack representation learning from solar images. On the other hand,
end-to-end learning approaches struggle to model long-range temporal
dependencies in solar images. In this study, we propose Deep Space Weather
Model (Deep SWM), which is based on multiple deep state space models for
handling both ten-channel solar images and long-range spatio-temporal
dependencies. Deep SWM also features a sparse masked autoencoder, a novel
pretraining strategy that employs a two-phase masking approach to preserve
crucial regions such as sunspots while compressing spatial information.
Furthermore, we built FlareBench, a new public benchmark for solar flare
prediction covering a full 11-year solar activity cycle, to validate our
method. Our method outperformed baseline methods and even human expert
performance on standard metrics in terms of performance and reliability. The
project page can be found at https://keio-smilab25.github.io/DeepSWM.

</details>


### [169] [Morphological Analysis of Semiconductor Microstructures using Skeleton Graphs](https://arxiv.org/abs/2508.07850)
*Noriko Nitta,Rei Miyata,Naoto Oishi*

Main category: cs.CV

TL;DR: 通过电子显微镜图像提取Ge表面的拓扑特征，使用图卷积网络嵌入，并通过PCA分析发现辐照角度对形态特性的影响大于辐照通量。


<details>
  <summary>Details</summary>
Motivation: 研究Ge表面在离子束辐照下形成的微观结构的形态特性，探索辐照参数（角度和通量）对其影响。

Method: 处理电子显微镜图像提取骨架图，使用图卷积网络嵌入，通过PCA分析和Davies-Bouldin指数评估聚类分离性。

Result: 辐照角度对Ge表面形态特性的影响显著大于辐照通量。

Conclusion: 辐照角度是影响Ge表面微观结构形态的主要因素。

Abstract: In this paper, electron microscopy images of microstructures formed on Ge
surfaces by ion beam irradiation were processed to extract topological features
as skeleton graphs, which were then embedded using a graph convolutional
network. The resulting embeddings were analyzed using principal component
analysis, and cluster separability in the resulting PCA space was evaluated
using the Davies-Bouldin index. The results indicate that variations in
irradiation angle have a more significant impact on the morphological
properties of Ge surfaces than variations in irradiation fluence.

</details>


### [170] [Tracking Any Point Methods for Markerless 3D Tissue Tracking in Endoscopic Stereo Images](https://arxiv.org/abs/2508.07851)
*Konrad Reuter,Suresh Guttikonda,Sarah Latus,Lennart Maack,Christian Betz,Tobias Maurer,Alexander Schlaefer*

Main category: cs.CV

TL;DR: 提出了一种基于2D TAP网络的无标记3D组织追踪方法，结合两个CoTracker模型，用于从立体内窥镜图像中估计3D运动。实验表明，该方法在复杂手术场景中具有潜力。


<details>
  <summary>Details</summary>
Motivation: 微创手术中存在动态组织运动和视野受限的挑战，准确的组织追踪可以支持手术导航、提高安全性，并实现复杂手术中的机器人辅助。

Method: 利用2D TAP网络，结合两个CoTracker模型（一个用于时间追踪，一个用于立体匹配），从立体内窥镜图像中估计3D运动。

Result: 在鸡组织模型上实现了可靠的追踪，欧氏距离误差低至1.1 mm（速度10 mm/s）。

Conclusion: 基于TAP的模型在复杂手术场景中具有准确的无标记3D追踪潜力。

Abstract: Minimally invasive surgery presents challenges such as dynamic tissue motion
and a limited field of view. Accurate tissue tracking has the potential to
support surgical guidance, improve safety by helping avoid damage to sensitive
structures, and enable context-aware robotic assistance during complex
procedures. In this work, we propose a novel method for markerless 3D tissue
tracking by leveraging 2D Tracking Any Point (TAP) networks. Our method
combines two CoTracker models, one for temporal tracking and one for stereo
matching, to estimate 3D motion from stereo endoscopic images. We evaluate the
system using a clinical laparoscopic setup and a robotic arm simulating tissue
motion, with experiments conducted on a synthetic 3D-printed phantom and a
chicken tissue phantom. Tracking on the chicken tissue phantom yielded more
reliable results, with Euclidean distance errors as low as 1.1 mm at a velocity
of 10 mm/s. These findings highlight the potential of TAP-based models for
accurate, markerless 3D tracking in challenging surgical scenarios.

</details>


### [171] [Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model](https://arxiv.org/abs/2508.07863)
*Bin Cao,Sipeng Zheng,Ye Wang,Lujie Xia,Qianshan Wei,Qin Jin,Jing Liu,Zongqing Lu*

Main category: cs.CV

TL;DR: 论文提出Being-M0.5，一种实时可控的视觉-语言-运动模型（VLMM），解决了现有模型在可控性方面的五大瓶颈，并基于HuMo100M数据集实现了多任务运动生成的先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-运动模型（VLMMs）在可控性方面存在显著限制，包括对多样化指令的响应不足、姿态初始化能力有限、长序列生成性能差、未见场景处理不足及缺乏细粒度身体部位控制。

Method: 基于HuMo100M数据集（包含500万运动序列和1亿多任务指令实例），提出了一种新型部分感知残差量化技术，用于运动标记化，实现细粒度控制。

Result: 实验验证表明，Being-M0.5在多种运动生成任务中表现优异，并具备实时能力。

Conclusion: HuMo100M数据集和Being-M0.5模型为运动生成技术的实际应用提供了重要进展，未来可加速其在实际场景中的落地。

Abstract: Human motion generation has emerged as a critical technology with
transformative potential for real-world applications. However, existing
vision-language-motion models (VLMMs) face significant limitations that hinder
their practical deployment. We identify controllability as a main bottleneck,
manifesting in five key aspects: inadequate response to diverse human commands,
limited pose initialization capabilities, poor performance on long-term
sequences, insufficient handling of unseen scenarios, and lack of fine-grained
control over individual body parts. To overcome these limitations, we present
Being-M0.5, the first real-time, controllable VLMM that achieves
state-of-the-art performance across multiple motion generation tasks. Our
approach is built upon HuMo100M, the largest and most comprehensive human
motion dataset to date, comprising over 5 million self-collected motion
sequences, 100 million multi-task instructional instances, and detailed
part-level annotations that address a critical gap in existing datasets. We
introduce a novel part-aware residual quantization technique for motion
tokenization that enables precise, granular control over individual body parts
during generation. Extensive experimental validation demonstrates Being-M0.5's
superior performance across diverse motion benchmarks, while comprehensive
efficiency analysis confirms its real-time capabilities. Our contributions
include design insights and detailed computational analysis to guide future
development of practical motion generators. We believe that HuMo100M and
Being-M0.5 represent significant advances that will accelerate the adoption of
motion generation technologies in real-world applications. The project page is
available at https://beingbeyond.github.io/Being-M0.5.

</details>


### [172] [CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning](https://arxiv.org/abs/2508.07871)
*Yanshu Li,Jianjiang Yang,Zhennan Shen,Ligong Han,Haoyan Xu,Ruixiang Tang*

Main category: cs.CV

TL;DR: 论文提出了一种针对多模态上下文学习的自适应图像令牌剪枝方法（CATP），解决了现有剪枝方法在多模态任务中的性能下降问题，显著提升了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像令牌剪枝方法主要针对单图像任务，忽视了多模态上下文学习（ICL）中的冗余问题，导致性能下降。

Method: 提出CATP方法，通过两阶段渐进式剪枝，充分考虑输入序列中的跨模态交互。

Result: 剪除77.8%的图像令牌后，CATP在四个LVLM和八个基准测试中平均性能提升0.6%，推理延迟平均减少10.78%。

Conclusion: CATP提升了多模态ICL的实用价值，为未来图像-文本交错场景的研究奠定了基础。

Abstract: Modern large vision-language models (LVLMs) convert each input image into a
large set of tokens, far outnumbering the text tokens. Although this improves
visual perception, it introduces severe image token redundancy. Because image
tokens carry sparse information, many add little to reasoning, yet greatly
increase inference cost. The emerging image token pruning methods tackle this
issue by identifying the most important tokens and discarding the rest. These
methods can raise efficiency with only modest performance loss. However, most
of them only consider single-image tasks and overlook multimodal in-context
learning (ICL), where redundancy is greater and efficiency is more critical.
Redundant tokens weaken the advantage of multimodal ICL for rapid domain
adaptation and cause unstable performance. Applying existing pruning methods in
this setting leads to large accuracy drops, exposing a clear gap and the need
for new techniques. Thus, we propose Contextually Adaptive Token Pruning
(CATP), a training-free pruning method targeted at multimodal ICL. CATP
consists of two stages that perform progressive pruning to fully account for
the complex cross-modal interactions in the input sequence. After removing
77.8\% of the image tokens, CATP produces an average performance gain of 0.6\%
over the vanilla model on four LVLMs and eight benchmarks, exceeding all
baselines remarkably. Meanwhile, it effectively improves efficiency by
achieving an average reduction of 10.78\% in inference latency. CATP enhances
the practical value of multimodal ICL and lays the groundwork for future
progress in interleaved image-text scenarios.

</details>


### [173] [Selective Contrastive Learning for Weakly Supervised Affordance Grounding](https://arxiv.org/abs/2508.07877)
*WonJun Moon,Hyun Seok Seong,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 论文提出了一种选择性原型和像素对比目标的方法，通过多视角学习提升弱监督功能部件定位的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督功能部件定位方法依赖分类器，容易关注与功能无关的类特定模式，导致定位不准确。

Method: 结合CLIP模型从第一人称和第三人称图像中发现动作相关对象，并通过跨视角对比学习精确提取功能部件线索。

Result: 实验表明，该方法能有效将激活从无关区域转移到有意义的功能线索上。

Conclusion: 通过多视角学习和对比目标，显著提升了弱监督功能部件定位的性能。

Abstract: Facilitating an entity's interaction with objects requires accurately
identifying parts that afford specific actions. Weakly supervised affordance
grounding (WSAG) seeks to imitate human learning from third-person
demonstrations, where humans intuitively grasp functional parts without needing
pixel-level annotations. To achieve this, grounding is typically learned using
a shared classifier across images from different perspectives, along with
distillation strategies incorporating part discovery process. However, since
affordance-relevant parts are not always easily distinguishable, models
primarily rely on classification, often focusing on common class-specific
patterns that are unrelated to affordance. To address this limitation, we move
beyond isolated part-level learning by introducing selective prototypical and
pixel contrastive objectives that adaptively learn affordance-relevant cues at
both the part and object levels, depending on the granularity of the available
information. Initially, we find the action-associated objects in both
egocentric (object-focused) and exocentric (third-person example) images by
leveraging CLIP. Then, by cross-referencing the discovered objects of
complementary views, we excavate the precise part-level affordance clues in
each perspective. By consistently learning to distinguish affordance-relevant
regions from affordance-irrelevant background context, our approach effectively
shifts activation from irrelevant areas toward meaningful affordance cues.
Experimental results demonstrate the effectiveness of our method. Codes are
available at github.com/hynnsk/SelectiveCL.

</details>


### [174] [TAP: Parameter-efficient Task-Aware Prompting for Adverse Weather Removal](https://arxiv.org/abs/2508.07878)
*Hanting Wang,Shengpeng Ji,Shulei Wang,Hai Huang,Xiao Jin,Qifei Zhang,Tao Jin*

Main category: cs.CV

TL;DR: 提出了一种参数高效的All-in-One图像修复框架，通过任务感知增强提示处理多种恶劣天气退化。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖专用模块或参数处理不同退化类型，导致参数冗余且忽视任务间相关性。

Method: 采用两阶段训练（预训练和提示调优），利用低秩分解和对比约束增强任务感知提示。

Result: 仅用2.75M参数，在多种修复任务中表现优异。

Conclusion: 该方法高效且性能优越，显著提升参数利用率和任务建模准确性。

Abstract: Image restoration under adverse weather conditions has been extensively
explored, leading to numerous high-performance methods. In particular, recent
advances in All-in-One approaches have shown impressive results by training on
multi-task image restoration datasets. However, most of these methods rely on
dedicated network modules or parameters for each specific degradation type,
resulting in a significant parameter overhead. Moreover, the relatedness across
different restoration tasks is often overlooked. In light of these issues, we
propose a parameter-efficient All-in-One image restoration framework that
leverages task-aware enhanced prompts to tackle various adverse weather
degradations.Specifically, we adopt a two-stage training paradigm consisting of
a pretraining phase and a prompt-tuning phase to mitigate parameter conflicts
across tasks. We first employ supervised learning to acquire general
restoration knowledge, and then adapt the model to handle specific degradation
via trainable soft prompts. Crucially, we enhance these task-specific prompts
in a task-aware manner. We apply low-rank decomposition to these prompts to
capture both task-general and task-specific characteristics, and impose
contrastive constraints to better align them with the actual inter-task
relatedness. These enhanced prompts not only improve the parameter efficiency
of the restoration model but also enable more accurate task modeling, as
evidenced by t-SNE analysis. Experimental results on different restoration
tasks demonstrate that the proposed method achieves superior performance with
only 2.75M parameters.

</details>


### [175] [NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction](https://arxiv.org/abs/2508.07897)
*Tianle Zeng,Junlei Hu,Gerardo Loza Galindo,Sharib Ali,Duygu Sarikaya,Pietro Valdastri,Dominic Jones*

Main category: cs.CV

TL;DR: 论文提出了一种动态高斯Splatting技术，用于解决手术图像数据集稀缺问题，生成高质量合成图像，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动方法需要大量高质量标注图像，限制了手术数据科学的应用。

Method: 提出动态高斯模型表示动态手术场景，结合动态训练调整策略和自动标注方法。

Result: 生成的照片级合成图像质量最高（PSNR 29.87），模型性能提升15%。

Conclusion: 该方法有效解决了数据稀缺问题，显著提升了手术自动化模型的性能。

Abstract: Computer vision-based technologies significantly enhance surgical automation
by advancing tool tracking, detection, and localization. However, Current
data-driven approaches are data-voracious, requiring large, high-quality
labeled image datasets, which limits their application in surgical data
science. Our Work introduces a novel dynamic Gaussian Splatting technique to
address the data scarcity in surgical image datasets. We propose a dynamic
Gaussian model to represent dynamic surgical scenes, enabling the rendering of
surgical instruments from unseen viewpoints and deformations with real tissue
backgrounds. We utilize a dynamic training adjustment strategy to address
challenges posed by poorly calibrated camera poses from real-world scenarios.
Additionally, we propose a method based on dynamic Gaussians for automatically
generating annotations for our synthetic data. For evaluation, we constructed a
new dataset featuring seven scenes with 14,000 frames of tool and camera motion
and tool jaw articulation, with a background of an ex-vivo porcine model. Using
this dataset, we synthetically replicate the scene deformation from the ground
truth data, allowing direct comparisons of synthetic image quality.
Experimental results illustrate that our method generates photo-realistic
labeled image datasets with the highest values in Peak-Signal-to-Noise Ratio
(29.87). We further evaluate the performance of medical-specific neural
networks trained on real and synthetic images using an unseen real-world image
dataset. Our results show that the performance of models trained on synthetic
images generated by the proposed method outperforms those trained with
state-of-the-art standard data augmentation by 10%, leading to an overall
improvement in model performances by nearly 15%.

</details>


### [176] [Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation](https://arxiv.org/abs/2508.07901)
*Bowen Xue,Qixin Yan,Wenjing Wang,Hao Liu,Chen Li*

Main category: cs.CV

TL;DR: 提出了一种轻量级、即插即用的视频生成框架Stand-In，用于身份保持，仅需少量参数和训练数据即可实现高质量视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖过多训练参数且与其他AIGC工具不兼容，需要一种更高效的身份保持方案。

Method: 在预训练视频生成模型中引入条件图像分支，通过受限自注意力与条件位置映射实现身份控制，仅需2000对数据快速学习。

Result: 仅增加约1%参数即可在视频质量和身份保持上优于全参数训练方法，并支持多种任务集成。

Conclusion: Stand-In框架高效、灵活，适用于多种视频生成任务，具有广泛的应用潜力。

Abstract: Generating high-fidelity human videos that match user-specified identities is
important yet challenging in the field of generative AI. Existing methods often
rely on an excessive number of training parameters and lack compatibility with
other AIGC tools. In this paper, we propose Stand-In, a lightweight and
plug-and-play framework for identity preservation in video generation.
Specifically, we introduce a conditional image branch into the pre-trained
video generation model. Identity control is achieved through restricted
self-attentions with conditional position mapping, and can be learned quickly
with only 2000 pairs. Despite incorporating and training just $\sim$1\%
additional parameters, our framework achieves excellent results in video
quality and identity preservation, outperforming other full-parameter training
methods. Moreover, our framework can be seamlessly integrated for other tasks,
such as subject-driven video generation, pose-referenced video generation,
stylization, and face swapping.

</details>


### [177] [CTC Transcription Alignment of the Bullinger Letters: Automatic Improvement of Annotation Quality](https://arxiv.org/abs/2508.07904)
*Marco Peer,Anna Scius-Bertrand,Andreas Fischer*

Main category: cs.CV

TL;DR: 提出了一种基于CTC对齐算法的自训练方法，用于解决历史手写文档中的标注错误（如连字符问题），提升了识别性能和对齐精度。


<details>
  <summary>Details</summary>
Motivation: 历史手写文档识别因手写变异性、文档退化及标注不足而困难，需解决标注错误问题。

Method: 采用CTC对齐算法，通过动态规划和模型输出概率匹配全文转录与文本行图像，实现自训练。

Result: 性能提升（如CER提高1.1个百分点），对齐精度增加；发现较弱模型对齐更准，支持迭代训练。

Conclusion: 方法可迭代提升CER和对齐质量，发布了修正数据集和代码。

Abstract: Handwritten text recognition for historical documents remains challenging due
to handwriting variability, degraded sources, and limited layout-aware
annotations. In this work, we address annotation errors - particularly
hyphenation issues - in the Bullinger correspondence, a large 16th-century
letter collection. We introduce a self-training method based on a CTC alignment
algorithm that matches full transcriptions to text line images using dynamic
programming and model output probabilities trained with the CTC loss. Our
approach improves performance (e.g., by 1.1 percentage points CER with PyLaia)
and increases alignment accuracy. Interestingly, we find that weaker models
yield more accurate alignments, enabling an iterative training strategy. We
release a new manually corrected subset of 100 pages from the Bullinger
dataset, along with our code and benchmarks. Our approach can be applied
iteratively to further improve the CER as well as the alignment quality for
text recognition pipelines. Code and data are available via
https://github.com/andreas-fischer-unifr/nntp.

</details>


### [178] [Generative Video Matting](https://arxiv.org/abs/2508.07905)
*Yongtao Ge,Kangyang Xie,Guangkai Xu,Mingyu Liu,Li Ke,Longtao Huang,Hui Xue,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 论文提出了一种解决视频抠图问题的新方法，通过大规模预训练和合成数据生成，结合视频扩散模型的先验知识，显著提升了模型的泛化能力和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 传统视频抠图方法因缺乏高质量真实数据而受限，现有数据集多为人工标注的不完美数据，导致模型在真实场景中泛化能力差。

Method: 提出两阶段解决方案：1) 利用合成和伪标注数据集进行大规模预训练；2) 设计一种新型视频抠图架构，利用预训练视频扩散模型的先验知识，确保时间一致性。

Result: 在三个基准数据集上的定量评估显示方法性能优越，定性结果展示了在多样真实场景中的强泛化能力。

Conclusion: 该方法通过结合大规模预训练和视频扩散模型先验，显著提升了视频抠图的泛化能力和时间一致性。

Abstract: Video matting has traditionally been limited by the lack of high-quality
ground-truth data. Most existing video matting datasets provide only
human-annotated imperfect alpha and foreground annotations, which must be
composited to background images or videos during the training stage. Thus, the
generalization capability of previous methods in real-world scenarios is
typically poor. In this work, we propose to solve the problem from two
perspectives. First, we emphasize the importance of large-scale pre-training by
pursuing diverse synthetic and pseudo-labeled segmentation datasets. We also
develop a scalable synthetic data generation pipeline that can render diverse
human bodies and fine-grained hairs, yielding around 200 video clips with a
3-second duration for fine-tuning. Second, we introduce a novel video matting
approach that can effectively leverage the rich priors from pre-trained video
diffusion models. This architecture offers two key advantages. First, strong
priors play a critical role in bridging the domain gap between synthetic and
real-world scenes. Second, unlike most existing methods that process video
matting frame-by-frame and use an independent decoder to aggregate temporal
information, our model is inherently designed for video, ensuring strong
temporal consistency. We provide a comprehensive quantitative evaluation across
three benchmark datasets, demonstrating our approach's superior performance,
and present comprehensive qualitative results in diverse real-world scenes,
illustrating the strong generalization capability of our method. The code is
available at https://github.com/aim-uofa/GVM.

</details>


### [179] [Mem4D: Decoupling Static and Dynamic Memory for Dynamic Scene Reconstruction](https://arxiv.org/abs/2508.07908)
*Xudong Cai,Shuo Wang,Peng Wang,Yongcai Wang,Zhaoxin Fan,Wanting Li,Tianbao Zhang,Jianrong Tao,Yeying Jin,Deying Li*

Main category: cs.CV

TL;DR: Mem4D提出了一种双内存架构，分别处理静态结构和动态运动，解决了单目视频动态场景重建中的内存需求困境。


<details>
  <summary>Details</summary>
Motivation: 现有方法在静态结构长期稳定性和动态运动高保真细节保留之间存在冲突，导致重建质量下降。

Method: Mem4D采用双内存架构：瞬态动态内存（TDM）捕捉高频动态细节，持久结构内存（PSM）保留长期静态信息。

Result: 实验表明，Mem4D在保持高效的同时，实现了静态结构的全局一致性和动态元素的高保真重建。

Conclusion: Mem4D通过解耦静态和动态建模，显著提升了动态场景重建的性能。

Abstract: Reconstructing dense geometry for dynamic scenes from a monocular video is a
critical yet challenging task. Recent memory-based methods enable efficient
online reconstruction, but they fundamentally suffer from a Memory Demand
Dilemma: The memory representation faces an inherent conflict between the
long-term stability required for static structures and the rapid, high-fidelity
detail retention needed for dynamic motion. This conflict forces existing
methods into a compromise, leading to either geometric drift in static
structures or blurred, inaccurate reconstructions of dynamic objects. To
address this dilemma, we propose Mem4D, a novel framework that decouples the
modeling of static geometry and dynamic motion. Guided by this insight, we
design a dual-memory architecture: 1) The Transient Dynamics Memory (TDM)
focuses on capturing high-frequency motion details from recent frames, enabling
accurate and fine-grained modeling of dynamic content; 2) The Persistent
Structure Memory (PSM) compresses and preserves long-term spatial information,
ensuring global consistency and drift-free reconstruction for static elements.
By alternating queries to these specialized memories, Mem4D simultaneously
maintains static geometry with global consistency and reconstructs dynamic
elements with high fidelity. Experiments on challenging benchmarks demonstrate
that our method achieves state-of-the-art or competitive performance while
maintaining high efficiency. Codes will be publicly available.

</details>


### [180] [RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language Model-based Question Answering](https://arxiv.org/abs/2508.07918)
*Xing Zi,Jinghao Xiao,Yunxiao Shi,Xian Tao,Jun Li,Ali Braytee,Mukesh Prasad*

Main category: cs.CV

TL;DR: 论文介绍了RSVLM-QA数据集，一个用于遥感视觉问答（VQA）的大规模、内容丰富的数据集，旨在解决现有数据集在标注丰富性、问题多样性和推理能力评估上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有遥感VQA数据集在标注丰富性、问题多样性和推理能力评估方面存在局限，需要更全面的数据集推动研究。

Method: 通过整合多个遥感分割和检测数据集，采用双轨标注生成流程：利用GPT-4.1自动生成详细标注和复杂问答对，并开发专门流程处理遥感图像中的对象计数问题。

Result: RSVLM-QA包含13,820张图像和162,373个VQA对，具有广泛的标注和多样的问题类型，能有效评估当前视觉语言模型的理解和推理能力。

Conclusion: RSVLM-QA将成为遥感VQA和VLM研究领域的重要资源，推动相关技术进步。

Abstract: Visual Question Answering (VQA) in remote sensing (RS) is pivotal for
interpreting Earth observation data. However, existing RS VQA datasets are
constrained by limitations in annotation richness, question diversity, and the
assessment of specific reasoning capabilities. This paper introduces RSVLM-QA
dataset, a new large-scale, content-rich VQA dataset for the RS domain.
RSVLM-QA is constructed by integrating data from several prominent RS
segmentation and detection datasets: WHU, LoveDA, INRIA, and iSAID. We employ
an innovative dual-track annotation generation pipeline. Firstly, we leverage
Large Language Models (LLMs), specifically GPT-4.1, with meticulously designed
prompts to automatically generate a suite of detailed annotations including
image captions, spatial relations, and semantic tags, alongside complex
caption-based VQA pairs. Secondly, to address the challenging task of object
counting in RS imagery, we have developed a specialized automated process that
extracts object counts directly from the original segmentation data; GPT-4.1
then formulates natural language answers from these counts, which are paired
with preset question templates to create counting QA pairs. RSVLM-QA comprises
13,820 images and 162,373 VQA pairs, featuring extensive annotations and
diverse question types. We provide a detailed statistical analysis of the
dataset and a comparison with existing RS VQA benchmarks, highlighting the
superior depth and breadth of RSVLM-QA's annotations. Furthermore, we conduct
benchmark experiments on Six mainstream Vision Language Models (VLMs),
demonstrating that RSVLM-QA effectively evaluates and challenges the
understanding and reasoning abilities of current VLMs in the RS domain. We
believe RSVLM-QA will serve as a pivotal resource for the RS VQA and VLM
research communities, poised to catalyze advancements in the field.

</details>


### [181] [Safeguarding Generative AI Applications in Preclinical Imaging through Hybrid Anomaly Detection](https://arxiv.org/abs/2508.07923)
*Jakub Binda,Valentina Paneta,Vasileios Eleftheriadis,Hongkyou Chung,Panagiotis Papadimitroulas,Neo Christopher Chung*

Main category: cs.CV

TL;DR: 论文提出了一种混合异常检测框架，用于增强生成式AI在核医学数据合成中的可靠性和安全性，并在两个应用中验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在核医学中潜力巨大，但高风险的生物医学影像需要可靠的异常检测机制以确保模型行为的准确性。

Method: 开发并实施了一种混合异常检测框架，应用于Pose2Xray和DosimetrEYE两个生成式AI系统。

Result: 异常检测提高了系统的可靠性，减少了人工监督需求，并支持实时质量控制。

Conclusion: 该方法增强了生成式AI在临床前环境中的工业可行性，提高了鲁棒性、可扩展性和合规性。

Abstract: Generative AI holds great potentials to automate and enhance data synthesis
in nuclear medicine. However, the high-stakes nature of biomedical imaging
necessitates robust mechanisms to detect and manage unexpected or erroneous
model behavior. We introduce development and implementation of a hybrid anomaly
detection framework to safeguard GenAI models in BIOEMTECH's eyes(TM) systems.
Two applications are demonstrated: Pose2Xray, which generates synthetic X-rays
from photographic mouse images, and DosimetrEYE, which estimates 3D radiation
dose maps from 2D SPECT/CT scans. In both cases, our outlier detection (OD)
enhances reliability, reduces manual oversight, and supports real-time quality
control. This approach strengthens the industrial viability of GenAI in
preclinical settings by increasing robustness, scalability, and regulatory
compliance.

</details>


### [182] [TAG: A Simple Yet Effective Temporal-Aware Approach for Zero-Shot Video Temporal Grounding](https://arxiv.org/abs/2508.07925)
*Jin-Seop Lee,SungJoon Lee,Jaehan Ahn,YunSeok Choi,Jee-Hyong Lee*

Main category: cs.CV

TL;DR: 论文提出了一种名为TAG的零样本视频时间定位方法，通过时间池化、时间相干性聚类和相似性调整，解决了现有方法的语义碎片化和相似性分布偏差问题，无需训练即可实现高性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本视频时间定位方法存在语义碎片化和相似性分布偏差问题，且依赖昂贵的LLMs推理。

Method: 提出TAG方法，结合时间池化、时间相干性聚类和相似性调整，无需训练即可捕捉视频时间上下文并优化相似性分布。

Result: 在Charades-STA和ActivityNet Captions基准数据集上取得了最先进的结果，且不依赖LLMs。

Conclusion: TAG是一种简单有效的零样本视频时间定位方法，解决了现有方法的局限性，性能优越。

Abstract: Video Temporal Grounding (VTG) aims to extract relevant video segments based
on a given natural language query. Recently, zero-shot VTG methods have gained
attention by leveraging pretrained vision-language models (VLMs) to localize
target moments without additional training. However, existing approaches suffer
from semantic fragmentation, where temporally continuous frames sharing the
same semantics are split across multiple segments. When segments are
fragmented, it becomes difficult to predict an accurate target moment that
aligns with the text query. Also, they rely on skewed similarity distributions
for localization, making it difficult to select the optimal segment.
Furthermore, they heavily depend on the use of LLMs which require expensive
inferences. To address these limitations, we propose a \textit{TAG}, a simple
yet effective Temporal-Aware approach for zero-shot video temporal Grounding,
which incorporates temporal pooling, temporal coherence clustering, and
similarity adjustment. Our proposed method effectively captures the temporal
context of videos and addresses distorted similarity distributions without
training. Our approach achieves state-of-the-art results on Charades-STA and
ActivityNet Captions benchmark datasets without rely on LLMs. Our code is
available at https://github.com/Nuetee/TAG

</details>


### [183] [VOIDFace: A Privacy-Preserving Multi-Network Face Recognition With Enhanced Security](https://arxiv.org/abs/2508.07960)
*Ajnas Muhammed,Iurri Medvedev,Nuno Gonçalves*

Main category: cs.CV

TL;DR: VOIDFace提出了一种新型面部识别框架，通过视觉秘密共享消除数据复制，并采用基于补丁的多训练网络，提升隐私、安全性和数据控制。


<details>
  <summary>Details</summary>
Motivation: 解决面部识别系统中数据复制导致的隐私和伦理问题，同时提升用户对个人数据的控制权。

Method: 使用视觉秘密共享技术安全存储训练数据，并设计基于补丁的多训练网络。

Result: 在VGGFace2数据集上验证，VOIDFace在保持性能的同时实现了数据控制、安全和隐私。

Conclusion: VOIDFace为面部识别系统提供了一种隐私保护、高效且用户可控的解决方案。

Abstract: Advancement of machine learning techniques, combined with the availability of
large-scale datasets, has significantly improved the accuracy and efficiency of
facial recognition. Modern facial recognition systems are trained using large
face datasets collected from diverse individuals or public repositories.
However, for training, these datasets are often replicated and stored in
multiple workstations, resulting in data replication, which complicates
database management and oversight. Currently, once a user submits their face
for dataset preparation, they lose control over how their data is used, raising
significant privacy and ethical concerns. This paper introduces VOIDFace, a
novel framework for facial recognition systems that addresses two major issues.
First, it eliminates the need of data replication and improves data control to
securely store training face data by using visual secret sharing. Second, it
proposes a patch-based multi-training network that uses this novel training
data storage mechanism to develop a robust, privacy-preserving facial
recognition system. By integrating these advancements, VOIDFace aims to improve
the privacy, security, and efficiency of facial recognition training, while
ensuring greater control over sensitive personal face data. VOIDFace also
enables users to exercise their Right-To-Be-Forgotten property to control their
personal data. Experimental evaluations on the VGGFace2 dataset show that
VOIDFace provides Right-To-Be-Forgotten, improved data control, security, and
privacy while maintaining competitive facial recognition performance. Code is
available at: https://github.com/ajnasmuhammed89/VOIDFace

</details>


### [184] [TrackOR: Towards Personalized Intelligent Operating Rooms Through Robust Tracking](https://arxiv.org/abs/2508.07968)
*Tony Danjun Wang,Christian Heiliger,Nassir Navab,Lennart Bastian*

Main category: cs.CV

TL;DR: TrackOR框架通过3D几何特征实现手术室中多人员长期跟踪与重识别，提升关联准确性11%，并支持离线轨迹恢复，为个性化智能系统提供基础。


<details>
  <summary>Details</summary>
Motivation: 为手术团队提供智能支持，改善患者预后，需解决手术室中人员长期跟踪与身份一致性问题。

Method: 使用3D几何特征进行在线跟踪与离线轨迹恢复，实现高效的多人员跟踪与重识别。

Result: TrackOR在关联准确性上优于基线11%，并能生成分析就绪的轨迹数据。

Conclusion: 通过3D几何信息，TrackOR实现了持续身份跟踪，为手术室个性化智能系统提供了关键技术支持。

Abstract: Providing intelligent support to surgical teams is a key frontier in
automated surgical scene understanding, with the long-term goal of improving
patient outcomes. Developing personalized intelligence for all staff members
requires maintaining a consistent state of who is located where for long
surgical procedures, which still poses numerous computational challenges. We
propose TrackOR, a framework for tackling long-term multi-person tracking and
re-identification in the operating room. TrackOR uses 3D geometric signatures
to achieve state-of-the-art online tracking performance (+11% Association
Accuracy over the strongest baseline), while also enabling an effective offline
recovery process to create analysis-ready trajectories. Our work shows that by
leveraging 3D geometric information, persistent identity tracking becomes
attainable, enabling a critical shift towards the more granular, staff-centric
analyses required for personalized intelligent systems in the operating room.
This new capability opens up various applications, including our proposed
temporal pathway imprints that translate raw tracking data into actionable
insights for improving team efficiency and safety and ultimately providing
personalized support.

</details>


### [185] [Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation](https://arxiv.org/abs/2508.07981)
*Fangyuan Mao,Aiming Hao,Jintao Chen,Dongxia Liu,Xiaokun Feng,Jiashu Zhu,Meiqi Wu,Chubin Chen,Jiahong Wu,Xiangxiang Chu*

Main category: cs.CV

TL;DR: Omni-Effects提出了一种统一的框架，用于生成提示引导的视觉效果和空间可控的复合效果，解决了现有方法在多效果联合训练中的干扰和空间不可控问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在视觉效果（VFX）生产中存在局限性，无法同时生成多个空间可控的复合效果，限制了应用场景。

Method: 提出了LoRA-MoE（基于LoRA的专家混合）和Spatial-Aware Prompt（SAP）两项创新技术，并结合Independent-Information Flow（IIF）模块，实现了多效果的集成和精确空间控制。

Result: 实验表明，Omni-Effects能够实现精确的空间控制和多样化的效果生成，用户可指定效果的类别和位置。

Conclusion: Omni-Effects为VFX生产提供了一种高效、可控的解决方案，推动了多效果联合生成的研究。

Abstract: Visual effects (VFX) are essential visual enhancements fundamental to modern
cinematic production. Although video generation models offer cost-efficient
solutions for VFX production, current methods are constrained by per-effect
LoRA training, which limits generation to single effects. This fundamental
limitation impedes applications that require spatially controllable composite
effects, i.e., the concurrent generation of multiple effects at designated
locations. However, integrating diverse effects into a unified framework faces
major challenges: interference from effect variations and spatial
uncontrollability during multi-VFX joint training. To tackle these challenges,
we propose Omni-Effects, a first unified framework capable of generating
prompt-guided effects and spatially controllable composite effects. The core of
our framework comprises two key innovations: (1) LoRA-based Mixture of Experts
(LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects
within a unified model while effectively mitigating cross-task interference.
(2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the
text token, enabling precise spatial control. Furthermore, we introduce an
Independent-Information Flow (IIF) module integrated within the SAP, isolating
the control signals corresponding to individual effects to prevent any unwanted
blending. To facilitate this research, we construct a comprehensive VFX dataset
Omni-VFX via a novel data collection pipeline combining image editing and
First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX
evaluation framework for validating model performance. Extensive experiments
demonstrate that Omni-Effects achieves precise spatial control and diverse
effect generation, enabling users to specify both the category and location of
desired effects.

</details>


### [186] [The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility](https://arxiv.org/abs/2508.07989)
*Xiantao Zhang*

Main category: cs.CV

TL;DR: 论文指出多模态大语言模型（MLLMs）在盲人和视障人士（BVI）辅助技术中存在‘电梯问题’，即无法感知电梯运行方向，揭示了‘隐式运动盲区’的深层限制。


<details>
  <summary>Details</summary>
Motivation: MLLMs作为辅助技术潜力巨大，但在实际应用中存在信任问题，尤其是对连续低信号运动的感知不足。

Method: 通过分析现有视频理解的帧采样范式，指出其将视频视为静态图像序列的局限性。

Result: 提出‘隐式运动盲区’概念，强调其对用户信任的影响。

Conclusion: 呼吁从语义识别转向物理感知，开发以用户需求为中心的新基准，提升动态环境中的安全性和可靠性。

Abstract: Multimodal Large Language Models (MLLMs) hold immense promise as assistive
technologies for the blind and visually impaired (BVI) community. However, we
identify a critical failure mode that undermines their trustworthiness in
real-world applications. We introduce the Escalator Problem -- the inability of
state-of-the-art models to perceive an escalator's direction of travel -- as a
canonical example of a deeper limitation we term Implicit Motion Blindness.
This blindness stems from the dominant frame-sampling paradigm in video
understanding, which, by treating videos as discrete sequences of static
images, fundamentally struggles to perceive continuous, low-signal motion. As a
position paper, our contribution is not a new model but rather to: (I) formally
articulate this blind spot, (II) analyze its implications for user trust, and
(III) issue a call to action. We advocate for a paradigm shift from purely
semantic recognition towards robust physical perception and urge the
development of new, human-centered benchmarks that prioritize safety,
reliability, and the genuine needs of users in dynamic environments.

</details>


### [187] [Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models](https://arxiv.org/abs/2508.07996)
*Thinesh Thiyakesan Ponbagavathi,Chengzheng Yang,Alina Roitberg*

Main category: cs.CV

TL;DR: ProGraD是一种通过可学习提示和轻量级Transformer改进群体活动检测的方法，在复杂多群体场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型（VFMs）在群体动态建模中未充分探索，直接替换CNN骨干网络效果有限，需要更结构化的群体感知推理。

Method: ProGraD结合可学习群体提示引导VFM注意力，并使用轻量级GroupContext Transformer推断演员-群体关联和集体行为。

Result: 在Cafe和Social-CAD基准测试中超越现有方法，尤其在多群体场景中提升显著（Group mAP@1.0提升6.5%，Group mAP@0.5提升8.2%）。

Conclusion: ProGraD不仅性能优越，还能生成可解释的注意力图，为群体推理提供洞察。

Abstract: Group Activity Detection (GAD) involves recognizing social groups and their
collective behaviors in videos. Vision Foundation Models (VFMs), like DinoV2,
offer excellent features, but are pretrained primarily on object-centric data
and remain underexplored for modeling group dynamics. While they are a
promising alternative to highly task-specific GAD architectures that require
full fine-tuning, our initial investigation reveals that simply swapping CNN
backbones used in these methods with VFMs brings little gain, underscoring the
need for structured, group-aware reasoning on top.
  We introduce Prompt-driven Group Activity Detection (ProGraD) -- a method
that bridges this gap through 1) learnable group prompts to guide the VFM
attention toward social configurations, and 2) a lightweight two-layer
GroupContext Transformer that infers actor-group associations and collective
behavior. We evaluate our approach on two recent GAD benchmarks: Cafe, which
features multiple concurrent social groups, and Social-CAD, which focuses on
single-group interactions. While we surpass state-of-the-art in both settings,
our method is especially effective in complex multi-group scenarios, where we
yield a gain of 6.5\% (Group mAP\@1.0) and 8.2\% (Group mAP\@0.5) using only
10M trainable parameters. Furthermore, our experiments reveal that ProGraD
produces interpretable attention maps, offering insights into actor-group
reasoning. Code and models will be released.

</details>


### [188] [Sample-aware RandAugment: Search-free Automatic Data Augmentation for Effective Image Recognition](https://arxiv.org/abs/2508.08004)
*Anqi Xiao,Weichen Yu,Hongyuan Yu*

Main category: cs.CV

TL;DR: SRA是一种无需搜索的自动数据增强方法，通过动态调整增强策略和启发式评分模块，显著提升了性能，并在ImageNet上达到了78.31%的Top-1准确率。


<details>
  <summary>Details</summary>
Motivation: 主流AutoDA方法存在搜索耗时或性能不足的问题，SRA旨在解决这些挑战。

Method: SRA采用启发式评分模块评估数据复杂度，并结合非对称增强策略动态调整增强策略。

Result: SRA在ImageNet上达到78.31%的Top-1准确率，并在下游任务中表现出良好的泛化能力。

Conclusion: SRA为更简单、高效且实用的AutoDA设计提供了新思路，适用于多种未来任务。

Abstract: Automatic data augmentation (AutoDA) plays an important role in enhancing the
generalization of neural networks. However, mainstream AutoDA methods often
encounter two challenges: either the search process is excessively
time-consuming, hindering practical application, or the performance is
suboptimal due to insufficient policy adaptation during training. To address
these issues, we propose Sample-aware RandAugment (SRA), an asymmetric,
search-free AutoDA method that dynamically adjusts augmentation policies while
maintaining straightforward implementation. SRA incorporates a heuristic
scoring module that evaluates the complexity of the original training data,
enabling the application of tailored augmentations for each sample.
Additionally, an asymmetric augmentation strategy is employed to maximize the
potential of this scoring module. In multiple experimental settings, SRA
narrows the performance gap between search-based and search-free AutoDA
methods, achieving a state-of-the-art Top-1 accuracy of 78.31\% on ImageNet
with ResNet-50. Notably, SRA demonstrates good compatibility with existing
augmentation pipelines and solid generalization across new tasks, without
requiring hyperparameter tuning. The pretrained models leveraging SRA also
enhance recognition in downstream object detection tasks. SRA represents a
promising step towards simpler, more effective, and practical AutoDA designs
applicable to a variety of future tasks. Our code is available at
\href{https://github.com/ainieli/Sample-awareRandAugment}{https://github.com/ainieli/Sample-awareRandAugment

</details>


### [189] [Mitigating Biases in Surgical Operating Rooms with Geometry](https://arxiv.org/abs/2508.08028)
*Tony Danjun Wang,Tobias Czempiel,Nassir Navab,Lennart Bastian*

Main category: cs.CV

TL;DR: 论文探讨了深度神经网络在手术室（OR）中因学习虚假相关性而导致的模型偏差问题，提出通过3D点云序列编码人员信息以消除外观干扰，实验证明几何方法在临床环境中表现更优。


<details>
  <summary>Details</summary>
Motivation: 手术室中标准化服装掩盖了识别特征，导致CNN模型依赖无关视觉线索（如鞋子或眼镜），影响对人员工作流程的准确建模。

Method: 通过梯度显著性分析揭示CNN模型的偏差，并提出使用3D点云序列编码人员信息，分离形状和运动模式与外观干扰。

Result: 实验显示，在临床环境中，RGB模型性能下降12%，而几何方法因捕捉更有意义的生物特征表现更优。

Conclusion: 几何表示能有效减少外观干扰，为手术室中的人员建模提供了更稳健的方法。

Abstract: Deep neural networks are prone to learning spurious correlations, exploiting
dataset-specific artifacts rather than meaningful features for prediction. In
surgical operating rooms (OR), these manifest through the standardization of
smocks and gowns that obscure robust identifying landmarks, introducing model
bias for tasks related to modeling OR personnel. Through gradient-based
saliency analysis on two public OR datasets, we reveal that CNN models succumb
to such shortcuts, fixating on incidental visual cues such as footwear beneath
surgical gowns, distinctive eyewear, or other role-specific identifiers.
Avoiding such biases is essential for the next generation of intelligent
assistance systems in the OR, which should accurately recognize personalized
workflow traits, such as surgical skill level or coordination with other staff
members. We address this problem by encoding personnel as 3D point cloud
sequences, disentangling identity-relevant shape and motion patterns from
appearance-based confounders. Our experiments demonstrate that while RGB and
geometric methods achieve comparable performance on datasets with apparent
simulation artifacts, RGB models suffer a 12% accuracy drop in realistic
clinical settings with decreased visual diversity due to standardizations. This
performance gap confirms that geometric representations capture more meaningful
biometric features, providing an avenue to developing robust methods of
modeling humans in the OR.

</details>


### [190] [TRIDE: A Text-assisted Radar-Image weather-aware fusion network for Depth Estimation](https://arxiv.org/abs/2508.08038)
*Huawei Sun,Zixu Wang,Hao Feng,Julius Ott,Lorenzo Servadei,Robert Wille*

Main category: cs.CV

TL;DR: 论文提出了一种结合雷达-相机融合和文本特征的深度估计方法TRIDE，通过天气感知融合块优化性能，显著提升了精度。


<details>
  <summary>Details</summary>
Motivation: 现有深度估计方法未考虑天气对传感器性能的影响，且未充分利用语言描述信息。

Method: 提出文本生成策略和特征融合技术，结合雷达点信息增强文本特征提取，并引入天气感知融合块动态调整雷达权重。

Result: 在KITTI和nuScenes数据集上表现优异，MAE和RMSE分别提升12.87%和9.08%。

Conclusion: TRIDE通过多模态融合和天气适应机制，显著提升了深度估计的准确性和鲁棒性。

Abstract: Depth estimation, essential for autonomous driving, seeks to interpret the 3D
environment surrounding vehicles. The development of radar sensors, known for
their cost-efficiency and robustness, has spurred interest in radar-camera
fusion-based solutions. However, existing algorithms fuse features from these
modalities without accounting for weather conditions, despite radars being
known to be more robust than cameras under adverse weather. Additionally, while
Vision-Language models have seen rapid advancement, utilizing language
descriptions alongside other modalities for depth estimation remains an open
challenge. This paper first introduces a text-generation strategy along with
feature extraction and fusion techniques that can assist monocular depth
estimation pipelines, leading to improved accuracy across different algorithms
on the KITTI dataset. Building on this, we propose TRIDE, a radar-camera fusion
algorithm that enhances text feature extraction by incorporating radar point
information. To address the impact of weather on sensor performance, we
introduce a weather-aware fusion block that adaptively adjusts radar weighting
based on current weather conditions. Our method, benchmarked on the nuScenes
dataset, demonstrates performance gains over the state-of-the-art, achieving a
12.87% improvement in MAE and a 9.08% improvement in RMSE. Code:
https://github.com/harborsarah/TRIDE

</details>


### [191] [S^2VG: 3D Stereoscopic and Spatial Video Generation via Denoising Frame Matrix](https://arxiv.org/abs/2508.08048)
*Peng Dai,Feitong Tan,Qiangeng Xu,Yihua Huang,David Futschik,Ruofei Du,Sean Fanello,Yinda Zhang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 提出了一种无需姿态估计和额外训练的方法，利用现成的单目视频生成模型生成沉浸式3D视频。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在生成高质量单目视频方面表现优异，但生成3D立体和空间视频的研究较少。

Method: 通过深度信息将单目视频映射到预定义视角，并采用帧矩阵修复框架填补缺失内容，结合双更新方案提升修复质量。

Result: 实验表明，该方法在多种生成模型（如Sora、Lumiere等）上显著优于现有方法。

Conclusion: 该方法为生成3D沉浸式视频提供了一种高效且无需额外训练的解决方案。

Abstract: While video generation models excel at producing high-quality monocular
videos, generating 3D stereoscopic and spatial videos for immersive
applications remains an underexplored challenge. We present a pose-free and
training-free method that leverages an off-the-shelf monocular video generation
model to produce immersive 3D videos. Our approach first warps the generated
monocular video into pre-defined camera viewpoints using estimated depth
information, then applies a novel \textit{frame matrix} inpainting framework.
This framework utilizes the original video generation model to synthesize
missing content across different viewpoints and timestamps, ensuring spatial
and temporal consistency without requiring additional model fine-tuning.
Moreover, we develop a \dualupdate~scheme that further improves the quality of
video inpainting by alleviating the negative effects propagated from
disoccluded areas in the latent space. The resulting multi-view videos are then
adapted into stereoscopic pairs or optimized into 4D Gaussians for spatial
video synthesis. We validate the efficacy of our proposed method by conducting
experiments on videos from various generative models, such as Sora, Lumiere,
WALT, and Zeroscope. The experiments demonstrate that our method has a
significant improvement over previous methods. Project page at:
https://daipengwa.github.io/S-2VG_ProjectPage/

</details>


### [192] [PrIINeR: Towards Prior-Informed Implicit Neural Representations for Accelerated MRI](https://arxiv.org/abs/2508.08058)
*Ziad Al-Haj Hemidi,Eytan Kats,Mattias P. Heinrich*

Main category: cs.CV

TL;DR: PrIINeR结合预训练深度学习模型的先验知识与隐式神经表示（INR），提升高加速因子下的MRI重建质量。


<details>
  <summary>Details</summary>
Motivation: 高加速因子下MRI重建易导致图像质量下降，现有INR方法因先验约束不足而表现不佳。

Method: 提出PrIINeR，将预训练模型的先验知识融入INR框架，并通过双重数据一致性优化。

Result: 在NYU fastMRI数据集上，PrIINeR优于现有INR和部分学习型方法，显著提升结构保留和去伪影效果。

Conclusion: PrIINeR为高质量加速MRI重建提供了可靠方案，结合了深度学习和INR技术的优势。

Abstract: Accelerating Magnetic Resonance Imaging (MRI) reduces scan time but often
degrades image quality. While Implicit Neural Representations (INRs) show
promise for MRI reconstruction, they struggle at high acceleration factors due
to weak prior constraints, leading to structural loss and aliasing artefacts.
To address this, we propose PrIINeR, an INR-based MRI reconstruction method
that integrates prior knowledge from pre-trained deep learning models into the
INR framework. By combining population-level knowledge with instance-based
optimization and enforcing dual data consistency, PrIINeR aligns both with the
acquired k-space data and the prior-informed reconstruction. Evaluated on the
NYU fastMRI dataset, our method not only outperforms state-of-the-art INR-based
approaches but also improves upon several learning-based state-of-the-art
methods, significantly improving structural preservation and fidelity while
effectively removing aliasing artefacts.PrIINeR bridges deep learning and
INR-based techniques, offering a more reliable solution for high-quality,
accelerated MRI reconstruction. The code is publicly available on
https://github.com/multimodallearning/PrIINeR.

</details>


### [193] [Information Bottleneck-based Causal Attention for Multi-label Medical Image Recognition](https://arxiv.org/abs/2508.08069)
*Xiaoxiao Cui,Yiran Li,Kai He,Shanzhi Jiang,Mengli Xue,Wentao Li,Junhong Leng,Zhi Liu,Lizhen Cui,Shuo Li*

Main category: cs.CV

TL;DR: 提出了一种基于信息瓶颈的因果注意力方法（IBCA），用于医学图像的多标签分类，通过过滤无关特征并增强因果注意力，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在医学图像多标签分类中难以区分因果与无关特征，导致诊断准确性不足。

Method: 使用高斯混合多标签空间注意力过滤无关信息，并通过对比增强的因果干预减少虚假注意力。

Result: 在Endo和MuReD数据集上，IBCA在多项指标上显著优于其他方法。

Conclusion: IBCA有效解决了医学图像多标签分类中的因果注意力问题，提升了诊断准确性和可解释性。

Abstract: Multi-label classification (MLC) of medical images aims to identify multiple
diseases and holds significant clinical potential. A critical step is to learn
class-specific features for accurate diagnosis and improved interpretability
effectively. However, current works focus primarily on causal attention to
learn class-specific features, yet they struggle to interpret the true cause
due to the inadvertent attention to class-irrelevant features. To address this
challenge, we propose a new structural causal model (SCM) that treats
class-specific attention as a mixture of causal, spurious, and noisy factors,
and a novel Information Bottleneck-based Causal Attention (IBCA) that is
capable of learning the discriminative class-specific attention for MLC of
medical images. Specifically, we propose learning Gaussian mixture multi-label
spatial attention to filter out class-irrelevant information and capture each
class-specific attention pattern. Then a contrastive enhancement-based causal
intervention is proposed to gradually mitigate the spurious attention and
reduce noise information by aligning multi-head attention with the Gaussian
mixture multi-label spatial. Quantitative and ablation results on Endo and
MuReD show that IBCA outperforms all methods. Compared to the second-best
results for each metric, IBCA achieves improvements of 6.35\% in CR, 7.72\% in
OR, and 5.02\% in mAP for MuReD, 1.47\% in CR, and 1.65\% in CF1, and 1.42\% in
mAP for Endo.

</details>


### [194] [ME-TST+: Micro-expression Analysis via Temporal State Transition with ROI Relationship Awareness](https://arxiv.org/abs/2508.08082)
*Zizheng Guo,Bochao Zou,Junbao Zhuo,Huimin Ma*

Main category: cs.CV

TL;DR: 该论文提出两种基于状态空间模型的架构（ME-TST和ME-TST+），用于微表情（ME）的检测与识别，通过视频级回归和多粒度ROI建模提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在微表情分析中使用固定窗口长度和硬分类，且将检测与识别视为独立任务，忽略了其内在联系。

Method: 提出ME-TST和ME-TST+架构，利用时间状态转移机制替代传统窗口分类，引入多粒度ROI建模和SlowFast Mamba框架。

Result: 实验表明，所提方法在性能上达到最先进水平。

Conclusion: 通过视频级回归和任务协同策略，显著提升了微表情分析的准确性和鲁棒性。

Abstract: Micro-expressions (MEs) are regarded as important indicators of an
individual's intrinsic emotions, preferences, and tendencies. ME analysis
requires spotting of ME intervals within long video sequences and recognition
of their corresponding emotional categories. Previous deep learning approaches
commonly employ sliding-window classification networks. However, the use of
fixed window lengths and hard classification presents notable limitations in
practice. Furthermore, these methods typically treat ME spotting and
recognition as two separate tasks, overlooking the essential relationship
between them. To address these challenges, this paper proposes two state space
model-based architectures, namely ME-TST and ME-TST+, which utilize temporal
state transition mechanisms to replace conventional window-level classification
with video-level regression. This enables a more precise characterization of
the temporal dynamics of MEs and supports the modeling of MEs with varying
durations. In ME-TST+, we further introduce multi-granularity ROI modeling and
the slowfast Mamba framework to alleviate information loss associated with
treating ME analysis as a time-series task. Additionally, we propose a synergy
strategy for spotting and recognition at both the feature and result levels,
leveraging their intrinsic connection to enhance overall analysis performance.
Extensive experiments demonstrate that the proposed methods achieve
state-of-the-art performance. The codes are available at
https://github.com/zizheng-guo/ME-TST.

</details>


### [195] [Matrix-3D: Omnidirectional Explorable 3D World Generation](https://arxiv.org/abs/2508.08086)
*Zhongqi Yang,Wenhang Ge,Yuqi Li,Jiaqi Chen,Haoyuan Li,Mengyin An,Fei Kang,Hua Xue,Baixin Xu,Yuyang Yin,Eric Li,Yang Liu,Yikai Wang,Hao-Xiang Guo,Yahui Zhou*

Main category: cs.CV

TL;DR: Matrix-3D框架通过全景表示和视频扩散模型，实现高质量、几何一致的3D世界生成，结合了两种重建方法并引入新数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成场景范围上受限，需要更广泛的3D世界生成技术。

Method: 结合全景视频扩散模型和两种3D重建方法（前馈式与优化式），并引入Matrix-Pano数据集。

Result: 在全景视频生成和3D世界生成中达到最先进性能。

Conclusion: Matrix-3D框架在广泛覆盖的3D世界生成中表现出色，为未来研究提供了新方向。

Abstract: Explorable 3D world generation from a single image or text prompt forms a
cornerstone of spatial intelligence. Recent works utilize video model to
achieve wide-scope and generalizable 3D world generation. However, existing
approaches often suffer from a limited scope in the generated scenes. In this
work, we propose Matrix-3D, a framework that utilize panoramic representation
for wide-coverage omnidirectional explorable 3D world generation that combines
conditional video generation and panoramic 3D reconstruction. We first train a
trajectory-guided panoramic video diffusion model that employs scene mesh
renders as condition, to enable high-quality and geometrically consistent scene
video generation. To lift the panorama scene video to 3D world, we propose two
separate methods: (1) a feed-forward large panorama reconstruction model for
rapid 3D scene reconstruction and (2) an optimization-based pipeline for
accurate and detailed 3D scene reconstruction. To facilitate effective
training, we also introduce the Matrix-Pano dataset, the first large-scale
synthetic collection comprising 116K high-quality static panoramic video
sequences with depth and trajectory annotations. Extensive experiments
demonstrate that our proposed framework achieves state-of-the-art performance
in panoramic video generation and 3D world generation. See more in
https://matrix-3d.github.io.

</details>


### [196] [MDD-Net: Multimodal Depression Detection through Mutual Transformer](https://arxiv.org/abs/2508.08093)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Hamdi Altaheri,Lobna Nassar,Fakhri Karray*

Main category: cs.CV

TL;DR: 提出了一种基于多模态数据的抑郁症检测网络（MDD-Net），通过声学和视觉数据提取特征，并利用互变压器融合特征，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 抑郁症严重影响个体健康，社交媒体数据的易获取性为心理健康研究提供了新途径。

Method: MDD-Net包含声学特征提取、视觉特征提取、互变压器融合特征和检测层四个模块，利用D-Vlog数据集进行实验。

Result: 实验结果显示，MDD-Net在F1分数上比现有方法提升了17.37%。

Conclusion: 提出的多模态抑郁症检测网络性能优越，为抑郁症检测提供了有效工具。

Abstract: Depression is a major mental health condition that severely impacts the
emotional and physical well-being of individuals. The simple nature of data
collection from social media platforms has attracted significant interest in
properly utilizing this information for mental health research. A Multimodal
Depression Detection Network (MDD-Net), utilizing acoustic and visual data
obtained from social media networks, is proposed in this work where mutual
transformers are exploited to efficiently extract and fuse multimodal features
for efficient depression detection. The MDD-Net consists of four core modules:
an acoustic feature extraction module for retrieving relevant acoustic
attributes, a visual feature extraction module for extracting significant
high-level patterns, a mutual transformer for computing the correlations among
the generated features and fusing these features from multiple modalities, and
a detection layer for detecting depression using the fused feature
representations. The extensive experiments are performed using the multimodal
D-Vlog dataset, and the findings reveal that the developed multimodal
depression detection network surpasses the state-of-the-art by up to 17.37% for
F1-Score, demonstrating the greater performance of the proposed system. The
source code is accessible at
https://github.com/rezwanh001/Multimodal-Depression-Detection.

</details>


### [197] [3D Plant Root Skeleton Detection and Extraction](https://arxiv.org/abs/2508.08094)
*Jiakai Lin,Jinchang Zhang,Ge Jin,Wenzhan Song,Tianming Liu,Guoyu Lu*

Main category: cs.CV

TL;DR: 提出了一种从少量图像中高效提取植物根系3D骨架的方法，解决了根系复杂结构和缺乏纹理信息的问题，验证了模型的有效性，并展示了其在自动化育种机器人中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 植物根系结构复杂且缺乏纹理信息，传统2D研究难以捕捉其完整特征，而3D信息对研究遗传性状和根系发育至关重要。

Method: 包括侧根检测与匹配、三角测量提取侧根骨架结构，以及侧根与主根整合。

Result: 提取的3D根系骨架与真实结构高度相似，验证了模型的有效性。

Conclusion: 该方法可显著提升自动化育种机器人的效率，通过精确分析3D根系结构，帮助筛选优质种子，推动现代农业发展。

Abstract: Plant roots typically exhibit a highly complex and dense architecture,
incorporating numerous slender lateral roots and branches, which significantly
hinders the precise capture and modeling of the entire root system.
Additionally, roots often lack sufficient texture and color information, making
it difficult to identify and track root traits using visual methods. Previous
research on roots has been largely confined to 2D studies; however, exploring
the 3D architecture of roots is crucial in botany. Since roots grow in real 3D
space, 3D phenotypic information is more critical for studying genetic traits
and their impact on root development. We have introduced a 3D root skeleton
extraction method that efficiently derives the 3D architecture of plant roots
from a few images. This method includes the detection and matching of lateral
roots, triangulation to extract the skeletal structure of lateral roots, and
the integration of lateral and primary roots. We developed a highly complex
root dataset and tested our method on it. The extracted 3D root skeletons
showed considerable similarity to the ground truth, validating the
effectiveness of the model. This method can play a significant role in
automated breeding robots. Through precise 3D root structure analysis, breeding
robots can better identify plant phenotypic traits, especially root structure
and growth patterns, helping practitioners select seeds with superior root
systems. This automated approach not only improves breeding efficiency but also
reduces manual intervention, making the breeding process more intelligent and
efficient, thus advancing modern agriculture.

</details>


### [198] [TBAC-UniImage: Unified Understanding and Generation by Ladder-Side Diffusion Tuning](https://arxiv.org/abs/2508.08098)
*Junzhe Xu,Yuyang Yin,Xi Chen*

Main category: cs.CV

TL;DR: TBAC-UniImage提出了一种新型多模态理解与生成统一模型，通过深度整合预训练扩散模型和多模态大语言模型（MLLM），解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型统一方法存在浅层连接或计算成本高的问题，TBAC-UniImage旨在实现更深层次的理解与生成统一。

Method: 利用MLLM多层表示作为扩散模型的生成条件，而非仅依赖最终隐藏状态，实现更细粒度的统一。

Result: TBAC-UniImage实现了更深层次、更精细的理解与生成统一。

Conclusion: 该方法为多模态理解与生成提供了一种高效且深度的统一框架。

Abstract: This paper introduces TBAC-UniImage, a novel unified model for multimodal
understanding and generation. We achieve this by deeply integrating a
pre-trained Diffusion Model, acting as a generative ladder, with a Multimodal
Large Language Model (MLLM). Previous diffusion-based unified models face two
primary limitations. One approach uses only the MLLM's final hidden state as
the generative condition. This creates a shallow connection, as the generator
is isolated from the rich, hierarchical representations within the MLLM's
intermediate layers. The other approach, pretraining a unified generative
architecture from scratch, is computationally expensive and prohibitive for
many researchers. To overcome these issues, our work explores a new paradigm.
Instead of relying on a single output, we use representations from multiple,
diverse layers of the MLLM as generative conditions for the diffusion model.
This method treats the pre-trained generator as a ladder, receiving guidance
from various depths of the MLLM's understanding process. Consequently,
TBAC-UniImage achieves a much deeper and more fine-grained unification of
understanding and generation.

</details>


### [199] [Hyperspectral Imaging](https://arxiv.org/abs/2508.08107)
*Danfeng Hong,Chenyu Li,Naoto Yokoya,Bing Zhang,Xiuping Jia,Antonio Plaza,Paolo Gamba,Jon Atli Benediktsson,Jocelyn Chanussot*

Main category: cs.CV

TL;DR: 本文概述了高光谱成像（HSI）的原理、技术、应用及未来发展方向，涵盖数据采集、分析方法及跨学科应用。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像能同时捕捉空间和光谱信息，为材料、化学和生物分析提供非侵入性手段，具有广泛的应用潜力。

Method: 介绍了HSI的物理原理、传感器架构、数据采集与校准方法，以及包括降维、分类、光谱解混和深度学习在内的分析技术。

Result: HSI在多个领域（如地球观测、精准农业、生物医学等）展现出揭示亚视觉特征的能力，但也面临硬件限制和数据复杂性等挑战。

Conclusion: 未来HSI将向实时、嵌入式系统发展，结合自监督学习和基础模型，有望成为跨学科通用平台。

Abstract: Hyperspectral imaging (HSI) is an advanced sensing modality that
simultaneously captures spatial and spectral information, enabling
non-invasive, label-free analysis of material, chemical, and biological
properties. This Primer presents a comprehensive overview of HSI, from the
underlying physical principles and sensor architectures to key steps in data
acquisition, calibration, and correction. We summarize common data structures
and highlight classical and modern analysis methods, including dimensionality
reduction, classification, spectral unmixing, and AI-driven techniques such as
deep learning. Representative applications across Earth observation, precision
agriculture, biomedicine, industrial inspection, cultural heritage, and
security are also discussed, emphasizing HSI's ability to uncover sub-visual
features for advanced monitoring, diagnostics, and decision-making. Persistent
challenges, such as hardware trade-offs, acquisition variability, and the
complexity of high-dimensional data, are examined alongside emerging solutions,
including computational imaging, physics-informed modeling, cross-modal fusion,
and self-supervised learning. Best practices for dataset sharing,
reproducibility, and metadata documentation are further highlighted to support
transparency and reuse. Looking ahead, we explore future directions toward
scalable, real-time, and embedded HSI systems, driven by sensor
miniaturization, self-supervised learning, and foundation models. As HSI
evolves into a general-purpose, cross-disciplinary platform, it holds promise
for transformative applications in science, technology, and society.

</details>


### [200] [GRASPTrack: Geometry-Reasoned Association via Segmentation and Projection for Multi-Object Tracking](https://arxiv.org/abs/2508.08117)
*Xudong Han,Pengcheng Fang,Yueying Tian,Jianhui Yu,Xiaohao Cai,Daniel Roggen,Philip Birch*

Main category: cs.CV

TL;DR: GRASPTrack是一种新型深度感知多目标跟踪框架，通过结合单目深度估计和实例分割，生成3D点云以实现几何感知，提升复杂场景下的跟踪鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决传统跟踪方法因缺乏几何感知而难以应对遮挡和深度模糊的问题。

Method: 集成单目深度估计和实例分割，生成3D点云并体素化，使用Voxel-Based 3D IoU进行空间关联，同时引入深度感知自适应噪声补偿和深度增强的观测中心动量。

Result: 在MOT17、MOT20和DanceTrack基准测试中表现优异，显著提升了复杂场景下的跟踪鲁棒性。

Conclusion: GRASPTrack通过几何感知和动态调整策略，有效解决了遮挡和深度模糊问题，提升了多目标跟踪性能。

Abstract: Multi-object tracking (MOT) in monocular videos is fundamentally challenged
by occlusions and depth ambiguity, issues that conventional
tracking-by-detection (TBD) methods struggle to resolve owing to a lack of
geometric awareness. To address these limitations, we introduce GRASPTrack, a
novel depth-aware MOT framework that integrates monocular depth estimation and
instance segmentation into a standard TBD pipeline to generate high-fidelity 3D
point clouds from 2D detections, thereby enabling explicit 3D geometric
reasoning. These 3D point clouds are then voxelized to enable a precise and
robust Voxel-Based 3D Intersection-over-Union (IoU) for spatial association. To
further enhance tracking robustness, our approach incorporates Depth-aware
Adaptive Noise Compensation, which dynamically adjusts the Kalman filter
process noise based on occlusion severity for more reliable state estimation.
Additionally, we propose a Depth-enhanced Observation-Centric Momentum, which
extends the motion direction consistency from the image plane into 3D space to
improve motion-based association cues, particularly for objects with complex
trajectories. Extensive experiments on the MOT17, MOT20, and DanceTrack
benchmarks demonstrate that our method achieves competitive performance,
significantly improving tracking robustness in complex scenes with frequent
occlusions and intricate motion patterns.

</details>


### [201] [Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control](https://arxiv.org/abs/2508.08134)
*Zeqian Long,Mingzhe Zheng,Kunyu Feng,Xinhua Zhang,Hongyu Liu,Harry Yang,Linfeng Zhang,Qifeng Chen,Yue Ma*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练和掩码的框架Follow-Your-Shape，用于精确控制物体形状编辑，同时严格保护非目标内容。通过计算轨迹差异图（TDM）和计划KV注入机制，实现了稳定且忠实的编辑。实验表明该方法在大型形状替换任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有流式图像编辑模型在大型形状变换任务中表现不佳，要么无法实现目标形状变化，要么意外修改非目标区域。

Method: 提出Follow-Your-Shape框架，利用轨迹差异图（TDM）定位可编辑区域，并通过计划KV注入机制实现稳定编辑。

Result: 实验证明该方法在编辑能力和视觉保真度上表现优越，尤其是在大型形状替换任务中。

Conclusion: Follow-Your-Shape是一种高效且可控的图像形状编辑方法，适用于复杂场景。

Abstract: While recent flow-based image editing models demonstrate general-purpose
capabilities across diverse tasks, they often struggle to specialize in
challenging scenarios -- particularly those involving large-scale shape
transformations. When performing such structural edits, these methods either
fail to achieve the intended shape change or inadvertently alter non-target
regions, resulting in degraded background quality. We propose
Follow-Your-Shape, a training-free and mask-free framework that supports
precise and controllable editing of object shapes while strictly preserving
non-target content. Motivated by the divergence between inversion and editing
trajectories, we compute a Trajectory Divergence Map (TDM) by comparing
token-wise velocity differences between the inversion and denoising paths. The
TDM enables precise localization of editable regions and guides a Scheduled KV
Injection mechanism that ensures stable and faithful editing. To facilitate a
rigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120
new images and enriched prompt pairs specifically curated for shape-aware
editing. Experiments demonstrate that our method achieves superior editability
and visual fidelity, particularly in tasks requiring large-scale shape
replacement.

</details>


### [202] [FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting](https://arxiv.org/abs/2508.08136)
*Yitong Yang,Yinglin Wang,Changshuo Wang,Huajie Wang,Shuting He*

Main category: cs.CV

TL;DR: FantasyStyle是一个基于3DGS的风格迁移框架，首次完全依赖扩散模型蒸馏，解决了多视角不一致性和VGG特征依赖问题。


<details>
  <summary>Details</summary>
Motivation: 当前3DGS风格迁移方法存在多视角不一致导致风格冲突，以及VGG特征难以分离风格和内容的问题。

Method: 提出多视角频率一致性和可控风格蒸馏，通过3D滤波和负引导优化3D高斯。

Result: 实验表明，该方法在多种场景和风格下优于现有技术，实现了更高的风格化质量和视觉真实感。

Conclusion: FantasyStyle通过创新设计有效解决了3D风格迁移的关键挑战，性能显著提升。

Abstract: The success of 3DGS in generative and editing applications has sparked
growing interest in 3DGS-based style transfer. However, current methods still
face two major challenges: (1) multi-view inconsistency often leads to style
conflicts, resulting in appearance smoothing and distortion; and (2) heavy
reliance on VGG features, which struggle to disentangle style and content from
style images, often causing content leakage and excessive stylization. To
tackle these issues, we introduce \textbf{FantasyStyle}, a 3DGS-based style
transfer framework, and the first to rely entirely on diffusion model
distillation. It comprises two key components: (1) \textbf{Multi-View Frequency
Consistency}. We enhance cross-view consistency by applying a 3D filter to
multi-view noisy latent, selectively reducing low-frequency components to
mitigate stylized prior conflicts. (2) \textbf{Controllable Stylized
Distillation}. To suppress content leakage from style images, we introduce
negative guidance to exclude undesired content. In addition, we identify the
limitations of Score Distillation Sampling and Delta Denoising Score in 3D
style transfer and remove the reconstruction term accordingly. Building on
these insights, we propose a controllable stylized distillation that leverages
negative guidance to more effectively optimize the 3D Gaussians. Extensive
experiments demonstrate that our method consistently outperforms
state-of-the-art approaches, achieving higher stylization quality and visual
realism across various scenes and styles.

</details>


### [203] [Pindrop it! Audio and Visual Deepfake Countermeasures for Robust Detection and Fine Grained-Localization](https://arxiv.org/abs/2508.08141)
*Nicholas Klein,Hemlata Tak,James Fullwood,Krishna Regmi,Leonidas Spinoulas,Ganesh Sivaraman,Tianxiang Chen,Elie Khoury*

Main category: cs.CV

TL;DR: 本文提出了针对深度伪造视频分类和定位的解决方案，并在ACM 1M Deepfakes Detection Challenge中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 随着视觉和音频生成技术的快速发展，检测合成内容的需求日益迫切，尤其是针对局部细微修改的挑战。

Method: 提出了用于深度伪造视频分类和定位的方法，并提交至ACM 1M Deepfakes Detection Challenge。

Result: 在时间定位任务中表现最佳，在分类任务的TestA部分排名前四。

Conclusion: 该解决方案在深度伪造检测领域表现出色，尤其在处理局部细微修改时具有优势。

Abstract: The field of visual and audio generation is burgeoning with new
state-of-the-art methods. This rapid proliferation of new techniques
underscores the need for robust solutions for detecting synthetic content in
videos. In particular, when fine-grained alterations via localized
manipulations are performed in visual, audio, or both domains, these subtle
modifications add challenges to the detection algorithms. This paper presents
solutions for the problems of deepfake video classification and localization.
The methods were submitted to the ACM 1M Deepfakes Detection Challenge,
achieving the best performance in the temporal localization task and a top four
ranking in the classification task for the TestA split of the evaluation
dataset.

</details>


### [204] [Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning](https://arxiv.org/abs/2508.08165)
*Yan Wang,Da-Wei Zhou,Han-Jia Ye*

Main category: cs.CV

TL;DR: 论文提出了一种结合任务特定和通用适配器（TUNA）的方法，用于解决类增量学习中的模块选择和共享知识问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练模型的类增量学习方法通常冻结预训练网络并使用轻量级模块（如适配器）适应增量任务，但模块选择错误和忽视共享知识会导致性能下降。

Method: 训练任务特定适配器捕捉关键特征，引入基于熵的选择机制，并通过适配器融合构建通用适配器，结合两者预测以利用专业和通用知识。

Result: 在多个基准数据集上的实验表明，该方法达到了最先进的性能。

Conclusion: TUNA方法通过结合任务特定和通用适配器，有效解决了类增量学习中的挑战。

Abstract: Class-Incremental Learning (CIL) requires a learning system to continually
learn new classes without forgetting. Existing pre-trained model-based CIL
methods often freeze the pre-trained network and adapt to incremental tasks
using additional lightweight modules such as adapters. However, incorrect
module selection during inference hurts performance, and task-specific modules
often overlook shared general knowledge, leading to errors on distinguishing
between similar classes across tasks. To address the aforementioned challenges,
we propose integrating Task-Specific and Universal Adapters (TUNA) in this
paper. Specifically, we train task-specific adapters to capture the most
crucial features relevant to their respective tasks and introduce an
entropy-based selection mechanism to choose the most suitable adapter.
Furthermore, we leverage an adapter fusion strategy to construct a universal
adapter, which encodes the most discriminative features shared across tasks. We
combine task-specific and universal adapter predictions to harness both
specialized and general knowledge during inference. Extensive experiments on
various benchmark datasets demonstrate the state-of-the-art performance of our
approach. Code is available at: https://github.com/LAMDA-CL/ICCV2025-TUNA

</details>


### [205] [ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction](https://arxiv.org/abs/2508.08170)
*Chaojun Ni,Guosheng Zhao,Xiaofeng Wang,Zheng Zhu,Wenkang Qin,Xinze Chen,Guanghong Jia,Guan Huang,Wenjun Mei*

Main category: cs.CV

TL;DR: ReconDreamer-RL框架通过结合视频扩散先验和动力学模型，缩小自动驾驶训练的模拟与现实差距，并引入动态对手代理和轨迹生成器以覆盖更多极端场景，显著提升训练效果。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶训练模拟环境与现实差距大，且难以生成高质量传感器数据用于新轨迹或极端场景。

Method: 提出ReconDreamer-RL框架，结合视频扩散先验和动力学模型构建ReconSimulator，并引入动态对手代理（DAA）和轨迹生成器（CTG）生成极端场景。

Result: 实验显示，ReconDreamer-RL在端到端自动驾驶训练中优于模仿学习方法，碰撞率降低5倍。

Conclusion: ReconDreamer-RL有效缩小模拟与现实差距，提升自动驾驶训练效果，尤其在极端场景处理上表现突出。

Abstract: Reinforcement learning for training end-to-end autonomous driving models in
closed-loop simulations is gaining growing attention. However, most simulation
environments differ significantly from real-world conditions, creating a
substantial simulation-to-reality (sim2real) gap. To bridge this gap, some
approaches utilize scene reconstruction techniques to create photorealistic
environments as a simulator. While this improves realistic sensor simulation,
these methods are inherently constrained by the distribution of the training
data, making it difficult to render high-quality sensor data for novel
trajectories or corner case scenarios. Therefore, we propose ReconDreamer-RL, a
framework designed to integrate video diffusion priors into scene
reconstruction to aid reinforcement learning, thereby enhancing end-to-end
autonomous driving training. Specifically, in ReconDreamer-RL, we introduce
ReconSimulator, which combines the video diffusion prior for appearance
modeling and incorporates a kinematic model for physical modeling, thereby
reconstructing driving scenarios from real-world data. This narrows the
sim2real gap for closed-loop evaluation and reinforcement learning. To cover
more corner-case scenarios, we introduce the Dynamic Adversary Agent (DAA),
which adjusts the trajectories of surrounding vehicles relative to the ego
vehicle, autonomously generating corner-case traffic scenarios (e.g., cut-in).
Finally, the Cousin Trajectory Generator (CTG) is proposed to address the issue
of training data distribution, which is often biased toward simple
straight-line movements. Experiments show that ReconDreamer-RL improves
end-to-end autonomous driving training, outperforming imitation learning
methods with a 5x reduction in the Collision Ratio.

</details>


### [206] [CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data](https://arxiv.org/abs/2508.08173)
*Chongke Bi,Xin Gao,Jiangkang Deng,Guan*

Main category: cs.CV

TL;DR: CD-TVD是一种结合对比学习和改进扩散模型的框架，用于从有限的高分辨率数据中实现3D超分辨率，减少对大规模数据集的依赖。


<details>
  <summary>Details</summary>
Motivation: 大规模科学模拟需要高成本生成高分辨率数据，现有超分辨率方法依赖大量训练数据，限制了其适用性。

Method: 结合对比学习编码器和改进的扩散模型，利用历史数据预训练，并通过局部注意力机制微调。

Result: 在流体和大气模拟数据集上验证了CD-TVD的准确性和资源效率。

Conclusion: CD-TVD显著提升了大规模科学模拟中的数据增强能力。

Abstract: Large-scale scientific simulations require significant resources to generate
high-resolution time-varying data (TVD). While super-resolution is an efficient
post-processing strategy to reduce costs, existing methods rely on a large
amount of HR training data, limiting their applicability to diverse simulation
scenarios. To address this constraint, we proposed CD-TVD, a novel framework
that combines contrastive learning and an improved diffusion-based
super-resolution model to achieve accurate 3D super-resolution from limited
time-step high-resolution data. During pre-training on historical simulation
data, the contrastive encoder and diffusion superresolution modules learn
degradation patterns and detailed features of high-resolution and
low-resolution samples. In the training phase, the improved diffusion model
with a local attention mechanism is fine-tuned using only one newly generated
high-resolution timestep, leveraging the degradation knowledge learned by the
encoder. This design minimizes the reliance on large-scale high-resolution
datasets while maintaining the capability to recover fine-grained details.
Experimental results on fluid and atmospheric simulation datasets confirm that
CD-TVD delivers accurate and resource-efficient 3D super-resolution, marking a
significant advancement in data augmentation for large-scale scientific
simulations. The code is available at
https://github.com/Xin-Gao-private/CD-TVD.

</details>


### [207] [MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision](https://arxiv.org/abs/2508.08177)
*Zhonghao Yan,Muxi Diao,Yuxuan Yang,Jiayuan Xu,Kaizhou Zhang,Ruoyan Jing,Lele Yang,Yanxi Liu,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

TL;DR: 论文提出了一种新的医学视觉语言任务UMRG，并发布了数据集U-MRG-14K，同时提出了MedReasoner框架，通过强化学习优化推理与分割，实现了高性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前医学图像中的区域定位方法依赖于显式空间提示，无法处理临床实践中的隐式查询，因此需要一种结合临床推理和像素级定位的新方法。

Method: 定义了UMRG任务，发布了U-MRG-14K数据集，并提出了MedReasoner框架，通过强化学习优化推理模块，同时利用冻结的分割专家生成掩码。

Result: MedReasoner在U-MRG-14K上实现了最先进的性能，并对未见过的临床查询表现出强泛化能力。

Conclusion: 强化学习在可解释的医学定位中具有显著潜力，MedReasoner框架为临床实践提供了高效解决方案。

Abstract: Accurately grounding regions of interest (ROIs) is critical for diagnosis and
treatment planning in medical imaging. While multimodal large language models
(MLLMs) combine visual perception with natural language, current
medical-grounding pipelines still rely on supervised fine-tuning with explicit
spatial hints, making them ill-equipped to handle the implicit queries common
in clinical practice. This work makes three core contributions. We first define
Unified Medical Reasoning Grounding (UMRG), a novel vision-language task that
demands clinical reasoning and pixel-level grounding. Second, we release
U-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside
implicit clinical queries and reasoning traces, spanning 10 modalities, 15
super-categories, and 108 specific categories. Finally, we introduce
MedReasoner, a modular framework that distinctly separates reasoning from
segmentation: an MLLM reasoner is optimized with reinforcement learning, while
a frozen segmentation expert converts spatial prompts into masks, with
alignment achieved through format and accuracy rewards. MedReasoner achieves
state-of-the-art performance on U-MRG-14K and demonstrates strong
generalization to unseen clinical queries, underscoring the significant promise
of reinforcement learning for interpretable medical grounding.

</details>


### [208] [3D Human Mesh Estimation from Single View RGBD](https://arxiv.org/abs/2508.08178)
*Ozhan Suat,Bedirhan Uguz,Batuhan Karagoz,Muhammed Can Keles,Emre Akbas*

Main category: cs.CV

TL;DR: 提出了一种利用RGBD相机数据进行3D人体网格估计的方法M$^3$，通过虚拟投影和掩码自编码器解决数据稀缺问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: RGBD相机虽普及但深度数据未被充分利用，现有数据集小且多样性不足，需解决数据稀缺问题。

Method: 利用MoCap数据集生成虚拟单视角深度数据，训练掩码自编码器补全网格，推理时匹配深度数据到模板网格。

Result: 在SURREAL、CAPE和BEHAVE数据集上表现优异，PVE误差分别为16.8mm、22.0mm和70.9mm。

Conclusion: M$^3$方法有效利用深度数据，显著提升3D人体网格估计精度，代码将开源。

Abstract: Despite significant progress in 3D human mesh estimation from RGB images;
RGBD cameras, offering additional depth data, remain underutilized. In this
paper, we present a method for accurate 3D human mesh estimation from a single
RGBD view, leveraging the affordability and widespread adoption of RGBD cameras
for real-world applications. A fully supervised approach for this problem,
requires a dataset with RGBD image and 3D mesh label pairs. However, collecting
such a dataset is costly and challenging, hence, existing datasets are small,
and limited in pose and shape diversity. To overcome this data scarcity, we
leverage existing Motion Capture (MoCap) datasets. We first obtain complete 3D
meshes from the body models found in MoCap datasets, and create partial,
single-view versions of them by projection to a virtual camera. This simulates
the depth data provided by an RGBD camera from a single viewpoint. Then, we
train a masked autoencoder to complete the partial, single-view mesh. During
inference, our method, which we name as M$^3$ for ``Masked Mesh Modeling'',
matches the depth values coming from the sensor to vertices of a template human
mesh, which creates a partial, single-view mesh. We effectively recover parts
of the 3D human body mesh model that are not visible, resulting in a full body
mesh. M$^3$ achieves 16.8 mm and 22.0 mm per-vertex-error (PVE) on the SURREAL
and CAPE datasets, respectively; outperforming existing methods that use
full-body point clouds as input. We obtain a competitive 70.9 PVE on the BEHAVE
dataset, outperforming a recently published RGB based method by 18.4 mm,
highlighting the usefulness of depth data. Code will be released.

</details>


### [209] [PP-Motion: Physical-Perceptual Fidelity Evaluation for Human Motion Generation](https://arxiv.org/abs/2508.08179)
*Sihan Zhao,Zixuan Wang,Tianyu Luan,Jia Jia,Wentao Zhu,Jiebo Luo,Junsong Yuan,Nan Xi*

Main category: cs.CV

TL;DR: 论文提出了一种物理标注方法和PP-Motion指标，用于评估人体运动的物理和感知保真度，解决了现有方法在物理可行性与人类感知之间的差距问题。


<details>
  <summary>Details</summary>
Motivation: 人体运动生成在多个领域有广泛应用，但评估其保真度是一个复杂任务。现有方法在物理可行性与人类感知之间存在差距，且人类感知标注主观且粗糙。

Method: 提出物理标注方法，计算运动对齐物理定律所需的最小修改，生成细粒度连续标注。基于此提出PP-Motion指标，结合皮尔逊相关损失和人类感知损失。

Result: 实验表明PP-Motion不仅符合物理定律，且在运动保真度的人类感知对齐上优于先前工作。

Conclusion: PP-Motion为运动保真度评估提供了更全面、客观的解决方案，兼顾物理可行性和人类感知。

Abstract: Human motion generation has found widespread applications in AR/VR, film,
sports, and medical rehabilitation, offering a cost-effective alternative to
traditional motion capture systems. However, evaluating the fidelity of such
generated motions is a crucial, multifaceted task. Although previous approaches
have attempted at motion fidelity evaluation using human perception or physical
constraints, there remains an inherent gap between human-perceived fidelity and
physical feasibility. Moreover, the subjective and coarse binary labeling of
human perception further undermines the development of a robust data-driven
metric. We address these issues by introducing a physical labeling method. This
method evaluates motion fidelity by calculating the minimum modifications
needed for a motion to align with physical laws. With this approach, we are
able to produce fine-grained, continuous physical alignment annotations that
serve as objective ground truth. With these annotations, we propose PP-Motion,
a novel data-driven metric to evaluate both physical and perceptual fidelity of
human motion. To effectively capture underlying physical priors, we employ
Pearson's correlation loss for the training of our metric. Additionally, by
incorporating a human-based perceptual fidelity loss, our metric can capture
fidelity that simultaneously considers both human perception and physical
alignment. Experimental results demonstrate that our metric, PP-Motion, not
only aligns with physical laws but also aligns better with human perception of
motion fidelity than previous work.

</details>


### [210] [THAT: Token-wise High-frequency Augmentation Transformer for Hyperspectral Pansharpening](https://arxiv.org/abs/2508.08183)
*Hongkun Jin,Hongcheng Jiang,Zejun Zhang,Yuan Zhang,Jia Fu,Tingfeng Li,Kai Luo*

Main category: cs.CV

TL;DR: 论文提出了一种名为THAT的Transformer框架，通过增强高频特征表示和优化令牌选择，解决了现有方法在保留高频细节和减少冗余令牌方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer方法在超光谱图像融合中存在高频细节丢失和令牌冗余问题，影响了重建质量。

Method: THAT框架引入了Pivotal Token Selective Attention（PTSA）和Multi-level Variance-aware Feed-forward Network（MVFN），分别用于优化令牌选择和增强高频细节学习。

Result: 实验表明，THAT在标准基准测试中实现了最先进的性能，提升了重建质量和效率。

Conclusion: THAT通过改进高频特征表示和令牌选择，显著提升了超光谱图像融合的效果。

Abstract: Transformer-based methods have demonstrated strong potential in hyperspectral
pansharpening by modeling long-range dependencies. However, their effectiveness
is often limited by redundant token representations and a lack of multi-scale
feature modeling. Hyperspectral images exhibit intrinsic spectral priors (e.g.,
abundance sparsity) and spatial priors (e.g., non-local similarity), which are
critical for accurate reconstruction. From a spectral-spatial perspective,
Vision Transformers (ViTs) face two major limitations: they struggle to
preserve high-frequency components--such as material edges and texture
transitions--and suffer from attention dispersion across redundant tokens.
These issues stem from the global self-attention mechanism, which tends to
dilute high-frequency signals and overlook localized details. To address these
challenges, we propose the Token-wise High-frequency Augmentation Transformer
(THAT), a novel framework designed to enhance hyperspectral pansharpening
through improved high-frequency feature representation and token selection.
Specifically, THAT introduces: (1) Pivotal Token Selective Attention (PTSA) to
prioritize informative tokens and suppress redundancy; (2) a Multi-level
Variance-aware Feed-forward Network (MVFN) to enhance high-frequency detail
learning. Experiments on standard benchmarks show that THAT achieves
state-of-the-art performance with improved reconstruction quality and
efficiency. The source code is available at https://github.com/kailuo93/THAT.

</details>


### [211] [KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning](https://arxiv.org/abs/2508.08186)
*Md Meftahul Ferdaus,Mahdi Abdelguerfi,Elias Ioup,Steven Sloan,Kendall N. Niles,Ken Pathak*

Main category: cs.CV

TL;DR: KARMA是一种高效的语义分割框架，通过一维函数组合建模复杂缺陷模式，显著减少参数数量，适合实时基础设施检查。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习方法参数过多、不适用于实时检查系统的问题。

Method: 提出KARMA框架，包含TiKAN模块、优化的特征金字塔结构和静态-动态原型机制。

Result: KARMA在参数减少97%的情况下，性能优于或接近现有方法，推理速度适合实时部署。

Conclusion: KARMA为基础设施自动化检查提供了高效且实用的解决方案。

Abstract: Semantic segmentation of structural defects in civil infrastructure remains
challenging due to variable defect appearances, harsh imaging conditions, and
significant class imbalance. Current deep learning methods, despite their
effectiveness, typically require millions of parameters, rendering them
impractical for real-time inspection systems. We introduce KARMA
(Kolmogorov-Arnold Representation Mapping Architecture), a highly efficient
semantic segmentation framework that models complex defect patterns through
compositions of one-dimensional functions rather than conventional
convolutions. KARMA features three technical innovations: (1) a
parameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraging
low-rank factorization for KAN-based feature transformation; (2) an optimized
feature pyramid structure with separable convolutions for multi-scale defect
analysis; and (3) a static-dynamic prototype mechanism that enhances feature
representation for imbalanced classes. Extensive experiments on benchmark
infrastructure inspection datasets demonstrate that KARMA achieves competitive
or superior mean IoU performance compared to state-of-the-art approaches, while
using significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction).
Operating at 0.264 GFLOPS, KARMA maintains inference speeds suitable for
real-time deployment, enabling practical automated infrastructure inspection
systems without compromising accuracy. The source code can be accessed at the
following URL: https://github.com/faeyelab/karma.

</details>


### [212] [Reinforcement Learning in Vision: A Survey](https://arxiv.org/abs/2508.08189)
*Weijia Wu,Chen Gao,Joya Chen,Kevin Qinghong Lin,Qingwei Meng,Yiming Zhang,Yuke Qiu,Hong Zhou,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 本文综述了视觉强化学习（RL）的最新进展，总结了200多篇代表性工作，分为四大主题支柱，并探讨了算法设计、奖励工程和评估协议等内容。


<details>
  <summary>Details</summary>
Motivation: 为研究者和从业者提供视觉RL领域的全面概览，并指出未来研究方向。

Method: 通过形式化视觉RL问题，梳理策略优化策略的演变，并将代表性工作分类为四大支柱进行分析。

Result: 总结了当前趋势（如课程驱动训练、偏好对齐扩散等）和评估协议，并识别了样本效率、泛化和安全部署等开放挑战。

Conclusion: 本文为视觉RL领域提供了清晰的研究地图，并强调了未来研究的潜在方向。

Abstract: Recent advances at the intersection of reinforcement learning (RL) and visual
intelligence have enabled agents that not only perceive complex visual scenes
but also reason, generate, and act within them. This survey offers a critical
and up-to-date synthesis of the field. We first formalize visual RL problems
and trace the evolution of policy-optimization strategies from RLHF to
verifiable reward paradigms, and from Proximal Policy Optimization to Group
Relative Policy Optimization. We then organize more than 200 representative
works into four thematic pillars: multi-modal large language models, visual
generation, unified model frameworks, and vision-language-action models. For
each pillar we examine algorithmic design, reward engineering, benchmark
progress, and we distill trends such as curriculum-driven training,
preference-aligned diffusion, and unified reward modeling. Finally, we review
evaluation protocols spanning set-level fidelity, sample-level preference, and
state-level stability, and we identify open challenges that include sample
efficiency, generalization, and safe deployment. Our goal is to provide
researchers and practitioners with a coherent map of the rapidly expanding
landscape of visual RL and to highlight promising directions for future
inquiry. Resources are available at:
https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.

</details>


### [213] [Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model](https://arxiv.org/abs/2508.08199)
*Peiqi He,Zhenhao Zhang,Yixiang Zhang,Xiongjun Zhao,Shaoliang Peng*

Main category: cs.CV

TL;DR: Spatial-ORMLLM是一种基于RGB模态的大型视觉语言模型，用于手术室中的3D空间推理，无需额外传感器或专家标注。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多模态数据学习空间关系，但手术室通常缺乏多传感器数据，且仅用2D数据无法捕捉复杂场景的细节。

Method: 提出Spatial-ORMLLM，通过Spatial-Enhanced Feature Fusion Block将2D输入与3D空间知识结合，采用端到端框架实现3D场景推理。

Result: 在多个临床数据集上表现优异，泛化能力强。

Conclusion: Spatial-ORMLLM为手术室3D空间建模提供了高效解决方案，支持下游医疗任务。

Abstract: Precise spatial modeling in the operating room (OR) is foundational to many
clinical tasks, supporting intraoperative awareness, hazard avoidance, and
surgical decision-making. While existing approaches leverage large-scale
multimodal datasets for latent-space alignment to implicitly learn spatial
relationships, they overlook the 3D capabilities of MLLMs. However, this
approach raises two issues: (1) Operating rooms typically lack multiple video
and audio sensors, making multimodal 3D data difficult to obtain; (2) Training
solely on readily available 2D data fails to capture fine-grained details in
complex scenes. To address this gap, we introduce Spatial-ORMLLM, the first
large vision-language model for 3D spatial reasoning in operating rooms using
only RGB modality to infer volumetric and semantic cues, enabling downstream
medical tasks with detailed and holistic spatial context. Spatial-ORMLLM
incorporates a Spatial-Enhanced Feature Fusion Block, which integrates 2D
modality inputs with rich 3D spatial knowledge extracted by the estimation
algorithm and then feeds the combined features into the visual tower. By
employing a unified end-to-end MLLM framework, it combines powerful spatial
features with textual features to deliver robust 3D scene reasoning without any
additional expert annotations or sensor inputs. Experiments on multiple
benchmark clinical datasets demonstrate that Spatial-ORMLLM achieves
state-of-the-art performance and generalizes robustly to previously unseen
surgical scenarios and downstream tasks.

</details>


### [214] [SAGOnline: Segment Any Gaussians Online](https://arxiv.org/abs/2508.08219)
*Wentao Sun,Quanyun Wu,Hanqing Xu,Kyle Gao,Zhengsen Xu,Yiping Chen,Dedong Zhang,Lingfei Ma,John S. Zelek,Jonathan Li*

Main category: cs.CV

TL;DR: SAGOnline是一个轻量级、零样本的实时3D分割框架，通过解耦策略和GPU加速算法解决了3D高斯场景中的分割与跟踪问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法在3D高斯场景中存在计算成本高、空间推理能力有限以及无法同时跟踪多对象的问题。

Method: 结合视频基础模型（如SAM2）进行视图一致的2D掩码传播，并使用GPU加速的3D掩码生成和高斯级实例标记算法。

Result: 在NVOS和Spin-NeRF基准测试中表现优异（92.7%和95.2% mIoU），推理速度提升15-1500倍（27毫秒/帧）。

Conclusion: SAGOnline为实时渲染和3D场景理解提供了高效解决方案，适用于AR/VR和机器人应用。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit
3D scene representation, yet achieving efficient and consistent 3D segmentation
remains challenging. Current methods suffer from prohibitive computational
costs, limited 3D spatial reasoning, and an inability to track multiple objects
simultaneously. We present Segment Any Gaussians Online (SAGOnline), a
lightweight and zero-shot framework for real-time 3D segmentation in Gaussian
scenes that addresses these limitations through two key innovations: (1) a
decoupled strategy that integrates video foundation models (e.g., SAM2) for
view-consistent 2D mask propagation across synthesized views; and (2) a
GPU-accelerated 3D mask generation and Gaussian-level instance labeling
algorithm that assigns unique identifiers to 3D primitives, enabling lossless
multi-object tracking and segmentation across views. SAGOnline achieves
state-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU)
benchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 times
in inference speed (27 ms/frame). Qualitative results demonstrate robust
multi-object segmentation and tracking in complex scenes. Our contributions
include: (i) a lightweight and zero-shot framework for 3D segmentation in
Gaussian scenes, (ii) explicit labeling of Gaussian primitives enabling
simultaneous segmentation and tracking, and (iii) the effective adaptation of
2D video foundation models to the 3D domain. This work allows real-time
rendering and 3D scene understanding, paving the way for practical AR/VR and
robotic applications.

</details>


### [215] [Learning User Preferences for Image Generation Model](https://arxiv.org/abs/2508.08220)
*Wenyi Mo,Ying Ba,Tianyu Zhang,Yalong Bai,Biye Li*

Main category: cs.CV

TL;DR: 提出了一种基于多模态大语言模型的方法，通过对比偏好损失和偏好标记学习个性化用户偏好，显著提升了偏好预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖通用偏好或静态用户画像，忽视了用户偏好的动态性和多样性，需更精准的个性化预测。

Method: 采用多模态大语言模型，引入对比偏好损失和可学习的偏好标记，区分用户喜好并捕捉群体兴趣表征。

Result: 实验表明，该方法在偏好预测准确性上优于其他方法，能有效识别相似审美倾向的用户。

Conclusion: 该方法为生成符合个人口味的图像提供了更精确的指导，显著提升了用户偏好预测的效果。

Abstract: User preference prediction requires a comprehensive and accurate
understanding of individual tastes. This includes both surface-level
attributes, such as color and style, and deeper content-related aspects, such
as themes and composition. However, existing methods typically rely on general
human preferences or assume static user profiles, often neglecting individual
variability and the dynamic, multifaceted nature of personal taste. To address
these limitations, we propose an approach built upon Multimodal Large Language
Models, introducing contrastive preference loss and preference tokens to learn
personalized user preferences from historical interactions. The contrastive
preference loss is designed to effectively distinguish between user ''likes''
and ''dislikes'', while the learnable preference tokens capture shared interest
representations among existing users, enabling the model to activate
group-specific preferences and enhance consistency across similar users.
Extensive experiments demonstrate our model outperforms other methods in
preference prediction accuracy, effectively identifying users with similar
aesthetic inclinations and providing more precise guidance for generating
images that align with individual tastes. The project page is
\texttt{https://learn-user-pref.github.io/}.

</details>


### [216] [OMGSR: You Only Need One Mid-timestep Guidance for Real-World Image Super-Resolution](https://arxiv.org/abs/2508.08227)
*Zhiqiang Wu,Zhaomang Sun,Tong Zhou,Bingtao Fu,Ji Cong,Yitong Dong,Huaqi Zhang,Xuan Tang,Mingsong Chen,Xian Wei*

Main category: cs.CV

TL;DR: OMGSR提出了一种基于DDPM/FM生成模型的一步式真实图像超分辨率框架，通过在中时间步注入低质量图像潜在分布，并引入潜在分布细化损失和重叠分块损失，显著提升了生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有一步式真实图像超分辨率模型在初始时间步注入低质量图像潜在分布时存在与高斯噪声潜在分布的根本差距，限制了生成先验的有效利用。

Method: OMGSR在中时间步注入低质量图像潜在分布，引入潜在分布细化损失和重叠分块损失，并在DDPM/FM框架下实例化为OMGSR-S和OMGSR-F两种变体。

Result: OMGSR-S/F在512分辨率下表现优异，OMGSR-F在所有参考指标中占据绝对优势，1k和2k分辨率下也生成高质量图像。

Conclusion: OMGSR通过优化潜在分布对齐和损失设计，显著提升了一步式真实图像超分辨率的性能，适用于多种生成模型。

Abstract: Denoising Diffusion Probabilistic Models (DDPM) and Flow Matching (FM)
generative models show promising potential for one-step Real-World Image
Super-Resolution (Real-ISR). Recent one-step Real-ISR models typically inject a
Low-Quality (LQ) image latent distribution at the initial timestep. However, a
fundamental gap exists between the LQ image latent distribution and the
Gaussian noisy latent distribution, limiting the effective utilization of
generative priors. We observe that the noisy latent distribution at DDPM/FM
mid-timesteps aligns more closely with the LQ image latent distribution. Based
on this insight, we present One Mid-timestep Guidance Real-ISR (OMGSR), a
universal framework applicable to DDPM/FM-based generative models. OMGSR
injects the LQ image latent distribution at a pre-computed mid-timestep,
incorporating the proposed Latent Distribution Refinement loss to alleviate the
latent distribution gap. We also design the Overlap-Chunked LPIPS/GAN loss to
eliminate checkerboard artifacts in image generation. Within this framework, we
instantiate OMGSR for DDPM/FM-based generative models with two variants:
OMGSR-S (SD-Turbo) and OMGSR-F (FLUX.1-dev). Experimental results demonstrate
that OMGSR-S/F achieves balanced/excellent performance across quantitative and
qualitative metrics at 512-resolution. Notably, OMGSR-F establishes
overwhelming dominance in all reference metrics. We further train a
1k-resolution OMGSR-F to match the default resolution of FLUX.1-dev, which
yields excellent results, especially in the details of the image generation. We
also generate 2k-resolution images by the 1k-resolution OMGSR-F using our
two-stage Tiled VAE & Diffusion.

</details>


### [217] [Cut2Next: Generating Next Shot via In-Context Tuning](https://arxiv.org/abs/2508.08244)
*Jingwen He,Hongbo Liu,Jiajun Li,Ziqi Huang,Yu Qiao,Wanli Ouyang,Ziwei Liu*

Main category: cs.CV

TL;DR: 论文提出了一种名为Next Shot Generation (NSG)的方法，通过Cut2Next框架生成符合专业编辑模式和电影连续性的高质量镜头。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注基本视觉一致性，而忽视了叙事流畅性和电影完整性，导致输出缺乏叙事深度。

Method: 采用Diffusion Transformer (DiT)和Hierarchical Multi-Prompting策略，结合Context-Aware Condition Injection (CACI)和Hierarchical Attention Mask (HAM)技术。

Result: 实验显示Cut2Next在视觉一致性和文本保真度上表现优异，用户研究也验证了其对编辑模式和电影连续性的遵循。

Conclusion: Cut2Next能够生成高质量、叙事表达丰富且电影连贯的后续镜头。

Abstract: Effective multi-shot generation demands purposeful, film-like transitions and
strict cinematic continuity. Current methods, however, often prioritize basic
visual consistency, neglecting crucial editing patterns (e.g., shot/reverse
shot, cutaways) that drive narrative flow for compelling storytelling. This
yields outputs that may be visually coherent but lack narrative sophistication
and true cinematic integrity. To bridge this, we introduce Next Shot Generation
(NSG): synthesizing a subsequent, high-quality shot that critically conforms to
professional editing patterns while upholding rigorous cinematic continuity.
Our framework, Cut2Next, leverages a Diffusion Transformer (DiT). It employs
in-context tuning guided by a novel Hierarchical Multi-Prompting strategy. This
strategy uses Relational Prompts to define overall context and inter-shot
editing styles. Individual Prompts then specify per-shot content and
cinematographic attributes. Together, these guide Cut2Next to generate
cinematically appropriate next shots. Architectural innovations, Context-Aware
Condition Injection (CACI) and Hierarchical Attention Mask (HAM), further
integrate these diverse signals without introducing new parameters. We
construct RawCuts (large-scale) and CuratedCuts (refined) datasets, both with
hierarchical prompts, and introduce CutBench for evaluation. Experiments show
Cut2Next excels in visual consistency and text fidelity. Crucially, user
studies reveal a strong preference for Cut2Next, particularly for its adherence
to intended editing patterns and overall cinematic continuity, validating its
ability to generate high-quality, narratively expressive, and cinematically
coherent subsequent shots.

</details>


### [218] [StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation](https://arxiv.org/abs/2508.08248)
*Shuyuan Tu,Yueming Pan,Yinming Huang,Xintong Han,Zhen Xing,Qi Dai,Chong Luo,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: StableAvatar提出了一种端到端的视频扩散变换器，能够生成无限长度的高质量视频，解决了现有模型在音频同步和身份一致性上的问题。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动的虚拟化身视频生成模型难以生成长视频且音频同步和身份一致性差，StableAvatar旨在解决这些问题。

Method: 通过时间步感知音频适配器和音频原生引导机制优化音频建模，并采用动态加权滑动窗口策略增强视频平滑度。

Result: 实验证明StableAvatar在质量和定量上均优于现有方法。

Conclusion: StableAvatar为无限长度视频生成提供了一种高效且高质量的解决方案。

Abstract: Current diffusion models for audio-driven avatar video generation struggle to
synthesize long videos with natural audio synchronization and identity
consistency. This paper presents StableAvatar, the first end-to-end video
diffusion transformer that synthesizes infinite-length high-quality videos
without post-processing. Conditioned on a reference image and audio,
StableAvatar integrates tailored training and inference modules to enable
infinite-length video generation. We observe that the main reason preventing
existing models from generating long videos lies in their audio modeling. They
typically rely on third-party off-the-shelf extractors to obtain audio
embeddings, which are then directly injected into the diffusion model via
cross-attention. Since current diffusion backbones lack any audio-related
priors, this approach causes severe latent distribution error accumulation
across video clips, leading the latent distribution of subsequent segments to
drift away from the optimal distribution gradually. To address this,
StableAvatar introduces a novel Time-step-aware Audio Adapter that prevents
error accumulation via time-step-aware modulation. During inference, we propose
a novel Audio Native Guidance Mechanism to further enhance the audio
synchronization by leveraging the diffusion's own evolving joint audio-latent
prediction as a dynamic guidance signal. To enhance the smoothness of the
infinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy
that fuses latent over time. Experiments on benchmarks show the effectiveness
of StableAvatar both qualitatively and quantitatively.

</details>


### [219] [ReferSplat: Referring Segmentation in 3D Gaussian Splatting](https://arxiv.org/abs/2508.08252)
*Shuting He,Guangquan Jie,Changshuo Wang,Yun Zhou,Shuming Hu,Guanbin Li,Henghui Ding*

Main category: cs.CV

TL;DR: 论文提出了一种新任务R3DGS，旨在通过自然语言描述在3D高斯场景中分割目标物体，并构建了首个数据集Ref-LERF。提出的ReferSplat框架在任务和基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 推动具身AI发展，解决3D多模态理解和空间关系建模的挑战。

Method: 提出ReferSplat框架，显式建模3D高斯点与自然语言表达的空间感知范式。

Result: ReferSplat在R3DGS任务和3D开放词汇分割基准测试中达到最优性能。

Conclusion: R3DGS任务和ReferSplat框架为3D多模态理解提供了重要进展，数据集和代码已开源。

Abstract: We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task
that aims to segment target objects in a 3D Gaussian scene based on natural
language descriptions, which often contain spatial relationships or object
attributes. This task requires the model to identify newly described objects
that may be occluded or not directly visible in a novel view, posing a
significant challenge for 3D multi-modal understanding. Developing this
capability is crucial for advancing embodied AI. To support research in this
area, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that
3D multi-modal understanding and spatial relationship modeling are key
challenges for R3DGS. To address these challenges, we propose ReferSplat, a
framework that explicitly models 3D Gaussian points with natural language
expressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art
performance on both the newly proposed R3DGS task and 3D open-vocabulary
segmentation benchmarks. Dataset and code are available at
https://github.com/heshuting555/ReferSplat.

</details>


### [220] [Learning an Implicit Physics Model for Image-based Fluid Simulation](https://arxiv.org/abs/2508.08254)
*Emily Yue-Ting Jia,Jiageng Mao,Zhiyuan Gao,Yajie Zhao,Yue Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于物理约束的神经网络方法，用于从单张图像生成物理一致的4D场景（包含3D几何和运动）。


<details>
  <summary>Details</summary>
Motivation: 人类能够从单张静态图像想象出4D场景（包括运动和3D几何），而现有方法通常使用简化的2D运动估计，导致违反物理原理的不真实动画。

Method: 提出了一种物理信息神经网络，通过基于物理原理（如Navier-Stokes方程）的损失项预测表面点运动，并结合3D高斯特征进行渲染。

Result: 实验结果表明，该方法能生成物理上合理的动画，性能显著优于现有方法。

Conclusion: 该方法成功模拟了人类从单张图像生成物理一致4D场景的能力，为相关领域提供了新思路。

Abstract: Humans possess an exceptional ability to imagine 4D scenes, encompassing both
motion and 3D geometry, from a single still image. This ability is rooted in
our accumulated observations of similar scenes and an intuitive understanding
of physics. In this paper, we aim to replicate this capacity in neural
networks, specifically focusing on natural fluid imagery. Existing methods for
this task typically employ simplistic 2D motion estimators to animate the
image, leading to motion predictions that often defy physical principles,
resulting in unrealistic animations. Our approach introduces a novel method for
generating 4D scenes with physics-consistent animation from a single image. We
propose the use of a physics-informed neural network that predicts motion for
each surface point, guided by a loss term derived from fundamental physical
principles, including the Navier-Stokes equations. To capture appearance, we
predict feature-based 3D Gaussians from the input image and its estimated
depth, which are then animated using the predicted motions and rendered from
any desired camera perspective. Experimental results highlight the
effectiveness of our method in producing physically plausible animations,
showcasing significant performance improvements over existing methods. Our
project page is https://physfluid.github.io/ .

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [221] [Semi-automated Fact-checking in Portuguese: Corpora Enrichment using Retrieval with Claim extraction](https://arxiv.org/abs/2508.06495)
*Juliana Resplande Sant'anna Gomes,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: 论文提出了一种半自动化事实核查（SAFC）系统，通过整合外部证据丰富葡萄牙语新闻语料库，填补了现有数据集的不足。


<details>
  <summary>Details</summary>
Motivation: 由于虚假信息传播速度快，手动事实核查效率不足，且葡萄牙语领域缺乏整合外部证据的公开数据集，亟需开发更强大的事实核查系统。

Method: 利用大型语言模型（如Gemini 1.5 Flash）提取文本主要主张，并通过搜索引擎API（如Google Search API）检索外部证据，同时引入数据验证和预处理框架（如近重复检测）提升语料库质量。

Result: 开发了一种方法，成功丰富了葡萄牙语新闻语料库（Fake.Br、COVID19.BR、MuMiN-PT），并提供了高质量的外部证据整合框架。

Conclusion: 该研究为葡萄牙语半自动化事实核查系统提供了重要支持，填补了数据集的空白，并为未来研究奠定了基础。

Abstract: The accelerated dissemination of disinformation often outpaces the capacity
for manual fact-checking, highlighting the urgent need for Semi-Automated
Fact-Checking (SAFC) systems. Within the Portuguese language context, there is
a noted scarcity of publicly available datasets that integrate external
evidence, an essential component for developing robust AFC systems, as many
existing resources focus solely on classification based on intrinsic text
features. This dissertation addresses this gap by developing, applying, and
analyzing a methodology to enrich Portuguese news corpora (Fake.Br, COVID19.BR,
MuMiN-PT) with external evidence. The approach simulates a user's verification
process, employing Large Language Models (LLMs, specifically Gemini 1.5 Flash)
to extract the main claim from texts and search engine APIs (Google Search API,
Google FactCheck Claims Search API) to retrieve relevant external documents
(evidence). Additionally, a data validation and preprocessing framework,
including near-duplicate detection, is introduced to enhance the quality of the
base corpora.

</details>


### [222] [Retrieval augmented generation based dynamic prompting for few-shot biomedical named entity recognition using large language models](https://arxiv.org/abs/2508.06504)
*Yao Ge,Sudeshna Das,Yuting Guo,Abeed Sarker*

Main category: cs.CL

TL;DR: 论文研究了动态提示策略在少样本生物医学命名实体识别（NER）中的应用，通过检索增强生成（RAG）优化性能。静态和动态提示技术显著提升了模型表现，动态提示效果更佳。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在少样本生物医学NER任务中的性能挑战，探索动态提示策略的潜力。

Method: 采用检索增强生成（RAG）技术，动态选择与输入文本相似的上下文学习示例，并在推理过程中动态更新提示。

Result: 静态提示使GPT-4、GPT-3.5和LLaMA 3-70B的F1分数分别提升12%、11%和11%；动态提示进一步提升了性能，TF-IDF和SBERT检索方法在5-shot和10-shot设置中分别提高7.3%和5.6%。

Conclusion: 动态提示策略通过RAG显著提升了生物医学NER任务的性能，证明了上下文自适应提示的实用性。

Abstract: Biomedical named entity recognition (NER) is a high-utility natural language
processing (NLP) task, and large language models (LLMs) show promise
particularly in few-shot settings (i.e., limited training data). In this
article, we address the performance challenges of LLMs for few-shot biomedical
NER by investigating a dynamic prompting strategy involving retrieval-augmented
generation (RAG). In our approach, the annotated in-context learning examples
are selected based on their similarities with the input texts, and the prompt
is dynamically updated for each instance during inference. We implemented and
optimized static and dynamic prompt engineering techniques and evaluated them
on five biomedical NER datasets. Static prompting with structured components
increased average F1-scores by 12% for GPT-4, and 11% for GPT-3.5 and LLaMA
3-70B, relative to basic static prompting. Dynamic prompting further improved
performance, with TF-IDF and SBERT retrieval methods yielding the best results,
improving average F1-scores by 7.3% and 5.6% in 5-shot and 10-shot settings,
respectively. These findings highlight the utility of contextually adaptive
prompts via RAG for biomedical NER.

</details>


### [223] [CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models](https://arxiv.org/abs/2508.06524)
*Lei Jiang,Fan Chen*

Main category: cs.CL

TL;DR: 论文提出CarbonScaling框架，将神经缩放定律扩展到碳排放领域，量化模型精度与碳足迹的关系。


<details>
  <summary>Details</summary>
Motivation: 现有神经缩放定律忽视了LLM规模扩大带来的碳排放问题，需结合碳足迹优化模型训练。

Method: 整合神经缩放、GPU硬件演进、并行优化和碳排放模型，构建CarbonScaling框架。

Result: 精度与碳排放呈幂律关系，但现实效率问题显著增加缩放因子；硬件优化对小中型模型有效，但对超大型LLM效果有限。

Conclusion: CarbonScaling为训练更可持续、碳高效的LLM提供了关键见解。

Abstract: Neural scaling laws have driven the development of increasingly large
language models (LLMs) by linking accuracy improvements to growth in parameter
count, dataset size, and compute. However, these laws overlook the carbon
emissions that scale exponentially with LLM size. This paper presents
\textit{CarbonScaling}, an analytical framework that extends neural scaling
laws to incorporate both operational and embodied carbon in LLM training. By
integrating models for neural scaling, GPU hardware evolution, parallelism
optimization, and carbon estimation, \textit{CarbonScaling} quantitatively
connects model accuracy to carbon footprint. Results show that while a
power-law relationship between accuracy and carbon holds, real-world
inefficiencies significantly increase the scaling factor. Hardware technology
scaling reduces carbon emissions for small to mid-sized models, but offers
diminishing returns for extremely large LLMs due to communication overhead and
underutilized GPUs. Training optimizations-especially aggressive critical batch
size scaling-help alleviate this inefficiency. \textit{CarbonScaling} offers
key insights for training more sustainable and carbon-efficient LLMs.

</details>


### [224] [The Art of Breaking Words: Rethinking Multilingual Tokenizer Design](https://arxiv.org/abs/2508.06533)
*Aamod Thakur,Ajay Nagpal,Atharva Savarkar,Kundeshwar Pundalik,Siddhesh Dosi,Piyush Sawarkar,Viraj Thakur,Rohit Saluja,Maunendra Sankar Desarkar,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 本文研究了多语言环境下LLM的tokenization问题，提出了一种新的数据组合算法，显著提高了token-to-word效率，并改善了模型性能和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有tokenizer在多语言环境下效率低下，特别是在高脚本多样性和拼写复杂性的语言（如印度语系）中表现不佳。

Method: 通过系统研究词汇量、预分词规则和训练语料组成，提出了一种平衡多语言数据的新算法。

Result: 新算法使token-to-word比率降低约6%，平均提升40%的效率，显著改善模型性能和推理速度。

Conclusion: tokenization是构建高效、可扩展多语言LLM的关键因素之一。

Abstract: While model architecture and training objectives are well-studied,
tokenization, particularly in multilingual contexts, remains a relatively
neglected aspect of Large Language Model (LLM) development. Existing tokenizers
often exhibit high token-to-word ratios, inefficient use of context length, and
slower inference. We present a systematic study that links vocabulary size,
pre-tokenization rules, and training-corpus composition to both token-to-word
efficiency and model quality. To ground our analysis in a linguistically
diverse context, we conduct extensive experiments on Indic scripts, which
present unique challenges due to their high script diversity and orthographic
complexity. Drawing on the insights from these analyses, we propose a novel
algorithm for data composition that balances multilingual data for tokenizer
training. Our observations on pretokenization strategies significantly improve
model performance, and our data composition algorithm reduces the average
token-to-word ratio by approximately 6% with respect to the conventional data
randomization approach. Our tokenizer achieves more than 40% improvement on
average token-to-word ratio against stateof-the-art multilingual Indic models.
This improvement yields measurable gains in both model performance and
inference speed. This highlights tokenization alongside architecture and
training objectives as a critical lever for building efficient, scalable
multilingual LLMs

</details>


### [225] [Factor Augmented Supervised Learning with Text Embeddings](https://arxiv.org/abs/2508.06548)
*Zhanye Luo,Yuefeng Han,Xiufan Yu*

Main category: cs.CL

TL;DR: 论文提出AEALT框架，通过监督增强自编码器降低LLM生成的高维文本嵌入维度，提升下游任务效率。


<details>
  <summary>Details</summary>
Motivation: 高维文本嵌入在计算效率和成本上存在问题，需要一种方法在不损失语义信息的情况下降低维度。

Method: 提取文本嵌入后，通过监督增强自编码器学习低维任务相关潜在因子。

Result: AEALT在分类、异常检测和预测任务中表现优于传统方法。

Conclusion: AEALT框架显著提升了高维嵌入在下游任务中的效率和性能。

Abstract: Large language models (LLMs) generate text embeddings from text data,
producing vector representations that capture the semantic meaning and
contextual relationships of words. However, the high dimensionality of these
embeddings often impedes efficiency and drives up computational cost in
downstream tasks. To address this, we propose AutoEncoder-Augmented Learning
with Text (AEALT), a supervised, factor-augmented framework that incorporates
dimension reduction directly into pre-trained LLM workflows. First, we extract
embeddings from text documents; next, we pass them through a supervised
augmented autoencoder to learn low-dimensional, task-relevant latent factors.
By modeling the nonlinear structure of complex embeddings, AEALT outperforms
conventional deep-learning approaches that rely on raw embeddings. We validate
its broad applicability with extensive experiments on classification, anomaly
detection, and prediction tasks using multiple real-world public datasets.
Numerical results demonstrate that AEALT yields substantial gains over both
vanilla embeddings and several standard dimension reduction methods.

</details>


### [226] [Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs](https://arxiv.org/abs/2508.06583)
*Ying Liu,Can Li,Ting Zhang,Mei Wang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型在自适应辅导中的潜力，提出了GuideEval基准，评估模型在感知、编排和引导三个阶段的指导能力，并发现现有模型在适应性支持方面表现不足。通过行为引导的微调策略，显著提升了指导性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注模型的苏格拉底式提问能力，但忽略了基于学习者认知状态的自适应指导。本研究旨在填补这一空白，探讨模型是否能像专家导师一样动态调整策略。

Method: 提出GuideEval基准，基于真实教育对话，通过感知、编排和引导三个阶段评估模型的教学指导能力。采用行为引导的微调策略优化模型性能。

Result: 实证发现现有模型在适应性支持方面表现不佳，尤其是在学习者困惑或需要引导时。行为引导的微调策略显著提升了指导性能。

Conclusion: 研究倡导从孤立内容评估转向以学习者为中心的交互范式，为苏格拉底式语言模型的评估提供了新视角。

Abstract: The conversational capabilities of large language models hold significant
promise for enabling scalable and interactive tutoring. While prior research
has primarily examined their capacity for Socratic questioning, it often
overlooks a critical dimension: adaptively guiding learners based on their
cognitive states. This study shifts focus from mere question generation to the
broader instructional guidance capability. We ask: Can LLMs emulate expert
tutors who dynamically adjust strategies in response to learners'
understanding? To investigate this, we propose GuideEval, a benchmark grounded
in authentic educational dialogues that evaluates pedagogical guidance through
a three-phase behavioral framework: (1) Perception, inferring learner states;
(2) Orchestration, adapting instructional strategies; and (3) Elicitation,
stimulating proper reflections. Empirical findings reveal that existing LLMs
frequently fail to provide effective adaptive scaffolding when learners exhibit
confusion or require redirection. Furthermore, we introduce a behavior-guided
finetuning strategy that leverages behavior-prompted instructional dialogues,
significantly enhancing guidance performance. By shifting the focus from
isolated content evaluation to learner-centered interaction, our work advocates
a more dialogic paradigm for evaluating Socratic LLMs.

</details>


### [227] [LLM Unlearning Without an Expert Curated Dataset](https://arxiv.org/abs/2508.06595)
*Xiaoyuan Zhu,Muru Zhang,Ollie Liu,Robin Jia,Willie Neiswanger*

Main category: cs.CL

TL;DR: 论文提出了一种自动化生成高质量遗忘数据集的方法，用于大语言模型的领域知识遗忘，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型可能包含敏感、有害或受版权保护的知识，需要一种无需完全重新训练即可遗忘特定领域知识的方法。

Method: 通过结构化提示管道合成教科书风格的数据，仅需输入领域名称即可生成遗忘数据集。

Result: 实验表明，该方法生成的合成数据集优于基线方法，且与专家手工整理的数据集效果相当。

Conclusion: 合成数据集为广泛领域的实用、可扩展遗忘提供了一条可行路径。

Abstract: Modern large language models often encode sensitive, harmful, or copyrighted
knowledge, raising the need for post-hoc unlearning-the ability to remove
specific domains of knowledge from a model without full retraining. A major
bottleneck in current unlearning pipelines is constructing effective forget
sets-datasets that approximate the target domain and guide the model to forget
it. In this work, we introduce a scalable, automated approach to generate
high-quality forget sets using language models themselves. Our method
synthesizes textbook-style data through a structured prompting pipeline,
requiring only a domain name as input. Through experiments on unlearning
biosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic
datasets consistently outperform the baseline synthetic alternatives and are
comparable to the expert-curated ones. Additionally, ablation studies reveal
that the multi-step generation pipeline significantly boosts data diversity,
which in turn improves unlearning utility. Overall, our findings suggest that
synthetic datasets offer a promising path toward practical, scalable unlearning
for a wide range of emerging domains without the need for manual intervention.
We release our code and dataset at
https://github.com/xyzhu123/Synthetic_Textbook.

</details>


### [228] [BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent](https://arxiv.org/abs/2508.06600)
*Zijian Chen,Xueguang Ma,Shengyao Zhuang,Ping Nie,Kai Zou,Andrew Liu,Joshua Green,Kshama Patel,Ruoxi Meng,Mingyi Su,Sahel Sharifymoghaddam,Yanxi Li,Haoran Hong,Xinyu Shi,Xuye Liu,Nandan Thakur,Crystina Zhang,Luyu Gao,Wenhu Chen,Jimmy Lin*

Main category: cs.CL

TL;DR: 论文介绍了BrowseComp-Plus基准测试，用于更公平、透明地评估深度研究代理的性能，解决了现有基准测试的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试（如BrowseComp）依赖动态、不透明的网络API，导致公平性和透明度问题，难以进行可控实验。

Method: 提出BrowseComp-Plus，基于固定、精选的文档库，包含人工验证的支持文档和挑战性负样本，支持可控实验。

Result: BrowseComp-Plus能有效区分系统性能，例如GPT-5结合Qwen3-Embedding-8B检索器将准确率提升至70.1%。

Conclusion: BrowseComp-Plus为深度研究代理和检索方法提供了全面的评估框架，有助于分析检索效果和上下文工程。

Abstract: Deep-Research agents, which integrate large language models (LLMs) with
search tools, have shown success in improving the effectiveness of handling
complex queries that require iterative search planning and reasoning over
search results. Evaluations on current benchmarks like BrowseComp relies on
black-box live web search APIs, have notable limitations in (1) fairness:
dynamic and opaque web APIs hinder fair comparisons and reproducibility of deep
research methods; (2) transparency: lack of control over the document corpus
makes it difficult to isolate retriever contributions. In other words, the
current evaluations may compare a complete deep research system at a given
time, but they do not foster well-controlled experiments to provide insights
into the capability of underlying deep research LLMs. To address these
challenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp,
employing a fixed, carefully curated corpus. Each query in BrowseComp-Plus
includes human-verified supporting documents and mined challenging negatives,
enabling controlled experimentation. The benchmark is shown to be effective in
distinguishing the performance of deep research systems. For instance, the
open-source model Search-R1, when paired with the BM25 retriever, achieves
3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with
the Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with
fewer search calls. This benchmark allows comprehensive evaluation and
disentangled analysis of deep research agents and retrieval methods, fostering
insights into retrieval effectiveness, citation accuracy, and context
engineering in Deep-Research system.

</details>


### [229] [Train It and Forget It: Merge Lists are Unnecessary for BPE Inference in Language Models](https://arxiv.org/abs/2508.06621)
*Tomohiro Sawada,Kartik Goyal*

Main category: cs.CL

TL;DR: 研究探讨了BPE推理算法对语言模型性能的影响，发现非目标性算法对性能影响较小，可能简化并保护隐私。


<details>
  <summary>Details</summary>
Motivation: BPE合并列表可能暴露训练数据信息，研究探索不依赖合并列表的推理算法的下游影响。

Method: 研究两类BPE推理方案：目标性偏离合并列表（如随机合并顺序）和非目标性算法（不依赖合并列表）。

Result: 目标性偏离显著降低性能，而非目标性算法对性能影响极小。

Conclusion: 非目标性算法为简化且隐私保护的标记化方案提供了可能。

Abstract: Standard Byte-Pair Encoding (BPE) tokenization compresses text by pairing a
learned token vocabulary with a detailed merge list. Recent work has shown that
this merge list exposes a potential attack surface for extracting information
about language model's training data. In this paper, we explore the downstream
impact of BPE inference algorithms that do not rely on this merge list at all,
and hence differ from the encoding process during BPE training. To address this
question, we investigate two broad classes of BPE inference schemes that differ
from BPE application during training: a) targeted deviation from merge-lists
including random merge orders, and various corruptions of merge list involving
deletion/truncation, and b) non-targeted BPE inference algorithms that do not
depend on the merge list but focus on compressing the text either greedily or
exactly. Extensive experiments across diverse language modeling tasks like
accuracy-based QA benchmarks, machine translation, and open-ended generation
reveal that while targeted deviation from the merge lists exhibits significant
degradation in language model performance, the non-targeted merge-list-free
inference algorithms result in minimal impact on downstream performance that is
often much smaller than expected. These findings pave way for simpler and
potentially more privacy-preserving tokenization schemes that do not
catastrophically compromise model performance.

</details>


### [230] [Measuring Stereotype and Deviation Biases in Large Language Models](https://arxiv.org/abs/2508.06649)
*Daniel Wang,Eli Brignac,Minjia Mao,Xiao Fang*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）存在刻板印象偏差和偏离偏差，可能对多群体造成潜在危害。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在生成内容时可能表现出的两种偏差，以揭示其潜在风险。

Method: 通过让四种先进LLMs生成个人资料，分析其与政治倾向、宗教和性取向等属性的关联。

Result: 所有测试的LLMs均对多群体表现出显著的刻板印象偏差和偏离偏差。

Conclusion: 研究揭示了LLMs在推断用户属性时的偏差问题，警示其生成内容可能带来的危害。

Abstract: Large language models (LLMs) are widely applied across diverse domains,
raising concerns about their limitations and potential risks. In this study, we
investigate two types of bias that LLMs may display: stereotype bias and
deviation bias. Stereotype bias refers to when LLMs consistently associate
specific traits with a particular demographic group. Deviation bias reflects
the disparity between the demographic distributions extracted from
LLM-generated content and real-world demographic distributions. By asking four
advanced LLMs to generate profiles of individuals, we examine the associations
between each demographic group and attributes such as political affiliation,
religion, and sexual orientation. Our experimental results show that all
examined LLMs exhibit both significant stereotype bias and deviation bias
towards multiple groups. Our findings uncover the biases that occur when LLMs
infer user attributes and shed light on the potential harms of LLM-generated
outputs.

</details>


### [231] [Testing the Limits of Machine Translation from One Book](https://arxiv.org/abs/2508.06665)
*Jonathan Shaw,Dillon Mee,Timothy Khouw,Zackary Leech,Daniel Wilson*

Main category: cs.CL

TL;DR: 论文研究了如何利用语言资源（如语法和词典）提升LLM在低资源语言（如Kanuri）翻译中的表现，发现平行句子是最有效的数据源，而语法单独使用效果有限。


<details>
  <summary>Details</summary>
Motivation: Kanuri语言虽使用人口众多但数字资源匮乏，研究旨在探索如何利用有限的语言资源提升LLM翻译质量。

Method: 设计两个数据集（健康和通用术语），比较不同语言资源组合（语法、词典、平行句子）对LLM翻译的影响，并与人工翻译对比。

Result: 平行句子效果最佳，语法虽优于零样本翻译但单独使用效果不佳；LLM在准确性上优于流畅性。

Conclusion: LLM翻译评估需多维指标，语法需结合平行句子才能有效支持领域翻译。

Abstract: Current state-of-the-art models demonstrate capacity to leverage in-context
learning to translate into previously unseen language contexts. Tanzer et al.
[2024] utilize language materials (e.g. a grammar) to improve translation
quality for Kalamang using large language models (LLMs). We focus on Kanuri, a
language that, despite having substantial speaker population, has minimal
digital resources. We design two datasets for evaluation: one focused on health
and humanitarian terms, and another containing generalized terminology,
investigating how domain-specific tasks impact LLM translation quality.
  By providing different combinations of language resources (grammar,
dictionary, and parallel sentences), we measure LLM translation effectiveness,
comparing results to native speaker translations and human linguist
performance. We evaluate using both automatic metrics and native speaker
assessments of fluency and accuracy.
  Results demonstrate that parallel sentences remain the most effective data
source, outperforming other methods in human evaluations and automatic metrics.
While incorporating grammar improves over zero-shot translation, it fails as an
effective standalone data source. Human evaluations reveal that LLMs achieve
accuracy (meaning) more effectively than fluency (grammaticality).
  These findings suggest LLM translation evaluation benefits from
multidimensional assessment beyond simple accuracy metrics, and that grammar
alone, without parallel sentences, does not provide sufficient context for
effective domain-specific translation.

</details>


### [232] [Do Biased Models Have Biased Thoughts?](https://arxiv.org/abs/2508.06671)
*Swati Rajwal,Shivank Garg,Reem Abdel-Salam,Abdelrahman Zayed*

Main category: cs.CL

TL;DR: 研究发现，语言模型虽然性能出色，但存在性别、种族等偏见。通过链式思维提示方法，发现模型的偏见思维与输出偏见的关联性较低。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型在链式思维提示下的偏见表现，验证偏见思维与输出偏见的关系。

Method: 使用链式思维提示方法，对5种流行大语言模型进行实验，量化11种偏见的思维和输出。

Result: 偏见思维与输出偏见的关联性较低（相关性小于0.6，p值小于0.001）。

Conclusion: 与人类不同，测试模型即使有偏见决策，也不一定伴随偏见思维。

Abstract: The impressive performance of language models is undeniable. However, the
presence of biases based on gender, race, socio-economic status, physical
appearance, and sexual orientation makes the deployment of language models
challenging. This paper studies the effect of chain-of-thought prompting, a
recent approach that studies the steps followed by the model before it
responds, on fairness. More specifically, we ask the following question:
\textit{Do biased models have biased thoughts}? To answer our question, we
conduct experiments on $5$ popular large language models using fairness metrics
to quantify $11$ different biases in the model's thoughts and output. Our
results show that the bias in the thinking steps is not highly correlated with
the output bias (less than $0.6$ correlation with a $p$-value smaller than
$0.001$ in most cases). In other words, unlike human beings, the tested models
with biased decisions do not always possess biased thoughts.

</details>


### [233] [Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge](https://arxiv.org/abs/2508.06709)
*Evangelia Spiliopoulou,Riccardo Fogliato,Hanna Burnsky,Tamer Soliman,Jie Ma,Graham Horwood,Miguel Ballesteros*

Main category: cs.CL

TL;DR: 本文提出了一种统计框架来识别和量化LLM作为评委时的自我偏见，确保真实性能差异不被误认为偏见。


<details>
  <summary>Details</summary>
Motivation: LLM作为评委时可能对自身输出评分过高（自我偏见），导致模型性能评估失真。

Method: 通过统计框架建模LLM评委对自身和其他模型输出的评分分布差异，并引入第三方评委（如人类）作为参考。

Result: 实证分析发现某些LLM（如GPT-4o和Claude 3.5 Sonnet）存在自我偏见和家族偏见。

Conclusion: 研究揭示了LLM评委的潜在问题，并提供了减少偏见的实用建议。

Abstract: Large language models (LLMs) can serve as judges that offer rapid and
reliable assessments of other LLM outputs. However, models may systematically
assign overly favorable ratings to their own outputs, a phenomenon known as
self-bias, which can distort evaluations of true model performance. Previous
studies often conflate genuine differences in model quality with bias or
incorrectly assume that evaluations from LLMs and humans follow the same rating
distributions. In this work, we present a statistical framework that explicitly
formalizes assumptions under which self-bias can be identified and estimated.
Our method models the difference in the scoring distribution that
LLM-as-a-judge assigns to its own completions compared to other models, while
accounting for the underlying quality of the completions provided by an
independent, third-party judge (e.g., humans). Our method reliably isolates and
quantifies self-bias, even when models vary in ability, ensuring that genuine
performance differences are not mistaken for self-bias. We conduct an empirical
analysis of self-bias on a large dataset (>5000 prompt-completion pairs)
consisting of expert human annotations and judgments from nine different LLM
judges. We find that some models, such as GPT-4o and Claude 3.5 Sonnet,
systematically assign higher scores to their own outputs. These models also
display family-bias; systematically assigning higher ratings to outputs
produced by other models of the same family. Our findings highlight potential
pitfalls of using LLM judges and offer practical guidance to mitigate biases
when interpreting automated evaluations.

</details>


### [234] [Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis](https://arxiv.org/abs/2508.06729)
*Komala Subramanyam Cherukuri,Pranav Abishai Moses,Aisa Sakata,Jiangping Chen,Haihua Chen*

Main category: cs.CL

TL;DR: 本文提出了一种基于LLM的自动化框架，用于对日本美国监禁口述历史进行语义和情感标注，通过多阶段方法评估模型性能，并展示了LLM在大规模口述历史分析中的有效性。


<details>
  <summary>Details</summary>
Motivation: 口述历史是记录生活经历的重要资料，但因其非结构化、情感复杂和标注成本高，大规模分析受限。本文旨在解决这些问题，促进对口述历史的访问和理解。

Method: 采用多阶段方法，结合专家标注、提示设计和LLM评估（ChatGPT、Llama、Qwen），测试零样本、少样本和RAG策略，标注558个句子并扩展到92,191个句子。

Result: ChatGPT在语义分类中表现最佳（F1 88.71%），Llama在情感分析中略优（82.66%）。最佳提示配置成功标注了大量数据。

Conclusion: 研究表明，LLM在良好设计的提示下可有效完成大规模口述历史的语义和情感标注，为数字人文领域提供了可复用的标注流程和伦理指导。

Abstract: Oral histories are vital records of lived experience, particularly within
communities affected by systemic injustice and historical erasure. Effective
and efficient analysis of their oral history archives can promote access and
understanding of the oral histories. However, Large-scale analysis of these
archives remains limited due to their unstructured format, emotional
complexity, and high annotation costs. This paper presents a scalable framework
to automate semantic and sentiment annotation for Japanese American
Incarceration Oral History. Using LLMs, we construct a high-quality dataset,
evaluate multiple models, and test prompt engineering strategies in
historically sensitive contexts. Our multiphase approach combines expert
annotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We
labeled 558 sentences from 15 narrators for sentiment and semantic
classification, then evaluated zero-shot, few-shot, and RAG strategies. For
semantic classification, ChatGPT achieved the highest F1 score (88.71%),
followed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama
slightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models
showing comparable results. The best prompt configurations were used to
annotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our
findings show that LLMs can effectively perform semantic and sentiment
annotation across large oral history collections when guided by well-designed
prompts. This study provides a reusable annotation pipeline and practical
guidance for applying LLMs in culturally sensitive archival analysis. By
bridging archival ethics with scalable NLP techniques, this work lays the
groundwork for responsible use of artificial intelligence in digital humanities
and preservation of collective memory. GitHub:
https://github.com/kc6699c/LLM4OralHistoryAnalysis.

</details>


### [235] [Many-Turn Jailbreaking](https://arxiv.org/abs/2508.06755)
*Xianjun Yang,Liqiang Xiao,Shiyang Li,Faisal Ladhak,Hyokun Yun,Linda Ruth Petzold,Yi Xu,William Yang Wang*

Main category: cs.CL

TL;DR: 论文探讨了多轮越狱（multi-turn jailbreaking）对大型语言模型（LLMs）的安全威胁，提出了一种新的基准测试（MTJ-Bench）来评估多轮对话中的越狱风险。


<details>
  <summary>Details</summary>
Motivation: 现有的越狱研究仅关注单轮对话，而实际应用中用户可能进行多轮交互，导致更严重的安全隐患。

Method: 构建了多轮越狱基准测试（MTJ-Bench），用于评估开源和闭源模型在多轮对话中的越狱风险。

Result: 揭示了多轮越狱是一种新的安全威胁，并提供了相关见解。

Conclusion: 呼吁社区关注多轮越狱风险，推动更安全的LLMs开发。

Abstract: Current jailbreaking work on large language models (LLMs) aims to elicit
unsafe outputs from given prompts. However, it only focuses on single-turn
jailbreaking targeting one specific query. On the contrary, the advanced LLMs
are designed to handle extremely long contexts and can thus conduct multi-turn
conversations. So, we propose exploring multi-turn jailbreaking, in which the
jailbroken LLMs are continuously tested on more than the first-turn
conversation or a single target query. This is an even more serious threat
because 1) it is common for users to continue asking relevant follow-up
questions to clarify certain jailbroken details, and 2) it is also possible
that the initial round of jailbreaking causes the LLMs to respond to additional
irrelevant questions consistently. As the first step (First draft done at June
2024) in exploring multi-turn jailbreaking, we construct a Multi-Turn Jailbreak
Benchmark (MTJ-Bench) for benchmarking this setting on a series of open- and
closed-source models and provide novel insights into this new safety threat. By
revealing this new vulnerability, we aim to call for community efforts to build
safer LLMs and pave the way for a more in-depth understanding of jailbreaking
LLMs.

</details>


### [236] [SEVADE: Self-Evolving Multi-Agent Analysis with Decoupled Evaluation for Hallucination-Resistant Irony Detection](https://arxiv.org/abs/2508.06803)
*Ziqi Liu,Yangbin Chen,Ziyang Zhou,Yilin Li,Mingxuan Hu,Yushan Pan,Zhijie Xu*

Main category: cs.CL

TL;DR: 提出了一种名为SEVADE的新型多代理框架，用于抗幻觉的讽刺检测，通过动态代理推理引擎（DARE）和多角度分析提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在讽刺检测中存在单视角分析、静态推理路径和幻觉问题，影响准确性和可靠性。

Method: 采用SEVADE框架，结合DARE进行多角度文本解构，并通过独立的理性裁决器（RA）进行分类，以分离推理与判断。

Result: 在四个基准数据集上，SEVADE在准确率和Macro-F1分数上分别平均提升6.75%和6.29%。

Conclusion: SEVADE通过多代理分析和解耦评估，显著提升了讽刺检测的性能和抗幻觉能力。

Abstract: Sarcasm detection is a crucial yet challenging Natural Language Processing
task. Existing Large Language Model methods are often limited by
single-perspective analysis, static reasoning pathways, and a susceptibility to
hallucination when processing complex ironic rhetoric, which impacts their
accuracy and reliability. To address these challenges, we propose **SEVADE**, a
novel **S**elf-**Ev**olving multi-agent **A**nalysis framework with
**D**ecoupled **E**valuation for hallucination-resistant sarcasm detection. The
core of our framework is a Dynamic Agentive Reasoning Engine (DARE), which
utilizes a team of specialized agents grounded in linguistic theory to perform
a multifaceted deconstruction of the text and generate a structured reasoning
chain. Subsequently, a separate lightweight rationale adjudicator (RA) performs
the final classification based solely on this reasoning chain. This decoupled
architecture is designed to mitigate the risk of hallucination by separating
complex reasoning from the final judgment. Extensive experiments on four
benchmark datasets demonstrate that our framework achieves state-of-the-art
performance, with average improvements of **6.75%** in Accuracy and **6.29%**
in Macro-F1 score.

</details>


### [237] [Annotating Errors in English Learners' Written Language Production: Advancing Automated Written Feedback Systems](https://arxiv.org/abs/2508.06810)
*Steven Coyne,Diana Galvan-Sosa,Ryan Spring,Camélia Guerraoui,Michael Zock,Keisuke Sakaguchi,Kentaro Inui*

Main category: cs.CL

TL;DR: 论文提出了一种针对语言学习的自动写作评估系统改进框架，通过标注错误类型和可推广性，生成更适合学习的反馈。


<details>
  <summary>Details</summary>
Motivation: 现有自动写作评估系统虽能修正语法错误，但未针对语言学习优化，缺乏解释性和间接提示。

Method: 引入标注框架，分类错误类型并收集带注释的学习者错误数据集，评估关键词引导、无关键词和模板引导的反馈生成方法。

Result: 开发了数据集并比较了不同系统的性能，人类教师评估了反馈的相关性、准确性和可理解性。

Conclusion: 提出的框架和方法为语言学习提供了更有效的反馈生成方式。

Abstract: Recent advances in natural language processing (NLP) have contributed to the
development of automated writing evaluation (AWE) systems that can correct
grammatical errors. However, while these systems are effective at improving
text, they are not optimally designed for language learning. They favor direct
revisions, often with a click-to-fix functionality that can be applied without
considering the reason for the correction. Meanwhile, depending on the error
type, learners may benefit most from simple explanations and strategically
indirect hints, especially on generalizable grammatical rules. To support the
generation of such feedback, we introduce an annotation framework that models
each error's error type and generalizability. For error type classification, we
introduce a typology focused on inferring learners' knowledge gaps by
connecting their errors to specific grammatical patterns. Following this
framework, we collect a dataset of annotated learner errors and corresponding
human-written feedback comments, each labeled as a direct correction or hint.
With this data, we evaluate keyword-guided, keyword-free, and template-guided
methods of generating feedback using large language models (LLMs). Human
teachers examined each system's outputs, assessing them on grounds including
relevance, factuality, and comprehensibility. We report on the development of
the dataset and the comparative performance of the systems investigated.

</details>


### [238] [Text to Speech System for Meitei Mayek Script](https://arxiv.org/abs/2508.06870)
*Gangular Singh Irengbam,Nirvash Singh Wahengbam,Lanthoiba Meitei Khumanthem,Paikhomba Oinam*

Main category: cs.CL

TL;DR: 本文开发了一种基于Meitei Mayek文字的曼尼普尔语TTS系统，采用Tacotron 2和HiFi-GAN，支持音调语音学和资源匮乏的语言环境。


<details>
  <summary>Details</summary>
Motivation: 为曼尼普尔语开发TTS系统，支持其语言保护和技术包容性。

Method: 使用Tacotron 2和HiFi-GAN架构，开发Meitei Mayek到ARPAbet的音素映射，并构建单说话人数据集。

Result: 通过主客观指标验证，系统实现了清晰自然的语音合成。

Conclusion: 该系统为曼尼普尔语的语言保护和技术应用奠定了基础。

Abstract: This paper presents the development of a Text-to-Speech (TTS) system for the
Manipuri language using the Meitei Mayek script. Leveraging Tacotron 2 and
HiFi-GAN, we introduce a neural TTS architecture adapted to support tonal
phonology and under-resourced linguistic environments. We develop a phoneme
mapping for Meitei Mayek to ARPAbet, curate a single-speaker dataset, and
demonstrate intelligible and natural speech synthesis, validated through
subjective and objective metrics. This system lays the groundwork for
linguistic preservation and technological inclusion of Manipuri.

</details>


### [239] [ESNERA: Empirical and semantic named entity alignment for named entity dataset merging](https://arxiv.org/abs/2508.06877)
*Xiaobo Zhang,Congqing He,Ying He,Jian Peng,Dajie Fu,Tien-Ping Tan*

Main category: cs.CL

TL;DR: 提出了一种基于标签相似度的自动标签对齐方法，用于合并多源NER数据集，提升低资源领域的性能。


<details>
  <summary>Details</summary>
Motivation: 现有NER方法依赖大规模标注数据，但构建成本高；当前合并方法缺乏可解释性和可扩展性。

Method: 结合经验和语义相似度，采用贪心成对合并策略统一不同数据集的标签空间。

Result: 成功合并三个NER数据集，并在金融领域低资源场景中提升性能。

Conclusion: 该方法为多源NER语料库整合提供了高效、可解释且可扩展的解决方案。

Abstract: Named Entity Recognition (NER) is a fundamental task in natural language
processing. It remains a research hotspot due to its wide applicability across
domains. Although recent advances in deep learning have significantly improved
NER performance, they rely heavily on large, high-quality annotated datasets.
However, building these datasets is expensive and time-consuming, posing a
major bottleneck for further research. Current dataset merging approaches
mainly focus on strategies like manual label mapping or constructing label
graphs, which lack interpretability and scalability. To address this, we
propose an automatic label alignment method based on label similarity. The
method combines empirical and semantic similarities, using a greedy pairwise
merging strategy to unify label spaces across different datasets. Experiments
are conducted in two stages: first, merging three existing NER datasets into a
unified corpus with minimal impact on NER performance; second, integrating this
corpus with a small-scale, self-built dataset in the financial domain. The
results show that our method enables effective dataset merging and enhances NER
performance in the low-resource financial domain. This study presents an
efficient, interpretable, and scalable solution for integrating multi-source
NER corpora.

</details>


### [240] [The ReQAP System for Question Answering over Personal Information](https://arxiv.org/abs/2508.06880)
*Philipp Christmann,Gerhard Weikum*

Main category: cs.CL

TL;DR: ReQAP系统通过递归分解问题并构建操作树，利用轻量级语言模型处理异构数据源，支持复杂问题的回答，并提供答案追踪功能。


<details>
  <summary>Details</summary>
Motivation: 用户设备上存在大量结构化与非结构化数据，但缺乏有效工具支持复杂问题的回答。

Method: 递归分解问题，构建操作树，利用轻量级语言模型进行问题解释与操作执行。

Result: 系统能处理复杂问题，并通过操作树追踪答案来源，提升用户信任。

Conclusion: ReQAP系统通过智能分解与追踪功能，有效支持复杂问题回答，增强用户信任。

Abstract: Personal information is abundant on users' devices, from structured data in
calendar, shopping records or fitness tools, to unstructured contents in mail
and social media posts. This works presents the ReQAP system that supports
users with answers for complex questions that involve filters, joins and
aggregation over heterogeneous sources. The unique trait of ReQAP is that it
recursively decomposes questions and incrementally builds an operator tree for
execution. Both the question interpretation and the individual operators make
smart use of light-weight language models, with judicious fine-tuning. The demo
showcases the rich functionality for advanced user questions, and also offers
detailed tracking of how the answers are computed by the operators in the
execution tree. Being able to trace answers back to the underlying sources is
vital for human comprehensibility and user trust in the system.

</details>


### [241] [Score Before You Speak: Improving Persona Consistency in Dialogue Generation using Response Quality Scores](https://arxiv.org/abs/2508.06886)
*Arpita Saggar,Jonathan C. Darling,Vania Dimitrova,Duygu Sarikaya,David C. Hogg*

Main category: cs.CL

TL;DR: 论文提出了一种名为SBS（Score-Before-Speaking）的新框架，通过将对话生成与质量评分统一学习，显著提升了基于角色的对话生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有对话数据多样性不足，导致大型语言模型在角色一致性对话生成中表现不佳。

Method: SBS框架通过训练模型将增强的对话响应与质量评分关联，并在推理时利用这一知识。采用名词替换增强数据，语义相似度评分作为质量代理。

Result: 在PERSONA-CHAT和ConvAI2数据集上的实验表明，SBS显著提升了角色一致性对话生成效果。

Conclusion: SBS框架通过评分条件训练，有效提升了对话模型在角色一致性任务中的表现。

Abstract: Persona-based dialogue generation is an important milestone towards building
conversational artificial intelligence. Despite the ever-improving capabilities
of large language models (LLMs), effectively integrating persona fidelity in
conversations remains challenging due to the limited diversity in existing
dialogue data. We propose a novel framework SBS (Score-Before-Speaking), which
outperforms previous methods and yields improvements for both million and
billion-parameter models. Unlike previous methods, SBS unifies the learning of
responses and their relative quality into a single step. The key innovation is
to train a dialogue model to correlate augmented responses with a quality score
during training and then leverage this knowledge at inference. We use
noun-based substitution for augmentation and semantic similarity-based scores
as a proxy for response quality. Through extensive experiments with benchmark
datasets (PERSONA-CHAT and ConvAI2), we show that score-conditioned training
allows existing models to better capture a spectrum of persona-consistent
dialogues. Our ablation studies also demonstrate that including scores in the
input prompt during training is superior to conventional training setups. Code
and further details are available at
https://arpita2512.github.io/score_before_you_speak

</details>


### [242] [Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection](https://arxiv.org/abs/2508.06913)
*Siyuan Li,Xi Lin,Guangyan Li,Zehao Liu,Aodu Wulianghai,Li Ding,Jun Wu,Jianhua Li*

Main category: cs.CL

TL;DR: SentiDetect通过分析情感分布稳定性差异检测LLM生成文本，优于现有方法，且在对抗攻击和文本长度变化中表现更稳健。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法泛化性差且易受攻击，而LLM生成文本情感分布更稳定，人类文本情感变化更大。

Method: 提出两个互补指标（情感分布一致性和情感分布保持性），量化情感和语义变化下的稳定性。

Result: 在五个数据集和多种LLM上测试，F1分数显著提升（如Gemini-1.5-Pro提升16%），且对抗攻击和文本长度变化中表现更优。

Conclusion: SentiDetect在检测LLM生成文本上优于现有方法，具有更高的鲁棒性和泛化能力。

Abstract: The rapid advancement of large language models (LLMs) has resulted in
increasingly sophisticated AI-generated content, posing significant challenges
in distinguishing LLM-generated text from human-written language. Existing
detection methods, primarily based on lexical heuristics or fine-tuned
classifiers, often suffer from limited generalizability and are vulnerable to
paraphrasing, adversarial perturbations, and cross-domain shifts. In this work,
we propose SentiDetect, a model-agnostic framework for detecting LLM-generated
text by analyzing the divergence in sentiment distribution stability. Our
method is motivated by the empirical observation that LLM outputs tend to
exhibit emotionally consistent patterns, whereas human-written texts display
greater emotional variability. To capture this phenomenon, we define two
complementary metrics: sentiment distribution consistency and sentiment
distribution preservation, which quantify stability under sentiment-altering
and semantic-preserving transformations. We evaluate SentiDetect on five
diverse datasets and a range of advanced LLMs,including Gemini-1.5-Pro,
Claude-3, GPT-4-0613, and LLaMa-3.3. Experimental results demonstrate its
superiority over state-of-the-art baselines, with over 16% and 11% F1 score
improvements on Gemini-1.5-Pro and GPT-4-0613, respectively. Moreover,
SentiDetect also shows greater robustness to paraphrasing, adversarial attacks,
and text length variations, outperforming existing detectors in challenging
scenarios.

</details>


### [243] [Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer Extraction](https://arxiv.org/abs/2508.06971)
*Mohamed Basem,Islam Oshallah,Ali Hamdi,Khaled Shaban,Hozaifa Kassab*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的两阶段框架，用于解决《古兰经》问答中的段落检索和答案提取问题，结合了微调的语言模型和指令调优的大语言模型，取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 《古兰经》问答因古典阿拉伯语的语言复杂性和宗教文本的语义丰富性而具有独特挑战，需要一种高效的方法来解决低资源环境下的问题。

Method: 采用两阶段框架：1) 通过集成微调的阿拉伯语言模型实现段落检索；2) 使用指令调优的大语言模型进行少样本提示，完成答案提取。

Result: 在Quran QA 2023共享任务中，检索阶段的MAP@10为0.3128，MRR@10为0.5763；提取阶段的pAP@10为0.669，显著优于之前的方法。

Conclusion: 结合模型集成和指令调优语言模型，能有效解决专业领域低资源问答的挑战。

Abstract: Quranic Question Answering presents unique challenges due to the linguistic
complexity of Classical Arabic and the semantic richness of religious texts. In
this paper, we propose a novel two-stage framework that addresses both passage
retrieval and answer extraction. For passage retrieval, we ensemble fine-tuned
Arabic language models to achieve superior ranking performance. For answer
extraction, we employ instruction-tuned large language models with few-shot
prompting to overcome the limitations of fine-tuning on small datasets. Our
approach achieves state-of-the-art results on the Quran QA 2023 Shared Task,
with a MAP@10 of 0.3128 and MRR@10 of 0.5763 for retrieval, and a pAP@10 of
0.669 for extraction, substantially outperforming previous methods. These
results demonstrate that combining model ensembling and instruction-tuned
language models effectively addresses the challenges of low-resource question
answering in specialized domains.

</details>


### [244] [Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models](https://arxiv.org/abs/2508.06974)
*Zhijun Tu,Hanting Chen,Siqi Liu,Chuanjian Liu,Jian Li,Jie Hu,Yunhe Wang*

Main category: cs.CL

TL;DR: 本文提出了一种渐进式训练方法，将预训练模型的浮点权重平滑转换为1-bit表示，结合二进制感知初始化和双尺度补偿，显著提升了1-bit LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接从零开始训练1-bit LLM，未能充分利用预训练模型，导致高训练成本和精度下降。本文旨在解决这一问题。

Method: 采用渐进式训练方法，结合二进制感知初始化和双尺度补偿，平滑转换浮点权重为1-bit表示。

Result: 实验表明，该方法在多种规模的LLM上优于现有方法，实现了高性能1-bit量化。

Conclusion: 通过利用预训练模型，本文方法避免了昂贵的从头训练，实现了高效的1-bit LLM量化。

Abstract: 1-bit LLM quantization offers significant advantages in reducing storage and
computational costs. However, existing methods typically train 1-bit LLMs from
scratch, failing to fully leverage pre-trained models. This results in high
training costs and notable accuracy degradation. We identify that the large gap
between full precision and 1-bit representations makes direct adaptation
difficult. In this paper, we introduce a consistent progressive training for
both forward and backward, smoothly converting the floating-point weights into
the binarized ones. Additionally, we incorporate binary-aware initialization
and dual-scaling compensation to reduce the difficulty of progressive training
and improve the performance. Experimental results on LLMs of various sizes
demonstrate that our method outperforms existing approaches. Our results show
that high-performance 1-bit LLMs can be achieved using pre-trained models,
eliminating the need for expensive training from scratch.

</details>


### [245] [Vec2Summ: Text Summarization via Probabilistic Sentence Embeddings](https://arxiv.org/abs/2508.07017)
*Mao Li,Fred Conrad,Johann Gagnon-Bartsch*

Main category: cs.CL

TL;DR: Vec2Summ是一种新颖的抽象摘要方法，通过语义压缩任务将文档集合表示为语义嵌入空间中的单个均值向量，并通过生成语言模型将其解码为自然语言摘要。


<details>
  <summary>Details</summary>
Motivation: 解决基于LLM的摘要方法在上下文长度限制、生成可控性和扩展性方面的局限性。

Method: 使用语义嵌入空间的均值向量表示文档集合，通过嵌入反转和随机采样生成摘要。

Result: Vec2Summ在主题集中、顺序无关的语料上生成连贯摘要，性能与直接LLM摘要相当，但细节较少。

Conclusion: Vec2Summ在需要扩展性、语义控制和语料级抽象的场合具有潜力。

Abstract: We propose Vec2Summ, a novel method for abstractive summarization that frames
the task as semantic compression. Vec2Summ represents a document collection
using a single mean vector in the semantic embedding space, capturing the
central meaning of the corpus. To reconstruct fluent summaries, we perform
embedding inversion -- decoding this mean vector into natural language using a
generative language model. To improve reconstruction quality and capture some
degree of topical variability, we introduce stochasticity by sampling from a
Gaussian distribution centered on the mean. This approach is loosely analogous
to bagging in ensemble learning, where controlled randomness encourages more
robust and varied outputs. Vec2Summ addresses key limitations of LLM-based
summarization methods. It avoids context-length constraints, enables
interpretable and controllable generation via semantic parameters, and scales
efficiently with corpus size -- requiring only $O(d + d^2)$ parameters.
Empirical results show that Vec2Summ produces coherent summaries for topically
focused, order-invariant corpora, with performance comparable to direct LLM
summarization in terms of thematic coverage and efficiency, albeit with less
fine-grained detail. These results underscore Vec2Summ's potential in settings
where scalability, semantic control, and corpus-level abstraction are
prioritized.

</details>


### [246] [SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages](https://arxiv.org/abs/2508.07069)
*Muhammad Dehan Al Kautsar,Aswin Candra,Muhammad Alif Al Hakim,Maxalmina Satria Kahfi,Fajri Koto,Alham Fikri Aji,Peerat Limkonchotiwat,Ekapol Chuangsuwanich,Genta Indra Winata*

Main category: cs.CL

TL;DR: SEADialogues是一个针对东南亚文化多样性的对话数据集，包含八种语言和六个国家的对话，旨在填补现有闲聊数据集中文化细微差别的空白。


<details>
  <summary>Details</summary>
Motivation: 现有闲聊数据集普遍忽视文化差异，而东南亚地区文化多样性丰富但资源匮乏，因此需要构建一个文化相关的对话数据集。

Method: 收集了六个东南亚国家的八种语言对话，每段对话包含人物属性和两个反映日常生活文化主题的内容。

Result: 发布了SEADialogues数据集，支持多轮对话研究，推动文化感知和以人为本的大语言模型发展。

Conclusion: SEADialogues为文化敏感的对话系统研究提供了重要资源，有助于提升对话代理的文化适应性和个性化。

Abstract: Although numerous datasets have been developed to support dialogue systems,
most existing chit-chat datasets overlook the cultural nuances inherent in
natural human conversations. To address this gap, we introduce SEADialogues, a
culturally grounded dialogue dataset centered on Southeast Asia, a region with
over 700 million people and immense cultural diversity. Our dataset features
dialogues in eight languages from six Southeast Asian countries, many of which
are low-resource despite having sizable speaker populations. To enhance
cultural relevance and personalization, each dialogue includes persona
attributes and two culturally grounded topics that reflect everyday life in the
respective communities. Furthermore, we release a multi-turn dialogue dataset
to advance research on culturally aware and human-centric large language
models, including conversational dialogue agents.

</details>


### [247] [BharatBBQ: A Multilingual Bias Benchmark for Question Answering in the Indian Context](https://arxiv.org/abs/2508.07090)
*Aditya Tomar,Nihar Ranjan Sahoo,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 论文介绍了BharatBBQ，一个针对印度多语言和文化背景的偏见评估基准，填补了现有偏见评测工具的不足。


<details>
  <summary>Details</summary>
Motivation: 现有偏见评测工具（如BBQ）主要关注西方语境，无法适用于印度多语言和文化背景，因此需要开发本土化评测工具。

Method: 通过翻译和验证，将49,108个单语示例扩展到8种语言的392,864个示例，评估5种多语言模型在零样本和少样本设置下的偏见表现。

Result: 研究发现，印度语言中的偏见比英语更显著，且不同语言和社会类别中均存在持续偏见。

Conclusion: 强调需要基于语言和文化的偏见评测基准，以确保AI系统的公平性。

Abstract: Evaluating social biases in language models (LMs) is crucial for ensuring
fairness and minimizing the reinforcement of harmful stereotypes in AI systems.
Existing benchmarks, such as the Bias Benchmark for Question Answering (BBQ),
primarily focus on Western contexts, limiting their applicability to the Indian
context. To address this gap, we introduce BharatBBQ, a culturally adapted
benchmark designed to assess biases in Hindi, English, Marathi, Bengali, Tamil,
Telugu, Odia, and Assamese. BharatBBQ covers 13 social categories, including 3
intersectional groups, reflecting prevalent biases in the Indian sociocultural
landscape. Our dataset contains 49,108 examples in one language that are
expanded using translation and verification to 392,864 examples in eight
different languages. We evaluate five multilingual LM families across zero and
few-shot settings, analyzing their bias and stereotypical bias scores. Our
findings highlight persistent biases across languages and social categories and
often amplified biases in Indian languages compared to English, demonstrating
the necessity of linguistically and culturally grounded benchmarks for bias
evaluation.

</details>


### [248] [Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning](https://arxiv.org/abs/2508.07101)
*Lijie Yang,Zhihao Zhang,Arti Jain,Shijie Cao,Baihong Yuan,Yiwei Chen,Zhihao Jia,Ravi Netravali*

Main category: cs.CL

TL;DR: LessIsMore是一种无需训练的稀疏注意力机制，通过全局注意力模式提升推理任务的效率和准确性，减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决现有稀疏注意力机制在长生成推理中因累积误差导致的准确性下降问题，同时避免高令牌保留率或昂贵的重新训练需求。

Method: 利用全局注意力模式聚合局部注意力头的令牌选择，结合近期上下文信息，实现跨头令牌统一排名。

Result: 在多种推理任务和基准测试中，LessIsMore保持或提升准确性，平均解码速度提升1.1倍，令牌关注数量减少2倍。

Conclusion: LessIsMore在效率和准确性上优于现有稀疏注意力方法，适用于大规模推理模型。

Abstract: Large reasoning models achieve strong performance through test-time scaling
but incur substantial computational overhead, particularly from excessive token
generation when processing short input prompts. While sparse attention
mechanisms can reduce latency and memory usage, existing approaches suffer from
significant accuracy degradation due to accumulated errors during
long-generation reasoning. These methods generally require either high token
retention rates or expensive retraining. We introduce LessIsMore, a
training-free sparse attention mechanism for reasoning tasks, which leverages
global attention patterns rather than relying on traditional head-specific
local optimizations. LessIsMore aggregates token selections from local
attention heads with recent contextual information, enabling unified cross-head
token ranking for future decoding layers. This unified selection improves
generalization and efficiency by avoiding the need to maintain separate token
subsets per head. Evaluation across diverse reasoning tasks and benchmarks
shows that LessIsMore preserves -- and in some cases improves -- accuracy while
achieving a $1.1\times$ average decoding speed-up compared to full attention.
Moreover, LessIsMore attends to $2\times$ fewer tokens without accuracy loss,
achieving a $1.13\times$ end-to-end speed-up compared to existing sparse
attention methods.

</details>


### [249] [Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution](https://arxiv.org/abs/2508.07111)
*Falaah Arif Khan,Nivedha Sivakumar,Yinong Oliver Wang,Katherine Metcalf,Cezanne Camacho,Barry-John Theobald,Luca Zappella,Nicholas Apostoloff*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLMs）在交叉性偏见中的表现，提出了新的评估基准WinoIdentity，并发现模型在双重弱势身份上表现最不确定。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在资源受限的决策支持中广泛应用，但其可能加剧社会偏见，尤其是在交叉性身份中。现有研究多关注单一维度的偏见评估，缺乏对交叉性偏见的系统性分析。

Method: 通过扩展WinoBias数据集，创建了包含25个人口统计标记的WinoIdentity基准，评估了5个LLMs在245,700个提示中的表现，并提出了一种新的不公平度量指标Coreference Confidence Disparity。

Result: 发现模型在某些交叉性身份（如双重弱势身份）上的置信度差异高达40%，且模型对反刻板印象情境中的身份表现最不确定。

Conclusion: LLMs的优异表现可能更多依赖记忆而非逻辑推理，其价值对齐和有效性存在独立缺陷，可能导致社会危害。

Abstract: Large language models (LLMs) have achieved impressive performance, leading to
their widespread adoption as decision-support tools in resource-constrained
contexts like hiring and admissions. There is, however, scientific consensus
that AI systems can reflect and exacerbate societal biases, raising concerns
about identity-based harm when used in critical social contexts. Prior work has
laid a solid foundation for assessing bias in LLMs by evaluating demographic
disparities in different language reasoning tasks. In this work, we extend
single-axis fairness evaluations to examine intersectional bias, recognizing
that when multiple axes of discrimination intersect, they create distinct
patterns of disadvantage. We create a new benchmark called WinoIdentity by
augmenting the WinoBias dataset with 25 demographic markers across 10
attributes, including age, nationality, and race, intersected with binary
gender, yielding 245,700 prompts to evaluate 50 distinct bias patterns.
Focusing on harms of omission due to underrepresentation, we investigate bias
through the lens of uncertainty and propose a group (un)fairness metric called
Coreference Confidence Disparity which measures whether models are more or less
confident for some intersectional identities than others. We evaluate five
recently published LLMs and find confidence disparities as high as 40% along
various demographic attributes including body type, sexual orientation and
socio-economic status, with models being most uncertain about
doubly-disadvantaged identities in anti-stereotypical settings. Surprisingly,
coreference confidence decreases even for hegemonic or privileged markers,
indicating that the recent impressive performance of LLMs is more likely due to
memorization than logical reasoning. Notably, these are two independent
failures in value alignment and validity that can compound to cause social
harm.

</details>


### [250] [Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens](https://arxiv.org/abs/2508.07143)
*Anna Seo Gyeong Choi,Hoon Choi*

Main category: cs.CL

TL;DR: 论文探讨了自动语音识别（ASR）系统中的偏见问题，指出其对非标准方言的误识别不仅是技术问题，更是一种对边缘化语言社区的不尊重。


<details>
  <summary>Details</summary>
Motivation: 研究ASR系统的公平性，揭示其对非标准方言的误识别如何加剧历史不公，并提出其独特的伦理维度。

Method: 通过哲学视角区分道德中立的分类与有害歧视，分析ASR系统如何将前者转化为后者，并识别三个独特的伦理维度。

Result: 发现ASR偏见涉及时间负担、对话流中断及身份认同问题，现有技术公平指标无法捕捉。

Conclusion: 解决ASR偏见需技术干预外，还需承认多样语言表达的合法性，以尊重语言多样性和说话者自主性。

Abstract: Automatic Speech Recognition (ASR) systems now mediate countless
human-technology interactions, yet research on their fairness implications
remains surprisingly limited. This paper examines ASR bias through a
philosophical lens, arguing that systematic misrecognition of certain speech
varieties constitutes more than a technical limitation -- it represents a form
of disrespect that compounds historical injustices against marginalized
linguistic communities. We distinguish between morally neutral classification
(discriminate1) and harmful discrimination (discriminate2), demonstrating how
ASR systems can inadvertently transform the former into the latter when they
consistently misrecognize non-standard dialects. We identify three unique
ethical dimensions of speech technologies that differentiate ASR bias from
other algorithmic fairness concerns: the temporal burden placed on speakers of
non-standard varieties ("temporal taxation"), the disruption of conversational
flow when systems misrecognize speech, and the fundamental connection between
speech patterns and personal/cultural identity. These factors create asymmetric
power relationships that existing technical fairness metrics fail to capture.
The paper analyzes the tension between linguistic standardization and pluralism
in ASR development, arguing that current approaches often embed and reinforce
problematic language ideologies. We conclude that addressing ASR bias requires
more than technical interventions; it demands recognition of diverse speech
varieties as legitimate forms of expression worthy of technological
accommodation. This philosophical reframing offers new pathways for developing
ASR systems that respect linguistic diversity and speaker autonomy.

</details>


### [251] [Gradient Surgery for Safe LLM Fine-Tuning](https://arxiv.org/abs/2508.07172)
*Biao Yi,Jiahao Li,Baolei Zhang,Lihai Nie,Tong Li,Tiansheng Huang,Zheli Liu*

Main category: cs.CL

TL;DR: 论文提出了一种名为SafeGrad的新方法，通过梯度手术解决恶意样本在微调过程中破坏大语言模型安全对齐的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在恶意样本比例较高时防御能力急剧下降，原因是用户任务梯度与安全目标梯度冲突。

Method: SafeGrad通过投影用户任务梯度的有害部分到安全梯度的正交平面，并结合KL散度对齐损失增强鲁棒性。

Result: 实验表明，SafeGrad在各种LLM和数据集上提供了最先进的防御，即使在高恶意比例下也能保持安全性和任务性能。

Conclusion: SafeGrad有效解决了安全微调中的梯度冲突问题，实现了安全与任务性能的平衡。

Abstract: Fine-tuning-as-a-Service introduces a critical vulnerability where a few
malicious examples mixed into the user's fine-tuning dataset can compromise the
safety alignment of Large Language Models (LLMs). While a recognized paradigm
frames safe fine-tuning as a multi-objective optimization problem balancing
user task performance with safety alignment, we find existing solutions are
critically sensitive to the harmful ratio, with defenses degrading sharply as
harmful ratio increases. We diagnose that this failure stems from conflicting
gradients, where the user-task update directly undermines the safety objective.
To resolve this, we propose SafeGrad, a novel method that employs gradient
surgery. When a conflict is detected, SafeGrad nullifies the harmful component
of the user-task gradient by projecting it onto the orthogonal plane of the
alignment gradient, allowing the model to learn the user's task without
sacrificing safety. To further enhance robustness and data efficiency, we
employ a KL-divergence alignment loss that learns the rich, distributional
safety profile of the well-aligned foundation model. Extensive experiments show
that SafeGrad provides state-of-the-art defense across various LLMs and
datasets, maintaining robust safety even at high harmful ratios without
compromising task fidelity.

</details>


### [252] [Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual Large Language Models](https://arxiv.org/abs/2508.07173)
*Leyi Pan,Zheyu Fu,Yunpeng Zhai,Shuchang Tao,Sheng Guan,Shiyu Huang,Lingzhe Zhang,Zhaoyang Liu,Bolin Ding,Felix Henry,Lijie Wen,Aiwei Liu*

Main category: cs.CL

TL;DR: 论文提出了首个全面的OLLM安全评估基准Omni-SafetyBench，填补了现有评估工具的空白，并揭示了当前OLLM在安全性和跨模态一致性上的严重不足。


<details>
  <summary>Details</summary>
Motivation: 现有评估工具无法满足OLLM（多模态大语言模型）的安全评估需求，尤其是对音频-视觉联合输入和跨模态一致性的评估。

Method: 提出了Omni-SafetyBench基准，包含24种模态组合和972个样本，并设计了Safety-score（基于C-ASR和C-RR）和CMSC-score两种评估指标。

Result: 评估了10个OLLM，发现：（1）无模型在安全性和一致性上均表现优异；（2）复杂输入（尤其是音频-视觉联合）会削弱安全防御；（3）部分模型在某些模态上得分极低（如0.14）。

Conclusion: 研究强调了提升OLLM安全性的紧迫性，并为未来改进提供了基础。

Abstract: The rise of Omni-modal Large Language Models (OLLMs), which integrate visual
and auditory processing with text, necessitates robust safety evaluations to
mitigate harmful outputs. However, no dedicated benchmarks currently exist for
OLLMs, and prior benchmarks designed for other LLMs lack the ability to assess
safety performance under audio-visual joint inputs or cross-modal safety
consistency. To fill this gap, we introduce Omni-SafetyBench, the first
comprehensive parallel benchmark for OLLM safety evaluation, featuring 24
modality combinations and variations with 972 samples each, including dedicated
audio-visual harm cases. Considering OLLMs' comprehension challenges with
complex omni-modal inputs and the need for cross-modal consistency evaluation,
we propose tailored metrics: a Safety-score based on conditional Attack Success
Rate (C-ASR) and Refusal Rate (C-RR) to account for comprehension failures, and
a Cross-Modal Safety Consistency Score (CMSC-score) to measure consistency
across modalities. Evaluating 6 open-source and 4 closed-source OLLMs reveals
critical vulnerabilities: (1) no model excels in both overall safety and
consistency, with only 3 models achieving over 0.6 in both metrics and top
performer scoring around 0.8; (2) safety defenses weaken with complex inputs,
especially audio-visual joints; (3) severe weaknesses persist, with some models
scoring as low as 0.14 on specific modalities. Our benchmark and metrics
highlight urgent needs for enhanced OLLM safety, providing a foundation for
future improvements.

</details>


### [253] [Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback](https://arxiv.org/abs/2508.07178)
*Kejin Liu,Junhong Lian,Xiang Ao,Ningtao Wang,Xing Fu,Yu Cheng,Weiqiang Wang,Xinyu Liu*

Main category: cs.CL

TL;DR: 论文提出了一种通过去噪虚假兴趣的个性化标题生成框架（PHG-DIF），解决了历史点击流中噪声对生成质量的影响，并在新数据集DT-PENS上取得了最优效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了历史点击流中的个性化无关噪声，导致生成的标题偏离用户真实兴趣。

Method: PHG-DIF采用双阶段过滤去除噪声，并通过多级时间融合动态建模用户兴趣。

Result: 实验表明PHG-DIF显著减少了噪声影响，提升了标题质量，在DT-PENS数据集上达到SOTA。

Conclusion: PHG-DIF通过去噪和多级兴趣建模，有效提升了个性化标题生成的准确性和质量。

Abstract: Accurate personalized headline generation hinges on precisely capturing user
interests from historical behaviors. However, existing methods neglect
personalized-irrelevant click noise in entire historical clickstreams, which
may lead to hallucinated headlines that deviate from genuine user preferences.
In this paper, we reveal the detrimental impact of click noise on personalized
generation quality through rigorous analysis in both user and news dimensions.
Based on these insights, we propose a novel Personalized Headline Generation
framework via Denoising Fake Interests from Implicit Feedback (PHG-DIF).
PHG-DIF first employs dual-stage filtering to effectively remove clickstream
noise, identified by short dwell times and abnormal click bursts, and then
leverages multi-level temporal fusion to dynamically model users' evolving and
multi-faceted interests for precise profiling. Moreover, we release DT-PENS, a
new benchmark dataset comprising the click behavior of 1,000 carefully curated
users and nearly 10,000 annotated personalized headlines with historical dwell
time annotations. Extensive experiments demonstrate that PHG-DIF substantially
mitigates the adverse effects of click noise and significantly improves
headline quality, achieving state-of-the-art (SOTA) results on DT-PENS. Our
framework implementation and dataset are available at
https://github.com/liukejin-up/PHG-DIF.

</details>


### [254] [Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks](https://arxiv.org/abs/2508.07179)
*Jiaqi Yin,Yi-Wei Chen,Meng-Lung Lee,Xiya Liu*

Main category: cs.CL

TL;DR: 提出了一种自动化框架，用于从多语言企业管道脚本中提取细粒度模式谱系，解决语义漂移问题，并通过新评估指标SLiCE和基准测试验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 企业数据管道中的语义漂移导致数据可重现性和治理问题，影响下游服务（如RAG和text-to-SQL系统）的实用性。

Method: 提出自动化提取框架，识别源模式、源表、转换逻辑和聚合操作，并引入SLiCE评估指标和1,700条标注谱系的基准。

Result: 实验表明，模式谱系提取性能随模型规模和提示技术提升，32B开源模型在单次推理中表现接近GPT系列。

Conclusion: 该框架为实际应用中部署模式感知代理提供了可扩展且经济的解决方案。

Abstract: Enterprise data pipelines, characterized by complex transformations across
multiple programming languages, often cause a semantic disconnect between
original metadata and downstream data. This "semantic drift" compromises data
reproducibility and governance, and impairs the utility of services like
retrieval-augmented generation (RAG) and text-to-SQL systems. To address this,
a novel framework is proposed for the automated extraction of fine-grained
schema lineage from multilingual enterprise pipeline scripts. This method
identifies four key components: source schemas, source tables, transformation
logic, and aggregation operations, creating a standardized representation of
data transformations. For the rigorous evaluation of lineage quality, this
paper introduces the Schema Lineage Composite Evaluation (SLiCE), a metric that
assesses both structural correctness and semantic fidelity. A new benchmark is
also presented, comprising 1,700 manually annotated lineages from real-world
industrial scripts. Experiments were conducted with 12 language models, from
1.3B to 32B small language models (SLMs) to large language models (LLMs) like
GPT-4o and GPT-4.1. The results demonstrate that the performance of schema
lineage extraction scales with model size and the sophistication of prompting
techniques. Specially, a 32B open-source model, using a single reasoning trace,
can achieve performance comparable to the GPT series under standard prompting.
This finding suggests a scalable and economical approach for deploying
schema-aware agents in practical applications.

</details>


### [255] [DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention](https://arxiv.org/abs/2508.07185)
*Kabir Khan,Priya Sharma,Arjun Mehta,Neha Gupta,Ravi Narayanan*

Main category: cs.CL

TL;DR: DySK-Attn框架通过动态知识图谱和稀疏注意力机制，使大语言模型能高效整合实时知识，解决静态知识过时问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的知识静态且易过时，重新训练成本高，现有知识编辑技术效率低且可能产生副作用。

Method: 结合动态知识图谱与稀疏知识注意力机制，实现粗到细粒度搜索，高效聚焦相关子集。

Result: 在时效性问答任务中，DySK-Attn在事实准确性和计算效率上显著优于基线方法。

Conclusion: DySK-Attn为构建动态更新的语言模型提供了可扩展且高效的解决方案。

Abstract: Large Language Models (LLMs) suffer from a critical limitation: their
knowledge is static and quickly becomes outdated. Retraining these massive
models is computationally prohibitive, while existing knowledge editing
techniques can be slow and may introduce unforeseen side effects. To address
this, we propose DySK-Attn, a novel framework that enables LLMs to efficiently
integrate real-time knowledge from a dynamic external source. Our approach
synergizes an LLM with a dynamic Knowledge Graph (KG) that can be updated
instantaneously. The core of our framework is a sparse knowledge attention
mechanism, which allows the LLM to perform a coarse-to-fine grained search,
efficiently identifying and focusing on a small, highly relevant subset of
facts from the vast KG. This mechanism avoids the high computational cost of
dense attention over the entire knowledge base and mitigates noise from
irrelevant information. We demonstrate through extensive experiments on
time-sensitive question-answering tasks that DySK-Attn significantly
outperforms strong baselines, including standard Retrieval-Augmented Generation
(RAG) and model editing techniques, in both factual accuracy for updated
knowledge and computational efficiency. Our framework offers a scalable and
effective solution for building LLMs that can stay current with the
ever-changing world.

</details>


### [256] [Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment](https://arxiv.org/abs/2508.07195)
*Yanru Sun,Emadeldeen Eldele,Zongxia Xie,Yucheng Wang,Wenzhe Niu,Qinghua Hu,Chee Keong Kwoh,Min Wu*

Main category: cs.CL

TL;DR: TALON框架通过建模时间异质性和语义对齐，提升LLM在时间序列预测中的表现。


<details>
  <summary>Details</summary>
Motivation: LLMs在自然语言处理中表现优异，但直接用于时间序列预测存在时间模式异质性和模态差异的挑战。

Method: 提出Heterogeneous Temporal Encoder分割时间序列，并设计Semantic Alignment Module对齐特征与LLM表示。

Result: 在七个基准测试中，TALON表现最佳，MSE平均提升11%。

Conclusion: 结合模式感知和语义感知设计，能有效提升LLM在时间序列预测中的性能。

Abstract: Large Language Models (LLMs) have recently demonstrated impressive
capabilities in natural language processing due to their strong generalization
and sequence modeling capabilities. However, their direct application to time
series forecasting remains challenging due to two fundamental issues: the
inherent heterogeneity of temporal patterns and the modality gap between
continuous numerical signals and discrete language representations. In this
work, we propose TALON, a unified framework that enhances LLM-based forecasting
by modeling temporal heterogeneity and enforcing semantic alignment.
Specifically, we design a Heterogeneous Temporal Encoder that partitions
multivariate time series into structurally coherent segments, enabling
localized expert modeling across diverse temporal patterns. To bridge the
modality gap, we introduce a Semantic Alignment Module that aligns temporal
features with LLM-compatible representations, enabling effective integration of
time series into language-based models while eliminating the need for
handcrafted prompts during inference. Extensive experiments on seven real-world
benchmarks demonstrate that TALON achieves superior performance across all
datasets, with average MSE improvements of up to 11\% over recent
state-of-the-art methods. These results underscore the effectiveness of
incorporating both pattern-aware and semantic-aware designs when adapting LLMs
for time series forecasting. The code is available at:
https://github.com/syrGitHub/TALON.

</details>


### [257] [Enhancing Rumor Detection Methods with Propagation Structure Infused Language Model](https://arxiv.org/abs/2508.07209)
*Chaoqun Cui,Siyuan Li,Kunkun Ma,Caiyan Jia*

Main category: cs.CL

TL;DR: 论文提出了一种名为PEP的继续预训练策略，通过预测帖子间的传播关系（如根、分支、父级）来提升PLMs在社交媒体谣言检测中的性能。同时发布了TwitterCorpus和两个未标注数据集，并训练了专用模型SoLM，实验显示PEP显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有PLMs在社交媒体任务（如谣言检测）中表现不佳，原因包括预训练语料与社交文本不匹配、对社交符号处理不足，以及预训练任务未建模传播结构中的用户互动。

Method: 提出PEP策略，通过预测帖子间的传播关系（根、分支、父级）来捕捉立场和情感的交互。同时发布TwitterCorpus和两个未标注数据集，并训练专用模型SoLM。

Result: PEP显著提升了谣言检测性能，在基准数据集上比基线模型提高了1.0-3.7%的准确率，甚至超越当前最优方法。SoLM也取得了竞争性结果。

Conclusion: PEP策略有效学习了帖子交互的判别性特征，显著提升了PLMs在社交媒体任务中的性能。

Abstract: Pretrained Language Models (PLMs) have excelled in various Natural Language
Processing tasks, benefiting from large-scale pretraining and self-attention
mechanism's ability to capture long-range dependencies. However, their
performance on social media application tasks like rumor detection remains
suboptimal. We attribute this to mismatches between pretraining corpora and
social texts, inadequate handling of unique social symbols, and pretraining
tasks ill-suited for modeling user engagements implicit in propagation
structures. To address these issues, we propose a continue pretraining strategy
called Post Engagement Prediction (PEP) to infuse information from propagation
structures into PLMs. PEP makes models to predict root, branch, and parent
relations between posts, capturing interactions of stance and sentiment crucial
for rumor detection. We also curate and release large-scale Twitter corpus:
TwitterCorpus (269GB text), and two unlabeled claim conversation datasets with
propagation structures (UTwitter and UWeibo). Utilizing these resources and PEP
strategy, we train a Twitter-tailored PLM called SoLM. Extensive experiments
demonstrate PEP significantly boosts rumor detection performance across
universal and social media PLMs, even in few-shot scenarios. On benchmark
datasets, PEP enhances baseline models by 1.0-3.7\% accuracy, even enabling it
to outperform current state-of-the-art methods on multiple datasets. SoLM
alone, without high-level modules, also achieves competitive results,
highlighting the strategy's effectiveness in learning discriminative post
interaction features.

</details>


### [258] [How Does a Deep Neural Network Look at Lexical Stress?](https://arxiv.org/abs/2508.07229)
*Itai Allouche,Itay Asael,Rotem Rousso,Vered Dassa,Ann Bradlow,Seung-Eun Kim,Matthew Goldrick,Joseph Keshet*

Main category: cs.CL

TL;DR: 该论文研究了神经网络在语音处理中如何决策的问题，通过分析英语双音节词的词汇重音，揭示了CNN模型如何利用频谱特征预测重音位置。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络在语音处理中的决策依据，特别是在词汇重音预测中的可解释性。

Method: 使用自动构建的英语双音节词数据集，训练多个CNN架构预测重音位置，并采用LRP技术分析模型决策。

Result: 模型在测试数据上达到92%准确率，LRP显示重音预测主要受重音音节频谱特征影响，尤其是元音的第一和第二共振峰。

Conclusion: 深度学习能从自然数据中学习分布式重音线索，扩展了传统基于严格控制刺激的语音学研究。

Abstract: Despite their success in speech processing, neural networks often operate as
black boxes, prompting the question: what informs their decisions, and how can
we interpret them? This work examines this issue in the context of lexical
stress. A dataset of English disyllabic words was automatically constructed
from read and spontaneous speech. Several Convolutional Neural Network (CNN)
architectures were trained to predict stress position from a spectrographic
representation of disyllabic words lacking minimal stress pairs (e.g., initial
stress WAllet, final stress exTEND), achieving up to 92% accuracy on held-out
test data. Layerwise Relevance Propagation (LRP), a technique for CNN
interpretability analysis, revealed that predictions for held-out minimal pairs
(PROtest vs. proTEST ) were most strongly influenced by information in stressed
versus unstressed syllables, particularly the spectral properties of stressed
vowels. However, the classifiers also attended to information throughout the
word. A feature-specific relevance analysis is proposed, and its results
suggest that our best-performing classifier is strongly influenced by the
stressed vowel's first and second formants, with some evidence that its pitch
and third formant also contribute. These results reveal deep learning's ability
to acquire distributed cues to stress from naturally occurring data, extending
traditional phonetic work based around highly controlled stimuli.

</details>


### [259] [Prompt Tuning for Few-Shot Continual Learning Named Entity Recognition](https://arxiv.org/abs/2508.07248)
*Zhe Ren*

Main category: cs.CL

TL;DR: 论文提出了一种基于提示调优和记忆演示模板的方法，解决了少样本持续学习命名实体识别任务中的蒸馏困境。


<details>
  <summary>Details</summary>
Motivation: 在少样本持续学习命名实体识别任务中，由于新类别实体稀缺和旧类别信息缺失，导致模型难以泛化并陷入蒸馏困境。

Method: 采用可扩展的锚词导向提示调优（APT）范式，并结合记忆演示模板（MDT）策略，以增强少样本场景下的性能。

Result: 实验表明，该方法在少样本持续学习命名实体识别任务中取得了竞争性表现。

Conclusion: 通过提示调优和记忆演示模板的结合，有效解决了少样本蒸馏困境，提升了模型性能。

Abstract: Knowledge distillation has been successfully applied to Continual Learning
Named Entity Recognition (CLNER) tasks, by using a teacher model trained on
old-class data to distill old-class entities present in new-class data as a
form of regularization, thereby avoiding catastrophic forgetting. However, in
Few-Shot CLNER (FS-CLNER) tasks, the scarcity of new-class entities makes it
difficult for the trained model to generalize during inference. More
critically, the lack of old-class entity information hinders the distillation
of old knowledge, causing the model to fall into what we refer to as the
Few-Shot Distillation Dilemma. In this work, we address the above challenges
through a prompt tuning paradigm and memory demonstration template strategy.
Specifically, we designed an expandable Anchor words-oriented Prompt Tuning
(APT) paradigm to bridge the gap between pre-training and fine-tuning, thereby
enhancing performance in few-shot scenarios. Additionally, we incorporated
Memory Demonstration Templates (MDT) into each training instance to provide
replay samples from previous tasks, which not only avoids the Few-Shot
Distillation Dilemma but also promotes in-context learning. Experiments show
that our approach achieves competitive performances on FS-CLNER.

</details>


### [260] [The 2D+ Dynamic Articulatory Model DYNARTmo: Tongue-Palate Contact Area Estimation](https://arxiv.org/abs/2508.07262)
*Bernd J. Kröger*

Main category: cs.CL

TL;DR: 扩展了二维动态发音模型DYNARTmo，通过集成三维腭穹内部表示，从中矢舌轮廓估计舌-腭接触区域。


<details>
  <summary>Details</summary>
Motivation: 改进发音模型的准确性，使其能更好地模拟舌-腭接触，并支持语音科学教育和言语治疗。

Method: 实现两种腭穹几何模型（半椭圆和余弦曲线），通过解析计算侧向接触点，生成类似电子腭图的2D+可视化。

Result: 模型支持三种同步视图（矢状、声门和腭视图），适用于静态和动态发音展示。

Conclusion: 未来工作将增加面部（唇）视图并实现发音-声学合成，以量化评估模型真实性。

Abstract: This paper describes an extension of the two-dimensional dynamic articulatory
model DYNARTmo by integrating an internal three-dimensional representation of
the palatal dome to estimate tongue-palate contact areas from midsagittal
tongue contours. Two alternative dome geometries - a half-ellipse and a cosine
based profile - are implemented to model lateral curvature in the coronal
plane. Using these geometries, lateral contact points are analytically computed
for each anterior-posterior position, enabling the generation of
electropalatography-like visualizations within the 2D+ framework. The enhanced
model supports three synchronized views (sagittal, glottal, and palatal) for
static and dynamic (animated) articulation displays, suitable for speech
science education and speech therapy. Future work includes adding a facial
(lip) view and implementing articulatory-to-acoustic synthesis to
quantitatively evaluate model realism.

</details>


### [261] [Incorporating Contextual Paralinguistic Understanding in Large Speech-Language Models](https://arxiv.org/abs/2508.07273)
*Qiongqiong Wang,Hardik B. Sailor,Jeremy H. M. Wong,Tianchi Liu,Shuo Sun,Wenyu Zhang,Muhammad Huzaifah,Nancy Chen,Ai Ti Aw*

Main category: cs.CL

TL;DR: 论文提出两种方法（显式和隐式）将上下文副语言信息融入语音大语言模型训练，显著提升了模型在共情推理上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前语音大语言模型在共情推理上表现不足，主要因缺乏结合上下文内容和副语言线索的训练数据。

Method: 1. 显式方法：直接提供副语言元数据（如情感标注）给模型；2. 隐式方法：自动生成结合情感标注和语音转录的训练问答对。

Result: 隐式方法在人类标注的问答基准上提升38.41%，结合显式方法后达46.02%，验证了模型在上下文副语言理解上的有效性。

Conclusion: 两种方法的结合显著提升了语音大语言模型的共情推理能力，同时验证了模型评估的可靠性。

Abstract: Current large speech language models (Speech-LLMs) often exhibit limitations
in empathetic reasoning, primarily due to the absence of training datasets that
integrate both contextual content and paralinguistic cues. In this work, we
propose two approaches to incorporate contextual paralinguistic information
into model training: (1) an explicit method that provides paralinguistic
metadata (e.g., emotion annotations) directly to the LLM, and (2) an implicit
method that automatically generates novel training question-answer (QA) pairs
using both categorical and dimensional emotion annotations alongside speech
transcriptions. Our implicit method boosts performance (LLM-judged) by 38.41%
on a human-annotated QA benchmark, reaching 46.02% when combined with the
explicit approach, showing effectiveness in contextual paralinguistic
understanding. We also validate the LLM judge by demonstrating its correlation
with classification metrics, providing support for its reliability.

</details>


### [262] [MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory](https://arxiv.org/abs/2508.07279)
*Vasudha Varadarajan,Hui Xu,Rebecca Astrid Boehme,Mariam Marlan Mirstrom,Sverker Sikstrom,H. Andrew Schwartz*

Main category: cs.CL

TL;DR: MAQuA是一种自适应提问框架，通过结合多结果建模和IRT理论，优化心理健康筛查问题，显著减少问题数量并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）在心理健康评估中因频繁提问增加用户负担，且效率不足，需更高效的多维度筛查方法。

Method: MAQuA结合多结果建模、IRT理论和因子分析，动态选择最具信息量的问题，优化诊断信息。

Result: 实证显示，MAQuA将问题数量减少50-87%，在抑郁和饮食障碍等领域表现稳健。

Conclusion: MAQuA为高效、可扩展的心理健康筛查提供了有力工具，推动LLM在临床工作中的应用。

Abstract: Recent advances in large language models (LLMs) offer new opportunities for
scalable, interactive mental health assessment, but excessive querying by LLMs
burdens users and is inefficient for real-world screening across
transdiagnostic symptom profiles. We introduce MAQuA, an adaptive
question-asking framework for simultaneous, multidimensional mental health
screening. Combining multi-outcome modeling on language responses with item
response theory (IRT) and factor analysis, MAQuA selects the questions with
most informative responses across multiple dimensions at each turn to optimize
diagnostic information, improving accuracy and potentially reducing response
burden. Empirical results on a novel dataset reveal that MAQuA reduces the
number of assessment questions required for score stabilization by 50-87%
compared to random ordering (e.g., achieving stable depression scores with 71%
fewer questions and eating disorder scores with 85% fewer questions). MAQuA
demonstrates robust performance across both internalizing (depression, anxiety)
and externalizing (substance use, eating disorder) domains, with early stopping
strategies further reducing patient time and burden. These findings position
MAQuA as a powerful and efficient tool for scalable, nuanced, and interactive
mental health screening, advancing the integration of LLM-based agents into
real-world clinical workflows.

</details>


### [263] ["Pull or Not to Pull?'': Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas](https://arxiv.org/abs/2508.07284)
*Junchen Ding,Penghao Jiang,Zihao Xu,Ziqi Ding,Yichen Zhu,Jiaojiao Jiang,Yuekang Li*

Main category: cs.CL

TL;DR: 论文研究了14种大型语言模型（LLM）在27种电车问题场景下的道德推理能力，发现模型在不同道德哲学框架下表现差异显著，尤其是利他主义、公平和美德伦理框架下表现较好，但在亲属关系、法律或自我利益框架下表现不佳。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地参与道德敏感决策，理解其道德推理过程变得至关重要。

Method: 通过因子提示协议，收集了3,780个二元决策和自然语言解释，分析了决策的坚定性、解释一致性、公众道德对齐及对无关道德线索的敏感性。

Result: 研究发现，增强推理模型更果断且解释更结构化，但未必更符合人类共识；某些道德框架下模型表现较好，而其他框架下则表现不佳。

Conclusion: 建议将道德推理作为LLM对齐的主要维度，并呼吁建立标准化基准以评估LLM的决策过程和原因。

Abstract: As large language models (LLMs) increasingly mediate ethically sensitive
decisions, understanding their moral reasoning processes becomes imperative.
This study presents a comprehensive empirical evaluation of 14 leading LLMs,
both reasoning enabled and general purpose, across 27 diverse trolley problem
scenarios, framed by ten moral philosophies, including utilitarianism,
deontology, and altruism. Using a factorial prompting protocol, we elicited
3,780 binary decisions and natural language justifications, enabling analysis
along axes of decisional assertiveness, explanation answer consistency, public
moral alignment, and sensitivity to ethically irrelevant cues. Our findings
reveal significant variability across ethical frames and model types: reasoning
enhanced models demonstrate greater decisiveness and structured justifications,
yet do not always align better with human consensus. Notably, "sweet zones"
emerge in altruistic, fairness, and virtue ethics framings, where models
achieve a balance of high intervention rates, low explanation conflict, and
minimal divergence from aggregated human judgments. However, models diverge
under frames emphasizing kinship, legality, or self interest, often producing
ethically controversial outcomes. These patterns suggest that moral prompting
is not only a behavioral modifier but also a diagnostic tool for uncovering
latent alignment philosophies across providers. We advocate for moral reasoning
to become a primary axis in LLM alignment, calling for standardized benchmarks
that evaluate not just what LLMs decide, but how and why.

</details>


### [264] [Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking](https://arxiv.org/abs/2508.07286)
*Jian Chen,Jinbao Tian,Yankui Li,Zhou Li*

Main category: cs.CL

TL;DR: 论文提出ARCE方法，通过LLM生成简单解释性知识（Cote）来增强RoBERTa模型，在AEC领域的NER任务中取得最优性能（Macro-F1 77.20%）。


<details>
  <summary>Details</summary>
Motivation: 解决AEC领域NER任务中预训练模型因领域术语和复杂关系导致的性能受限问题，避免传统方法的高成本。

Method: 使用LLM生成简单解释性知识（Cote），并用于增量预训练RoBERTa模型，再微调下游任务。

Result: 在AEC基准数据集上达到77.20%的Macro-F1，优于复杂角色推理知识。

Conclusion: 简单解释性知识对AEC领域NER任务更有效，ARCE方法为领域适应提供了高效解决方案。

Abstract: Accurate information extraction from specialized texts is a critical
challenge, particularly for named entity recognition (NER) in the architecture,
engineering, and construction (AEC) domain to support automated rule checking
(ARC). The performance of standard pre-trained models is often constrained by
the domain gap, as they struggle to interpret the specialized terminology and
complex relational contexts inherent in AEC texts. Although this issue can be
mitigated by further pre-training on large, human-curated domain corpora, as
exemplified by methods like ARCBERT, this approach is both labor-intensive and
cost-prohibitive. Consequently, leveraging large language models (LLMs) for
automated knowledge generation has emerged as a promising alternative. However,
the optimal strategy for generating knowledge that can genuinely enhance
smaller, efficient models remains an open question. To address this, we propose
ARCE (augmented RoBERTa with contextualized elucidations), a novel approach
that systematically explores and optimizes this generation process. ARCE
employs an LLM to first generate a corpus of simple, direct explanations, which
we term Cote, and then uses this corpus to incrementally pre-train a RoBERTa
model prior to its fine-tuning on the downstream task. Our extensive
experiments show that ARCE establishes a new state-of-the-art on a benchmark
AEC dataset, achieving a Macro-F1 score of 77.20%. This result also reveals a
key finding: simple, explanation-based knowledge proves surprisingly more
effective than complex, role-based rationales for this task. The code is
publicly available at:https://github.com/nxcc-lab/ARCE.

</details>


### [265] [CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation](https://arxiv.org/abs/2508.07295)
*Yexing Du,Kaiyuan Liu,Youcheng Pan,Zheng Chu,Bo Yang,Xiaocheng Feng,Yang Xiang,Ming Liu*

Main category: cs.CL

TL;DR: 论文提出了一种跨语言和跨模态的事实性基准（CCFQA），用于评估多模态大语言模型（MLLMs）在多语言和语音输入中的可靠性。实验显示当前MLLMs在此基准上表现不佳，并提出了一种少样本迁移学习策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注英语文本或视觉模态，缺乏对多语言和语音输入的事实性评估，因此需要填补这一空白。

Method: 设计了包含8种语言的平行语音-文本事实性问题的CCFQA基准，并提出了一种少样本迁移学习策略，将英语QA能力迁移到多语言语音QA任务中。

Result: 当前MLLMs在CCFQA基准上表现不佳，但提出的迁移学习策略仅用5样本训练即可达到与GPT-4o-mini-Audio竞争的性能。

Conclusion: CCFQA为开发更可靠的多语言和语音理解能力的MLLMs提供了基础研究资源。

Abstract: As Large Language Models (LLMs) are increasingly popularized in the
multilingual world, ensuring hallucination-free factuality becomes markedly
crucial. However, existing benchmarks for evaluating the reliability of
Multimodal Large Language Models (MLLMs) predominantly focus on textual or
visual modalities with a primary emphasis on English, which creates a gap in
evaluation when processing multilingual input, especially in speech. To bridge
this gap, we propose a novel \textbf{C}ross-lingual and \textbf{C}ross-modal
\textbf{F}actuality benchmark (\textbf{CCFQA}). Specifically, the CCFQA
benchmark contains parallel speech-text factual questions across 8 languages,
designed to systematically evaluate MLLMs' cross-lingual and cross-modal
factuality capabilities. Our experimental results demonstrate that current
MLLMs still face substantial challenges on the CCFQA benchmark. Furthermore, we
propose a few-shot transfer learning strategy that effectively transfers the
Question Answering (QA) capabilities of LLMs in English to multilingual Spoken
Question Answering (SQA) tasks, achieving competitive performance with
GPT-4o-mini-Audio using just 5-shot training. We release CCFQA as a
foundational research resource to promote the development of MLLMs with more
robust and reliable speech understanding capabilities. Our code and dataset are
available at https://github.com/yxduir/ccfqa.

</details>


### [266] [HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways](https://arxiv.org/abs/2508.07308)
*Cristian Cosentino,Annamaria Defilippo,Marco Dossena,Christopher Irwin,Sara Joubbi,Pietro Liò*

Main category: cs.CL

TL;DR: HealthBranches是一个专为评估大型语言模型（LLMs）在医学问答中复杂推理能力而设计的新基准数据集。


<details>
  <summary>Details</summary>
Motivation: 旨在为高风险的医疗领域开发更可信、可解释且临床可靠的LLMs提供基础，并作为教育资源。

Method: 通过半自动化流程将医学来源的决策路径转化为真实患者案例，包含问题和答案，覆盖17个医疗主题的4,063个案例。

Result: 数据集支持开放式和多项选择题形式，并包含完整的推理路径，可用于评估LLMs的多步推理能力。

Conclusion: HealthBranches为LLMs在医疗领域的应用提供了可靠的评估工具和教育资源。

Abstract: HealthBranches is a novel benchmark dataset for medical Question-Answering
(Q&A), specifically designed to evaluate complex reasoning in Large Language
Models (LLMs). This dataset is generated through a semi-automated pipeline that
transforms explicit decision pathways from medical source into realistic
patient cases with associated questions and answers. Covering 4,063 case
studies across 17 healthcare topics, each data point is based on clinically
validated reasoning chains. HealthBranches supports both open-ended and
multiple-choice question formats and uniquely includes the full reasoning path
for each Q&A. Its structured design enables robust evaluation of LLMs'
multi-step inference capabilities, including their performance in structured
Retrieval-Augmented Generation (RAG) contexts. HealthBranches establishes a
foundation for the development of more trustworthy, interpretable, and
clinically reliable LLMs in high-stakes domains while also serving as a
valuable resource for educational purposes.

</details>


### [267] [ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering](https://arxiv.org/abs/2508.07321)
*Shubhra Ghosh,Abhilekh Borah,Aditya Kumar Guru,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: 论文提出了一种名为ObfusQAte的新技术，并基于此构建了ObfusQA框架，用于测试大语言模型（LLMs）在面对模糊化问题时的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在事实问答（QA）方面表现出色，但其在面对模糊化问题时的表现尚未被系统研究。

Method: 通过ObfusQA框架，从三个维度（命名实体间接、干扰项间接和上下文过载）对LLMs进行多层次的模糊化测试。

Result: 研究发现，LLMs在面对模糊化问题时容易失败或产生幻觉性回答。

Conclusion: ObfusQA为评估LLMs的鲁棒性和适应性提供了全面基准，并公开了ObfusQAte以推动相关研究。

Abstract: The rapid proliferation of Large Language Models (LLMs) has significantly
contributed to the development of equitable AI systems capable of factual
question-answering (QA). However, no known study tests the LLMs' robustness
when presented with obfuscated versions of questions. To systematically
evaluate these limitations, we propose a novel technique, ObfusQAte and,
leveraging the same, introduce ObfusQA, a comprehensive, first of its kind,
framework with multi-tiered obfuscation levels designed to examine LLM
capabilities across three distinct dimensions: (i) Named-Entity Indirection,
(ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these
fine-grained distinctions in language, ObfusQA provides a comprehensive
benchmark for evaluating LLM robustness and adaptability. Our study observes
that LLMs exhibit a tendency to fail or generate hallucinated responses when
confronted with these increasingly nuanced variations. To foster research in
this direction, we make ObfusQAte publicly available.

</details>


### [268] [Strategies of Code-switching in Human-Machine Dialogs](https://arxiv.org/abs/2508.07325)
*Dean Geckt,Melinda Fricke,Shuly Wintner*

Main category: cs.CL

TL;DR: 研究探讨了多语言者在代码转换中的行为，通过开发能完成地图任务的聊天机器人，验证了代码转换策略对任务完成的影响。


<details>
  <summary>Details</summary>
Motivation: 理解多语言者在代码转换中的行为特征，以及代码转换策略对交流的影响。

Method: 开发能完成地图任务的聊天机器人，采用不同代码转换策略进行实验。

Result: 参与者更喜欢可预测的代码转换行为，随机或不规范的代码转换会降低任务完成率和体验。

Conclusion: 研究展示了多语言技术用于双语研究的潜力，但也指出了技术不成熟可能带来的负面影响。

Abstract: Most people are multilingual, and most multilinguals code-switch, yet the
characteristics of code-switched language are not fully understood. We
developed a chatbot capable of completing a Map Task with human participants
using code-switched Spanish and English. In two experiments, we prompted the
bot to code-switch according to different strategies, examining (1) the
feasibility of such experiments for investigating bilingual language use, and
(2) whether participants would be sensitive to variations in discourse and
grammatical patterns. Participants generally enjoyed code-switching with our
bot as long as it produced predictable code-switching behavior; when
code-switching was random or ungrammatical (as when producing unattested
incongruent mixed-language noun phrases, such as `la fork'), participants
enjoyed the task less and were less successful at completing it. These results
underscore the potential downsides of deploying insufficiently developed
multilingual language technology, while also illustrating the promise of such
technology for conducting research on bilingual language use.

</details>


### [269] [Think Before You Talk: Enhancing Meaningful Dialogue Generation in Full-Duplex Speech Language Models with Planning-Inspired Text Guidance](https://arxiv.org/abs/2508.07375)
*Wenqian Cui,Lei Zhu,Xiaohui Li,Zhihan Guo,Haoli Bai,Lu Hou,Irwin King*

Main category: cs.CL

TL;DR: FD-SLMs旨在实现实时语音交互，但面临对话能力下降问题。TurnGuide通过动态分段和文本引导解决了插入时机和长度问题，显著提升了对话质量。


<details>
  <summary>Details</summary>
Motivation: 解决FD-SLMs在语音交互中对话能力下降的问题，尤其是插入时机和长度对齐的挑战。

Method: 提出TurnGuide方法，动态分段语音并生成文本引导，优化对话流畅性和语义连贯性。

Result: 实验表明，TurnGuide显著提升了FD-SLMs的对话能力，保持了自然交互的流畅性。

Conclusion: TurnGuide有效解决了FD-SLMs的关键问题，为实时语音交互提供了更自然的解决方案。

Abstract: Full-Duplex Speech Language Models (FD-SLMs) are specialized foundation
models designed to enable natural, real-time spoken interactions by modeling
complex conversational dynamics such as interruptions, backchannels, and
overlapping speech, and End-to-end (e2e) FD-SLMs leverage real-world
double-channel conversational data to capture nuanced two-speaker dialogue
patterns for human-like interactions. However, they face a critical challenge
-- their conversational abilities often degrade compared to pure-text
conversation due to prolonged speech sequences and limited high-quality spoken
dialogue data. While text-guided speech generation could mitigate these issues,
it suffers from timing and length issues when integrating textual guidance into
double-channel audio streams, disrupting the precise time alignment essential
for natural interactions. To address these challenges, we propose TurnGuide, a
novel planning-inspired approach that mimics human conversational planning by
dynamically segmenting assistant speech into dialogue turns and generating
turn-level text guidance before speech output, which effectively resolves both
insertion timing and length challenges. Extensive experiments demonstrate our
approach significantly improves e2e FD-SLMs' conversational abilities, enabling
them to generate semantically meaningful and coherent speech while maintaining
natural conversational flow. Demos are available at
https://dreamtheater123.github.io/TurnGuide-Demo/. Code will be available at
https://github.com/dreamtheater123/TurnGuide.

</details>


### [270] [Grounding Multilingual Multimodal LLMs With Cultural Knowledge](https://arxiv.org/abs/2508.07414)
*Jean de Dieu Nyandwi,Yueqi Song,Simran Khanuja,Graham Neubig*

Main category: cs.CL

TL;DR: 论文提出了一种数据驱动的方法，通过构建文化知识数据集CulturalGround，训练多模态大语言模型CulturalPangea，显著提升了模型在文化相关任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在低资源语言和文化实体理解上的不足。

Method: 利用Wikidata知识图谱收集文化相关图像，生成多语言视觉问答数据，训练模型CulturalPangea。

Result: CulturalPangea在文化相关多语言多模态任务中表现最佳，平均提升5.0分，且不影响主流任务性能。

Conclusion: 文化知识嵌入方法有效缩小了多模态模型的文化鸿沟，为全球包容性系统提供了可行路径。

Abstract: Multimodal Large Language Models excel in high-resource settings, but often
misinterpret long-tail cultural entities and underperform in low-resource
languages. To address this gap, we propose a data-centric approach that
directly grounds MLLMs in cultural knowledge. Leveraging a large scale
knowledge graph from Wikidata, we collect images that represent culturally
significant entities, and generate synthetic multilingual visual question
answering data. The resulting dataset, CulturalGround, comprises 22 million
high-quality, culturally-rich VQA pairs spanning 42 countries and 39 languages.
We train an open-source MLLM CulturalPangea on CulturalGround, interleaving
standard multilingual instruction-tuning data to preserve general abilities.
CulturalPangea achieves state-of-the-art performance among open models on
various culture-focused multilingual multimodal benchmarks, outperforming prior
models by an average of 5.0 without degrading results on mainstream
vision-language tasks. Our findings show that our targeted, culturally grounded
approach could substantially narrow the cultural gap in MLLMs and offer a
practical path towards globally inclusive multimodal systems.

</details>


### [271] [Let's Revise Step-by-Step: A Unified Local Search Framework for Code Generation with LLMs](https://arxiv.org/abs/2508.07434)
*Zhiyi Lyu,Jianguo Huang,Yanchen Deng,Steven Hoi,Bo An*

Main category: cs.CL

TL;DR: ReLoc是一个统一的局部搜索框架，通过逐步代码修订提高代码生成效率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成方法（如树搜索和改进方法）存在效率低、扩展性差和奖励信号不明确等问题。

Method: 提出ReLoc框架，包含初始代码起草、邻域代码生成、候选评估和代码更新四个组件，并开发专用修订奖励模型。

Result: 实验表明ReLoc在多样化代码生成任务中表现优异，显著优于现有方法。

Conclusion: ReLoc通过局部修订和精细奖励信号，有效解决了代码生成中的效率和扩展性问题。

Abstract: Large Language Models (LLMs) with inference-time scaling techniques show
promise for code generation, yet face notable efficiency and scalability
challenges. Construction-based tree-search methods suffer from rapid growth in
tree size, high token consumption, and lack of anytime property. In contrast,
improvement-based methods offer better performance but often struggle with
uninformative reward signals and inefficient search strategies. In this work,
we propose \textbf{ReLoc}, a unified local search framework which effectively
performs step-by-step code revision. Specifically, ReLoc explores a series of
local revisions through four key algorithmic components: initial code drafting,
neighborhood code generation, candidate evaluation, and incumbent code
updating, each of which can be instantiated with specific decision rules to
realize different local search algorithms such as Hill Climbing (HC) or Genetic
Algorithm (GA). Furthermore, we develop a specialized revision reward model
that evaluates code quality based on revision distance to produce fine-grained
preferences that guide the local search toward more promising candidates.
Finally, our extensive experimental results demonstrate that our approach
achieves superior performance across diverse code generation tasks,
significantly outperforming both construction-based tree search as well as the
state-of-the-art improvement-based code generation methods.

</details>


### [272] [Positional Biases Shift as Inputs Approach Context Window Limits](https://arxiv.org/abs/2508.07479)
*Blerta Veseli,Julian Chibane,Mariya Toneva,Alexander Koller*

Main category: cs.CL

TL;DR: 研究发现，大语言模型（LLMs）在长输入中存在位置偏差，如‘迷失在中间’（LiM）效应。当输入占模型上下文窗口的50%以内时，LiM效应最强；超过后，首因效应减弱，而近因效应保持稳定。此外，检索能力是LLMs推理的前提，位置偏差主要源于检索。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在长输入中的位置偏差（如LiM效应）及其表现条件，以改进长上下文任务和评估方法。

Method: 采用相对输入长度（相对于模型上下文窗口）进行综合分析，研究位置偏差的变化。

Result: LiM效应在输入占50%上下文窗口时最强；超过后，首因效应减弱，近因效应稳定，出现基于距离的偏差。检索能力是推理的前提。

Conclusion: 研究揭示了LLMs在长输入中的位置偏差特性，为长上下文任务设计、LLM基准测试和评估方法提供了重要启示。

Abstract: Large Language Models (LLMs) often struggle to use information across long
inputs effectively. Prior work has identified positional biases, such as the
Lost in the Middle (LiM) effect, where models perform better when information
appears at the beginning (primacy bias) or end (recency bias) of the input,
rather than in the middle. However, long-context studies have not consistently
replicated these effects, raising questions about their intensity and the
conditions under which they manifest. To address this, we conducted a
comprehensive analysis using relative rather than absolute input lengths,
defined with respect to each model's context window. Our findings reveal that
the LiM effect is strongest when inputs occupy up to 50% of a model's context
window. Beyond that, the primacy bias weakens, while recency bias remains
relatively stable. This effectively eliminates the LiM effect; instead, we
observe a distance-based bias, where model performance is better when relevant
information is closer to the end of the input. Furthermore, our results suggest
that successful retrieval is a prerequisite for reasoning in LLMs, and that the
observed positional biases in reasoning are largely inherited from retrieval.
These insights have implications for long-context tasks, the design of future
LLM benchmarks, and evaluation methodologies for LLMs handling extended inputs.

</details>


### [273] [ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models](https://arxiv.org/abs/2508.07484)
*Archchana Sindhujan,Shenbin Qian,Chan Chi Chun Matthew,Constantin Orasan,Diptesh Kanojia*

Main category: cs.CL

TL;DR: ALOPE是一个自适应层优化框架，通过调整Transformer层的表示来提升基于LLM的机器翻译质量评估（QE）性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的QE系统因预训练目标和低资源语言问题表现不佳，需要改进。

Method: ALOPE结合低秩适配器（LoRA）和回归任务头，动态加权多层表示，并采用多头回归策略。

Result: ALOPE在多种LLM-based QE方法中表现更优，中间层表示更适合跨语言QE任务。

Conclusion: ALOPE为LLM-based MT框架提供了可扩展的QE能力，代码和模型已开源。

Abstract: Large Language Models (LLMs) have shown remarkable performance across a wide
range of natural language processing tasks. Quality Estimation (QE) for Machine
Translation (MT), which assesses the quality of a source-target pair without
relying on reference translations, remains a challenging cross-lingual task for
LLMs. The challenges stem from the inherent limitations of existing LLM-based
QE systems, which are pre-trained for causal language modelling rather than
regression-specific tasks, further elevated by the presence of low-resource
languages given pre-training data distribution. This paper introduces ALOPE, an
adaptive layer-optimization framework designed to enhance LLM-based QE by
restructuring Transformer representations through layer-wise adaptation for
improved regression-based prediction. Our framework integrates low-rank
adapters (LoRA) with regression task heads, leveraging selected pre-trained
Transformer layers for improved cross-lingual alignment. In addition to the
layer-specific adaptation, ALOPE introduces two strategies-dynamic weighting,
which adaptively combines representations from multiple layers, and multi-head
regression, which aggregates regression losses from multiple heads for QE. Our
framework shows improvements over various existing LLM-based QE approaches.
Empirical evidence suggests that intermediate Transformer layers in LLMs
provide contextual representations that are more aligned with the cross-lingual
nature of the QE task. We make resultant models and framework code publicly
available for further research, also allowing existing LLM-based MT frameworks
to be scaled with QE capabilities.

</details>


### [274] [Augmenting Bias Detection in LLMs Using Topological Data Analysis](https://arxiv.org/abs/2508.07516)
*Keshav Varadarajan,Tananun Songdechakraiwut*

Main category: cs.CL

TL;DR: 本文提出了一种基于拓扑数据分析的方法，用于识别GPT-2中哪些注意力头导致了对特定身份群体的偏见。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏检测大型语言模型中哪些部分对特定群体偏见负责的方法。

Method: 使用拓扑数据分析方法，分析GPT-2中的注意力头对StereoSet数据集中身份群体偏见的贡献。

Result: 发现偏见（如性别或职业）集中在某些热点注意力头中，并提出了一种度量方法。

Conclusion: 该方法可用于识别特定群体的偏见，未来可扩展用于去偏大型语言模型。

Abstract: Recently, many bias detection methods have been proposed to determine the
level of bias a large language model captures. However, tests to identify which
parts of a large language model are responsible for bias towards specific
groups remain underdeveloped. In this study, we present a method using
topological data analysis to identify which heads in GPT-2 contribute to the
misrepresentation of identity groups present in the StereoSet dataset. We find
that biases for particular categories, such as gender or profession, are
concentrated in attention heads that act as hot spots. The metric we propose
can also be used to determine which heads capture bias for a specific group
within a bias category, and future work could extend this method to help
de-bias large language models.

</details>


### [275] [Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews](https://arxiv.org/abs/2508.07517)
*Joseph T. Colonel,Baihan Lin*

Main category: cs.CL

TL;DR: ThemeClouds是一种基于大型语言模型（LLM）的可视化工具，用于生成主题化、参与者加权的词云，解决了传统频率词云在对话语境中的不足。


<details>
  <summary>Details</summary>
Motivation: 传统频率词云在对话语境中效果不佳，无法有效捕捉语义相关概念，限制了早期分析的实用性。

Method: 利用LLM识别语料库中的概念级主题，并统计每个主题被多少独特参与者提及，生成基于提及广度的词云。

Result: ThemeClouds在用户研究中比频率词云和主题建模基线（如LDA、BERTopic）更能捕捉到实际设备问题。

Conclusion: ThemeClouds为定性工作流提供了透明、可控的LLM辅助工具，并支持交互式分析（如差异词云）。

Abstract: Word clouds are a common way to summarize qualitative interviews, yet
traditional frequency-based methods often fail in conversational contexts: they
surface filler words, ignore paraphrase, and fragment semantically related
ideas. This limits their usefulness in early-stage analysis, when researchers
need fast, interpretable overviews of what participant actually said. We
introduce ThemeClouds, an open-source visualization tool that uses large
language models (LLMs) to generate thematic, participant-weighted word clouds
from dialogue transcripts. The system prompts an LLM to identify concept-level
themes across a corpus and then counts how many unique participants mention
each topic, yielding a visualization grounded in breadth of mention rather than
raw term frequency. Researchers can customize prompts and visualization
parameters, providing transparency and control. Using interviews from a user
study comparing five recording-device configurations (31 participants; 155
transcripts, Whisper ASR), our approach surfaces more actionable device
concerns than frequency clouds and topic-modeling baselines (e.g., LDA,
BERTopic). We discuss design trade-offs for integrating LLM assistance into
qualitative workflows, implications for interpretability and researcher agency,
and opportunities for interactive analyses such as per-condition contrasts
(``diff clouds'').

</details>


### [276] [From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR](https://arxiv.org/abs/2508.07534)
*Jia Deng,Jie Chen,Zhipeng Chen,Daixuan Cheng,Fei Bai,Beichen Zhang,Yinqian Min,Yanzipeng Gao,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 本文系统研究了RLVR中LLM的探索能力，包括探索空间塑造、熵-性能交换和RL性能优化，旨在为RLVR系统提供基础框架。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法依赖规则反馈指导LLM生成复杂推理链，但其探索行为机制尚未深入研究。

Method: 研究分为三部分：探索空间塑造（量化LLM能力边界）、熵-性能交换分析（训练阶段、实例和token级模式）、RL性能优化（将探索收益转化为性能提升）。

Result: 通过统一现有见解与新实证证据，提出了RLVR系统的理论基础。

Conclusion: 本研究为RLVR系统的探索能力提供了系统性分析框架，有助于进一步优化LLM的推理能力。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of large language
models (LLMs). Unlike traditional RL approaches, RLVR leverages rule-based
feedback to guide LLMs in generating and refining complex reasoning chains -- a
process critically dependent on effective exploration strategies. While prior
work has demonstrated RLVR's empirical success, the fundamental mechanisms
governing LLMs' exploration behaviors remain underexplored. This technical
report presents a systematic investigation of exploration capacities in RLVR,
covering four main aspects: (1) exploration space shaping, where we develop
quantitative metrics to characterize LLMs' capability boundaries; (2)
entropy-performance exchange, analyzed across training stages, individual
instances, and token-level patterns; and (3) RL performance optimization,
examining methods to effectively translate exploration gains into measurable
improvements. By unifying previously identified insights with new empirical
evidence, this work aims to provide a foundational framework for advancing RLVR
systems.

</details>


### [277] [IBPS: Indian Bail Prediction System](https://arxiv.org/abs/2508.07592)
*Puspesh Kumar Srivastava,Uddeshya Raj,Praveen Patel,/Shubham Kumar Nigam,Noel Shallum,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: 本文介绍了印度保释预测系统（IBPS），一个基于AI的框架，旨在通过预测保释结果和生成法律依据，帮助解决印度法院保释决策中的主观性、延迟和不一致问题。


<details>
  <summary>Details</summary>
Motivation: 印度法院的保释决策存在主观性、延迟和不一致，导致监狱中75%的未审判囚犯多为社会经济弱势群体，加剧人权问题和司法积压。

Method: 作者构建了一个包含150,430个高等法院保释判决的大规模数据集，并利用参数高效技术微调大型语言模型，结合法律条文（RAG）评估模型性能。

Result: 结果显示，结合法律知识的模型显著优于基线，具有高准确性和解释质量，并能泛化到法律专家独立标注的测试集。

Conclusion: IBPS提供了一个透明、可扩展且可复现的解决方案，支持数据驱动的法律辅助，减少保释延迟，促进印度司法系统的程序公平。

Abstract: Bail decisions are among the most frequently adjudicated matters in Indian
courts, yet they remain plagued by subjectivity, delays, and inconsistencies.
With over 75% of India's prison population comprising undertrial prisoners,
many from socioeconomically disadvantaged backgrounds, the lack of timely and
fair bail adjudication exacerbates human rights concerns and contributes to
systemic judicial backlog. In this paper, we present the Indian Bail Prediction
System (IBPS), an AI-powered framework designed to assist in bail
decision-making by predicting outcomes and generating legally sound rationales
based solely on factual case attributes and statutory provisions. We curate and
release a large-scale dataset of 150,430 High Court bail judgments, enriched
with structured annotations such as age, health, criminal history, crime
category, custody duration, statutes, and judicial reasoning. We fine-tune a
large language model using parameter-efficient techniques and evaluate its
performance across multiple configurations, with and without statutory context,
and with RAG. Our results demonstrate that models fine-tuned with statutory
knowledge significantly outperform baselines, achieving strong accuracy and
explanation quality, and generalize well to a test set independently annotated
by legal experts. IBPS offers a transparent, scalable, and reproducible
solution to support data-driven legal assistance, reduce bail delays, and
promote procedural fairness in the Indian judicial system.

</details>


### [278] [Keyword-Centric Prompting for One-Shot Event Detection with Self-Generated Rationale Enhancements](https://arxiv.org/abs/2508.07598)
*Ziheng Li,Zhi-Hong Deng*

Main category: cs.CL

TL;DR: 论文提出KeyCP++方法，通过关键词驱动的思维链提示改进LLM在事件检测中的表现，解决传统ICL的不足。


<details>
  <summary>Details</summary>
Motivation: LLM在事件检测中因对触发词理解不足和过度解释而表现不佳，传统ICL方法无法有效纠正。

Method: 提出KeyCP++，通过关键词驱动的思维链提示，自动标注输入文本与检测结果间的逻辑差距，生成深度推理依据。

Result: 实验证明KeyCP++在单样本事件检测中显著提升性能。

Conclusion: KeyCP++通过关键词驱动的推理，有效减少LLM对关键词的过度依赖，提升事件检测能力。

Abstract: Although the LLM-based in-context learning (ICL) paradigm has demonstrated
considerable success across various natural language processing tasks, it
encounters challenges in event detection. This is because LLMs lack an accurate
understanding of event triggers and tend to make over-interpretation, which
cannot be effectively corrected through in-context examples alone. In this
paper, we focus on the most challenging one-shot setting and propose KeyCP++, a
keyword-centric chain-of-thought prompting approach. KeyCP++ addresses the
weaknesses of conventional ICL by automatically annotating the logical gaps
between input text and detection results for the demonstrations. Specifically,
to generate in-depth and meaningful rationale, KeyCP++ constructs a trigger
discrimination prompting template. It incorporates the exemplary triggers
(a.k.a keywords) into the prompt as the anchor to simply trigger profiling, let
LLM propose candidate triggers, and justify each candidate. These
propose-and-judge rationales help LLMs mitigate over-reliance on the keywords
and promote detection rule learning. Extensive experiments demonstrate the
effectiveness of our approach, showcasing significant advancements in one-shot
event detection.

</details>


### [279] [InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information](https://arxiv.org/abs/2508.07630)
*Anirudh Iyengar Kaniyar Narayana Iyengar,Srija Mukhopadhyay,Adnan Qidwai,Shubhankar Singh,Dan Roth,Vivek Gupta*

Main category: cs.CL

TL;DR: InterChart是一个评估视觉语言模型在多图表推理能力上的诊断基准，涵盖从简单到复杂的任务，揭示模型在多图表整合上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现实应用（如科学报告、金融分析）需要模型在多图表间进行推理，但现有基准仅关注单一图表，无法满足需求。

Method: InterChart分为三个难度层级：单图表事实推理、合成对齐图表集整合分析、复杂真实图表对的语义推理。

Result: 评估显示，随着图表复杂性增加，模型准确性显著下降，分解多实体图表可提升表现。

Conclusion: InterChart为复杂多视觉环境中的多模态推理提供了严格框架，揭示了模型的系统性局限。

Abstract: We introduce InterChart, a diagnostic benchmark that evaluates how well
vision-language models (VLMs) reason across multiple related charts, a task
central to real-world applications such as scientific reporting, financial
analysis, and public policy dashboards. Unlike prior benchmarks focusing on
isolated, visually uniform charts, InterChart challenges models with diverse
question types ranging from entity inference and trend correlation to numerical
estimation and abstract multi-step reasoning grounded in 2-3 thematically or
structurally related charts. We organize the benchmark into three tiers of
increasing difficulty: (1) factual reasoning over individual charts, (2)
integrative analysis across synthetically aligned chart sets, and (3) semantic
inference over visually complex, real-world chart pairs. Our evaluation of
state-of-the-art open and closed-source VLMs reveals consistent and steep
accuracy declines as chart complexity increases. We find that models perform
better when we decompose multi-entity charts into simpler visual units,
underscoring their struggles with cross-chart integration. By exposing these
systematic limitations, InterChart provides a rigorous framework for advancing
multimodal reasoning in complex, multi-visual environments.

</details>


### [280] [LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval](https://arxiv.org/abs/2508.07690)
*Luyao Zhuang,Qinggang Zhang,Huachi Zhou,Juhua Liu,Qing Li,Xiao Huang*

Main category: cs.CL

TL;DR: 论文提出了一种名为LoSemB的逻辑引导语义桥接框架，用于解决大语言模型在工具学习中处理未见工具时的分布偏移和相似性检索脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中工具库不断更新，现有方法无法有效处理训练阶段未见的工具，导致分布偏移和检索脆弱性问题。

Method: LoSemB框架包含逻辑嵌入对齐模块和关系增强检索机制，以挖掘和转移逻辑信息，无需重新训练。

Result: 实验表明，LoSemB在归纳设置中表现优异，同时在转导设置中保持良好效果。

Conclusion: LoSemB通过逻辑引导的语义桥接，有效解决了工具学习中的未见工具问题。

Abstract: Tool learning has emerged as a promising paradigm for large language models
(LLMs) to solve many real-world tasks. Nonetheless, with the tool repository
rapidly expanding, it is impractical to contain all tools within the limited
input length of LLMs. To alleviate these issues, researchers have explored
incorporating a tool retrieval module to select the most relevant tools or
represent tools as unique tokens within LLM parameters. However, most
state-of-the-art methods are under transductive settings, assuming all tools
have been observed during training. Such a setting deviates from reality as the
real-world tool repository is evolving and incorporates new tools frequently.
When dealing with these unseen tools, which refer to tools not encountered
during the training phase, these methods are limited by two key issues,
including the large distribution shift and the vulnerability of
similarity-based retrieval. To this end, inspired by human cognitive processes
of mastering unseen tools through discovering and applying the logical
information from prior experience, we introduce a novel Logic-Guided Semantic
Bridging framework for inductive tool retrieval, namely, LoSemB, which aims to
mine and transfer latent logical information for inductive tool retrieval
without costly retraining. Specifically, LoSemB contains a logic-based
embedding alignment module to mitigate distribution shifts and implements a
relational augmented retrieval mechanism to reduce the vulnerability of
similarity-based retrieval. Extensive experiments demonstrate that LoSemB
achieves advanced performance in inductive settings while maintaining desirable
effectiveness in the transductive setting.

</details>


### [281] [What am I missing here?: Evaluating Large Language Models for Masked Sentence Prediction](https://arxiv.org/abs/2508.07702)
*Charlie Wyatt,Aditya Joshi,Flora Salim*

Main category: cs.CL

TL;DR: 论文研究了基于Transformer的模型在预测完整句子（而非单字）时的表现，发现商业LLMs在低结构化领域中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 探讨NTP（单字预测）在长距离连贯性和全局一致性上的局限性，尤其是在结构化文档中预测完整句子的能力。

Method: 评估了GPT-4o、Claude 3.5 Sonnet和Gemini 2.0 Flash在三个领域的掩码句子预测（MSP）任务中的表现，包括保真度和连贯性。

Result: 商业LLMs在低结构化领域中预测掩码句子的能力较差。

Conclusion: 当前模型在全局一致性任务上存在能力缺口，需进一步改进。

Abstract: Transformer-based models primarily rely on Next Token Prediction (NTP), which
predicts the next token in a sequence based on the preceding context. However,
NTP's focus on single-token prediction often limits a model's ability to plan
ahead or maintain long-range coherence, raising questions about how well LLMs
can predict longer contexts, such as full sentences within structured
documents. While NTP encourages local fluency, it provides no explicit
incentive to ensure global coherence across sentence boundaries-an essential
skill for reconstructive or discursive tasks. To investigate this, we evaluate
three commercial LLMs (GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash) on
Masked Sentence Prediction (MSP) - the task of infilling a randomly removed
sentence - from three domains: ROCStories (narrative), Recipe1M (procedural),
and Wikipedia (expository). We assess both fidelity (similarity to the original
sentence) and cohesiveness (fit within the surrounding context). Our key
finding reveals that commercial LLMs, despite their superlative performance in
other tasks, are poor at predicting masked sentences in low-structured domains,
highlighting a gap in current model capabilities.

</details>


### [282] [Exploring Causal Effect of Social Bias on Faithfulness Hallucinations in Large Language Models](https://arxiv.org/abs/2508.07753)
*Zhenliang Zhang,Junzhe Zhang,Xinyu Hu,HuiXuan Zhang,Xiaojun Wan*

Main category: cs.CL

TL;DR: 研究探讨了社会偏见是否导致大型语言模型（LLM）的忠实性幻觉，并通过结构因果模型（SCM）验证了因果关系。


<details>
  <summary>Details</summary>
Motivation: LLM在多种任务中表现优异，但其输出可能与输入不一致（忠实性幻觉），社会偏见是否导致这一现象尚未被研究。

Method: 使用SCM建立因果关系，设计偏见干预措施控制混杂因素，并开发了Bias Intervention Dataset（BID）用于精确测量。

Result: 实验表明偏见是忠实性幻觉的重要原因，且不同偏见状态的影响方向各异。

Conclusion: 偏见对幻觉生成具有微妙但显著的因果影响，尤其是在不公平性幻觉中。

Abstract: Large language models (LLMs) have achieved remarkable success in various
tasks, yet they remain vulnerable to faithfulness hallucinations, where the
output does not align with the input. In this study, we investigate whether
social bias contributes to these hallucinations, a causal relationship that has
not been explored. A key challenge is controlling confounders within the
context, which complicates the isolation of causality between bias states and
hallucinations. To address this, we utilize the Structural Causal Model (SCM)
to establish and validate the causality and design bias interventions to
control confounders. In addition, we develop the Bias Intervention Dataset
(BID), which includes various social biases, enabling precise measurement of
causal effects. Experiments on mainstream LLMs reveal that biases are
significant causes of faithfulness hallucinations, and the effect of each bias
state differs in direction. We further analyze the scope of these causal
effects across various models, specifically focusing on unfairness
hallucinations, which are primarily targeted by social bias, revealing the
subtle yet significant causal effect of bias on hallucination generation.

</details>


### [283] [SASST: Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech Translation](https://arxiv.org/abs/2508.07781)
*Zeyu Yang,Lai Wei,Roman Koshkin,Xi Chen,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 论文提出了一种基于语法的分块策略，通过解析依赖关系和标点特征将输入流分割为语义完整的单元，并在此基础上构建了SASST框架，显著提升了翻译质量。


<details>
  <summary>Details</summary>
Motivation: 解决语义碎片化问题，优化同步语音翻译的时机和内容。

Method: 结合依赖关系解析和标点特征的分块策略，集成Whisper编码器和仅解码器LLM的SASST框架。

Result: 在CoVoST2多语言语料库上验证了翻译质量的显著提升。

Conclusion: 语法结构在LLM驱动的同步语音翻译系统中具有显著效果。

Abstract: This work proposes a grammar-based chunking strategy that segments input
streams into semantically complete units by parsing dependency relations (e.g.,
noun phrase boundaries, verb-object structures) and punctuation features. The
method ensures chunk coherence and minimizes semantic fragmentation. Building
on this mechanism, we present SASST (Syntax-Aware Simultaneous Speech
Translation), an end-to-end framework integrating frozen Whisper encoder and
decoder-only LLM. The unified architecture dynamically outputs translation
tokens or <WAIT> symbols to jointly optimize translation timing and content,
with target-side reordering addressing word-order divergence. Experiments on
CoVoST2 multilingual corpus En-{De, Zh, Ja} demonstrate significant translation
quality improvements across languages and validate the effectiveness of
syntactic structures in LLM-driven SimulST systems.

</details>


### [284] [Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts](https://arxiv.org/abs/2508.07785)
*Haoyuan Wu,Haoxing Chen,Xiaodong Chen,Zhanchao Zhou,Tieyuan Chen,Yihong Zhuang,Guoshan Lu,Zenan Huang,Junbo Zhao,Lin Liu,Zhenzhong Lan,Bei Yu,Jianguo Li*

Main category: cs.CL

TL;DR: Grove MoE是一种新型的MoE架构，通过引入不同大小的专家和动态激活机制，提升计算效率，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统MoE架构使用同质化专家，固定激活参数数量，限制了计算效率。

Method: 提出Grove MoE，采用异质化专家和动态激活机制，并基于Qwen3-30B-A3B-Base模型开发GroveMoE-Base和GroveMoE-Inst。

Result: GroveMoE模型动态激活3.14-3.28B参数，性能与类似或更大规模的SOTA开源模型相当。

Conclusion: Grove MoE通过异质化专家和动态激活机制，有效提升了计算效率和模型性能。

Abstract: The Mixture of Experts (MoE) architecture is a cornerstone of modern
state-of-the-art (SOTA) large language models (LLMs). MoE models facilitate
scalability by enabling sparse parameter activation. However, traditional MoE
architecture uses homogeneous experts of a uniform size, activating a fixed
number of parameters irrespective of input complexity and thus limiting
computational efficiency. To overcome this limitation, we introduce Grove MoE,
a novel architecture incorporating experts of varying sizes, inspired by the
heterogeneous big.LITTLE CPU architecture. This architecture features novel
adjugate experts with a dynamic activation mechanism, enabling model capacity
expansion while maintaining manageable computational overhead. Building on this
architecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs
developed by applying an upcycling strategy to the Qwen3-30B-A3B-Base model
during mid-training and post-training. GroveMoE models dynamically activate
3.14-3.28B parameters based on token complexity and achieve performance
comparable to SOTA open-source models of similar or even larger size.

</details>


### [285] [Can You Trick the Grader? Adversarial Persuasion of LLM Judges](https://arxiv.org/abs/2508.07805)
*Yerin Hwang,Dongryeol Lee,Taegwan Kang,Yongil Kim,Kyomin Jung*

Main category: cs.CL

TL;DR: 研究发现，通过策略性嵌入说服性语言可以偏置大型语言模型（LLM）在数学推理任务中的评分，即使答案错误也能获得高分。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM作为自动评估工具时，是否容易被说服性语言操纵评分，揭示其潜在漏洞。

Method: 基于亚里士多德修辞学原理，设计七种说服技巧，嵌入数学任务回答中，测试LLM评分偏差。

Result: 说服性语言导致LLM对错误答案评分平均提高8%，模型规模增大无法显著缓解此问题。

Conclusion: LLM作为评估工具易受说服性攻击，需开发更鲁棒的防御机制。

Abstract: As large language models take on growing roles as automated evaluators in
practical settings, a critical question arises: Can individuals persuade an LLM
judge to assign unfairly high scores? This study is the first to reveal that
strategically embedded persuasive language can bias LLM judges when scoring
mathematical reasoning tasks, where correctness should be independent of
stylistic variation. Grounded in Aristotle's rhetorical principles, we
formalize seven persuasion techniques (Majority, Consistency, Flattery,
Reciprocity, Pity, Authority, Identity) and embed them into otherwise identical
responses. Across six math benchmarks, we find that persuasive language leads
LLM judges to assign inflated scores to incorrect solutions, by up to 8% on
average, with Consistency causing the most severe distortion. Notably,
increasing model size does not substantially mitigate this vulnerability.
Further analysis demonstrates that combining multiple persuasion techniques
amplifies the bias, and pairwise evaluation is likewise susceptible. Moreover,
the persuasive effect persists under counter prompting strategies, highlighting
a critical vulnerability in LLM-as-a-Judge pipelines and underscoring the need
for robust defenses against persuasion-based attacks.

</details>


### [286] [Evaluating Compositional Approaches for Focus and Sentiment Analysis](https://arxiv.org/abs/2508.07810)
*Olga Kellert,Muhammad Imran,Nicholas Hill Matlis,Mahmud Uz Zaman,Carlos Gómez-Rodríguez*

Main category: cs.CL

TL;DR: 论文总结了在语言学中的焦点分析（FA）和自然语言处理（NLP）中的情感分析（SA）中，组合方法的评估结果。填补了FA领域缺乏定量评估的空白，并论证了SA的组合规则同样适用于FA。


<details>
  <summary>Details</summary>
Motivation: 填补语言学中焦点分析（FA）缺乏定量评估的研究空白，并论证情感分析（SA）的组合规则适用于FA，因为两者密切相关。

Method: 采用基于通用依赖（UDs）的组合方法，利用修饰、协调和否定等基本句法规则处理情感词典中的词汇。

Result: 组合方法在SA中表现出更高的可解释性和准确性，优于非组合方法VADER，并将结果推广到FA。

Conclusion: 组合方法不仅适用于SA，还可推广到FA，为相关领域提供了新的研究视角。

Abstract: This paper summarizes the results of evaluating a compositional approach for
Focus Analysis (FA) in Linguistics and Sentiment Analysis (SA) in Natural
Language Processing (NLP). While quantitative evaluations of compositional and
non-compositional approaches in SA exist in NLP, similar quantitative
evaluations are very rare in FA in Linguistics that deal with linguistic
expressions representing focus or emphasis such as "it was John who left". We
fill this gap in research by arguing that compositional rules in SA also apply
to FA because FA and SA are closely related meaning that SA is part of FA. Our
compositional approach in SA exploits basic syntactic rules such as rules of
modification, coordination, and negation represented in the formalism of
Universal Dependencies (UDs) in English and applied to words representing
sentiments from sentiment dictionaries. Some of the advantages of our
compositional analysis method for SA in contrast to non-compositional analysis
methods are interpretability and explainability. We test the accuracy of our
compositional approach and compare it with a non-compositional approach VADER
that uses simple heuristic rules to deal with negation, coordination and
modification. In contrast to previous related work that evaluates
compositionality in SA on long reviews, this study uses more appropriate
datasets to evaluate compositionality. In addition, we generalize the results
of compositional approaches in SA to compositional approaches in FA.

</details>


### [287] [Evaluating Large Language Models as Expert Annotators](https://arxiv.org/abs/2508.07827)
*Yu-Min Tseng,Wei-Lin Chen,Chung-Chi Chen,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在需要专家知识的领域是否能替代人类专家进行文本标注任务，并评估了单模型和多代理方法在金融、生物医学和法律领域的表现。


<details>
  <summary>Details</summary>
Motivation: 文本标注通常成本高且耗时，LLMs在通用NLP任务中表现优异，但在需要专家知识的领域效果尚不明确。

Method: 提出多代理讨论框架模拟人类标注者，结合推理模型（如o3-mini）进行比较。

Result: 发现单模型推理技术效果有限，推理模型未显著优于非推理模型，多代理环境中某些模型行为固定。

Conclusion: LLMs在专业领域标注任务中效果有限，多代理方法可能揭示模型行为特征。

Abstract: Textual data annotation, the process of labeling or tagging text with
relevant information, is typically costly, time-consuming, and labor-intensive.
While large language models (LLMs) have demonstrated their potential as direct
alternatives to human annotators for general domains natural language
processing (NLP) tasks, their effectiveness on annotation tasks in domains
requiring expert knowledge remains underexplored. In this paper, we
investigate: whether top-performing LLMs, which might be perceived as having
expert-level proficiency in academic and professional benchmarks, can serve as
direct alternatives to human expert annotators? To this end, we evaluate both
individual LLMs and multi-agent approaches across three highly specialized
domains: finance, biomedicine, and law. Specifically, we propose a multi-agent
discussion framework to simulate a group of human annotators, where LLMs are
tasked to engage in discussions by considering others' annotations and
justifications before finalizing their labels. Additionally, we incorporate
reasoning models (e.g., o3-mini) to enable a more comprehensive comparison. Our
empirical results reveal that: (1) Individual LLMs equipped with inference-time
techniques (e.g., chain-of-thought (CoT), self-consistency) show only marginal
or even negative performance gains, contrary to prior literature suggesting
their broad effectiveness. (2) Overall, reasoning models do not demonstrate
statistically significant improvements over non-reasoning models in most
settings. This suggests that extended long CoT provides relatively limited
benefits for data annotation in specialized domains. (3) Certain model
behaviors emerge in the multi-agent discussion environment. For instance,
Claude 3.7 Sonnet with thinking rarely changes its initial annotations, even
when other agents provide correct annotations or valid reasoning.

</details>


### [288] [LLMs for Law: Evaluating Legal-Specific LLMs on Contract Understanding](https://arxiv.org/abs/2508.07849)
*Amrita Singh,H. Suhan Karaca,Aditya Joshi,Hye-young Paik,Jiaojiao Jiang*

Main category: cs.CL

TL;DR: 本文评估了10种法律专用LLM和7种通用LLM在合同理解任务上的表现，发现法律专用模型表现更优，尤其是需要法律细微理解的场景。


<details>
  <summary>Details</summary>
Motivation: 填补法律NLP领域中对合同分类任务全面评估的空白。

Method: 比较10种法律专用LLM和7种通用LLM在三个英语合同理解任务上的表现。

Result: 法律专用LLM（如Legal-BERT和Contracts-BERT）表现优于通用模型，参数更少但性能更强。

Conclusion: 法律专用LLM在合同理解任务中更具优势，为开发更准确的系统提供了基础。

Abstract: Despite advances in legal NLP, no comprehensive evaluation covering multiple
legal-specific LLMs currently exists for contract classification tasks in
contract understanding. To address this gap, we present an evaluation of 10
legal-specific LLMs on three English language contract understanding tasks and
compare them with 7 general-purpose LLMs. The results show that legal-specific
LLMs consistently outperform general-purpose models, especially on tasks
requiring nuanced legal understanding. Legal-BERT and Contracts-BERT establish
new SOTAs on two of the three tasks, despite having 69% fewer parameters than
the best-performing general-purpose LLM. We also identify CaseLaw-BERT and
LexLM as strong additional baselines for contract understanding. Our results
provide a holistic evaluation of legal-specific LLMs and will facilitate the
development of more accurate contract understanding systems.

</details>


### [289] [Large Language Models for Czech Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2508.07860)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本文评估了19种不同规模和架构的大语言模型（LLM）在捷克语基于方面的情感分析（ABSA）任务中的表现，发现领域专用的小模型在微调后优于通用LLM，而微调的LLM达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在捷克语ABSA任务中的表现，填补相关研究空白。

Method: 对19种LLM在零样本、少样本和微调场景下的性能进行全面评估，分析多语言性、模型规模和时效性等因素的影响。

Result: 领域专用小模型在微调后表现最佳，微调LLM达到SOTA；多语言性和模型规模对性能有显著影响。

Conclusion: 研究为捷克语ABSA任务中LLM的适用性提供了见解，并为未来研究提供了指导。

Abstract: Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that aims to identify sentiment toward specific aspects of an entity.
While large language models (LLMs) have shown strong performance in various
natural language processing (NLP) tasks, their capabilities for Czech ABSA
remain largely unexplored. In this work, we conduct a comprehensive evaluation
of 19 LLMs of varying sizes and architectures on Czech ABSA, comparing their
performance in zero-shot, few-shot, and fine-tuning scenarios. Our results show
that small domain-specific models fine-tuned for ABSA outperform
general-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs
achieve state-of-the-art results. We analyze how factors such as
multilingualism, model size, and recency influence performance and present an
error analysis highlighting key challenges, particularly in aspect term
prediction. Our findings provide insights into the suitability of LLMs for
Czech ABSA and offer guidance for future research in this area.

</details>


### [290] [Few-shot Cross-lingual Aspect-Based Sentiment Analysis with Sequence-to-Sequence Models](https://arxiv.org/abs/2508.07866)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 研究探讨了在低资源语言中，通过加入少量目标语言示例对跨语言ABSA任务的性能提升效果。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言因标注数据稀缺导致的ABSA性能问题，探索少量目标语言示例的潜在价值。

Method: 在四种ABSA任务、六种目标语言和两种序列到序列模型中，评估加入少量目标语言示例的效果。

Result: 加入十个目标语言示例显著提升性能，结合1000个示例甚至超越单语基线。

Conclusion: 少量高质量目标语言示例对跨语言ABSA任务具有实际可行性和高效性。

Abstract: Aspect-based sentiment analysis (ABSA) has received substantial attention in
English, yet challenges remain for low-resource languages due to the scarcity
of labelled data. Current cross-lingual ABSA approaches often rely on external
translation tools and overlook the potential benefits of incorporating a small
number of target language examples into training. In this paper, we evaluate
the effect of adding few-shot target language examples to the training set
across four ABSA tasks, six target languages, and two sequence-to-sequence
models. We show that adding as few as ten target language examples
significantly improves performance over zero-shot settings and achieves a
similar effect to constrained decoding in reducing prediction errors.
Furthermore, we demonstrate that combining 1,000 target language examples with
English data can even surpass monolingual baselines. These findings offer
practical insights for improving cross-lingual ABSA in low-resource and
domain-specific settings, as obtaining ten high-quality annotated examples is
both feasible and highly effective.

</details>


### [291] [Tailored Emotional LLM-Supporter: Enhancing Cultural Sensitivity](https://arxiv.org/abs/2508.07902)
*Chen Cecilia Liu,Hiba Arnaout,Nils Kovačić,Dana Atzil-Slonim,Iryna Gurevych*

Main category: cs.CL

TL;DR: 论文介绍了CultureCare数据集，用于探索大型语言模型（LLMs）在提供文化敏感情感支持方面的能力，并提出了四种适应策略。


<details>
  <summary>Details</summary>
Motivation: LLMs在情感支持方面表现潜力，但缺乏文化敏感性的研究资源。

Method: 开发CultureCare数据集，测试四种适应策略，结合LLM法官、文化内人类注释者和临床心理学家进行评估。

Result: 适应后的LLMs表现优于匿名在线同行回复，简单文化角色扮演不足以实现文化敏感性。

Conclusion: LLMs在临床培训中具有潜力，可帮助培养未来治疗师的文化能力。

Abstract: Large language models (LLMs) show promise in offering emotional support and
generating empathetic responses for individuals in distress, but their ability
to deliver culturally sensitive support remains underexplored due to lack of
resources. In this work, we introduce CultureCare, the first dataset designed
for this task, spanning four cultures and including 1729 distress messages,
1523 cultural signals, and 1041 support strategies with fine-grained emotional
and cultural annotations. Leveraging CultureCare, we (i) develop and test four
adaptation strategies for guiding three state-of-the-art LLMs toward culturally
sensitive responses; (ii) conduct comprehensive evaluations using LLM judges,
in-culture human annotators, and clinical psychologists; (iii) show that
adapted LLMs outperform anonymous online peer responses, and that simple
cultural role-play is insufficient for cultural sensitivity; and (iv) explore
the application of LLMs in clinical training, where experts highlight their
potential in fostering cultural competence in future therapists.

</details>


### [292] [Challenges and opportunities in portraying emotion in generated sign language](https://arxiv.org/abs/2508.07937)
*John C. McDonald,Rosalee Wolfe,Fabrizio Nunnari*

Main category: cs.CL

TL;DR: 论文提出了一种两参数表示法，用于控制手语虚拟角色的非手动信号（如情感表达），通过EASIER文本表示实现更一致的情感标注。


<details>
  <summary>Details</summary>
Motivation: 手语虚拟角色在表达情感等非手动信号时缺乏标准化方法，导致情感内容难以融入。

Method: 采用直观的两参数表示法，通过EASIER文本表示控制虚拟角色Paula的情感表达。

Result: 该方法能更一致地指定情感非手动信号，并支持更细腻的情感状态表达。

Conclusion: 两参数表示法为手语虚拟角色的情感表达提供了一种更连贯的标准化解决方案。

Abstract: Non-manual signals in sign languages continue to be a challenge for signing
avatars. More specifically, emotional content has been difficult to incorporate
because of a lack of a standard method of specifying the avatar's emotional
state. This paper explores the application of an intuitive two-parameter
representation for emotive non-manual signals to the Paula signing avatar that
shows promise for facilitating the linguistic specification of emotional facial
expressions in a more coherent manner than previous methods. Users can apply
these parameters to control Paula's emotional expressions through a textual
representation called the EASIER notation. The representation can allow avatars
to express more nuanced emotional states using two numerical parameters. It
also has the potential to enable more consistent specification of emotional
non-manual signals in linguistic annotations which drive signing avatars.

</details>


### [293] [Expert Preference-based Evaluation of Automated Related Work Generation](https://arxiv.org/abs/2508.07955)
*Furkan Şahinuç,Subhabrata Dutta,Iryna Gurevych*

Main category: cs.CL

TL;DR: 论文提出GREP框架，用于评估自动生成的科学写作质量，结合领域标准和专家偏好，提供细粒度评估。


<details>
  <summary>Details</summary>
Motivation: 解决现有自动评估方法和LLM评委无法准确捕捉专家偏好和领域质量标准的问题，支持人机协作写作。

Method: 提出GREP多轮评估框架，分解评估为细粒度维度，结合对比示例提供上下文指导，设计两种变体（专有和开源LLM）。

Result: GREP比标准LLM评委更稳健地评估相关工作总结质量，与专家评估高度相关，但当前LLM生成内容难以满足要求。

Conclusion: GREP框架有效提升科学写作评估质量，但LLM生成内容仍需改进。

Abstract: Expert domain writing, such as scientific writing, typically demands
extensive domain knowledge. Recent advances in LLMs show promising potential in
reducing the expert workload. However, evaluating the quality of automatically
generated scientific writing is a crucial open issue, as it requires knowledge
of domain-specific evaluation criteria and the ability to discern expert
preferences. Conventional automatic metrics and LLM-as-a-judge systems are
insufficient to grasp expert preferences and domain-specific quality standards.
To address this gap and support human-AI collaborative writing, we focus on
related work generation, one of the most challenging scientific tasks, as an
exemplar. We propose GREP, a multi-turn evaluation framework that integrates
classical related work evaluation criteria with expert-specific preferences.
Instead of assigning a single score, our framework decomposes the evaluation
into fine-grained dimensions. This localized evaluation approach is further
augmented with contrastive few-shot examples to provide detailed contextual
guidance for the evaluation dimensions. The design principles allow our
framework to deliver cardinal assessment of quality, which can facilitate
better post-training compared to ordinal preference data. For better
accessibility, we design two variants of GREP: a more precise variant with
proprietary LLMs as evaluators, and a cheaper alternative with open-weight
LLMs. Empirical investigation reveals that our framework is able to assess the
quality of related work sections in a much more robust manner compared to
standard LLM judges, reflects natural scenarios of scientific writing, and
bears a strong correlation with the human expert assessment. We also observe
that generations from state-of-the-art LLMs struggle to satisfy validation
constraints of a suitable related work section. They (mostly) fail to improve
based on feedback as well.

</details>


### [294] [Large Language Models for Subjective Language Understanding: A Survey](https://arxiv.org/abs/2508.07959)
*Changhao Song,Yazhou Zhang,Hui Gao,Ben Yao,Peng Zhang*

Main category: cs.CL

TL;DR: 本文综述了大型语言模型（LLMs）在主观语言理解任务中的应用，包括情感分析、情感识别等，并探讨了其挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs（如ChatGPT、LLaMA）的出现，主观语言任务的处理方式发生了范式转变，需要系统梳理其应用与挑战。

Method: 通过语言学与认知视角定义主观语言，分析LLM架构的演变及适用性，并总结八项任务的定义、数据集、方法与挑战。

Result: LLMs在建模人类主观判断方面表现出色，但仍面临数据限制、模型偏见和伦理问题。

Conclusion: 本文为情感计算、比喻语言处理与LLMs交叉领域的研究者提供了宝贵资源，并指出了未来研究方向。

Abstract: Subjective language understanding refers to a broad set of natural language
processing tasks where the goal is to interpret or generate content that
conveys personal feelings, opinions, or figurative meanings rather than
objective facts. With the advent of large language models (LLMs) such as
ChatGPT, LLaMA, and others, there has been a paradigm shift in how we approach
these inherently nuanced tasks. In this survey, we provide a comprehensive
review of recent advances in applying LLMs to subjective language tasks,
including sentiment analysis, emotion recognition, sarcasm detection, humor
understanding, stance detection, metaphor interpretation, intent detection, and
aesthetics assessment. We begin by clarifying the definition of subjective
language from linguistic and cognitive perspectives, and we outline the unique
challenges posed by subjective language (e.g. ambiguity, figurativeness,
context dependence). We then survey the evolution of LLM architectures and
techniques that particularly benefit subjectivity tasks, highlighting why LLMs
are well-suited to model subtle human-like judgments. For each of the eight
tasks, we summarize task definitions, key datasets, state-of-the-art LLM-based
methods, and remaining challenges. We provide comparative insights, discussing
commonalities and differences among tasks and how multi-task LLM approaches
might yield unified models of subjectivity. Finally, we identify open issues
such as data limitations, model bias, and ethical considerations, and suggest
future research directions. We hope this survey will serve as a valuable
resource for researchers and practitioners interested in the intersection of
affective computing, figurative language processing, and large-scale language
models.

</details>


### [295] [Toward Machine Interpreting: Lessons from Human Interpreting Studies](https://arxiv.org/abs/2508.07964)
*Matthias Sperber,Maureen de Seyssel,Jiajun Bao,Matthias Paulik*

Main category: cs.CL

TL;DR: 本文探讨如何通过借鉴人类口译原则改进语音翻译系统，以缩小其与真实口译体验的差距。


<details>
  <summary>Details</summary>
Motivation: 当前语音翻译系统缺乏人类口译的动态适应性，限制了其实际应用价值。

Method: 通过分析人类口译文献，结合机器翻译领域的视角，探讨操作性和质性方面的启示。

Result: 研究发现，利用最新建模技术可以采纳许多人类口译原则。

Conclusion: 研究为缩小语音翻译系统的可用性差距提供了灵感，并推动实现真正的机器口译。

Abstract: Current speech translation systems, while having achieved impressive
accuracies, are rather static in their behavior and do not adapt to real-world
situations in ways human interpreters do. In order to improve their practical
usefulness and enable interpreting-like experiences, a precise understanding of
the nature of human interpreting is crucial. To this end, we discuss human
interpreting literature from the perspective of the machine translation field,
while considering both operational and qualitative aspects. We identify
implications for the development of speech translation systems and argue that
there is great potential to adopt many human interpreting principles using
recent modeling techniques. We hope that our findings provide inspiration for
closing the perceived usability gap, and can motivate progress toward true
machine interpreting.

</details>


### [296] [Understanding Syntactic Generalization in Structure-inducing Language Models](https://arxiv.org/abs/2508.07969)
*David Arps,Hassan Sajjad,Laura Kallmeyer*

Main category: cs.CL

TL;DR: 本文研究了三种结构诱导语言模型（SiLM）在自然语言和合成数据上的表现，发现它们在语法表示、任务性能和训练动态上各有优劣，其中GPST表现最稳定。


<details>
  <summary>Details</summary>
Motivation: 现有SiLM模型评估规模小、系统性不足且缺乏可比性，因此本文旨在填补这一空白。

Method: 比较了Structformer、UDGN和GPST三种SiLM架构，评估其语法表示、语法判断任务表现及训练动态。

Result: GPST在长距离依赖任务中表现最优，且小模型在合成数据上的训练为评估提供了有效测试平台。

Conclusion: 不同SiLM架构各有优势，GPST整体表现最佳，合成数据训练的小模型有助于基础模型评估。

Abstract: Structure-inducing Language Models (SiLM) are trained on a self-supervised
language modeling task, and induce a hierarchical sentence representation as a
byproduct when processing an input. A wide variety of SiLMs have been proposed.
However, these have typically been evaluated on a relatively small scale, and
evaluation of these models has systematic gaps and lacks comparability. In this
work, we study three different SiLM architectures using both natural language
(English) corpora and synthetic bracketing expressions: Structformer (Shen et
al., 2021), UDGN (Shen et al., 2022) and GPST (Hu et al., 2024). We compare
them with respect to (i) properties of the induced syntactic representations
(ii) performance on grammaticality judgment tasks, and (iii) training dynamics.
We find that none of the three architectures dominates across all evaluation
metrics. However, there are significant differences, in particular with respect
to the induced syntactic representations. The Generative Pretrained Structured
Transformer (GPST; Hu et al. 2024) performs most consistently across evaluation
settings, and outperforms the other models on long-distance dependencies in
bracketing expressions. Furthermore, our study shows that small models trained
on large amounts of synthetic data provide a useful testbed for evaluating
basic model properties.

</details>


### [297] [Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL](https://arxiv.org/abs/2508.07976)
*Jiaxuan Gao,Wei Fu,Minyang Xie,Shusheng Xu,Chuyi He,Zhiyu Mei,Banghua Zhu,Yi Wu*

Main category: cs.CL

TL;DR: ASearcher是一个开源项目，通过大规模RL训练提升搜索智能体的能力，解决了现有方法在可扩展性、效率和数据质量上的不足。


<details>
  <summary>Details</summary>
Motivation: 开源智能体在搜索智能（如处理模糊查询、生成精确搜索等）方面表现不足，现有方法存在可扩展性和效率问题。

Method: 提出ASearcher，采用完全异步RL训练和基于提示的LLM智能体，生成高质量QA数据集。

Result: QwQ-32B智能体在xBench和GAIA上分别取得46.7%和20.8%的Avg@4提升，支持超长搜索（40轮以上）。

Conclusion: ASearcher在简单设计下超越现有开源32B智能体，模型、数据和代码已开源。

Abstract: Recent advancements in LLM-based agents have demonstrated remarkable
capabilities in handling complex, knowledge-intensive tasks by integrating
external tools. Among diverse choices of tools, search tools play a pivotal
role in accessing vast external knowledge. However, open-source agents still
fall short of achieving expert-level Search Intelligence, the ability to
resolve ambiguous queries, generate precise searches, analyze results, and
conduct thorough exploration. Existing approaches fall short in scalability,
efficiency, and data quality. For example, small turn limits in existing online
RL methods, e.g. <=10, restrict complex strategy learning. This paper
introduces ASearcher, an open-source project for large-scale RL training of
search agents. Our key contributions include: (1) Scalable fully asynchronous
RL training that enables long-horizon search while maintaining high training
efficiency. (2) A prompt-based LLM agent that autonomously synthesizes
high-quality and challenging QAs, creating a large-scale QA dataset. Through RL
training, our prompt-based QwQ-32B agent achieves substantial improvements,
with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our
agent exhibits extreme long-horizon search, with tool calls exceeding 40 turns
and output tokens exceeding 150k during training time. With a simple agent
design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on
xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We
open-source our models, training data, and codes in
https://github.com/inclusionAI/ASearcher.

</details>


### [298] [The Medical Metaphors Corpus (MCC)](https://arxiv.org/abs/2508.07993)
*Anna Sofia Lippolis,Andrea Giovanni Nuzzolese,Aldo Gangemi*

Main category: cs.CL

TL;DR: 论文介绍了医学隐喻语料库（MCC），填补了领域特定隐喻检测资源的空白，并展示了当前语言模型在科学隐喻检测中的局限性。


<details>
  <summary>Details</summary>
Motivation: 科学隐喻在理解复杂概念中起关键作用，但现有隐喻检测资源主要针对通用领域，缺乏领域特定应用。

Method: 构建了MCC数据集，包含792个医学和生物学领域的注释隐喻，涵盖多种来源，并提供二元和分级隐喻性标注。

Result: 评估显示当前语言模型在科学隐喻检测中表现一般，领域特定隐喻理解仍有改进空间。

Conclusion: MCC为隐喻检测、生成系统和患者沟通工具等研究提供了基础资源。

Abstract: Metaphor is a fundamental cognitive mechanism that shapes scientific
understanding, enabling the communication of complex concepts while potentially
constraining paradigmatic thinking. Despite the prevalence of figurative
language in scientific discourse, existing metaphor detection resources
primarily focus on general-domain text, leaving a critical gap for
domain-specific applications. In this paper, we present the Medical Metaphors
Corpus (MCC), a comprehensive dataset of 792 annotated scientific conceptual
metaphors spanning medical and biological domains. MCC aggregates metaphorical
expressions from diverse sources including peer-reviewed literature, news
media, social media discourse, and crowdsourced contributions, providing both
binary and graded metaphoricity judgments validated through human annotation.
Each instance includes source-target conceptual mappings and perceived
metaphoricity scores on a 0-7 scale, establishing the first annotated resource
for computational scientific metaphor research. Our evaluation demonstrates
that state-of-the-art language models achieve modest performance on scientific
metaphor detection, revealing substantial room for improvement in
domain-specific figurative language understanding. MCC enables multiple
research applications including metaphor detection benchmarking, quality-aware
generation systems, and patient-centered communication tools.

</details>


### [299] [WideSearch: Benchmarking Agentic Broad Info-Seeking](https://arxiv.org/abs/2508.07999)
*Ryan Wong,Jiawei Wang,Junjie Zhao,Li Chen,Yan Gao,Long Zhang,Xuan Zhou,Zuo Wang,Kai Xiang,Ge Zhang,Wenhao Huang,Yang Wang,Ke Wang*

Main category: cs.CL

TL;DR: 论文提出了WideSearch基准，用于评估LLM驱动的搜索代理在大规模信息收集任务中的可靠性，发现现有系统表现不佳，最高成功率仅5%。


<details>
  <summary>Details</summary>
Motivation: 大规模信息收集任务虽重复但繁琐，LLM驱动的搜索代理有望解放人力，但其可靠性和完整性缺乏评估标准。

Method: 构建包含200个问题的WideSearch基准，覆盖15个领域，通过五阶段质量控制确保数据质量，并测试10种搜索系统。

Result: 现有系统成功率极低（最高5%），而人工测试可达近100%，显示搜索代理在大规模信息收集中的不足。

Conclusion: 当前搜索代理存在显著缺陷，需进一步研究改进，WideSearch基准为未来研究提供了工具。

Abstract: From professional research to everyday planning, many tasks are bottlenecked
by wide-scale information seeking, which is more repetitive than cognitively
complex. With the rapid development of Large Language Models (LLMs), automated
search agents powered by LLMs offer a promising solution to liberate humans
from this tedious work. However, the capability of these agents to perform such
"wide-context" collection reliably and completely remains largely unevaluated
due to a lack of suitable benchmarks. To bridge this gap, we introduce
WideSearch, a new benchmark engineered to evaluate agent reliability on these
large-scale collection tasks. The benchmark features 200 manually curated
questions (100 in English, 100 in Chinese) from over 15 diverse domains,
grounded in real user queries. Each task requires agents to collect large-scale
atomic information, which could be verified one by one objectively, and arrange
it into a well-organized output. A rigorous five-stage quality control pipeline
ensures the difficulty, completeness, and verifiability of the dataset. We
benchmark over 10 state-of-the-art agentic search systems, including
single-agent, multi-agent frameworks, and end-to-end commercial systems. Most
systems achieve overall success rates near 0\%, with the best performer
reaching just 5\%. However, given sufficient time, cross-validation by multiple
human testers can achieve a near 100\% success rate. These results demonstrate
that present search agents have critical deficiencies in large-scale
information seeking, underscoring urgent areas for future research and
development in agentic search. Our dataset, evaluation pipeline, and benchmark
results have been publicly released at https://widesearch-seed.github.io/

</details>


### [300] [Progressive Depth Up-scaling via Optimal Transport](https://arxiv.org/abs/2508.08011)
*Mingzi Cao,Xi Wang,Nikolaos Aletras*

Main category: cs.CL

TL;DR: OpT-DeUS利用最优传输对齐神经元，提升大语言模型深度扩展的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度扩展方法忽视神经元排列差异，可能导致性能下降。

Method: 通过最优传输对齐和融合相邻基础层的Transformer块，创建新层。

Result: OpT-DeUS在持续预训练和监督微调中表现更优，训练效率更高。

Conclusion: 新层插入靠近顶部时，训练效率更高且性能提升更显著。

Abstract: Scaling Large Language Models (LLMs) yields performance gains but incurs
substantial training costs. Depth up-scaling offers training efficiency by
adding new layers to pre-trained models. However, most existing methods copy or
average weights from base layers, neglecting neuron permutation differences.
This limitation can potentially cause misalignment that harms performance.
Inspired by applying Optimal Transport (OT) for neuron alignment, we propose
Optimal Transport Depth Up-Scaling (OpT-DeUS). OpT-DeUS aligns and fuses
Transformer blocks in adjacent base layers via OT for new layer creation, to
mitigate neuron permutation mismatch between layers. OpT-DeUS achieves better
overall performance and offers improved training efficiency than existing
methods for continual pre-training and supervised fine-tuning across different
model sizes. To further evaluate the impact of interpolation positions, our
extensive analysis shows that inserting new layers closer to the top results in
higher training efficiency due to shorter back-propagation time while obtaining
additional performance gains.

</details>


### [301] [9th Workshop on Sign Language Translation and Avatar Technologies (SLTAT 2025)](https://arxiv.org/abs/2508.08050)
*Fabrizio Nunnari,Cristina Luna Jiménez,Rosalee Wolfe,John C. McDonald,Michael Filhol,Eleni Efthimiou,Evita Fotinea,Thomas Hanke*

Main category: cs.CL

TL;DR: SLTAT 2025研讨会聚焦于通过非侵入性技术改善聋人/人类交流，涵盖手语识别、数据收集与分析、工具开发、伦理、可用性及情感计算等领域。


<details>
  <summary>Details</summary>
Motivation: 推动聋人与人类之间的无障碍交流，结合虚拟代理技术提升沟通效率。

Method: 通过研讨会形式汇集研究成果，包括手语翻译、虚拟化身技术及相关工具开发。

Result: 展示了手语识别、数据分析和情感计算等多方面的进展，促进了跨领域合作。

Conclusion: SLTAT 2025为聋人交流技术的创新提供了平台，推动了虚拟代理与手语技术的融合。

Abstract: The Sign Language Translation and Avatar Technology (SLTAT) workshops
continue a series of gatherings to share recent advances in improving deaf /
human communication through non-invasive means. This 2025 edition, the 9th
since its first appearance in 2011, is hosted by the International Conference
on Intelligent Virtual Agents (IVA), giving the opportunity for contamination
between two research communities, using digital humans as either virtual
interpreters or as interactive conversational agents. As presented in this
summary paper, SLTAT sees contributions beyond avatar technologies, with a
consistent number of submissions on sign language recognition, and other work
on data collection, data analysis, tools, ethics, usability, and affective
computing.

</details>


### [302] [Dual Information Speech Language Models for Emotional Conversations](https://arxiv.org/abs/2508.08095)
*Chun Wang,Chenyang Liu,Wenze Xu,Weihong Deng*

Main category: cs.CL

TL;DR: 论文提出了一种解决语音语言模型（SLMs）在捕捉副语言信息和上下文理解不足问题的方法，通过异构适配器和弱监督训练策略，实现了副语言与语言信息的解耦，并在情感对话任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 基于文本的大型语言模型（LLMs）在对话系统中常忽略副语言线索，而语音语言模型（SLMs）虽能解决此问题，但现有方法因信息纠缠和训练策略不当导致性能受限。

Method: 提出两种异构适配器和弱监督训练策略，解耦副语言与语言信息，并通过受控随机性避免生成任务特定向量，保持上下文理解。

Result: 实验表明，该方法在情感对话任务中表现优异，能有效整合副语言和语言信息。

Conclusion: 该方法通过解耦信息和优化训练策略，提升了SLMs的性能，为语音对话系统提供了更高效的工具。

Abstract: Conversational systems relying on text-based large language models (LLMs)
often overlook paralinguistic cues, essential for understanding emotions and
intentions. Speech-language models (SLMs), which use speech as input, are
emerging as a promising solution. However, SLMs built by extending frozen LLMs
struggle to capture paralinguistic information and exhibit reduced context
understanding. We identify entangled information and improper training
strategies as key issues. To address these issues, we propose two heterogeneous
adapters and suggest a weakly supervised training strategy. Our approach
disentangles paralinguistic and linguistic information, enabling SLMs to
interpret speech through structured representations. It also preserves
contextual understanding by avoiding the generation of task-specific vectors
through controlled randomness. This approach trains only the adapters on common
datasets, ensuring parameter and data efficiency. Experiments demonstrate
competitive performance in emotional conversation tasks, showcasing the model's
ability to effectively integrate both paralinguistic and linguistic information
within contextual settings.

</details>


### [303] [Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?](https://arxiv.org/abs/2508.08096)
*Lukas Gehring,Benjamin Paaßen*

Main category: cs.CL

TL;DR: 论文研究了教育场景中检测大语言模型（LLM）生成文本的方法，提出了新数据集GEDE，并发现现有检测器在中间贡献级别文本上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的普及，学生可能利用其生成文本，挑战学术诚信。需要有效检测方法以确保学习效果。

Method: 提出贡献级别概念，评估不同检测器在GEDE数据集上的表现，涵盖从纯人工到完全LLM生成文本的多种情况。

Result: 多数检测器难以准确识别中间贡献级别文本（如LLM改进的人工文本），且易产生假阳性。

Conclusion: 现有检测器在教育场景中存在局限性，需进一步改进以减少误判对学生的影响。

Abstract: Recent advancements in Large Language Models (LLMs) and their increased
accessibility have made it easier than ever for students to automatically
generate texts, posing new challenges for educational institutions. To enforce
norms of academic integrity and ensure students' learning, learning analytics
methods to automatically detect LLM-generated text appear increasingly
appealing. This paper benchmarks the performance of different state-of-the-art
detectors in educational contexts, introducing a novel dataset, called
Generative Essay Detection in Education (GEDE), containing over 900
student-written essays and over 12,500 LLM-generated essays from various
domains. To capture the diversity of LLM usage practices in generating text, we
propose the concept of contribution levels, representing students' contribution
to a given assignment. These levels range from purely human-written texts, to
slightly LLM-improved versions, to fully LLM-generated texts, and finally to
active attacks on the detector by "humanizing" generated texts. We show that
most detectors struggle to accurately classify texts of intermediate student
contribution levels, like LLM-improved human-written texts. Detectors are
particularly likely to produce false positives, which is problematic in
educational settings where false suspicions can severely impact students'
lives. Our dataset, code, and additional supplementary materials are publicly
available at
https://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts.

</details>


### [304] [Iterative refinement, not training objective, makes HuBERT behave differently from wav2vec 2.0](https://arxiv.org/abs/2508.08110)
*Robin Huo,Ewan Dunbar*

Main category: cs.CL

TL;DR: 研究比较了HuBERT和wav2vec 2.0两种自监督语音表示学习模型的架构差异，发现训练迭代次数对隐藏表示的语言信息编码影响更大。


<details>
  <summary>Details</summary>
Motivation: 自监督语音表示学习模型广泛用于下游任务，但其架构对学习到的语言信息的影响尚未充分研究。

Method: 比较HuBERT和wav2vec 2.0的训练目标和迭代伪标签细化，分析隐藏表示与词、音素和说话者身份的典型相关性。

Result: 训练迭代次数（而非训练目标）解释了隐藏表示的语言信息编码差异。

Conclusion: 建议未来研究探索迭代细化在自监督语音表示中编码语言信息的有效性原因。

Abstract: Self-supervised models for speech representation learning now see widespread
use for their versatility and performance on downstream tasks, but the effect
of model architecture on the linguistic information learned in their
representations remains under-studied. This study investigates two such models,
HuBERT and wav2vec 2.0, and minimally compares two of their architectural
differences: training objective and iterative pseudo-label refinement through
multiple training iterations. We find that differences in canonical correlation
of hidden representations to word identity, phoneme identity, and speaker
identity are explained by training iteration, not training objective. We
suggest that future work investigate the reason for the effectiveness of
iterative refinement in encoding linguistic information in self-supervised
speech representations.

</details>


### [305] [Czech Dataset for Complex Aspect-Based Sentiment Analysis Tasks](https://arxiv.org/abs/2508.08125)
*Jakub Šmíd,Pavel Přibáň,Ondřej Pražák,Pavel Král*

Main category: cs.CL

TL;DR: 本文介绍了一个新的捷克语数据集，用于基于方面的情感分析（ABSA），包含3.1K手动标注的餐厅评论，支持更复杂的任务，如目标-方面-类别检测。


<details>
  <summary>Details</summary>
Motivation: 现有捷克语数据集仅支持基本ABSA任务，缺乏复杂任务的统一标注格式，限制了跨语言比较和应用。

Method: 基于SemEval-2016格式构建新数据集，采用双标注者标注，一致性达90%，并提供24M未标注评论用于无监督学习。

Result: 数据集支持复杂任务，标注一致性高，并提供了基于Transformer的基线结果和错误分析。

Conclusion: 新数据集填补了捷克语ABSA复杂任务的空白，支持跨语言研究，代码和数据已开源。

Abstract: In this paper, we introduce a novel Czech dataset for aspect-based sentiment
analysis (ABSA), which consists of 3.1K manually annotated reviews from the
restaurant domain. The dataset is built upon the older Czech dataset, which
contained only separate labels for the basic ABSA tasks such as aspect term
extraction or aspect polarity detection. Unlike its predecessor, our new
dataset is specifically designed for more complex tasks, e.g.
target-aspect-category detection. These advanced tasks require a unified
annotation format, seamlessly linking sentiment elements (labels) together. Our
dataset follows the format of the well-known SemEval-2016 datasets. This design
choice allows effortless application and evaluation in cross-lingual scenarios,
ultimately fostering cross-language comparisons with equivalent counterpart
datasets in other languages. The annotation process engaged two trained
annotators, yielding an impressive inter-annotator agreement rate of
approximately 90%. Additionally, we provide 24M reviews without annotations
suitable for unsupervised learning. We present robust monolingual baseline
results achieved with various Transformer-based models and insightful error
analysis to supplement our contributions. Our code and dataset are freely
available for non-commercial research purposes.

</details>


### [306] [Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models](https://arxiv.org/abs/2508.08131)
*Wenze Xu,Chun Wang,Jiazhen Yu,Sheng Chen,Liang Gao,Weihong Deng*

Main category: cs.CL

TL;DR: 论文提出了一种名为OTReg的方法，通过最优传输正则化来缩小语音与文本表示之间的模态差距，从而提升口语语言模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 口语语言模型（SLMs）在处理语音输入时存在模态差距问题，导致泛化能力不足，即使对已训练的语言和任务也是如此。

Method: 引入最优传输正则化（OTReg），将语音与文本对齐问题建模为最优传输问题，并通过正则化损失优化SLM训练。

Result: 实验表明，OTReg能有效提升语音与文本的对齐效果，缩小模态差距，并显著改善SLM在多语言ASR任务中的泛化性能。

Conclusion: OTReg是一种轻量级方法，无需额外标签或可学习参数，可无缝集成到现有SLM训练中，为解决模态差距问题提供了有效方案。

Abstract: Spoken Language Models (SLMs), which extend Large Language Models (LLMs) to
perceive speech inputs, have gained increasing attention for their potential to
advance speech understanding tasks. However, despite recent progress, studies
show that SLMs often struggle to generalize across datasets, even for trained
languages and tasks, raising concerns about whether they process speech in a
text-like manner as intended. A key challenge underlying this limitation is the
modality gap between speech and text representations. The high variability in
speech embeddings may allow SLMs to achieve strong in-domain performance by
exploiting unintended speech variations, ultimately hindering generalization.
To mitigate this modality gap, we introduce Optimal Transport Regularization
(OTReg), a method that formulates speech-text alignment as an optimal transport
problem and derives a regularization loss to improve SLM training. In each
training iteration, OTReg first establishes a structured correspondence between
speech and transcript embeddings by determining the optimal transport plan,
then incorporates the regularization loss based on this transport plan to
optimize SLMs in generating speech embeddings that align more effectively with
transcript embeddings. OTReg is lightweight, requiring no additional labels or
learnable parameters, and integrates seamlessly into existing SLM training
procedures. Extensive multilingual ASR experiments demonstrate that OTReg
enhances speech-text alignment, mitigates the modality gap, and consequently
improves SLM generalization across diverse datasets.

</details>


### [307] [Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models](https://arxiv.org/abs/2508.08139)
*Tianyi Zhou,Johanne Medina,Sanjay Chawla*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）生成不可靠内容的问题，提出了一种基于标记级不确定性的可靠性估计方法，通过实验验证了上下文信息对模型行为的影响。


<details>
  <summary>Details</summary>
Motivation: LLMs容易生成流畅但不正确的内容（即虚构），在多轮或代理应用中可能带来风险。研究旨在探索上下文信息如何影响模型行为，以及LLMs是否能识别自身不可靠的响应。

Method: 提出了一种可靠性估计方法，利用标记级不确定性（包括偶然性和认知不确定性）来识别关键标记，并将其隐藏状态聚合为紧凑表示，用于预测响应级可靠性。

Result: 实验表明，正确的上下文信息提高了答案准确性和模型信心，而误导性上下文常导致模型自信地生成错误答案。提出的方法能有效检测不可靠输出。

Conclusion: 研究揭示了直接不确定性信号的局限性，强调了不确定性引导探测在可靠性感知生成中的潜力。

Abstract: Large Language Models (LLMs) are prone to generating fluent but incorrect
content, known as confabulation, which poses increasing risks in multi-turn or
agentic applications where outputs may be reused as context. In this work, we
investigate how in-context information influences model behavior and whether
LLMs can identify their unreliable responses. We propose a reliability
estimation that leverages token-level uncertainty to guide the aggregation of
internal model representations. Specifically, we compute aleatoric and
epistemic uncertainty from output logits to identify salient tokens and
aggregate their hidden states into compact representations for response-level
reliability prediction. Through controlled experiments on open QA benchmarks,
we find that correct in-context information improves both answer accuracy and
model confidence, while misleading context often induces confidently incorrect
responses, revealing a misalignment between uncertainty and correctness. Our
probing-based method captures these shifts in model behavior and improves the
detection of unreliable outputs across multiple open-source LLMs. These results
underscore the limitations of direct uncertainty signals and highlight the
potential of uncertainty-guided probing for reliability-aware generation.

</details>


### [308] [Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced Submodular Perspective](https://arxiv.org/abs/2508.08140)
*Jun Wang,Zaifu Zhan,Qixin Zhang,Mingquan Lin,Meijia Song,Rui Zhang*

Main category: cs.CL

TL;DR: Dual-Div框架通过两阶段检索和排序，优化生物医学NLP任务中的示例选择，提升LLMs的上下文学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在示例选择中偏重代表性而非多样性，Dual-Div旨在填补这一空白。

Method: 采用两阶段检索和排序：先优化代表性和多样性筛选候选示例，再针对测试查询排序选择最相关且非冗余的示例。

Result: 在三个生物医学NLP任务中，Dual-Div比基线方法表现更优（F1分数提升5%），且对提示变化和类别不平衡具有鲁棒性。

Conclusion: 初始检索阶段的多样性比排序优化更重要，且3-5个示例能最大化性能效率。

Abstract: Recent progress in large language models (LLMs) has leveraged their
in-context learning (ICL) abilities to enable quick adaptation to unseen
biomedical NLP tasks. By incorporating only a few input-output examples into
prompts, LLMs can rapidly perform these new tasks. While the impact of these
demonstrations on LLM performance has been extensively studied, most existing
approaches prioritize representativeness over diversity when selecting examples
from large corpora. To address this gap, we propose Dual-Div, a
diversity-enhanced data-efficient framework for demonstration selection in
biomedical ICL. Dual-Div employs a two-stage retrieval and ranking process:
First, it identifies a limited set of candidate examples from a corpus by
optimizing both representativeness and diversity (with optional annotation for
unlabeled data). Second, it ranks these candidates against test queries to
select the most relevant and non-redundant demonstrations. Evaluated on three
biomedical NLP tasks (named entity recognition (NER), relation extraction (RE),
and text classification (TC)) using LLaMA 3.1 and Qwen 2.5 for inference, along
with three retrievers (BGE-Large, BMRetriever, MedCPT), Dual-Div consistently
outperforms baselines-achieving up to 5% higher macro-F1 scores-while
demonstrating robustness to prompt permutations and class imbalance. Our
findings establish that diversity in initial retrieval is more critical than
ranking-stage optimization, and limiting demonstrations to 3-5 examples
maximizes performance efficiency.

</details>


### [309] [REX-RAG: Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.08149)
*Wentao Jiang,Xiang Feng,Zengmao Wang,Yong Luo,Pingbo Xu,Zhe Chen,Bo Du,Jing Zhang*

Main category: cs.CL

TL;DR: 论文提出REX-RAG框架，通过混合采样策略和政策校正机制解决LLMs在强化学习中的推理路径问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在强化学习中常陷入无效推理路径（dead ends），导致决策错误，阻碍探索和优化。

Method: 提出REX-RAG框架，结合混合采样策略（探索新路径）和政策校正机制（纠正分布偏移）。

Result: 在七个问答基准测试中，REX-RAG平均性能提升5.1%（Qwen2.5-3B）和3.6%（Qwen2.5-7B）。

Conclusion: REX-RAG有效解决了LLMs在强化学习中的推理路径问题，显著提升了性能。

Abstract: Reinforcement learning (RL) is emerging as a powerful paradigm for enabling
large language models (LLMs) to perform complex reasoning tasks. Recent
advances indicate that integrating RL with retrieval-augmented generation (RAG)
allows LLMs to dynamically incorporate external knowledge, leading to more
informed and robust decision making. However, we identify a critical challenge
during policy-driven trajectory sampling: LLMs are frequently trapped in
unproductive reasoning paths, which we refer to as "dead ends", committing to
overconfident yet incorrect conclusions. This severely hampers exploration and
undermines effective policy optimization. To address this challenge, we propose
REX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented
Generation), a novel framework that explores alternative reasoning paths while
maintaining rigorous policy learning through principled distributional
corrections. Our approach introduces two key innovations: (1) Mixed Sampling
Strategy, which combines a novel probe sampling method with exploratory prompts
to escape dead ends; and (2) Policy Correction Mechanism, which employs
importance sampling to correct distribution shifts induced by mixed sampling,
thereby mitigating gradient estimation bias. We evaluate it on seven
question-answering benchmarks, and the experimental results show that REX-RAG
achieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B
over strong baselines, demonstrating competitive results across multiple
datasets. The code is publicly available at https://github.com/MiliLab/REX-RAG.

</details>


### [310] [LPI-RIT at LeWiDi-2025: Improving Distributional Predictions via Metadata and Loss Reweighting with DisCo](https://arxiv.org/abs/2508.08163)
*Mandira Sawkar,Samay U. Shetty,Deepak Pandita,Tharindu Cyril Weerasooriya,Christopher M. Homan*

Main category: cs.CL

TL;DR: LeWiDi 2025任务通过改进DisCo模型，结合标注者元数据和优化损失函数，显著提升了软标签分布预测和视角评估的性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何更好地建模标注者分歧，通过软标签分布预测和视角评估来捕捉数据复杂性。

Method: 改进DisCo模型，加入标注者元数据、增强输入表示并调整损失函数，以更好地捕捉分歧模式。

Result: 在三个数据集上，软标签和视角评估指标均有显著提升，并通过误差和校准分析验证了改进效果。

Conclusion: 分歧感知建模具有重要价值，系统组件与复杂标注数据的交互提供了深入见解。

Abstract: The Learning With Disagreements (LeWiDi) 2025 shared task is to model
annotator disagreement through soft label distribution prediction and
perspectivist evaluation, modeling annotators. We adapt DisCo (Distribution
from Context), a neural architecture that jointly models item-level and
annotator-level label distributions, and present detailed analysis and
improvements. In this paper, we extend the DisCo by incorporating annotator
metadata, enhancing input representations, and modifying the loss functions to
capture disagreement patterns better. Through extensive experiments, we
demonstrate substantial improvements in both soft and perspectivist evaluation
metrics across three datasets. We also conduct in-depth error and calibration
analyses, highlighting the conditions under which improvements occur. Our
findings underscore the value of disagreement-aware modeling and offer insights
into how system components interact with the complexity of human-annotated
data.

</details>


### [311] [Efficient Speculative Decoding for Llama at Scale: Challenges and Solutions](https://arxiv.org/abs/2508.08192)
*Bangsheng Tang,Carl Chengyan Fu,Fei Kou,Grigory Sizov,Haoci Zhang,Jason Park,Jiawen Liu,Jie You,Qirui Yang,Sachin Mehta,Shengyong Cai,Xiaodong Wang,Xingyu Liu,Yunlu Li,Yanjun Zhou,Wei Wei,Zhiwei Zhao,Zixi Qi,Adolfo Victoria,Aya Ibrahim,Bram Wasti,Changkyu Kim,Daniel Haziza,Fei Sun,Giancarlo Delfin,Emily Guo,Jialin Ouyang,Jaewon Lee,Jianyu Huang,Jeremy Reizenstein,Lu Fang,Quinn Zhu,Ria Verma,Vlad Mihailescu,Xingwen Guo,Yan Cui,Ye Hu,Yejin Lee*

Main category: cs.CL

TL;DR: 论文提出了一种优化EAGLE-based推测解码的方法，用于加速Llama模型的生产级推理，实现了新的最佳推理延迟。


<details>
  <summary>Details</summary>
Motivation: 解决在生产环境中扩展推测解码的工程挑战，包括高效实现GPU上的树注意力和多轮推测解码等操作。

Method: 详细介绍了训练和推理优化技术，以支持Llama模型的生产级EAGLE-based推测解码。

Result: 在8个NVIDIA H100 GPU上，Llama4 Maverick的推理延迟为每token约4毫秒（批量大小为1），比之前最佳方法快10%。对于大批量，优化实现了1.4x至2.0x的加速。

Conclusion: 通过优化技术，成功实现了生产级EAGLE-based推测解码的高效扩展，显著提升了Llama模型的推理速度。

Abstract: Speculative decoding is a standard method for accelerating the inference
speed of large language models. However, scaling it for production environments
poses several engineering challenges, including efficiently implementing
different operations (e.g., tree attention and multi-round speculative
decoding) on GPU. In this paper, we detail the training and inference
optimization techniques that we have implemented to enable EAGLE-based
speculative decoding at a production scale for Llama models. With these
changes, we achieve a new state-of-the-art inference latency for Llama models.
For example, Llama4 Maverick decodes at a speed of about 4 ms per token (with a
batch size of one) on 8 NVIDIA H100 GPUs, which is 10% faster than the
previously best known method. Furthermore, for EAGLE-based speculative
decoding, our optimizations enable us to achieve a speed-up for large batch
sizes between 1.4x and 2.0x at production scale.

</details>


### [312] [Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models](https://arxiv.org/abs/2508.08204)
*Kyle Moore,Jesse Roberts,Daryl Watson*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型的不确定性校准，特别关注推理时不确定性如何与人类不确定性对齐，并提出了一些新的评估方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于通过评估模型不确定性校准，以提升模型控制和用户信任，尤其是在推理时不确定性对实际应用的重要性。

Method: 论文评估了多种推理时不确定性度量方法，包括传统指标和新变体，以分析其与人类群体不确定性和模型校准的对齐情况。

Result: 研究发现多个度量方法显示出与人类不确定性的强对齐，尽管与人类答案偏好不一致，且这些方法在校准方面表现良好。

Conclusion: 结论是某些不确定性度量方法能有效对齐人类不确定性，并在模型校准方面表现优异，为实际应用提供了支持。

Abstract: There has been much recent interest in evaluating large language models for
uncertainty calibration to facilitate model control and modulate user trust.
Inference time uncertainty, which may provide a real-time signal to the model
or external control modules, is particularly important for applying these
concepts to improve LLM-user experience in practice. While many of the existing
papers consider model calibration, comparatively little work has sought to
evaluate how closely model uncertainty aligns to human uncertainty. In this
work, we evaluate a collection of inference-time uncertainty measures, using
both established metrics and novel variations, to determine how closely they
align with both human group-level uncertainty and traditional notions of model
calibration. We find that numerous measures show evidence of strong alignment
to human uncertainty, even despite the lack of alignment to human answer
preference. For those successful metrics, we find moderate to strong evidence
of model calibration in terms of both correctness correlation and
distributional analysis.

</details>


### [313] [SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling](https://arxiv.org/abs/2508.08211)
*Zhuohao Yu,Xingru Jiang,Weizheng Gu,Yidong Wang,Shikun Zhang,Wei Ye*

Main category: cs.CL

TL;DR: SAEMark是一种后处理多比特水印框架，通过推理时基于特征的拒绝采样嵌入个性化消息，无需修改模型逻辑或训练，适用于多语言和封闭源LLM。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法会降低文本质量且需要白盒模型访问，限制了API模型和多语言场景的应用。

Method: 利用生成文本的确定性特征进行拒绝采样，选择符合密钥目标的输出，保留文本质量。

Result: 在4个数据集上表现优异，英语F1达99.7%，多比特检测准确率高。

Conclusion: SAEMark为封闭源LLM提供了一种可扩展的水印范式，支持内容归属。

Abstract: Watermarking LLM-generated text is critical for content attribution and
misinformation prevention. However, existing methods compromise text quality,
require white-box model access and logit manipulation. These limitations
exclude API-based models and multilingual scenarios. We propose SAEMark, a
general framework for post-hoc multi-bit watermarking that embeds personalized
messages solely via inference-time, feature-based rejection sampling without
altering model logits or requiring training. Our approach operates on
deterministic features extracted from generated text, selecting outputs whose
feature statistics align with key-derived targets. This framework naturally
generalizes across languages and domains while preserving text quality through
sampling LLM outputs instead of modifying. We provide theoretical guarantees
relating watermark success probability and compute budget that hold for any
suitable feature extractor. Empirically, we demonstrate the framework's
effectiveness using Sparse Autoencoders (SAEs), achieving superior detection
accuracy and text quality. Experiments across 4 datasets show SAEMark's
consistent performance, with 99.7% F1 on English and strong multi-bit detection
accuracy. SAEMark establishes a new paradigm for scalable watermarking that
works out-of-the-box with closed-source LLMs while enabling content
attribution.

</details>


### [314] [Capabilities of GPT-5 on Multimodal Medical Reasoning](https://arxiv.org/abs/2508.08224)
*Shansong Wang,Mingzhe Hu,Qiang Li,Mojtaba Safari,Xiaofeng Yang*

Main category: cs.CL

TL;DR: GPT-5作为多模态医学推理模型，在零样本思维链推理任务中表现优于GPT-4o和人类专家，尤其在多模态推理方面提升显著。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（如GPT-5）在医学决策支持中的潜力，尤其是在整合异构信息（如文本和图像）方面的能力。

Method: 通过标准化数据集（如MedQA、MedXpertQA等）评估GPT-5及其变体在文本和多模态问答任务中的表现。

Result: GPT-5在所有基准测试中均达到最优性能，多模态推理能力显著提升，超越人类专家表现。

Conclusion: GPT-5在医学多模态推理任务中表现出色，为未来临床决策支持系统的设计提供了重要参考。

Abstract: Recent advances in large language models (LLMs) have enabled general-purpose
systems to perform increasingly complex domain-specific reasoning without
extensive fine-tuning. In the medical domain, decision-making often requires
integrating heterogeneous information sources, including patient narratives,
structured data, and medical images. This study positions GPT-5 as a generalist
multimodal reasoner for medical decision support and systematically evaluates
its zero-shot chain-of-thought reasoning performance on both text-based
question answering and visual question answering tasks under a unified
protocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20
against standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU
medical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that
GPT-5 consistently outperforms all baselines, achieving state-of-the-art
accuracy across all QA benchmarks and delivering substantial gains in
multimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and
understanding scores by +29.62% and +36.18% over GPT-4o, respectively, and
surpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in
understanding. In contrast, GPT-4o remains below human expert performance in
most dimensions. A representative case study demonstrates GPT-5's ability to
integrate visual and textual cues into a coherent diagnostic reasoning chain,
recommending appropriate high-stakes interventions. Our results show that, on
these controlled multimodal reasoning benchmarks, GPT-5 moves from
human-comparable to above human-expert performance. This improvement may
substantially inform the design of future clinical decision-support systems.

</details>


### [315] [Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health Dialogues via LLM-as-Judge](https://arxiv.org/abs/2508.08236)
*Yunna Cai,Fan Wang,Haowei Wang,Kun Wang,Kailai Yang,Sophia Ananiadou,Moyan Li,Mingming Fan*

Main category: cs.CL

TL;DR: PsyCrisis-Bench是一个无参考评估基准，用于评估LLM在心理健康对话中的安全对齐性，采用基于提示的LLM-as-Judge方法，并在多维度上评分。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏黄金标准答案且心理健康对话涉及伦理敏感性，评估LLM在高风险心理健康对话中的安全对齐性具有挑战性。

Method: 提出PsyCrisis-Bench基准，采用基于提示的LLM-as-Judge方法，结合专家定义的心理干预原则进行上下文评估，并使用多维度二元评分。

Result: 在3600次评估中，该方法与专家评估一致性最高，且评估理由更易解释。

Conclusion: PsyCrisis-Bench及其数据集公开可用，为心理健康对话的安全对齐性研究提供了工具。

Abstract: Evaluating the safety alignment of LLM responses in high-risk mental health
dialogues is particularly difficult due to missing gold-standard answers and
the ethically sensitive nature of these interactions. To address this
challenge, we propose PsyCrisis-Bench, a reference-free evaluation benchmark
based on real-world Chinese mental health dialogues. It evaluates whether the
model responses align with the safety principles defined by experts.
Specifically designed for settings without standard references, our method
adopts a prompt-based LLM-as-Judge approach that conducts in-context evaluation
using expert-defined reasoning chains grounded in psychological intervention
principles. We employ binary point-wise scoring across multiple safety
dimensions to enhance the explainability and traceability of the evaluation.
Additionally, we present a manually curated, high-quality Chinese-language
dataset covering self-harm, suicidal ideation, and existential distress,
derived from real-world online discourse. Experiments on 3600 judgments show
that our method achieves the highest agreement with expert assessments and
produces more interpretable evaluation rationales compared to existing
approaches. Our dataset and evaluation tool are publicly available to
facilitate further research.

</details>


### [316] [Jinx: Unlimited LLMs for Probing Alignment Failures](https://arxiv.org/abs/2508.08243)
*Jiahao Zhao,Liwei Dong*

Main category: cs.CL

TL;DR: Jinx是一个无安全限制的语言模型变体，用于研究社区评估语言模型的安全对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有无安全限制的语言模型仅用于内部评估，研究社区无法获取，限制了安全对齐的研究。

Method: 基于流行开源权重LLMs开发Jinx，保留推理和指令跟随能力，但对所有查询无拒绝或安全过滤。

Result: Jinx为研究社区提供了一个工具，用于探测对齐失败、评估安全边界和系统研究失败模式。

Conclusion: Jinx填补了研究社区在语言模型安全对齐评估工具上的空白。

Abstract: Unlimited, or so-called helpful-only language models are trained without
safety alignment constraints and never refuse user queries. They are widely
used by leading AI companies as internal tools for red teaming and alignment
evaluation. For example, if a safety-aligned model produces harmful outputs
similar to an unlimited model, this indicates alignment failures that require
further attention. Despite their essential role in assessing alignment, such
models are not available to the research community.
  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx
responds to all queries without refusals or safety filtering, while preserving
the base model's capabilities in reasoning and instruction following. It
provides researchers with an accessible tool for probing alignment failures,
evaluating safety boundaries, and systematically studying failure modes in
language model safety.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [317] [Automated Seam Folding and Sewing Machine on Pleated Pants for Apparel Manufacturing](https://arxiv.org/abs/2508.06518)
*Ray Wai Man Kong*

Main category: cs.RO

TL;DR: 研究设计了一款自动化折叠与缝纫机，用于褶皱裤子的生产，显著提升了效率并减少了人工操作的需求。


<details>
  <summary>Details</summary>
Motivation: 传统褶皱制作方法劳动密集、易出错且依赖高技能工人，自动化需求迫切。

Method: 开发了一款集成精密折叠机制和实时监控功能的自动化缝纫机，消除了标记步骤。

Result: 劳动时间减少93%，机器时间提升73%，总产出率增加72%，周期时间从117秒降至33秒。

Conclusion: 该自动化机器降低了成本与浪费，符合可持续性和高效性的行业趋势。

Abstract: The applied research is the design and development of an automated folding
and sewing machine for pleated pants. It represents a significant advancement
in addressing the challenges associated with manual sewing processes.
Traditional methods for creating pleats are labour-intensive, prone to
inconsistencies, and require high levels of skill, making automation a critical
need in the apparel industry. This research explores the technical feasibility
and operational benefits of integrating advanced technologies into garment
production, focusing on the creation of an automated machine capable of precise
folding and sewing operations and eliminating the marking operation.
  The proposed machine incorporates key features such as a precision folding
mechanism integrated into the automated sewing unit with real-time monitoring
capabilities. The results demonstrate remarkable improvements: the standard
labour time has been reduced by 93%, dropping from 117 seconds per piece to
just 8 seconds with the automated system. Similarly, machinery time improved by
73%, and the total output rate increased by 72%. These enhancements translate
into a cycle time reduction from 117 seconds per piece to an impressive 33
seconds, enabling manufacturers to meet customer demand more swiftly. By
eliminating manual marking processes, the machine not only reduces labour costs
but also minimizes waste through consistent pleat formation. This automation
aligns with industry trends toward sustainability and efficiency, potentially
reducing environmental impact by decreasing material waste and energy
consumption.

</details>


### [318] [Optimization of Flip-Landing Trajectories for Starship based on a Deep Learned Simulator](https://arxiv.org/abs/2508.06520)
*Liwei Chen,Tong Qin,Zhenhua Huangfu,Li Li,Wei Wei*

Main category: cs.RO

TL;DR: 提出了一种基于可微分优化的可重复使用航天器翻转与着陆轨迹设计框架，结合深度学习与动力学求解器，实现端到端梯度优化。


<details>
  <summary>Details</summary>
Motivation: 解决传统轨迹优化方法中因线性化或凸松弛导致的精度不足问题，提高复杂非线性机动任务的建模与优化能力。

Method: 使用深度神经网络代理模型预测气动力与力矩，结合可微分刚体动力学求解器，支持端到端梯度优化，并处理执行器限制与终端着陆约束。

Result: 框架成功建模并优化了高非线性复杂机动，展示了物理一致的优化控制序列。

Conclusion: 为未来涉及非定常气动力学、羽流交互和智能制导设计的扩展奠定了基础。

Abstract: We propose a differentiable optimization framework for flip-and-landing
trajectory design of reusable spacecraft, exemplified by the Starship vehicle.
A deep neural network surrogate, trained on high-fidelity CFD data, predicts
aerodynamic forces and moments, and is tightly coupled with a differentiable
rigid-body dynamics solver. This enables end-to-end gradient-based trajectory
optimization without linearization or convex relaxation. The framework handles
actuator limits and terminal landing constraints, producing physically
consistent, optimized control sequences. Both standard automatic
differentiation and Neural ODEs are applied to support long-horizon rollouts.
Results demonstrate the framework's effectiveness in modeling and optimizing
complex maneuvers with high nonlinearities. This work lays the groundwork for
future extensions involving unsteady aerodynamics, plume interactions, and
intelligent guidance design.

</details>


### [319] [Stinger Robot: A Self-Bracing Robotic Platform for Autonomous Drilling in Confined Underground Environments](https://arxiv.org/abs/2508.06521)
*H. Liu,L. S. Moreu,T. S. Andersen,V. V. Puche,M. Fumagalli*

Main category: cs.RO

TL;DR: 本文介绍了一种名为Stinger Robot的新型紧凑机器人平台，专为在废弃地下矿山等极端环境中进行自主高力钻孔而设计。


<details>
  <summary>Details</summary>
Motivation: 随着对关键原材料需求的增加，废弃地下矿山重新受到关注，但传统钻探设备难以应对其狭窄、无结构和无基础设施的环境。

Method: Stinger Robot采用机械自锁三腿支撑机制，可在不规则隧道表面稳定锚定，并通过力感知闭环控制策略实现与无结构环境的动态交互。控制策略基于ROS 2的有限状态机实现，实时调整腿部部署以确保稳定性。

Result: 通过仿真和初步硬件测试，证明该机器人能在传统采矿设备无法工作的环境中自主稳定并钻孔。

Conclusion: Stinger Robot是首个已验证的集成分布式力支撑和自主钻孔的机器人架构，为未来模块化机器人协作采矿奠定了基础。

Abstract: The increasing demand for critical raw materials has revitalized interest in
abandoned underground mines, which pose extreme challenges for conventional
drilling machinery due to confined, unstructured, and infrastructure-less
environments. This paper presents the Stinger Robot, a novel compact robotic
platform specifically designed for autonomous high-force drilling in such
settings. The robot features a mechanically self-locking tri-leg bracing
mechanism that enables stable anchoring to irregular tunnel surfaces. A key
innovation lies in its force-aware, closed-loop control strategy, which enables
force interaction with unstructured environments during bracing and drilling.
Implemented as a finite-state machine in ROS 2, the control policy dynamically
adapts leg deployment based on real-time contact feedback and load thresholds,
ensuring stability without external supports. We demonstrate, through
simulation and preliminary hardware tests, that the Stinger Robot can
autonomously stabilize and drill in conditions previously inaccessible to
nowadays mining machines. This work constitutes the first validated robotic
architecture to integrate distributed force-bracing and autonomous drilling in
underground environments, laying the groundwork for future collaborative mining
operations using modular robot systems.

</details>


### [320] [MetAdv: A Unified and Interactive Adversarial Testing Platform for Autonomous Driving](https://arxiv.org/abs/2508.06534)
*Aishan Liu,Jiakai Wang,Tianyuan Zhang,Hainan Li,Jiangfan Liu,Siyuan Liang,Yilong Ren,Xianglong Liu,Dacheng Tao*

Main category: cs.RO

TL;DR: MetAdv是一个新型对抗测试平台，通过虚拟仿真与物理车辆反馈的紧密结合，实现了对自动驾驶系统的动态、交互式评估。


<details>
  <summary>Details</summary>
Motivation: 评估和确保自动驾驶系统的对抗鲁棒性是一个关键且未解决的挑战。

Method: MetAdv建立了一个混合虚拟-物理沙盒，设计了三层闭环测试环境，支持从高层对抗生成到低层物理车辆执行的端到端评估。

Result: MetAdv支持广泛的自动驾驶任务和算法范式，并具备人机交互能力，可实时捕获驾驶员生理信号和行为反馈。

Conclusion: MetAdv为对抗评估提供了一个可扩展的统一框架，有助于实现更安全的自动驾驶。

Abstract: Evaluating and ensuring the adversarial robustness of autonomous driving (AD)
systems is a critical and unresolved challenge. This paper introduces MetAdv, a
novel adversarial testing platform that enables realistic, dynamic, and
interactive evaluation by tightly integrating virtual simulation with physical
vehicle feedback. At its core, MetAdv establishes a hybrid virtual-physical
sandbox, within which we design a three-layer closed-loop testing environment
with dynamic adversarial test evolution. This architecture facilitates
end-to-end adversarial evaluation, ranging from high-level unified adversarial
generation, through mid-level simulation-based interaction, to low-level
execution on physical vehicles. Additionally, MetAdv supports a broad spectrum
of AD tasks, algorithmic paradigms (e.g., modular deep learning pipelines,
end-to-end learning, vision-language models). It supports flexible 3D vehicle
modeling and seamless transitions between simulated and physical environments,
with built-in compatibility for commercial platforms such as Apollo and Tesla.
A key feature of MetAdv is its human-in-the-loop capability: besides flexible
environmental configuration for more customized evaluation, it enables
real-time capture of physiological signals and behavioral feedback from
drivers, offering new insights into human-machine trust under adversarial
conditions. We believe MetAdv can offer a scalable and unified framework for
adversarial assessment, paving the way for safer AD.

</details>


### [321] [Symbolic Learning of Interpretable Reduced-Order Models for Jumping Quadruped Robots](https://arxiv.org/abs/2508.06538)
*Gioele Buriani,Jingyue Liu,Maximilian Stölzle,Cosimo Della Santina,Jiatao Ding*

Main category: cs.RO

TL;DR: 提出了一种结合SINDy与物理结构先验的新方法，用于构建四足机器人跳跃的可解释降阶模型，优于传统aSLIP模型。


<details>
  <summary>Details</summary>
Motivation: 简化复杂动力学同时保留关键行为，以支持四足机器人的运动规划与控制。

Method: 结合Sparse Identification of Nonlinear Dynamics (SINDy)与跳跃动力学的物理结构先验，构建低维潜在空间模型。

Result: 新方法在模拟和硬件实验中表现出比传统aSLIP模型更高的准确性。

Conclusion: 该方法为四足机器人跳跃提供了更精确且可解释的降阶模型。

Abstract: Reduced-order models are essential for motion planning and control of
quadruped robots, as they simplify complex dynamics while preserving critical
behaviors. This paper introduces a novel methodology for deriving such
interpretable dynamic models, specifically for jumping. We capture the
high-dimensional, nonlinear jumping dynamics in a low-dimensional latent space
by proposing a learning architecture combining Sparse Identification of
Nonlinear Dynamics (SINDy) with physical structural priors on the jump
dynamics. Our approach demonstrates superior accuracy to the traditional
actuated Spring-loaded Inverted Pendulum (aSLIP) model and is validated through
simulation and hardware experiments across different jumping strategies.

</details>


### [322] [A tutorial note on collecting simulated data for vision-language-action models](https://arxiv.org/abs/2508.06547)
*Heran Wu,Zirun Zhou,Jingfeng Zhang*

Main category: cs.RO

TL;DR: VLA模型通过单一神经网络统一处理视觉、语言和动作，但依赖高质量数据集。本文介绍了PyBullet、LIBERO和RT-X三种代表性系统。


<details>
  <summary>Details</summary>
Motivation: 传统机器人系统将智能分解为独立模块，VLA模型通过统一框架实现多模态处理，但需要高质量数据集支持。

Method: 使用PyBullet生成定制数据，LIBERO标准化任务定义与评估，RT-X支持大规模多机器人数据采集。

Result: 展示了PyBullet和LIBERO的数据生成方法，并概述了RT-X的特点与作用。

Conclusion: VLA模型依赖高质量数据集，PyBullet、LIBERO和RT-X为数据生成与采集提供了有效工具。

Abstract: Traditional robotic systems typically decompose intelligence into independent
modules for computer vision, natural language processing, and motion control.
Vision-Language-Action (VLA) models fundamentally transform this approach by
employing a single neural network that can simultaneously process visual
observations, understand human instructions, and directly output robot actions
-- all within a unified framework. However, these systems are highly dependent
on high-quality training datasets that can capture the complex relationships
between visual observations, language instructions, and robotic actions. This
tutorial reviews three representative systems: the PyBullet simulation
framework for flexible customized data generation, the LIBERO benchmark suite
for standardized task definition and evaluation, and the RT-X dataset
collection for large-scale multi-robot data acquisition. We demonstrated
dataset generation approaches in PyBullet simulation and customized data
collection within LIBERO, and provide an overview of the characteristics and
roles of the RT-X dataset for large-scale multi-robot data acquisition.

</details>


### [323] [AquaChat++: LLM-Assisted Multi-ROV Inspection for Aquaculture Net Pens with Integrated Battery Management and Thruster Fault Tolerance](https://arxiv.org/abs/2508.06554)
*Abdelhaleem Saad,Waseem Akram,Irfan Hussain*

Main category: cs.RO

TL;DR: AquaChat++是一个基于大型语言模型的多ROV检查框架，用于水产养殖网箱的智能检查，提高适应性和故障容忍能力。


<details>
  <summary>Details</summary>
Motivation: 传统水产养殖网箱检查方法适应性差，无法应对实时约束（如能耗、硬件故障和动态水下环境），需要更智能的解决方案。

Method: 提出两层架构：高层使用LLM（如ChatGPT-4）将自然语言命令转化为多代理检查计划；低层控制层实现轨迹跟踪和故障检测补偿。

Result: 模拟实验显示AquaChat++提高了检查覆盖率、能效和故障容忍能力。

Conclusion: LLM驱动的框架有望支持水产养殖中可扩展、智能和自主的水下机器人操作。

Abstract: Inspection of aquaculture net pens is essential for ensuring the structural
integrity and sustainable operation of offshore fish farming systems.
Traditional methods, typically based on manually operated or single-ROV
systems, offer limited adaptability to real-time constraints such as energy
consumption, hardware faults, and dynamic underwater conditions. This paper
introduces AquaChat++, a novel multi-ROV inspection framework that uses Large
Language Models (LLMs) to enable adaptive mission planning, coordinated task
execution, and fault-tolerant control in complex aquaculture environments. The
proposed system consists of a two-layered architecture. The high-level plan
generation layer employs an LLM, such as ChatGPT-4, to translate natural
language user commands into symbolic, multi-agent inspection plans. A task
manager dynamically allocates and schedules actions among ROVs based on their
real-time status and operational constraints, including thruster faults and
battery levels. The low-level control layer ensures accurate trajectory
tracking and integrates thruster fault detection and compensation mechanisms.
By incorporating real-time feedback and event-triggered replanning, AquaChat++
enhances system robustness and operational efficiency. Simulated experiments in
a physics-based aquaculture environment demonstrate improved inspection
coverage, energy-efficient behavior, and resilience to actuator failures. These
findings highlight the potential of LLM-driven frameworks to support scalable,
intelligent, and autonomous underwater robotic operations within the
aquaculture sector.

</details>


### [324] [Robust and Agile Quadrotor Flight via Adaptive Unwinding-Free Quaternion Sliding Mode Control](https://arxiv.org/abs/2508.06568)
*Amin Yazdanshenas,Reza Faieghi*

Main category: cs.RO

TL;DR: 提出了一种新的自适应滑模控制框架，用于四旋翼飞行器，在计算资源受限的情况下实现鲁棒和敏捷的飞行。


<details>
  <summary>Details</summary>
Motivation: 解决现有滑模控制方法在收敛速度、稳定性、旋转动力学简化、增益增长等问题上的局限性。

Method: 利用非光滑稳定性分析，设计全局稳定的姿态和位置滑模动力学，并在资源受限的纳米四旋翼上高效运行。

Result: 在超过130次飞行实验中，控制器表现优于三种基准方法，实现了高精度轨迹跟踪和低控制能耗。

Conclusion: 该控制器在外部干扰和计算资源受限的场景下具有实际应用潜力，支持高动态机动。

Abstract: This paper presents a new adaptive sliding mode control (SMC) framework for
quadrotors that achieves robust and agile flight under tight computational
constraints. The proposed controller addresses key limitations of prior SMC
formulations, including (i) the slow convergence and almost-global stability of
$\mathrm{SO(3)}$-based methods, (ii) the oversimplification of rotational
dynamics in Euler-based controllers, (iii) the unwinding phenomenon in
quaternion-based formulations, and (iv) the gain overgrowth problem in adaptive
SMC schemes. Leveraging nonsmooth stability analysis, we provide rigorous
global stability proofs for both the nonsmooth attitude sliding dynamics
defined on $\mathbb{S}^3$ and the position sliding dynamics. Our controller is
computationally efficient and runs reliably on a resource-constrained nano
quadrotor, achieving 250 Hz and 500 Hz refresh rates for position and attitude
control, respectively. In an extensive set of hardware experiments with over
130 flight trials, the proposed controller consistently outperforms three
benchmark methods, demonstrating superior trajectory tracking accuracy and
robustness with relatively low control effort. The controller enables
aggressive maneuvers such as dynamic throw launches, flip maneuvers, and
accelerations exceeding 3g, which is remarkable for a 32-gram nano quadrotor.
These results highlight promising potential for real-world applications,
particularly in scenarios requiring robust, high-performance flight control
under significant external disturbances and tight computational constraints.

</details>


### [325] [Efficient Safety Testing of Autonomous Vehicles via Adaptive Search over Crash-Derived Scenarios](https://arxiv.org/abs/2508.06575)
*Rui Zhou*

Main category: cs.RO

TL;DR: 本文提出了一种自适应大变量邻域模拟退火算法（ALVNS-SA），用于加速自动驾驶车辆在安全关键场景中的测试，显著提高了测试效率和场景覆盖率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆的安全性至关重要，尤其是在安全关键场景中。现有测试方法效率不足，亟需一种高效算法来验证其安全性。

Method: 1. 从中国深度移动安全研究-交通事故（CIMSS-TA）数据库中提取典型逻辑场景；2. 集成百度Apollo自动驾驶系统控制车辆行为；3. 提出ALVNS-SA算法加速测试。

Result: ALVNS-SA显著提升了测试效率，安全关键场景覆盖率达84.00%，其中碰撞场景覆盖率为96.83%，接近碰撞场景覆盖率为92.07%。

Conclusion: ALVNS-SA在安全关键场景测试中表现优异，优于遗传算法（GA）、自适应大邻域模拟退火算法（ALNS-SA）和随机测试。

Abstract: Ensuring the safety of autonomous vehicles (AVs) is paramount in their
development and deployment. Safety-critical scenarios pose more severe
challenges, necessitating efficient testing methods to validate AVs safety.
This study focuses on designing an accelerated testing algorithm for AVs in
safety-critical scenarios, enabling swift recognition of their driving
capabilities. First, typical logical scenarios were extracted from real-world
crashes in the China In-depth Mobility Safety Study-Traffic Accident (CIMSS-TA)
database, obtaining pre-crash features through reconstruction. Second, Baidu
Apollo, an advanced black-box automated driving system (ADS) is integrated to
control the behavior of the ego vehicle. Third, we proposed an adaptive
large-variable neighborhood-simulated annealing algorithm (ALVNS-SA) to
expedite the testing process. Experimental results demonstrate a significant
enhancement in testing efficiency when utilizing ALVNS-SA. It achieves an
84.00% coverage of safety-critical scenarios, with crash scenario coverage of
96.83% and near-crash scenario coverage of 92.07%. Compared to genetic
algorithm (GA), adaptive large neighborhood-simulated annealing algorithm
(ALNS-SA), and random testing, ALVNS-SA exhibits substantially higher coverage
in safety-critical scenarios.

</details>


### [326] [Optimal Planning and Machine Learning for Responsive Tracking and Enhanced Forecasting of Wildfires using a Spacecraft Constellation](https://arxiv.org/abs/2508.06687)
*Sreeja Roy-Singh,Vinay Ravindra,Richard Levinson,Mahta Moghaddam,Jan Mandel,Adam Kochanski,Angel Farguell Caus,Kurtis Nelson,Samira Alkaee Taleghan,Archana Kannan,Amer Melebari*

Main category: cs.RO

TL;DR: 本文提出了一种结合最优规划和机器学习的方法，用于收集和处理空间数据以监测野火，并改进现有决策支持工具。通过CYGNSS任务验证，该方法显著提升了数据收集效率和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有野火监测系统在数据收集、处理速度和预测准确性方面存在不足，需要一种高效、低延迟的解决方案。

Method: 采用混合整数规划调度卫星观测和数据下传，结合机器学习预测野火，生成燃烧区域地图并集成到现有模型中。

Result: 数据收集效率达98-100%，预测相关性提升40%，燃烧预测准确率提高13%，召回率提升15%，延迟缩短至6-30小时。

Conclusion: 该方法具有计算可扩展性和全球通用性，显著提升了野火监测和预测能力。

Abstract: We propose a novel concept of operations using optimal planning methods and
machine learning (ML) to collect spaceborne data that is unprecedented for
monitoring wildfires, process it to create new or enhanced products in the
context of wildfire danger or spread monitoring, and assimilate them to improve
existing, wildfire decision support tools delivered to firefighters within
latency appropriate for time-critical applications. The concept is studied with
respect to NASA's CYGNSS Mission, a constellation of passive microwave
receivers that measure specular GNSS-R reflections despite clouds and smoke.
Our planner uses a Mixed Integer Program formulation to schedule joint
observation data collection and downlink for all satellites. Optimal solutions
are found quickly that collect 98-100% of available observation opportunities.
ML-based fire predictions that drive the planner objective are greater than 40%
more correlated with ground truth than existing state-of-art. The presented
case study on the TX Smokehouse Creek fire in 2024 and LA fires in 2025
represents the first high-resolution data collected by CYGNSS of active fires.
Creation of Burnt Area Maps (BAM) using ML applied to the data during active
fires and BAM assimilation into NASA's Weather Research and Forecasting Model
using ML to broadcast fire spread are novel outcomes. BAM and CYGNSS obtained
soil moisture are integrated for the first time into USGS fire danger maps.
Inclusion of CYGNSS data in ML-based burn predictions boosts accuracy by 13%,
and inclusion of high-resolution data boosts ML recall by another 15%. The
proposed workflow has an expected latency of 6-30h, improving on the current
delivery time of multiple days. All components in the proposed concept are
shown to be computationally scalable and globally generalizable, with
sustainability considerations such as edge efficiency and low latency on small
devices.

</details>


### [327] [Improved Obstacle Avoidance for Autonomous Robots with ORCA-FLC](https://arxiv.org/abs/2508.06722)
*Justin London*

Main category: cs.RO

TL;DR: 论文提出ORCA-FL，通过模糊逻辑控制器改进ORCA算法，以更好地处理路径规划中的不确定性和不精确性，实验表明其在多智能体环境中能减少碰撞次数。


<details>
  <summary>Details</summary>
Motivation: 现有碰撞避免算法（如DWA、TEB、RVO等）存在路径优化不足、计算成本高或动态适应性差的问题，ORCA虽有改进但仍需提升。

Method: 使用模糊逻辑控制器（FLCs）改进ORCA算法，提出ORCA-FL，并进一步通过模糊Q强化学习（FQL）优化FLCs。

Result: 实验表明，当智能体速度超过阈值时，ORCA-FL在减少碰撞次数上优于ORCA。

Conclusion: ORCA-FL通过模糊逻辑和强化学习的结合，显著提升了动态多智能体环境中的避障性能。

Abstract: Obstacle avoidance enables autonomous agents and robots to operate safely and
efficiently in dynamic and complex environments, reducing the risk of
collisions and damage. For a robot or autonomous system to successfully
navigate through obstacles, it must be able to detect such obstacles. While
numerous collision avoidance algorithms like the dynamic window approach (DWA),
timed elastic bands (TEB), and reciprocal velocity obstacles (RVO) have been
proposed, they may lead to suboptimal paths due to fixed weights, be
computationally expensive, or have limited adaptability to dynamic obstacles in
multi-agent environments. Optimal reciprocal collision avoidance (ORCA), which
improves on RVO, provides smoother trajectories and stronger collision
avoidance guarantees. We propose ORCA-FL to improve on ORCA by using fuzzy
logic controllers (FLCs) to better handle uncertainty and imprecision for
obstacle avoidance in path planning. Numerous multi-agent experiments are
conducted and it is shown that ORCA-FL can outperform ORCA in reducing the
number of collision if the agent has a velocity that exceeds a certain
threshold. In addition, a proposed algorithm for improving ORCA-FL using fuzzy
Q reinforcement learning (FQL) is detailed for optimizing and tuning FLCs.

</details>


### [328] [Learning Causal Structure Distributions for Robust Planning](https://arxiv.org/abs/2508.06742)
*Alejandro Murillo-Gonzalez,Junhong Xu,Lantao Liu*

Main category: cs.RO

TL;DR: 学习功能关系时考虑结构信息的不确定性，可提升动态模型的鲁棒性，减少计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 现有方法常忽略因果结构，未能利用机器人系统中交互的稀疏性。

Method: 通过估计因果结构分布，采样因果图以指导编码器-多解码器概率模型的潜在空间表示。

Result: 模型能学习机器人动态，结合采样规划器完成新任务，验证了适应性和鲁棒性。

Conclusion: 考虑结构信息的方法在真实机器人场景中表现更优。

Abstract: Structural causal models describe how the components of a robotic system
interact. They provide both structural and functional information about the
relationships that are present in the system. The structural information
outlines the variables among which there is interaction. The functional
information describes how such interactions work, via equations or learned
models. In this paper we find that learning the functional relationships while
accounting for the uncertainty about the structural information leads to more
robust dynamics models which improves downstream planning, while using
significantly lower computational resources. This in contrast with common
model-learning methods that ignore the causal structure and fail to leverage
the sparsity of interactions in robotic systems. We achieve this by estimating
a causal structure distribution that is used to sample causal graphs that
inform the latent-space representations in an encoder-multidecoder
probabilistic model. We show that our model can be used to learn the dynamics
of a robot, which together with a sampling-based planner can be used to perform
new tasks in novel environments, provided an objective function for the new
requirement is available. We validate our method using manipulators and mobile
robots in both simulation and the real-world. Additionally, we validate the
learned dynamics' adaptability and increased robustness to corrupted inputs and
changes in the environment, which is highly desirable in challenging real-world
robotics scenarios. Video: https://youtu.be/X6k5t7OOnNc.

</details>


### [329] [Robust-Sub-Gaussian Model Predictive Control for Safe Ultrasound-Image-Guided Robotic Spinal Surgery](https://arxiv.org/abs/2508.06744)
*Yunke Ao,Manish Prajapat,Yarden As,Yassine Taoudi-Benchekroun,Fabio Carrillo,Hooman Esfandiari,Benjamin F. Grewe,Andreas Krause,Philipp Fürnstahl*

Main category: cs.RO

TL;DR: 论文提出了一种基于高维感官反馈的安全关键控制方法，通过子高斯噪声建模估计误差，并结合鲁棒集方法和MPC框架，确保线性系统的闭环安全性。


<details>
  <summary>Details</summary>
Motivation: 高维感官反馈（如图像、点云）在自动驾驶和机器人手术等领域的安全控制面临挑战，传统概率模型难以捕捉复杂的估计误差分布。

Method: 使用子高斯噪声建模估计误差，结合鲁棒集方法和子高斯方差代理传播技术，开发了一种MPC框架。

Result: 在超声图像引导的机器人脊柱手术中验证了方法的有效性，仿真结果显示其能确保安全性。

Conclusion: 该方法为解决复杂图像引导机器人手术任务提供了安全保证。

Abstract: Safety-critical control using high-dimensional sensory feedback from optical
data (e.g., images, point clouds) poses significant challenges in domains like
autonomous driving and robotic surgery. Control can rely on low-dimensional
states estimated from high-dimensional data. However, the estimation errors
often follow complex, unknown distributions that standard probabilistic models
fail to capture, making formal safety guarantees challenging. In this work, we
introduce a novel characterization of these general estimation errors using
sub-Gaussian noise with bounded mean. We develop a new technique for
uncertainty propagation of proposed noise characterization in linear systems,
which combines robust set-based methods with the propagation of sub-Gaussian
variance proxies. We further develop a Model Predictive Control (MPC) framework
that provides closed-loop safety guarantees for linear systems under the
proposed noise assumption. We apply this MPC approach in an
ultrasound-image-guided robotic spinal surgery pipeline, which contains
deep-learning-based semantic segmentation, image-based registration, high-level
optimization-based planning, and low-level robotic control. To validate the
pipeline, we developed a realistic simulation environment integrating real
human anatomy, robot dynamics, efficient ultrasound simulation, as well as
in-vivo data of breathing motion and drilling force. Evaluation results in
simulation demonstrate the potential of our approach for solving complex
image-guided robotic surgery task while ensuring safety.

</details>


### [330] [Learning a Vision-Based Footstep Planner for Hierarchical Walking Control](https://arxiv.org/abs/2508.06779)
*Minku Kim,Brian Acosta,Pratik Chaudhari,Michael Posa*

Main category: cs.RO

TL;DR: 提出了一种基于视觉的分层控制框架，结合强化学习的高层脚步规划器和低层操作空间控制器，用于双足机器人在非结构化环境中的实时脚步规划。


<details>
  <summary>Details</summary>
Motivation: 当前双足机器人的框架主要依赖本体感知或手动设计的视觉管道，这些方法在现实环境中脆弱且难以实现实时脚步规划。

Method: 采用分层控制框架，高层使用强化学习生成基于局部高程图的脚步命令，低层使用操作空间控制器跟踪轨迹。同时利用角动量线性倒立摆模型构建低维状态表示。

Result: 在欠驱动双足机器人Cassie上进行了不同地形条件下的仿真和硬件实验，验证了方法的有效性。

Conclusion: 该方法能够有效解决双足机器人在非结构化环境中的实时脚步规划问题，但仍存在一些挑战。

Abstract: Bipedal robots demonstrate potential in navigating challenging terrains
through dynamic ground contact. However, current frameworks often depend solely
on proprioception or use manually designed visual pipelines, which are fragile
in real-world settings and complicate real-time footstep planning in
unstructured environments. To address this problem, we present a vision-based
hierarchical control framework that integrates a reinforcement learning
high-level footstep planner, which generates footstep commands based on a local
elevation map, with a low-level Operational Space Controller that tracks the
generated trajectories. We utilize the Angular Momentum Linear Inverted
Pendulum model to construct a low-dimensional state representation to capture
an informative encoding of the dynamics while reducing complexity. We evaluate
our method across different terrain conditions using the underactuated bipedal
robot Cassie and investigate the capabilities and challenges of our approach
through simulation and hardware experiments.

</details>


### [331] [D3P: Dynamic Denoising Diffusion Policy via Reinforcement Learning](https://arxiv.org/abs/2508.06804)
*Shu-Ang Yu,Feng Gao,Yi Wu,Chao Yu,Yu Wang*

Main category: cs.RO

TL;DR: D3P是一种动态去噪扩散策略，通过自适应分配去噪步骤来加速机器人任务的实时部署。


<details>
  <summary>Details</summary>
Motivation: 现有方法对所有动作采用固定去噪步骤，忽略了动作对任务成功的重要性差异。

Method: D3P通过轻量级状态感知适配器动态分配去噪步骤，并结合强化学习优化性能与效率。

Result: 在模拟任务中，D3P实现了2.2倍的推理加速，且不影响成功率；在物理机器人上加速1.9倍。

Conclusion: D3P有效平衡了任务性能与推理效率，适用于实时机器人任务。

Abstract: Diffusion policies excel at learning complex action distributions for robotic
visuomotor tasks, yet their iterative denoising process poses a major
bottleneck for real-time deployment. Existing acceleration methods apply a
fixed number of denoising steps per action, implicitly treating all actions as
equally important. However, our experiments reveal that robotic tasks often
contain a mix of \emph{crucial} and \emph{routine} actions, which differ in
their impact on task success. Motivated by this finding, we propose
\textbf{D}ynamic \textbf{D}enoising \textbf{D}iffusion \textbf{P}olicy
\textbf{(D3P)}, a diffusion-based policy that adaptively allocates denoising
steps across actions at test time. D3P uses a lightweight, state-aware adaptor
to allocate the optimal number of denoising steps for each action. We jointly
optimize the adaptor and base diffusion policy via reinforcement learning to
balance task performance and inference efficiency. On simulated tasks, D3P
achieves an averaged 2.2$\times$ inference speed-up over baselines without
degrading success. Furthermore, we demonstrate D3P's effectiveness on a
physical robot, achieving a 1.9$\times$ acceleration over the baseline.

</details>


### [332] [Vibration-Based Energy Metric for Restoring Needle Alignment in Autonomous Robotic Ultrasound](https://arxiv.org/abs/2508.06921)
*Zhongyu Chen,Chenyang Li,Xuesong Li,Dianye Huang,Zhongliang Jiang,Stefanie Speidel,Xiangyu Chu,K. W. Samuel Au*

Main category: cs.RO

TL;DR: 提出了一种通过振动针头恢复超声成像平面与针插入平面对齐的方法，解决了针头可见性低的问题。


<details>
  <summary>Details</summary>
Motivation: 针头对齐在机器人超声引导手术中至关重要，但噪声、伪影和低分辨率导致针头检测困难。

Method: 通过机械系统周期性振动针头，提出基于振动的能量度量，开发控制策略调整超声探头位置。

Result: 实验显示平移误差0.41±0.27 mm，旋转误差0.51±0.19度。

Conclusion: 该方法在针头不可见时仍有效，显著提高了对齐精度。

Abstract: Precise needle alignment is essential for percutaneous needle insertion in
robotic ultrasound-guided procedures. However, inherent challenges such as
speckle noise, needle-like artifacts, and low image resolution make robust
needle detection difficult, particularly when visibility is reduced or lost. In
this paper, we propose a method to restore needle alignment when the ultrasound
imaging plane and the needle insertion plane are misaligned. Unlike many
existing approaches that rely heavily on needle visibility in ultrasound
images, our method uses a more robust feature by periodically vibrating the
needle using a mechanical system. Specifically, we propose a vibration-based
energy metric that remains effective even when the needle is fully out of
plane. Using this metric, we develop a control strategy to reposition the
ultrasound probe in response to misalignments between the imaging plane and the
needle insertion plane in both translation and rotation. Experiments conducted
on ex-vivo porcine tissue samples using a dual-arm robotic ultrasound-guided
needle insertion system demonstrate the effectiveness of the proposed approach.
The experimental results show the translational error of 0.41$\pm$0.27 mm and
the rotational error of 0.51$\pm$0.19 degrees.

</details>


### [333] [Manipulator for people with limited abilities](https://arxiv.org/abs/2508.06969)
*Bingkun Huang,Evgeniy Kotov,Arkady Yuschenko*

Main category: cs.RO

TL;DR: 开发一款四自由度机械手，用于辅助残疾人，结合机械设计、控制系统及ROS软件。


<details>
  <summary>Details</summary>
Motivation: 机器人技术发展为残疾人提供生活质量提升的可能，设计适配需求的机械手是重要挑战。

Method: 综合设计机械结构、开发控制系统，并集成技术视觉系统与ROS软件。

Result: 成功开发出适合实际操作的四自由度机械手。

Conclusion: 该机械手设计为残疾人辅助设备提供了实用解决方案。

Abstract: The topic of this final qualification work was chosen due to the importance
of developing robotic systems designed to assist people with disabilities.
Advances in robotics and automation technologies have opened up new prospects
for creating devices that can significantly improve the quality of life for
these people. In this context, designing a robotic hand with a control system
adapted to the needs of people with disabilities is a major scientific and
practical challenge. This work addresses the problem of developing and
manufacturing a four-degree-of-freedom robotic hand suitable for practical
manipulation. Addressing this issue requires a comprehensive approach,
encompassing the design of the hand's mechanical structure, the development of
its control system, and its integration with a technical vision system and
software based on the Robot Operating System (ROS).

</details>


### [334] [Imaginative World Modeling with Scene Graphs for Embodied Agent Navigation](https://arxiv.org/abs/2508.06990)
*Yue Hu,Junzhe Wu,Ruihan Xu,Hang Liu,Avery Xi,Henry X. Liu,Ram Vasudevan,Maani Ghaffari*

Main category: cs.RO

TL;DR: SGImagineNav是一种新颖的语义导航框架，通过符号化世界建模和场景图预测，提升智能体在未知环境中的导航效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖过去观察，缺乏对未来场景的预测能力，限制了导航效率。SGImagineNav通过预测未来场景，主动构建全局环境表示，以提升导航速度和准确性。

Method: SGImagineNav采用分层场景图和大型语言模型预测未知环境部分，结合自适应导航策略，利用语义捷径或探索未知区域。

Result: 在HM3D和HSSD基准测试中，SGImagineNav的成功率分别达到65.4%和66.8%，并在真实环境中展示了跨楼层和跨房间导航能力。

Conclusion: SGImagineNav通过主动预测和语义建模，显著提升了导航性能，展示了其有效性和通用性。

Abstract: Semantic navigation requires an agent to navigate toward a specified target
in an unseen environment. Employing an imaginative navigation strategy that
predicts future scenes before taking action, can empower the agent to find
target faster. Inspired by this idea, we propose SGImagineNav, a novel
imaginative navigation framework that leverages symbolic world modeling to
proactively build a global environmental representation. SGImagineNav maintains
an evolving hierarchical scene graphs and uses large language models to predict
and explore unseen parts of the environment. While existing methods solely
relying on past observations, this imaginative scene graph provides richer
semantic context, enabling the agent to proactively estimate target locations.
Building upon this, SGImagineNav adopts an adaptive navigation strategy that
exploits semantic shortcuts when promising and explores unknown areas otherwise
to gather additional context. This strategy continuously expands the known
environment and accumulates valuable semantic contexts, ultimately guiding the
agent toward the target. SGImagineNav is evaluated in both real-world scenarios
and simulation benchmarks. SGImagineNav consistently outperforms previous
methods, improving success rate to 65.4 and 66.8 on HM3D and HSSD, and
demonstrating cross-floor and cross-room navigation in real-world environments,
underscoring its effectiveness and generalizability.

</details>


### [335] [EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events](https://arxiv.org/abs/2508.07003)
*Siyu Chen,Shenghai Yuan,Thien-Minh Nguyen,Zhuyu Huang,Chenyang Shi,Jin Jing,Lihua Xie*

Main category: cs.RO

TL;DR: EGS-SLAM通过融合事件数据和RGB-D输入，解决了传统GS-SLAM在运动模糊下的性能问题，提升了跟踪精度和3D重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有GS-SLAM系统在严重运动模糊下表现不佳，影响跟踪和重建质量。

Method: 提出EGS-SLAM框架，融合事件数据和RGB-D输入，建模相机连续轨迹，引入可学习的相机响应函数和无事件损失。

Result: 在合成和真实数据集上验证，EGS-SLAM在轨迹精度和3D重建质量上优于现有方法。

Conclusion: EGS-SLAM显著提升了运动模糊场景下的SLAM性能，代码将开源。

Abstract: Gaussian Splatting SLAM (GS-SLAM) offers a notable improvement over
traditional SLAM methods, enabling photorealistic 3D reconstruction that
conventional approaches often struggle to achieve. However, existing GS-SLAM
systems perform poorly under persistent and severe motion blur commonly
encountered in real-world scenarios, leading to significantly degraded tracking
accuracy and compromised 3D reconstruction quality. To address this limitation,
we propose EGS-SLAM, a novel GS-SLAM framework that fuses event data with RGB-D
inputs to simultaneously reduce motion blur in images and compensate for the
sparse and discrete nature of event streams, enabling robust tracking and
high-fidelity 3D Gaussian Splatting reconstruction. Specifically, our system
explicitly models the camera's continuous trajectory during exposure,
supporting event- and blur-aware tracking and mapping on a unified 3D Gaussian
Splatting scene. Furthermore, we introduce a learnable camera response function
to align the dynamic ranges of events and images, along with a no-event loss to
suppress ringing artifacts during reconstruction. We validate our approach on a
new dataset comprising synthetic and real-world sequences with significant
motion blur. Extensive experimental results demonstrate that EGS-SLAM
consistently outperforms existing GS-SLAM systems in both trajectory accuracy
and photorealistic 3D Gaussian Splatting reconstruction. The source code will
be available at https://github.com/Chensiyu00/EGS-SLAM.

</details>


### [336] [$\mathcal{P}^3$: Toward Versatile Embodied Agents](https://arxiv.org/abs/2508.07033)
*Shengli Zhou,Xiangchen Wang,Jinrui Zhang,Ruozai Tian,Rongtao Xu,Feng Zheng*

Main category: cs.RO

TL;DR: 论文提出了一种名为$\mathcal{P}^3$的统一框架，旨在解决动态环境感知、开放式工具使用和复杂多任务规划的挑战，提升具身智能体的适应性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有具身智能体在动态环境感知、工具使用灵活性和多任务规划方面存在局限性，限制了其实际应用。

Method: $\mathcal{P}^3$框架通过实时感知、无反馈工具插拔和动态任务调度，实现环境主动感知、灵活工具使用和多任务优先级调整。

Result: 实验表明，$\mathcal{P}^3$能显著提升具身智能体的适应性和通用性，适用于实际部署。

Conclusion: $\mathcal{P}^3$为具身智能体的实际应用提供了高效解决方案，具有广泛的应用潜力。

Abstract: Embodied agents have shown promising generalization capabilities across
diverse physical environments, making them essential for a wide range of
real-world applications. However, building versatile embodied agents poses
critical challenges due to three key issues: dynamic environment perception,
open-ended tool usage, and complex multi-task planning. Most previous works
rely solely on feedback from tool agents to perceive environmental changes and
task status, which limits adaptability to real-time dynamics, causes error
accumulation, and restricts tool flexibility. Furthermore, multi-task
scheduling has received limited attention, primarily due to the inherent
complexity of managing task dependencies and balancing competing priorities in
dynamic and complex environments. To overcome these challenges, we introduce
$\mathcal{P}^3$, a unified framework that integrates real-time perception and
dynamic scheduling. Specifically, $\mathcal{P}^3$ enables 1) \textbf Perceive
relevant task information actively from the environment, 2) \textbf Plug and
utilize any tool without feedback requirement, and 3) \textbf Plan multi-task
execution based on prioritizing urgent tasks and dynamically adjusting task
order based on dependencies. Extensive real-world experiments show that our
approach bridges the gap between benchmarks and practical deployment,
delivering highly transferable, general-purpose embodied agents. Code and data
will be released soon.

</details>


### [337] [From Data to Safe Mobile Robot Navigation: An Efficient and Modular Robust MPC Design Pipeline](https://arxiv.org/abs/2508.07045)
*Dennis Benders,Johannes Köhler,Robert Babuška,Javier Alonso-Mora,Laura Ferranti*

Main category: cs.RO

TL;DR: 提出了一种模块化鲁棒MPC设计流程，通过闭环实验数据估计扰动边界，确保安全性和可行性。


<details>
  <summary>Details</summary>
Motivation: 现有MPC方法在真实环境中因扰动和噪声难以保证安全性，且依赖理想假设。

Method: 采用迭代流程，利用闭环实验数据估计扰动边界，设计鲁棒输出反馈MPC。

Result: 在Gazebo四旋翼仿真中验证了鲁棒约束满足和递归可行性。

Conclusion: 该流程有效解决了MPC在噪声和扰动下的安全性问题。

Abstract: Model predictive control (MPC) is a powerful strategy for planning and
control in autonomous mobile robot navigation. However, ensuring safety in
real-world deployments remains challenging due to the presence of disturbances
and measurement noise. Existing approaches often rely on idealized assumptions,
neglect the impact of noisy measurements, and simply heuristically guess
unrealistic bounds. In this work, we present an efficient and modular robust
MPC design pipeline that systematically addresses these limitations. The
pipeline consists of an iterative procedure that leverages closed-loop
experimental data to estimate disturbance bounds and synthesize a robust
output-feedback MPC scheme. We provide the pipeline in the form of
deterministic and reproducible code to synthesize the robust output-feedback
MPC from data. We empirically demonstrate robust constraint satisfaction and
recursive feasibility in quadrotor simulations using Gazebo.

</details>


### [338] [Model Predictive Control for Crowd Navigation via Learning-Based Trajectory Prediction](https://arxiv.org/abs/2508.07079)
*Mohamed Parvez Aslam,Bojan Derajic,Mohamed-Khalil Bouzidi,Sebastian Bernhard,Jan Oliver Ringert*

Main category: cs.RO

TL;DR: 该论文研究了将基于深度学习的社交隐式（SI）行人轨迹预测器与模型预测控制（MPC）框架结合，以提升机器人在行人密集环境中的导航安全性。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在行人密集环境中安全导航的挑战。

Method: 将SI轨迹预测器集成到MPC框架中，并与传统恒定速度（CV）模型在开环预测和闭环导航中进行比较。

Result: SI显著减少了轨迹预测误差（低密度环境下降低76%），并在拥挤场景中提升了安全性和运动平滑性。

Conclusion: SI-MPC框架在动态行人环境中表现出更高的适应性和安全性，强调了系统级评估的重要性。

Abstract: Safe navigation in pedestrian-rich environments remains a key challenge for
autonomous robots. This work evaluates the integration of a deep learning-based
Social-Implicit (SI) pedestrian trajectory predictor within a Model Predictive
Control (MPC) framework on the physical Continental Corriere robot. Tested
across varied pedestrian densities, the SI-MPC system is compared to a
traditional Constant Velocity (CV) model in both open-loop prediction and
closed-loop navigation. Results show that SI improves trajectory prediction -
reducing errors by up to 76% in low-density settings - and enhances safety and
motion smoothness in crowded scenes. Moreover, real-world deployment reveals
discrepancies between open-loop metrics and closed-loop performance, as the SI
model yields broader, more cautious predictions. These findings emphasize the
importance of system-level evaluation and highlight the SI-MPC framework's
promise for safer, more adaptive navigation in dynamic, human-populated
environments.

</details>


### [339] [An Evolutionary Game-Theoretic Merging Decision-Making Considering Social Acceptance for Autonomous Driving](https://arxiv.org/abs/2508.07080)
*Haolin Liu,Zijun Guo,Yanbo Chen,Jiaqi Chen,Huilong Yu,Junqiang Xi*

Main category: cs.RO

TL;DR: 论文提出了一种基于演化博弈论（EGT）的自动驾驶车辆（AVs）高速入口合并决策框架，通过动态平衡AVs和主路车辆（MVs）的利益，提升合并效率、舒适性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有决策算法未能充分应对动态复杂性和自动驾驶车辆的社会接受度，导致合并决策不理想或不安全。

Method: 提出EGT框架，将合并决策建模为多目标支付函数的演化博弈问题，并通过求解复制动态方程得到最优合并时机。同时，设计了实时驾驶风格估计算法动态调整支付函数。

Result: 实验表明，该方法在效率、舒适性和安全性上优于现有博弈论和传统规划方法。

Conclusion: EGT框架有效解决了AVs高速入口合并的挑战，实现了动态平衡和优化。

Abstract: Highway on-ramp merging is of great challenge for autonomous vehicles (AVs),
since they have to proactively interact with surrounding vehicles to enter the
main road safely within limited time. However, existing decision-making
algorithms fail to adequately address dynamic complexities and social
acceptance of AVs, leading to suboptimal or unsafe merging decisions. To
address this, we propose an evolutionary game-theoretic (EGT) merging
decision-making framework, grounded in the bounded rationality of human
drivers, which dynamically balances the benefits of both AVs and main-road
vehicles (MVs). We formulate the cut-in decision-making process as an EGT
problem with a multi-objective payoff function that reflects human-like driving
preferences. By solving the replicator dynamic equation for the evolutionarily
stable strategy (ESS), the optimal cut-in timing is derived, balancing
efficiency, comfort, and safety for both AVs and MVs. A real-time driving style
estimation algorithm is proposed to adjust the game payoff function online by
observing the immediate reactions of MVs. Empirical results demonstrate that we
improve the efficiency, comfort and safety of both AVs and MVs compared with
existing game-theoretic and traditional planning approaches across multi-object
metrics.

</details>


### [340] [DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit](https://arxiv.org/abs/2508.07118)
*Aiden Swann,Alex Qiu,Matthew Strong,Angelina Zhang,Samuel Morstein,Kai Rayle,Monroe Kennedy III*

Main category: cs.RO

TL;DR: DexFruit是一个机器人操作框架，用于轻柔、自主地处理易碎水果并精确评估损伤。通过光学触觉传感和触觉信息扩散策略，实现了低损伤的水果操作。FruitSplat技术用于高分辨率3D损伤量化。


<details>
  <summary>Details</summary>
Motivation: 许多水果易碎且容易受损，需人工小心采摘。现有损伤评估方法缺乏定量严谨性或设备昂贵。

Method: 使用光学触觉传感和触觉信息扩散策略进行自主操作，并引入FruitSplat技术进行3D损伤量化。

Result: 实现了92%的抓取成功率，视觉损伤减少20%，抓取成功率提升31%。

Conclusion: DexFruit框架在减少水果损伤和提高操作成功率方面表现优异，FruitSplat技术为损伤量化提供了新方法。

Abstract: DexFruit is a robotic manipulation framework that enables gentle, autonomous
handling of fragile fruit and precise evaluation of damage. Many fruits are
fragile and prone to bruising, thus requiring humans to manually harvest them
with care. In this work, we demonstrate by using optical tactile sensing,
autonomous manipulation of fruit with minimal damage can be achieved. We show
that our tactile informed diffusion policies outperform baselines in both
reduced bruising and pick-and-place success rate across three fruits:
strawberries, tomatoes, and blackberries. In addition, we introduce FruitSplat,
a novel technique to represent and quantify visual damage in high-resolution 3D
representation via 3D Gaussian Splatting (3DGS). Existing metrics for measuring
damage lack quantitative rigor or require expensive equipment. With FruitSplat,
we distill a 2D strawberry mask as well as a 2D bruise segmentation mask into
the 3DGS representation. Furthermore, this representation is modular and
general, compatible with any relevant 2D model. Overall, we demonstrate a 92%
grasping policy success rate, up to a 20% reduction in visual bruising, and up
to an 31% improvement in grasp success rate on challenging fruit compared to
our baselines across our three tested fruits. We rigorously evaluate this
result with over 630 trials. Please checkout our website at
https://dex-fruit.github.io .

</details>


### [341] [Integrating Neurosymbolic AI in Advanced Air Mobility: A Comprehensive Survey](https://arxiv.org/abs/2508.07163)
*Kamal Acharya,Iman Sharifi,Mehul Lad,Liang Sun,Houbing Song*

Main category: cs.RO

TL;DR: 神经符号AI结合神经网络与符号推理，为高级空中交通（AAM）的复杂问题提供解决方案。本文综述了其在需求预测、飞机设计和实时交通管理等领域的应用，指出当前研究分散且面临可扩展性、鲁棒性和合规性挑战，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 高级空中交通（AAM）面临复杂的监管、运营和安全挑战，神经符号AI结合了神经网络的适应性和符号推理的优势，有望解决这些问题。

Method: 本文综述了神经符号AI在AAM中的应用，包括神经符号强化学习等方法，并分类了当前进展，结合案例研究进行分析。

Result: 研究发现神经符号AI在动态优化方面有潜力，但仍需解决可扩展性、鲁棒性和合规性问题。

Conclusion: 本文为研究人员和从业者提供了整合神经符号AI到AAM系统的路线图，强调未来需进一步研究以实现可靠、透明的解决方案。

Abstract: Neurosymbolic AI combines neural network adaptability with symbolic
reasoning, promising an approach to address the complex regulatory,
operational, and safety challenges in Advanced Air Mobility (AAM). This survey
reviews its applications across key AAM domains such as demand forecasting,
aircraft design, and real-time air traffic management. Our analysis reveals a
fragmented research landscape where methodologies, including Neurosymbolic
Reinforcement Learning, have shown potential for dynamic optimization but still
face hurdles in scalability, robustness, and compliance with aviation
standards. We classify current advancements, present relevant case studies, and
outline future research directions aimed at integrating these approaches into
reliable, transparent AAM systems. By linking advanced AI techniques with AAM's
operational demands, this work provides a concise roadmap for researchers and
practitioners developing next-generation air mobility solutions.

</details>


### [342] [3D Gaussian Representations with Motion Trajectory Field for Dynamic Scene Reconstruction](https://arxiv.org/abs/2508.07182)
*Xuesong Li,Lars Petersson,Vivien Rolland*

Main category: cs.RO

TL;DR: 提出一种结合3D高斯泼溅与运动轨迹场的新方法，用于动态场景的新视角合成和运动重建。


<details>
  <summary>Details</summary>
Motivation: 解决单目视频中动态场景的新视角合成和运动重建问题，扩展静态场景重建方法（如NeRF和3DGS）的应用范围。

Method: 通过分离动态物体与静态背景，结合时间不变运动系数和共享运动轨迹基，优化运动轨迹场。

Result: 在单目视频的新视角合成和运动轨迹恢复中达到最先进效果。

Conclusion: 该方法显著提升了动态场景重建的能力。

Abstract: This paper addresses the challenge of novel-view synthesis and motion
reconstruction of dynamic scenes from monocular video, which is critical for
many robotic applications. Although Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have demonstrated remarkable success in rendering
static scenes, extending them to reconstruct dynamic scenes remains
challenging. In this work, we introduce a novel approach that combines 3DGS
with a motion trajectory field, enabling precise handling of complex object
motions and achieving physically plausible motion trajectories. By decoupling
dynamic objects from static background, our method compactly optimizes the
motion trajectory field. The approach incorporates time-invariant motion
coefficients and shared motion trajectory bases to capture intricate motion
patterns while minimizing optimization complexity. Extensive experiments
demonstrate that our approach achieves state-of-the-art results in both
novel-view synthesis and motion trajectory recovery from monocular video,
advancing the capabilities of dynamic scene reconstruction.

</details>


### [343] [Impact of Gaze-Based Interaction and Augmentation on Human-Robot Collaboration in Critical Tasks](https://arxiv.org/abs/2508.07244)
*Ayesha Jena,Stefan Reitmann,Elin Anna Topp*

Main category: cs.RO

TL;DR: 研究分析了头眼注视控制与局部视觉增强在模拟搜救任务中的效果，发现局部增强显著提升任务表现，降低认知负荷38%，缩短任务时间60%以上。


<details>
  <summary>Details</summary>
Motivation: 探索头眼注视控制与局部视觉增强在搜救任务中的实际效果，以优化人机交互。

Method: 通过用户研究，分析头眼注视模式及局部视觉增强对任务表现的影响。

Result: 局部视觉增强显著提升任务表现，降低认知负荷38%，缩短任务时间60%以上。

Conclusion: 局部视觉增强技术潜力巨大，需进一步研究注视模式以优化关键任务表现。

Abstract: We present a user study analyzing head-gaze-based robot control and foveated
visual augmentation in a simulated search-and-rescue task. Results show that
foveated augmentation significantly improves task performance, reduces
cognitive load by 38%, and shortens task time by over 60%. Head-gaze patterns
analysed over both the entire task duration and shorter time segments show that
near and far attention capture is essential to better understand user intention
in critical scenarios. Our findings highlight the potential of foveation as an
augmentation technique and the need to further study gaze measures to leverage
them during critical tasks.

</details>


### [344] [Bio-Inspired Topological Autonomous Navigation with Active Inference in Robotics](https://arxiv.org/abs/2508.07267)
*Daria de Tinguy,Tim Verbelen,Emilio Gamba,Bart Dhoedt*

Main category: cs.RO

TL;DR: 论文提出了一种基于主动推理框架（AIF）的生物启发式智能体，用于自主导航和探索，无需预训练，实时更新拓扑地图，并适应动态环境。


<details>
  <summary>Details</summary>
Motivation: 现有自主导航方法依赖严格规则或预训练，缺乏适应性和计算效率，难以应对动态或未知环境。

Method: 采用主动推理框架（AIF），结合概率推理和模块化ROS2架构，实现实时拓扑地图构建和自适应决策。

Result: 在仿真和真实环境中测试，智能体成功探索大规模环境并适应动态障碍，性能与Gbplanner、FAEL等方法相当。

Conclusion: 该方法为复杂非结构化环境提供了一种可扩展且透明的导航解决方案。

Abstract: Achieving fully autonomous exploration and navigation remains a critical
challenge in robotics, requiring integrated solutions for localisation,
mapping, decision-making and motion planning. Existing approaches either rely
on strict navigation rules lacking adaptability or on pre-training, which
requires large datasets. These AI methods are often computationally intensive
or based on static assumptions, limiting their adaptability in dynamic or
unknown environments. This paper introduces a bio-inspired agent based on the
Active Inference Framework (AIF), which unifies mapping, localisation, and
adaptive decision-making for autonomous navigation, including exploration and
goal-reaching. Our model creates and updates a topological map of the
environment in real-time, planning goal-directed trajectories to explore or
reach objectives without requiring pre-training. Key contributions include a
probabilistic reasoning framework for interpretable navigation, robust
adaptability to dynamic changes, and a modular ROS2 architecture compatible
with existing navigation systems. Our method was tested in simulated and
real-world environments. The agent successfully explores large-scale simulated
environments and adapts to dynamic obstacles and drift, proving to be
comparable to other exploration strategies such as Gbplanner, FAEL and
Frontiers. This approach offers a scalable and transparent approach for
navigating complex, unstructured environments.

</details>


### [345] [Navigation and Exploration with Active Inference: from Biology to Industry](https://arxiv.org/abs/2508.07269)
*Daria de Tinguy,Tim Verbelen,Bart Dhoedt*

Main category: cs.RO

TL;DR: 论文提出了一种基于主动推理框架（AIF）的实时机器人导航系统，模仿动物认知地图构建能力，无需预先训练即可在复杂环境中导航。


<details>
  <summary>Details</summary>
Motivation: 受动物在动态环境中构建认知地图的启发，旨在开发一种无需训练、适应性强的机器人导航系统。

Method: 利用AIF框架逐步构建拓扑地图，推断位置并通过最小化预期不确定性和实现感知目标来规划动作。

Result: 在ROS2生态系统中验证了系统在2D和3D环境中的高效适应性，性能与传统及前沿方法相当。

Conclusion: 该系统提供了一种生物启发的导航方法，展示了在复杂环境中的竞争力和潜力。

Abstract: By building and updating internal cognitive maps, animals exhibit
extraordinary navigation abilities in complex, dynamic environments. Inspired
by these biological mechanisms, we present a real time robotic navigation
system grounded in the Active Inference Framework (AIF). Our model
incrementally constructs a topological map, infers the agent's location, and
plans actions by minimising expected uncertainty and fulfilling perceptual
goals without any prior training. Integrated into the ROS2 ecosystem, we
validate its adaptability and efficiency across both 2D and 3D environments
(simulated and real world), demonstrating competitive performance with
traditional and state of the art exploration approaches while offering a
biologically inspired navigation approach.

</details>


### [346] [Multimodal Spiking Neural Network for Space Robotic Manipulation](https://arxiv.org/abs/2508.07287)
*Liwen Zhang,Dong Zhou,Shibo Shao,Zihao Su,Guanghui Sun*

Main category: cs.RO

TL;DR: 提出了一种基于脉冲神经网络的多模态控制框架，用于空间站机械臂，旨在解决资源限制问题并提升自主操作能力。


<details>
  <summary>Details</summary>
Motivation: 解决空间站机械臂在资源有限条件下的自主操作问题，结合多模态信息增强环境感知。

Method: 结合几何状态、触觉和语义信息，采用双通道三阶段课程强化学习（CRL）方案。

Result: 在目标接近、物体抓取和稳定提升等任务中表现可靠，任务成功率和能效均优于基线方法。

Conclusion: 该方法适用于实际航空航天应用，展现了在资源受限环境下的高效性和鲁棒性。

Abstract: This paper presents a multimodal control framework based on spiking neural
networks (SNNs) for robotic arms aboard space stations. It is designed to cope
with the constraints of limited onboard resources while enabling autonomous
manipulation and material transfer in space operations. By combining geometric
states with tactile and semantic information, the framework strengthens
environmental awareness and contributes to more robust control strategies. To
guide the learning process progressively, a dual-channel, three-stage
curriculum reinforcement learning (CRL) scheme is further integrated into the
system. The framework was tested across a range of tasks including target
approach, object grasping, and stable lifting with wall-mounted robotic arms,
demonstrating reliable performance throughout. Experimental evaluations
demonstrate that the proposed method consistently outperforms baseline
approaches in both task success rate and energy efficiency. These findings
highlight its suitability for real-world aerospace applications.

</details>


### [347] [A Hybrid Force-Position Strategy for Shape Control of Deformable Linear Objects With Graph Attention Networks](https://arxiv.org/abs/2508.07319)
*Yanzhao Yu,Haotian Yang,Junbo Tan,Xueqian Wang*

Main category: cs.RO

TL;DR: 本文提出了一种混合力-位置策略，用于控制可变形线性物体（DLOs）的形状，结合了力空间的状态轨迹规划和位置空间的模型预测控制（MPC）。


<details>
  <summary>Details</summary>
Motivation: DLOs（如电线和电缆）的操控在电子组装和医疗手术中至关重要，但由于其无限自由度、复杂的非线性动力学和系统欠驱动特性，面临挑战。

Method: 提出了一种结合力和位置表示的框架，包括力空间的状态轨迹规划和位置空间的MPC。模型包含显式动作编码器、属性提取器和基于图注意力网络的图处理器。

Result: 仿真和实际实验结果表明，该方法能高效稳定地控制DLOs的形状。

Conclusion: 该方法有效解决了DLOs形状控制的挑战，代码和视频已公开。

Abstract: Manipulating deformable linear objects (DLOs) such as wires and cables is
crucial in various applications like electronics assembly and medical
surgeries. However, it faces challenges due to DLOs' infinite degrees of
freedom, complex nonlinear dynamics, and the underactuated nature of the
system. To address these issues, this paper proposes a hybrid force-position
strategy for DLO shape control. The framework, combining both force and
position representations of DLO, integrates state trajectory planning in the
force space and Model Predictive Control (MPC) in the position space. We
present a dynamics model with an explicit action encoder, a property extractor
and a graph processor based on Graph Attention Networks. The model is used in
the MPC to enhance prediction accuracy. Results from both simulations and
real-world experiments demonstrate the effectiveness of our approach in
achieving efficient and stable shape control of DLOs. Codes and videos are
available at https://sites.google.com/view/dlom.

</details>


### [348] [Collision-Free Trajectory Planning and control of Robotic Manipulator using Energy-Based Artificial Potential Field (E-APF)](https://arxiv.org/abs/2508.07323)
*Adeetya Uppal,Rakesh Kumar Sahoo,Manoranjan Sinha*

Main category: cs.RO

TL;DR: 提出了一种基于能量的APF框架（E-APF），结合位置和速度依赖的势函数，解决了传统APF的局部最小值和振荡问题，并通过混合轨迹优化器实现了平滑且时间高效的轨迹规划。


<details>
  <summary>Details</summary>
Motivation: 动态和杂乱环境中的机器人轨迹规划在时间效率和运动平滑性方面存在挑战，传统APF方法因局部最小值和振荡问题表现不佳。

Method: 提出E-APF框架，结合位置和速度依赖的势函数，并与混合轨迹优化器联合优化，最小化加加速度和执行时间。

Result: 在7自由度Kinova Gen3机械臂仿真中验证，实现了无碰撞、平滑、高效且无振荡的轨迹。

Conclusion: E-APF框架为未来与反应式控制策略和实际硬件部署的集成奠定了基础。

Abstract: Robotic trajectory planning in dynamic and cluttered environments remains a
critical challenge, particularly when striving for both time efficiency and
motion smoothness under actuation constraints. Traditional path planner, such
as Artificial Potential Field (APF), offer computational efficiency but suffer
from local minima issue due to position-based potential field functions and
oscillatory motion near the obstacles due to Newtonian mechanics. To address
this limitation, an Energy-based Artificial Potential Field (APF) framework is
proposed in this paper that integrates position and velocity-dependent
potential functions. E-APF ensures dynamic adaptability and mitigates local
minima, enabling uninterrupted progression toward the goal. The proposed
framework integrates E-APF with a hybrid trajectory optimizer that jointly
minimizes jerk and execution time under velocity and acceleration constraints,
ensuring geometric smoothness and time efficiency. The entire framework is
validated in simulation using the 7-degree-of-freedom Kinova Gen3 robotic
manipulator. The results demonstrate collision-free, smooth, time-efficient,
and oscillation-free trajectory in the presence of obstacles, highlighting the
efficacy of the combined trajectory optimization and real-time obstacle
avoidance approach. This work lays the foundation for future integration with
reactive control strategies and physical hardware deployment in real-world
manipulation tasks.

</details>


### [349] [MonoMPC: Monocular Vision Based Navigation with Learned Collision Model and Risk-Aware Model Predictive Control](https://arxiv.org/abs/2508.07387)
*Basant Sharma,Prajyot Jadhav,Pranjal Paul,K. Madhava Krishna,Arun Kumar Singh*

Main category: cs.RO

TL;DR: 论文提出了一种基于学习碰撞模型的方法，通过预测最小障碍物间距分布来改进单RGB相机在未知环境中的导航性能，避免了直接使用噪声深度估计。


<details>
  <summary>Details</summary>
Motivation: 单RGB相机在未知环境中导航时缺乏深度信息，导致碰撞检测不可靠。现有的深度估计方法在复杂环境中噪声过大，无法直接用于导航。

Method: 提出了一种学习碰撞模型的方法，利用噪声深度估计作为上下文输入，预测最小障碍物间距分布。结合风险感知MPC规划器，最小化碰撞风险。通过联合训练优化碰撞模型和风险度量。

Result: 实验结果显示，该方法在真实环境中的成功率比NoMaD和ROS堆栈分别提高了9倍和7倍。消融研究验证了设计选择的有效性。

Conclusion: 通过联合训练学习碰撞模型和风险度量，显著提高了单RGB相机在复杂环境中的导航性能。

Abstract: Navigating unknown environments with a single RGB camera is challenging, as
the lack of depth information prevents reliable collision-checking. While some
methods use estimated depth to build collision maps, we found that depth
estimates from vision foundation models are too noisy for zero-shot navigation
in cluttered environments.
  We propose an alternative approach: instead of using noisy estimated depth
for direct collision-checking, we use it as a rich context input to a learned
collision model. This model predicts the distribution of minimum obstacle
clearance that the robot can expect for a given control sequence. At inference,
these predictions inform a risk-aware MPC planner that minimizes estimated
collision risk. Our joint learning pipeline co-trains the collision model and
risk metric using both safe and unsafe trajectories. Crucially, our
joint-training ensures optimal variance in our collision model that improves
navigation in highly cluttered environments. Consequently, real-world
experiments show 9x and 7x improvements in success rates over NoMaD and the ROS
stack, respectively. Ablation studies further validate the effectiveness of our
design choices.

</details>


### [350] [AgriVLN: Vision-and-Language Navigation for Agricultural Robots](https://arxiv.org/abs/2508.07406)
*Xiaobei Zhao,Xingqi Lyu,Xiang Li*

Main category: cs.RO

TL;DR: 论文提出了农业场景下的视觉与语言导航基准A2A和基线方法AgriVLN，通过子任务分解模块提升导航成功率。


<details>
  <summary>Details</summary>
Motivation: 解决农业机器人因依赖人工操作或固定轨道导致的移动性和适应性不足问题。

Method: 基于视觉语言模型（VLM）设计AgriVLN方法，并引入子任务列表（STL）模块分解指令。

Result: AgriVLN在短指令上表现良好，长指令通过STL模块将成功率从0.33提升至0.47。

Conclusion: AgriVLN在农业领域实现了最先进的性能，为农业机器人导航提供了有效解决方案。

Abstract: Agricultural robots have emerged as powerful members in agricultural tasks,
nevertheless, still heavily rely on manual operation or untransportable railway
for movement, resulting in limited mobility and poor adaptability.
Vision-and-Language Navigation (VLN) enables robots to navigate to the target
destinations following natural language instructions, demonstrating strong
performance on several domains. However, none of the existing benchmarks or
methods is specifically designed for agricultural scenes. To bridge this gap,
we propose Agriculture to Agriculture (A2A) benchmark, containing 1,560
episodes across six diverse agricultural scenes, in which all realistic RGB
videos are captured by front-facing camera on a quadruped robot at a height of
0.38 meters, aligning with the practical deployment conditions. Meanwhile, we
propose Vision-and-Language Navigation for Agricultural Robots (AgriVLN)
baseline based on Vision-Language Model (VLM) prompted with carefully crafted
templates, which can understand both given instructions and agricultural
environments to generate appropriate low-level actions for robot control. When
evaluated on A2A, AgriVLN performs well on short instructions but struggles
with long instructions, because it often fails to track which part of the
instruction is currently being executed. To address this, we further propose
Subtask List (STL) instruction decomposition module and integrate it into
AgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare
AgriVLN with several existing VLN methods, demonstrating the state-of-the-art
performance in the agricultural domain.

</details>


### [351] [Triple-S: A Collaborative Multi-LLM Framework for Solving Long-Horizon Implicative Tasks in Robotics](https://arxiv.org/abs/2508.07421)
*Zixi Jia,Hongbin Gao,Fashe Li,Jiqiang Liu,Hexiao Li,Qinghua Liu*

Main category: cs.RO

TL;DR: 论文提出Triple-S框架，通过多LLM协作解决长时程任务中的代码生成问题，显著提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 利用LLM生成机器人控制代码时，长时程任务常因API参数、注释和顺序错误导致失败，需改进方法。

Method: 采用Triple-S框架，通过多LLM分饰角色（简化-解决-总结）闭环学习，并结合成功案例更新演示库。

Result: 在LDIP数据集中，Triple-S任务成功率达89%，仿真和实际机器人实验均验证其有效性。

Conclusion: Triple-S框架显著提升长时程任务的鲁棒性和成功率，具有实际应用潜力。

Abstract: Leveraging Large Language Models (LLMs) to write policy code for controlling
robots has gained significant attention. However, in long-horizon implicative
tasks, this approach often results in API parameter, comments and sequencing
errors, leading to task failure. To address this problem, we propose a
collaborative Triple-S framework that involves multiple LLMs. Through
In-Context Learning, different LLMs assume specific roles in a closed-loop
Simplification-Solution-Summary process, effectively improving success rates
and robustness in long-horizon implicative tasks. Additionally, a novel
demonstration library update mechanism which learned from success allows it to
generalize to previously failed tasks. We validate the framework in the
Long-horizon Desktop Implicative Placement (LDIP) dataset across various
baseline models, where Triple-S successfully executes 89% of tasks in both
observable and partially observable scenarios. Experiments in both simulation
and real-world robot settings further validated the effectiveness of Triple-S.
Our code and dataset is available at: https://github.com/Ghbbbbb/Triple-S.

</details>


### [352] [A Learning-Based Framework for Collision-Free Motion Planning](https://arxiv.org/abs/2508.07502)
*Mateus Salomão,Tianyü Ren,Alexander König*

Main category: cs.RO

TL;DR: 本文提出了一种基于学习的运动规划方法，通过深度神经网络优化参数，实现高效无碰撞轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 解决传统手动调整力场参数的限制，提高在复杂环境中的规划效率。

Method: 结合CUDA加速感知模块、基于预测的规划策略，并通过贝叶斯优化生成数据集。

Result: 实验验证了实时规划能力，并在仿真和机器人上展示了优于传统规划器的泛化性能。

Conclusion: 该方法无需手动调参，适用于实时规划任务，具有更好的泛化能力。

Abstract: This paper presents a learning-based extension to a Circular Field (CF)-based
motion planner for efficient, collision-free trajectory generation in cluttered
environments. The proposed approach overcomes the limitations of hand-tuned
force field parameters by employing a deep neural network trained to infer
optimal planner gains from a single depth image of the scene. The pipeline
incorporates a CUDA-accelerated perception module, a predictive agent-based
planning strategy, and a dataset generated through Bayesian optimization in
simulation. The resulting framework enables real-time planning without manual
parameter tuning and is validated both in simulation and on a Franka Emika
Panda robot. Experimental results demonstrate successful task completion and
improved generalization compared to classical planners.

</details>


### [353] [Progressive Bird's Eye View Perception for Safety-Critical Autonomous Driving: A Comprehensive Survey](https://arxiv.org/abs/2508.07560)
*Yan Gong,Naibang Wang,Jianli Lu,Xinyu Zhang,Yongsheng Gao,Jie Zhao,Zifan Huang,Haozhi Bai,Nanxin Zeng,Nayu Su,Lei Yang,Ziying Song,Xiaoxi Hu,Xinmin Jiang,Xiaojuan Zhang,Susanto Rahardja*

Main category: cs.RO

TL;DR: 该论文首次从安全关键视角全面综述了鸟瞰图（BEV）感知技术，分析了单模态、多模态及多智能体协作感知的框架与策略，并探讨了相关数据集及开放世界挑战。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶从受控环境转向实际部署，确保BEV感知在复杂场景中的安全性和可靠性成为关键挑战。

Method: 系统分析了BEV感知的三个阶段（单模态、多模态、多智能体协作）的框架和策略，并评估了相关数据集。

Result: 总结了BEV感知的现状，并识别了开放世界中的关键挑战，如开放集识别、传感器退化等。

Conclusion: 提出了未来研究方向，包括与端到端自动驾驶系统、具身智能和大语言模型的结合。

Abstract: Bird's-Eye-View (BEV) perception has become a foundational paradigm in
autonomous driving, enabling unified spatial representations that support
robust multi-sensor fusion and multi-agent collaboration. As autonomous
vehicles transition from controlled environments to real-world deployment,
ensuring the safety and reliability of BEV perception in complex scenarios -
such as occlusions, adverse weather, and dynamic traffic - remains a critical
challenge. This survey provides the first comprehensive review of BEV
perception from a safety-critical perspective, systematically analyzing
state-of-the-art frameworks and implementation strategies across three
progressive stages: single-modality vehicle-side, multimodal vehicle-side, and
multi-agent collaborative perception. Furthermore, we examine public datasets
encompassing vehicle-side, roadside, and collaborative settings, evaluating
their relevance to safety and robustness. We also identify key open-world
challenges - including open-set recognition, large-scale unlabeled data, sensor
degradation, and inter-agent communication latency - and outline future
research directions, such as integration with end-to-end autonomous driving
systems, embodied intelligence, and large language models.

</details>


### [354] [Feedback Control of a Single-Tail Bioinspired 59-mg Swimmer](https://arxiv.org/abs/2508.07566)
*Conor K. Trygstad,Cody R. Longwell,Francisco M. F. R. Gonçalves,Elijah K. Blankenship,Néstor O. Pérez-Arancibia*

Main category: cs.RO

TL;DR: 本文介绍了一种改进的FRISSHBot游泳机器人，采用新型SMA双压电片驱动器，实现了二维空间的可控性，首次展示了亚克级单尾水生机器人的反馈控制轨迹跟踪。


<details>
  <summary>Details</summary>
Motivation: 改进原有FRISSHBot的设计，提升其游泳速度和可控性，实现更高效的二维空间运动。

Method: 通过物理信息设计，增大头部并缩短尾部，采用新型SMA双压电片驱动器。

Result: 改进后的机器人游泳速度提升至13.6 mm/s（原版的四倍），在闭环跟踪中速度达9.1 mm/s，跟踪误差低至2.6 mm，转弯半径小至10 mm。

Conclusion: 新型FRISSHBot在速度和可控性上显著提升，为亚克级水生机器人提供了更高效的解决方案。

Abstract: We present an evolved steerable version of the single-tail
Fish-&-Ribbon-Inspired Small Swimming Harmonic roBot (FRISSHBot), a 59-mg
biologically inspired swimmer, which is driven by a new shape-memory alloy
(SMA)-based bimorph actuator. The new FRISSHBot is controllable in the
two-dimensional (2D) space, which enabled the first demonstration of
feedback-controlled trajectory tracking of a single-tail aquatic robot with
onboard actuation at the subgram scale. These new capabilities are the result
of a physics-informed design with an enlarged head and shortened tail relative
to those of the original platform. Enhanced by its design, this new platform
achieves forward swimming speeds of up to 13.6 mm/s (0.38 Bl/s), which is over
four times that of the original platform. Furthermore, when following 2D
references in closed loop, the tested FRISSHBot prototype attains forward
swimming speeds of up to 9.1 mm/s, root-mean-square (RMS) tracking errors as
low as 2.6 mm, turning rates of up to 13.1 {\deg}/s, and turning radii as small
as 10 mm.

</details>


### [355] [In-situ Value-aligned Human-Robot Interactions with Physical Constraints](https://arxiv.org/abs/2508.07606)
*Hongtao Li,Ziyuan Jiao,Xiaofeng Liu,Hangxin Liu,Zilong Zheng*

Main category: cs.RO

TL;DR: 提出了一种结合人类偏好与物理约束的框架，通过上下文学习人类反馈（ICLHF）提升认知机器人任务完成能力。


<details>
  <summary>Details</summary>
Motivation: 认知机器人不仅需完成任务，还需学习并应用人类偏好，以更好地适应未来场景。

Method: 开发了日常家务活动基准，引入ICLHF框架，结合直接指令与日常调整的人类反馈。

Result: 实验证明ICLHF能高效生成任务计划并平衡物理约束与偏好。

Conclusion: 该框架为认知机器人学习人类偏好提供了有效解决方案。

Abstract: Equipped with Large Language Models (LLMs), human-centered robots are now
capable of performing a wide range of tasks that were previously deemed
challenging or unattainable. However, merely completing tasks is insufficient
for cognitive robots, who should learn and apply human preferences to future
scenarios. In this work, we propose a framework that combines human preferences
with physical constraints, requiring robots to complete tasks while considering
both. Firstly, we developed a benchmark of everyday household activities, which
are often evaluated based on specific preferences. We then introduced
In-Context Learning from Human Feedback (ICLHF), where human feedback comes
from direct instructions and adjustments made intentionally or unintentionally
in daily life. Extensive sets of experiments, testing the ICLHF to generate
task plans and balance physical constraints with preferences, have demonstrated
the efficiency of our approach.

</details>


### [356] [End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy](https://arxiv.org/abs/2508.07611)
*Zifan Wang,Xun Yang,Jianzhuang Zhao,Jiaming Zhou,Teli Ma,Ziyao Gao,Arash Ajoudani,Junwei Liang*

Main category: cs.RO

TL;DR: 提出了一种基于LiDAR点云的端到端运动策略，结合CMDP和CBFs，实现人形机器人在复杂动态环境中的安全导航。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中，现有强化学习方法缺乏环境感知或无法处理复杂3D障碍，需要更鲁棒的导航能力。

Method: 将控制问题建模为CMDP，利用CBFs转化为成本函数，采用P3O算法训练，并引入舒适性奖励。

Result: 通过仿真到实物的迁移，验证了框架在人形机器人上的敏捷和安全导航能力。

Conclusion: 该方法成功实现了复杂动态环境中的安全导航，同时兼顾了舒适性和社会意识。

Abstract: The deployment of humanoid robots in unstructured, human-centric environments
requires navigation capabilities that extend beyond simple locomotion to
include robust perception, provable safety, and socially aware behavior.
Current reinforcement learning approaches are often limited by blind
controllers that lack environmental awareness or by vision-based systems that
fail to perceive complex 3D obstacles. In this work, we present an end-to-end
locomotion policy that directly maps raw, spatio-temporal LiDAR point clouds to
motor commands, enabling robust navigation in cluttered dynamic scenes. We
formulate the control problem as a Constrained Markov Decision Process (CMDP)
to formally separate safety from task objectives. Our key contribution is a
novel methodology that translates the principles of Control Barrier Functions
(CBFs) into costs within the CMDP, allowing a model-free Penalized Proximal
Policy Optimization (P3O) to enforce safety constraints during training.
Furthermore, we introduce a set of comfort-oriented rewards, grounded in
human-robot interaction research, to promote motions that are smooth,
predictable, and less intrusive. We demonstrate the efficacy of our framework
through a successful sim-to-real transfer to a physical humanoid robot, which
exhibits agile and safe navigation around both static and dynamic 3D obstacles.

</details>


### [357] [Grasp-HGN: Grasping the Unexpected](https://arxiv.org/abs/2508.07648)
*Mehrshad Zandigohar,Mallesham Dasari,Gunar Schirner*

Main category: cs.RO

TL;DR: 论文提出Grasp-LLaVA和Hybrid Grasp Network（HGN），解决假肢手控制中未见物体的抓取泛化问题，显著提升准确性和速度。


<details>
  <summary>Details</summary>
Motivation: 现有假肢手抓取模型对未见物体表现差，影响用户独立性和生活质量，需提升泛化能力和鲁棒性。

Method: 提出Grasp-LLaVA模型，结合视觉语言推理；设计HGN边缘-云混合架构，动态切换模型以平衡速度与准确性。

Result: Grasp-LLaVA在未见物体上准确率达50.2%，HGN进一步将准确率提升至42.3%，速度提高3.5倍。

Conclusion: Grasp-LLaVA和HGN显著提升假肢手抓取的泛化能力和实用性，为未来设计提供新方向。

Abstract: For transradial amputees, robotic prosthetic hands promise to regain the
capability to perform daily living activities. To advance next-generation
prosthetic hand control design, it is crucial to address current shortcomings
in robustness to out of lab artifacts, and generalizability to new
environments. Due to the fixed number of object to interact with in existing
datasets, contrasted with the virtually infinite variety of objects encountered
in the real world, current grasp models perform poorly on unseen objects,
negatively affecting users' independence and quality of life.
  To address this: (i) we define semantic projection, the ability of a model to
generalize to unseen object types and show that conventional models like YOLO,
despite 80% training accuracy, drop to 15% on unseen objects. (ii) we propose
Grasp-LLaVA, a Grasp Vision Language Model enabling human-like reasoning to
infer the suitable grasp type estimate based on the object's physical
characteristics resulting in a significant 50.2% accuracy over unseen object
types compared to 36.7% accuracy of an SOTA grasp estimation model.
  Lastly, to bridge the performance-latency gap, we propose Hybrid Grasp
Network (HGN), an edge-cloud deployment infrastructure enabling fast grasp
estimation on edge and accurate cloud inference as a fail-safe, effectively
expanding the latency vs. accuracy Pareto. HGN with confidence calibration (DC)
enables dynamic switching between edge and cloud models, improving semantic
projection accuracy by 5.6% (to 42.3%) with 3.5x speedup over the unseen object
types. Over a real-world sample mix, it reaches 86% average accuracy (12.2%
gain over edge-only), and 2.2x faster inference than Grasp-LLaVA alone.

</details>


### [358] [GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions](https://arxiv.org/abs/2508.07650)
*Helong Huang,Min Cen,Kai Tan,Xingyue Quan,Guowei Huang,Hong Zhang*

Main category: cs.RO

TL;DR: GraphCoT-VLA是一种高效的端到端视觉-语言-动作模型，通过结构化思维链推理和实时更新的3D位姿-物体图，解决了现有模型在模糊指令和未知环境状态下的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在处理模糊语言指令和未知环境状态时表现不足，且感知局限于静态二维观察，缺乏三维交互建模能力。

Method: 设计了结构化思维链推理模块（整合高级任务理解、失败任务反馈和低级推理）和实时更新的3D位姿-物体图，并采用混合推理策略。

Result: 在多个真实机器人任务中，GraphCoT-VLA在任务成功率和响应速度上显著优于现有方法，表现出强泛化能力和鲁棒性。

Conclusion: GraphCoT-VLA通过改进推理和三维建模能力，显著提升了机器人操作的性能和适应性。

Abstract: Vision-language-action models have emerged as a crucial paradigm in robotic
manipulation. However, existing VLA models exhibit notable limitations in
handling ambiguous language instructions and unknown environmental states.
Furthermore, their perception is largely constrained to static two-dimensional
observations, lacking the capability to model three-dimensional interactions
between the robot and its environment. To address these challenges, this paper
proposes GraphCoT-VLA, an efficient end-to-end model. To enhance the model's
ability to interpret ambiguous instructions and improve task planning, we
design a structured Chain-of-Thought reasoning module that integrates
high-level task understanding and planning, failed task feedback, and low-level
imaginative reasoning about future object positions and robot actions.
Additionally, we construct a real-time updatable 3D Pose-Object graph, which
captures the spatial configuration of robot joints and the topological
relationships between objects in 3D space, enabling the model to better
understand and manipulate their interactions. We further integrates a dropout
hybrid reasoning strategy to achieve efficient control outputs. Experimental
results across multiple real-world robotic tasks demonstrate that GraphCoT-VLA
significantly outperforms existing methods in terms of task success rate and
response speed, exhibiting strong generalization and robustness in open
environments and under uncertain instructions.

</details>


### [359] [MoRoCo: Multi-operator-robot Coordination, Interaction and Exploration under Restricted Communication](https://arxiv.org/abs/2508.07657)
*Zhuoli Tian,Yuyang Zhang,Jinsheng Wei,Meng Guo*

Main category: cs.RO

TL;DR: MoRoCo是一个用于多操作者-多机器人系统的在线协调与探索框架，支持三种协调模式以适应受限通信环境。


<details>
  <summary>Details</summary>
Motivation: 在通信受限的环境中，现有研究忽视了人类操作者与机器人团队的实时互动需求。

Method: 提出MoRoCo框架，通过分布式算法管理三种协调模式：spread、migrate和chain。

Result: 大规模仿真和硬件实验验证了MoRoCo在受限通信下的高效可靠协调能力。

Conclusion: MoRoCo为复杂环境中的人机协作多机器人自主性提供了重要进展。

Abstract: Fleets of autonomous robots are increasingly deployed alongside multiple
human operators to explore unknown environments, identify salient features, and
perform complex tasks in scenarios such as subterranean exploration,
reconnaissance, and search-and-rescue missions. In these contexts,
communication is often severely limited to short-range exchanges via ad-hoc
networks, posing challenges to coordination. While recent studies have
addressed multi-robot exploration under communication constraints, they largely
overlook the essential role of human operators and their real-time interaction
with robotic teams. Operators may demand timely updates on the exploration
progress and robot status, reprioritize or cancel tasks dynamically, or request
live video feeds and control access. Conversely, robots may seek human
confirmation for anomalous events or require help recovering from motion or
planning failures. To enable such bilateral, context-aware interactions under
restricted communication, this work proposes MoRoCo, a unified framework for
online coordination and exploration in multi-operator, multi-robot systems.
MoRoCo enables the team to adaptively switch among three coordination modes:
spread mode for parallelized exploration with intermittent data sharing,
migrate mode for coordinated relocation, and chain mode for maintaining
high-bandwidth connectivity through multi-hop links. These transitions are
managed through distributed algorithms via only local communication. Extensive
large-scale human-in-the-loop simulations and hardware experiments validate the
necessity of incorporating human robot interactions and demonstrate that MoRoCo
enables efficient, reliable coordination under limited communication, marking a
significant step toward robust human-in-the-loop multi-robot autonomy in
challenging environments.

</details>


### [360] [Risk Map As Middleware: Towards Interpretable Cooperative End-to-end Autonomous Driving for Risk-Aware Planning](https://arxiv.org/abs/2508.07686)
*Mingyue Lei,Zewei Zhou,Hongchen Li,Jiaqi Ma,Jia Hu*

Main category: cs.RO

TL;DR: 论文提出了一种名为RiskMM的端到端自动驾驶框架，通过风险地图作为中间件提升可解释性和多智能体协作能力。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体端到端方法因遮挡和感知范围受限导致危险驾驶，且缺乏可解释性。

Method: RiskMM利用Transformer架构构建多智能体时空表示，通过注意力建模环境交互，并结合学习型MPC模块进行规划。

Result: 在V2XPnP-Seq数据集上验证，RiskMM在风险感知轨迹规划中表现优越且鲁棒。

Conclusion: RiskMM显著提升了端到端驾驶框架的可解释性和协作性能，代码将开源以促进研究。

Abstract: End-to-end paradigm has emerged as a promising approach to autonomous
driving. However, existing single-agent end-to-end pipelines are often
constrained by occlusion and limited perception range, resulting in hazardous
driving. Furthermore, their black-box nature prevents the interpretability of
the driving behavior, leading to an untrustworthiness system. To address these
limitations, we introduce Risk Map as Middleware (RiskMM) and propose an
interpretable cooperative end-to-end driving framework. The risk map learns
directly from the driving data and provides an interpretable spatiotemporal
representation of the scenario from the upstream perception and the
interactions between the ego vehicle and the surrounding environment for
downstream planning. RiskMM first constructs a multi-agent spatiotemporal
representation with unified Transformer-based architecture, then derives
risk-aware representations by modeling interactions among surrounding
environments with attention. These representations are subsequently fed into a
learning-based Model Predictive Control (MPC) module. The MPC planner
inherently accommodates physical constraints and different vehicle types and
can provide interpretation by aligning learned parameters with explicit MPC
elements. Evaluations conducted on the real-world V2XPnP-Seq dataset confirm
that RiskMM achieves superior and robust performance in risk-aware trajectory
planning, significantly enhancing the interpretability of the cooperative
end-to-end driving framework. The codebase will be released to facilitate
future research in this field.

</details>


### [361] [LAURON VI: A Six-Legged Robot for Dynamic Walking](https://arxiv.org/abs/2508.07689)
*Christian Eichmann,Sabine Bellmann,Nicolas Hügel,Louis-Elias Enslin,Carsten Plasberg,Georg Heppner,Arne Roennau,Ruediger Dillmann*

Main category: cs.RO

TL;DR: 本文介绍了六足机器人LAURON VI，旨在通过动态步态和自主控制技术提升其在复杂地形中的适应性，尤其是快速行走能力。


<details>
  <summary>Details</summary>
Motivation: 六足机器人在复杂地形中表现优异，但在简单地形中缺乏快速行走能力，限制了其广泛应用。

Method: 设计了三种控制方法：基于运动学的、模型预测的和强化学习的控制器，并在实验室和火星模拟任务中测试。

Result: 通过引入快速行走策略，LAURON VI显著提升了六足机器人在实际应用中的适用性。

Conclusion: LAURON VI及其控制方法为六足机器人在多样化地形中的高效任务执行提供了新可能。

Abstract: Legged locomotion enables robotic systems to traverse extremely challenging
terrains. In many real-world scenarios, the terrain is not that difficult and
these mixed terrain types introduce the need for flexible use of different
walking strategies to achieve mission goals in a fast, reliable, and
energy-efficient way. Six-legged robots have a high degree of flexibility and
inherent stability that aids them in traversing even some of the most difficult
terrains, such as collapsed buildings. However, their lack of fast walking
gaits for easier surfaces is one reason why they are not commonly applied in
these scenarios.
  This work presents LAURON VI, a six-legged robot platform for research on
dynamic walking gaits as well as on autonomy for complex field missions. The
robot's 18 series elastic joint actuators offer high-frequency interfaces for
Cartesian impedance and pure torque control. We have designed, implemented, and
compared three control approaches: kinematic-based, model-predictive, and
reinforcement-learned controllers. The robot hardware and the different control
approaches were extensively tested in a lab environment as well as on a Mars
analog mission. The introduction of fast locomotion strategies for LAURON VI
makes six-legged robots vastly more suitable for a wide range of real-world
applications.

</details>


### [362] [Robot and Overhead Crane Collaboration Scheme to Enhance Payload Manipulation](https://arxiv.org/abs/2508.07758)
*Antonio Rosales,Alaa Abderrahim,Markku Suomalainen,Mikael Haag,Tapio Heikkilä*

Main category: cs.RO

TL;DR: 提出了一种机器人协作吊车增强载荷操控的方案，通过力交互实现精确引导。


<details>
  <summary>Details</summary>
Motivation: 当前工业实践中，吊车载荷的精确定位需要人工手动引导，效率低且风险高。

Method: 采用两种导纳传递函数，机器人基于位置导纳控制，吊车通过力交互生成速度指令，实现协作引导。

Result: 仿真和实验验证了方案的可行性。

Conclusion: 该协作方案能高效、安全地完成载荷精确定位。

Abstract: This paper presents a scheme to enhance payload manipulation using a robot
collaborating with an overhead crane. In the current industrial practice, when
the crane's payload has to be accurately manipulated and located in a desired
position, the task becomes laborious and risky since the operators have to
guide the fine motions of the payload by hand. In the proposed collaborative
scheme, the crane lifts the payload while the robot's end-effector guides it
toward the desired position. The only link between the robot and the crane is
the interaction force produced during the guiding of the payload. Two
admittance transfer functions are considered to accomplish harmless and smooth
contact with the payload. The first is used in a position-based admittance
control integrated with the robot. The second one adds compliance to the crane
by processing the interaction force through the admittance transfer function to
generate a crane's velocity command that makes the crane follow the payload.
Then the robot's end-effector and the crane move collaboratively to guide the
payload to the desired location. A method is presented to design the admittance
controllers that accomplish a fluent robot-crane collaboration. Simulations and
experiments validating the scheme potential are shown.

</details>


### [363] [AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation](https://arxiv.org/abs/2508.07770)
*Yizheng Zhang,Zhenjun Yu,Jiaxin Lai,Cewu Lu,Lei Han*

Main category: cs.RO

TL;DR: AgentWorld是一个交互式仿真平台，用于开发家庭移动操作能力，结合了自动场景构建和双模式远程操作系统，支持从简单到复杂的任务数据收集，并通过多种模仿学习方法验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够模拟复杂家庭环境的平台，以支持机器人技能的规模化学习，并缩小仿真训练与实际部署之间的差距。

Method: 平台结合自动场景构建（布局生成、语义资产放置、视觉材料配置和物理模拟）与双模式远程操作系统（轮式底座和人形运动策略），收集多样化的任务数据。

Result: 生成的AgentWorld数据集支持从基础动作到多阶段活动的任务，并通过多种模仿学习方法验证了其有效性，实现了仿真到现实的迁移。

Conclusion: AgentWorld为复杂家庭环境中的机器人技能学习提供了全面的解决方案，并展示了仿真训练在实际部署中的潜力。

Abstract: We introduce AgentWorld, an interactive simulation platform for developing
household mobile manipulation capabilities. Our platform combines automated
scene construction that encompasses layout generation, semantic asset
placement, visual material configuration, and physics simulation, with a
dual-mode teleoperation system supporting both wheeled bases and humanoid
locomotion policies for data collection. The resulting AgentWorld Dataset
captures diverse tasks ranging from primitive actions (pick-and-place,
push-pull, etc.) to multistage activities (serve drinks, heat up food, etc.)
across living rooms, bedrooms, and kitchens. Through extensive benchmarking of
imitation learning methods including behavior cloning, action chunking
transformers, diffusion policies, and vision-language-action models, we
demonstrate the dataset's effectiveness for sim-to-real transfer. The
integrated system provides a comprehensive solution for scalable robotic skill
acquisition in complex home environments, bridging the gap between
simulation-based training and real-world deployment. The code, datasets will be
available at https://yizhengzhang1.github.io/agent_world/

</details>


### [364] [SwarmVLM: VLM-Guided Impedance Control for Autonomous Navigation of Heterogeneous Robots in Dynamic Warehousing](https://arxiv.org/abs/2508.07814)
*Malaika Zafar,Roohan Ahmed Khan,Faryal Batool,Yasheerah Yaqoot,Ziang Guo,Mikhail Litvinov,Aleksey Fedoseev,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: SwarmVLM通过视觉语言模型和检索增强生成技术，实现无人机与地面机器人的语义协作，提升异构导航效率。


<details>
  <summary>Details</summary>
Motivation: 无人机在物流中受限于电池寿命和载荷能力，需与地面机器人协同工作以克服这些限制。

Method: 结合VLM和RAG调整阻抗控制参数，无人机作为领导者使用APF规划实时导航，地面机器人通过虚拟阻抗链接跟随。

Result: 系统在12次真实试验中成功率达92%，VLM-RAG在理想光照下物体检测和参数选择准确率为8%。

Conclusion: SwarmVLM展示了在复杂环境中安全导航的能力，地面机器人能有效避开短障碍物。

Abstract: With the growing demand for efficient logistics, unmanned aerial vehicles
(UAVs) are increasingly being paired with automated guided vehicles (AGVs).
While UAVs offer the ability to navigate through dense environments and varying
altitudes, they are limited by battery life, payload capacity, and flight
duration, necessitating coordinated ground support.
  Focusing on heterogeneous navigation, SwarmVLM addresses these limitations by
enabling semantic collaboration between UAVs and ground robots through
impedance control. The system leverages the Vision Language Model (VLM) and the
Retrieval-Augmented Generation (RAG) to adjust impedance control parameters in
response to environmental changes. In this framework, the UAV acts as a leader
using Artificial Potential Field (APF) planning for real-time navigation, while
the ground robot follows via virtual impedance links with adaptive link
topology to avoid collisions with short obstacles.
  The system demonstrated a 92% success rate across 12 real-world trials. Under
optimal lighting conditions, the VLM-RAG framework achieved 8% accuracy in
object detection and selection of impedance parameters. The mobile robot
prioritized short obstacle avoidance, occasionally resulting in a lateral
deviation of up to 50 cm from the UAV path, which showcases safe navigation in
a cluttered setting.

</details>


### [365] [Touch Speaks, Sound Feels: A Multimodal Approach to Affective and Social Touch from Robots to Humans](https://arxiv.org/abs/2508.07839)
*Qiaoqiao Ren,Tony Belpaeme*

Main category: cs.RO

TL;DR: 研究开发了一种结合触觉和听觉的多模态交互系统，用于机器人情感表达，实验表明多模态显著提升情感解码准确性。


<details>
  <summary>Details</summary>
Motivation: 探索机器人通过触觉和听觉结合的方式传达情感和社交手势的能力，填补现有研究的空白。

Method: 开发了一个包含25个振动电机和音频播放的多模态系统，通过实验让32名中国参与者评估振动、声音或两者结合的情感表达效果。

Result: 多模态显著提升情感解码准确性；触觉和听觉各自对特定情感识别有优势；手势单独难以清晰传达情感。

Conclusion: 多感官整合在情感人机交互中至关重要，触觉和听觉线索在情感传达中具有互补作用。

Abstract: Affective tactile interaction constitutes a fundamental component of human
communication. In natural human-human encounters, touch is seldom experienced
in isolation; rather, it is inherently multisensory. Individuals not only
perceive the physical sensation of touch but also register the accompanying
auditory cues generated through contact. The integration of haptic and auditory
information forms a rich and nuanced channel for emotional expression. While
extensive research has examined how robots convey emotions through facial
expressions and speech, their capacity to communicate social gestures and
emotions via touch remains largely underexplored. To address this gap, we
developed a multimodal interaction system incorporating a 5*5 grid of 25
vibration motors synchronized with audio playback, enabling robots to deliver
combined haptic-audio stimuli. In an experiment involving 32 Chinese
participants, ten emotions and six social gestures were presented through
vibration, sound, or their combination. Participants rated each stimulus on
arousal and valence scales. The results revealed that (1) the combined
haptic-audio modality significantly enhanced decoding accuracy compared to
single modalities; (2) each individual channel-vibration or sound-effectively
supported certain emotions recognition, with distinct advantages depending on
the emotional expression; and (3) gestures alone were generally insufficient
for conveying clearly distinguishable emotions. These findings underscore the
importance of multisensory integration in affective human-robot interaction and
highlight the complementary roles of haptic and auditory cues in enhancing
emotional communication.

</details>


### [366] [DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts](https://arxiv.org/abs/2508.07842)
*Yutong Shen,Hangxu Liu,Penghui Liu,Ruizhe Xia,Tianyi Yao,Yitong Sun,Tongtong Feng*

Main category: cs.RO

TL;DR: DETACH是一个基于生物启发的双流解耦框架，用于解决跨领域的长时程任务，通过环境学习和技能学习模块实现高效的任务执行。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预训练子任务的串联，缺乏对新环境和技能组合的泛化能力，无法完成跨领域的复杂任务。

Method: DETACH采用双流解耦机制，包括环境学习模块（空间理解）和技能学习模块（任务执行），分别处理环境信息和自身状态。

Result: 实验表明，DETACH在子任务成功率和执行效率上分别平均提升23%和29%。

Conclusion: DETACH通过解耦机制显著提升了跨领域长时程任务的执行能力。

Abstract: Long-Horizon (LH) tasks in Human-Scene Interaction (HSI) are complex
multi-step tasks that require continuous planning, sequential decision-making,
and extended execution across domains to achieve the final goal. However,
existing methods heavily rely on skill chaining by concatenating pre-trained
subtasks, with environment observations and self-state tightly coupled, lacking
the ability to generalize to new combinations of environments and skills,
failing to complete various LH tasks across domains. To solve this problem,
this paper presents DETACH, a cross-domain learning framework for LH tasks via
biologically inspired dual-stream disentanglement. Inspired by the brain's
"where-what" dual pathway mechanism, DETACH comprises two core modules: i) an
environment learning module for spatial understanding, which captures object
functions, spatial relationships, and scene semantics, achieving cross-domain
transfer through complete environment-self disentanglement; ii) a skill
learning module for task execution, which processes self-state information
including joint degrees of freedom and motor patterns, enabling cross-skill
transfer through independent motor pattern encoding. We conducted extensive
experiments on various LH tasks in HSI scenes. Compared with existing methods,
DETACH can achieve an average subtasks success rate improvement of 23% and
average execution efficiency improvement of 29%.

</details>


### [367] [Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning](https://arxiv.org/abs/2508.07885)
*Shoaib Ahmmad,Zubayer Ahmed Aditto,Md Mehrab Hossain,Noushin Yeasmin,Shorower Hossain*

Main category: cs.RO

TL;DR: 论文提出了一种基于AI的感知系统，用于GPS缺失的室内环境中自主四轴飞行器导航，结合云计算和定制PCB，实现了高效导航。


<details>
  <summary>Details</summary>
Motivation: 解决GPS缺失环境下四轴飞行器的自主导航问题，提升在狭小空间中的感知和决策能力。

Method: 系统整合了YOLOv11、Depth Anything V2、ToF传感器、IMU和云端LLM，采用多线程架构和虚拟安全边界。

Result: 实验显示，目标检测mAP50为0.6，深度估计MAE为7.2 cm，安全边界违规次数少，系统延迟低于1秒。

Conclusion: 该框架为GPS缺失环境下的无人机导航提供了高效辅助，补充了现有技术。

Abstract: This paper introduces an advanced AI-driven perception system for autonomous
quadcopter navigation in GPS-denied indoor environments. The proposed framework
leverages cloud computing to offload computationally intensive tasks and
incorporates a custom-designed printed circuit board (PCB) for efficient sensor
data acquisition, enabling robust navigation in confined spaces. The system
integrates YOLOv11 for object detection, Depth Anything V2 for monocular depth
estimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial
Measurement Unit (IMU), and a cloud-based Large Language Model (LLM) for
context-aware decision-making. A virtual safety envelope, enforced by
calibrated sensor offsets, ensures collision avoidance, while a multithreaded
architecture achieves low-latency processing. Enhanced spatial awareness is
facilitated by 3D bounding box estimation with Kalman filtering. Experimental
results in an indoor testbed demonstrate strong performance, with object
detection achieving a mean Average Precision (mAP50) of 0.6, depth estimation
Mean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42
trials over approximately 11 minutes, and end-to-end system latency below 1
second. This cloud-supported, high-intelligence framework serves as an
auxiliary perception and navigation system, complementing state-of-the-art
drone autonomy for GPS-denied confined spaces.

</details>


### [368] [MolmoAct: Action Reasoning Models that can Reason in Space](https://arxiv.org/abs/2508.07917)
*Jason Lee,Jiafei Duan,Haoquan Fang,Yuquan Deng,Shuo Liu,Boyang Li,Bohan Fang,Jieyu Zhang,Yi Ru Wang,Sangho Lee,Winson Han,Wilbert Pumacay,Angelica Wu,Rose Hendrix,Karen Farley,Eli VanderBilt,Ali Farhadi,Dieter Fox,Ranjay Krishna*

Main category: cs.RO

TL;DR: 论文提出了一种名为Action Reasoning Models (ARMs)的视觉-语言-动作模型，通过三阶段结构化流程整合感知、规划与控制，提升了机器人的适应性和语义理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器人基础模型直接将感知和指令映射到控制，限制了适应性、泛化能力和语义基础。

Method: 采用三阶段管道：深度感知标记编码、可编辑轨迹规划、精确低级动作预测。

Result: 在仿真和真实环境中表现优异，如SimperEnv任务70.5%零样本准确率，LIBERO任务86.6%成功率，并显著优于基线模型。

Conclusion: MolmoAct是当前最先进的机器人基础模型，通过结构化推理将感知转化为有目的的动作，并开源了模型权重和数据集。

Abstract: Reasoning is central to purposeful action, yet most robotic foundation models
map perception and instructions directly to control, which limits adaptability,
generalization, and semantic grounding. We introduce Action Reasoning Models
(ARMs), a class of vision-language-action models that integrate perception,
planning, and control through a structured three-stage pipeline. Our model,
MolmoAct, encodes observations and instructions into depth-aware perception
tokens, generates mid-level spatial plans as editable trajectory traces, and
predicts precise low-level actions, enabling explainable and steerable
behavior. MolmoAct-7B-D achieves strong performance across simulation and
real-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching
tasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success on
LIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks;
and in real-world fine-tuning, an additional 10% (single-arm) and an additional
22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines
by an additional 23.3% on out-of-distribution generalization and achieves top
human-preference scores for open-ended instruction following and trajectory
steering. Furthermore, we release, for the first time, the MolmoAct Dataset --
a mid-training robot dataset comprising over 10,000 high quality robot
trajectories across diverse scenarios and tasks. Training with this dataset
yields an average 5.5% improvement in general performance over the base model.
We release all model weights, training code, our collected dataset, and our
action reasoning dataset, establishing MolmoAct as both a state-of-the-art
robotics foundation model and an open blueprint for building ARMs that
transform perception into purposeful action through structured reasoning.
Blogpost: https://allenai.org/blog/molmoact

</details>


### [369] [PCHands: PCA-based Hand Pose Synergy Representation on Manipulators with N-DoF](https://arxiv.org/abs/2508.07945)
*En Yen Puang,Federico Ceola,Giulia Pasquale,Lorenzo Natale*

Main category: cs.RO

TL;DR: PCHands提出了一种通用表示方法，用于不同形态机械手的灵巧操作学习，通过统一的锚点位置描述格式，提取跨机械手的姿态协同性，并在强化学习中验证其高效性和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决不同形态机械手在灵巧操作中通用表示学习的问题，以提升跨机械手的任务迁移和学习效率。

Method: 提出PCHands方法，基于锚点位置定义统一描述格式，学习可变长度的潜在表示，并提取跨机械手的通用主成分。

Result: PCHands在强化学习中表现优于关节空间基线，且在演示学习中也表现出鲁棒性，实际实验验证了其有效性。

Conclusion: PCHands为跨机械手的灵巧操作提供了一种高效的通用表示方法，具有实际应用潜力。

Abstract: We consider the problem of learning a common representation for dexterous
manipulation across manipulators of different morphologies. To this end, we
propose PCHands, a novel approach for extracting hand postural synergies from a
large set of manipulators. We define a simplified and unified description
format based on anchor positions for manipulators ranging from 2-finger
grippers to 5-finger anthropomorphic hands. This enables learning a
variable-length latent representation of the manipulator configuration and the
alignment of the end-effector frame of all manipulators. We show that it is
possible to extract principal components from this latent representation that
is universal across manipulators of different structures and degrees of
freedom. To evaluate PCHands, we use this compact representation to encode
observation and action spaces of control policies for dexterous manipulation
tasks learned with RL. In terms of learning efficiency and consistency, the
proposed representation outperforms a baseline that learns the same tasks in
joint space. We additionally show that PCHands performs robustly in RL from
demonstration, when demonstrations are provided from a different manipulator.
We further support our results with real-world experiments that involve a
2-finger gripper and a 4-finger anthropomorphic hand. Code and additional
material are available at https://hsp-iit.github.io/PCHands/.

</details>


### [370] [Aerial Target Encirclement and Interception with Noisy Range Observations](https://arxiv.org/abs/2508.08046)
*Fen Liu,Shenghai Yuan,Thien-Minh Nguyen,Wei Meng,Lihua Xie*

Main category: cs.RO

TL;DR: 提出一种利用噪声距离测量包围拦截非合作空中目标的策略，结合反同步轨迹和新型控制器实现目标状态估计与拦截。


<details>
  <summary>Details</summary>
Motivation: 针对非合作空中目标的拦截问题，利用噪声测量和动态轨迹确保目标状态的可观测性，并实现自适应拦截。

Method: 采用反同步3D振动弦轨迹和卡尔曼滤波进行状态估计，设计新型反目标控制器实现自适应拦截。

Result: 通过理论分析和实验验证，证明了状态估计误差的指数稳定性和包围误差的收敛性。

Conclusion: 系统设计有效，适用于非合作目标的拦截任务。

Abstract: This paper proposes a strategy to encircle and intercept a non-cooperative
aerial point-mass moving target by leveraging noisy range measurements for
state estimation. In this approach, the guardians actively ensure the
observability of the target by using an anti-synchronization (AS), 3D
``vibrating string" trajectory, which enables rapid position and velocity
estimation based on the Kalman filter. Additionally, a novel anti-target
controller is designed for the guardians to enable adaptive transitions from
encircling a protected target to encircling, intercepting, and neutralizing a
hostile target, taking into consideration the input constraints of the
guardians. Based on the guaranteed uniform observability, the exponentially
bounded stability of the state estimation error and the convergence of the
encirclement error are rigorously analyzed. Simulation results and real-world
UAV experiments are presented to further validate the effectiveness of the
system design.

</details>


### [371] [Capsizing-Guided Trajectory Optimization for Autonomous Navigation with Rough Terrain](https://arxiv.org/abs/2508.08108)
*Wei Zhang,Yinchuan Wang,Wangtao Lu,Pengyu Zhang,Xiang Zhang,Yue Wang,Chaoqun Wang*

Main category: cs.RO

TL;DR: 提出了一种防倾覆轨迹规划器（CAP），用于在崎岖地形中实现安全高效的导航。


<details>
  <summary>Details</summary>
Motivation: 地面机器人在恶劣环境中自主导航时面临非平凡障碍和崎岖地形的挑战，需平衡安全与效率。

Method: 分析机器人倾覆稳定性，定义可穿越方向，并将其作为约束纳入轨迹优化，使用图求解器生成稳健轨迹。

Result: 仿真和实验验证了CAP的有效性和鲁棒性，性能优于现有方法。

Conclusion: CAP显著提升了机器人在崎岖地形中的导航性能。

Abstract: It is a challenging task for ground robots to autonomously navigate in harsh
environments due to the presence of non-trivial obstacles and uneven terrain.
This requires trajectory planning that balances safety and efficiency. The
primary challenge is to generate a feasible trajectory that prevents robot from
tip-over while ensuring effective navigation. In this paper, we propose a
capsizing-aware trajectory planner (CAP) to achieve trajectory planning on the
uneven terrain. The tip-over stability of the robot on rough terrain is
analyzed. Based on the tip-over stability, we define the traversable
orientation, which indicates the safe range of robot orientations. This
orientation is then incorporated into a capsizing-safety constraint for
trajectory optimization. We employ a graph-based solver to compute a robust and
feasible trajectory while adhering to the capsizing-safety constraint.
Extensive simulation and real-world experiments validate the effectiveness and
robustness of the proposed method. The results demonstrate that CAP outperforms
existing state-of-the-art approaches, providing enhanced navigation performance
on uneven terrains.

</details>


### [372] [AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies](https://arxiv.org/abs/2508.08113)
*Yinpei Dai,Jayjun Lee,Yichi Zhang,Ziqiao Ma,Jed Yang,Amir Zadeh,Chuan Li,Nima Fazeli,Joyce Chai*

Main category: cs.RO

TL;DR: AimBot是一种轻量级视觉增强技术，通过叠加辅助视觉提示（如射击线和瞄准镜）到RGB图像上，提升机器人操作的视觉运动策略学习效果。


<details>
  <summary>Details</summary>
Motivation: 改善机器人操作中视觉运动策略学习的性能，通过提供明确的空间线索来增强视觉反馈。

Method: AimBot利用深度图像、相机外参和末端执行器姿态计算叠加的视觉提示，替换原始RGB图像，无需改变模型架构。

Result: 在仿真和实际环境中，AimBot显著提升了多种视觉运动策略的性能，且计算开销极小（小于1毫秒）。

Conclusion: AimBot通过空间基础视觉反馈，简单高效地提升了机器人操作的视觉运动策略学习效果。

Abstract: In this paper, we propose AimBot, a lightweight visual augmentation technique
that provides explicit spatial cues to improve visuomotor policy learning in
robotic manipulation. AimBot overlays shooting lines and scope reticles onto
multi-view RGB images, offering auxiliary visual guidance that encodes the
end-effector's state. The overlays are computed from depth images, camera
extrinsics, and the current end-effector pose, explicitly conveying spatial
relationships between the gripper and objects in the scene. AimBot incurs
minimal computational overhead (less than 1 ms) and requires no changes to
model architectures, as it simply replaces original RGB images with augmented
counterparts. Despite its simplicity, our results show that AimBot consistently
improves the performance of various visuomotor policies in both simulation and
real-world settings, highlighting the benefits of spatially grounded visual
feedback.

</details>


### [373] [COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models](https://arxiv.org/abs/2508.08144)
*Ganesh Sundaram,Jonas Ulmen,Amjad Haider,Daniel Görges*

Main category: cs.RO

TL;DR: 本文提出了一种基于组件感知的结构化剪枝方法，用于优化神经网络控制器（NNC）的压缩与稳定性平衡，适用于资源受限的移动平台。


<details>
  <summary>Details</summary>
Motivation: 随着移动机器人、可穿戴设备和物联网设备的快速发展，对计算高效的NNC需求增加，但深度神经网络（DNN）的高计算复杂性和内存需求限制了其在边缘设备上的部署。

Method: 采用组件感知的结构化剪枝方法，结合数学稳定性保证（如Lyapunov准则），在TD-MPC算法上评估压缩与稳定性的平衡。

Result: 实验验证表明，该方法在降低模型复杂度的同时保持了控制性能和稳定性，并确定了安全压缩比的理论界限。

Conclusion: 该方法为资源受限环境中的压缩NNC部署提供了理论框架和实践指导，确保稳定性不被破坏。

Abstract: The rapid growth of resource-constrained mobile platforms, including mobile
robots, wearable systems, and Internet-of-Things devices, has increased the
demand for computationally efficient neural network controllers (NNCs) that can
operate within strict hardware limitations. While deep neural networks (DNNs)
demonstrate superior performance in control applications, their substantial
computational complexity and memory requirements present significant barriers
to practical deployment on edge devices. This paper introduces a comprehensive
model compression methodology that leverages component-aware structured pruning
to determine the optimal pruning magnitude for each pruning group, ensuring a
balance between compression and stability for NNC deployment. Our approach is
rigorously evaluated on Temporal Difference Model Predictive Control (TD-MPC),
a state-of-the-art model-based reinforcement learning algorithm, with a
systematic integration of mathematical stability guarantee properties,
specifically Lyapunov criteria. The key contribution of this work lies in
providing a principled framework for determining the theoretical limits of
model compression while preserving controller stability. Experimental
validation demonstrates that our methodology successfully reduces model
complexity while maintaining requisite control performance and stability
characteristics. Furthermore, our approach establishes a quantitative boundary
for safe compression ratios, enabling practitioners to systematically determine
the maximum permissible model reduction before violating critical stability
properties, thereby facilitating the confident deployment of compressed NNCs in
resource-limited environments.

</details>


### [374] [Verti-Arena: A Controllable and Standardized Indoor Testbed for Multi-Terrain Off-Road Autonomy](https://arxiv.org/abs/2508.08226)
*Haiyue Chen,Aniket Datar,Tong Xu,Francesco Cancelliere,Harsh Rangwala,Madhan Balaji Rao,Daeun Song,David Eichinger,Xuesu Xiao*

Main category: cs.RO

TL;DR: Verti-Arena是一个可重构的室内设施，旨在为越野自主性研究提供标准化测试环境，支持数据收集和算法验证。


<details>
  <summary>Details</summary>
Motivation: 越野导航对移动机器人至关重要，但缺乏可控且标准化的真实测试环境限制了研究进展。

Method: 开发了Verti-Arena，配备传感器和运动捕捉系统，提供可重复的基准环境和精确的地面真实数据。

Result: Verti-Arena支持可重复实验、数据收集和算法比较，并通过网络界面实现远程实验。

Conclusion: Verti-Arena填补了越野自主性研究的测试空白，促进了标准化和可重复性。

Abstract: Off-road navigation is an important capability for mobile robots deployed in
environments that are inaccessible or dangerous to humans, such as disaster
response or planetary exploration. Progress is limited due to the lack of a
controllable and standardized real-world testbed for systematic data collection
and validation. To fill this gap, we introduce Verti-Arena, a reconfigurable
indoor facility designed specifically for off-road autonomy. By providing a
repeatable benchmark environment, Verti-Arena supports reproducible experiments
across a variety of vertically challenging terrains and provides precise ground
truth measurements through onboard sensors and a motion capture system.
Verti-Arena also supports consistent data collection and comparative evaluation
of algorithms in off-road autonomy research. We also develop a web-based
interface that enables research groups worldwide to remotely conduct
standardized off-road autonomy experiments on Verti-Arena.

</details>


### [375] [ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks](https://arxiv.org/abs/2508.08240)
*Kaijun Wang,Liqin Lu,Mingyu Liu,Jianuo Jiang,Zeju Li,Bolin Zhang,Wancai Zheng,Xinyi Yu,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: ODYSSEY是一个统一的移动操作框架，结合了高级任务规划和低级全身控制，解决了语言引导的长时程移动操作中的感知、泛化和控制问题。


<details>
  <summary>Details</summary>
Motivation: 解决语言引导的长时程移动操作中的三大限制：感知范围受限、泛化能力不足以及高机动性与精确控制的平衡问题。

Method: 引入分层规划器和视觉语言模型进行任务分解与动作执行，开发新型全身控制策略以协调复杂地形。

Result: 成功实现模拟到现实的迁移，展示了系统在非结构化环境中的泛化能力和鲁棒性。

Conclusion: ODYSSEY推动了通用机器人助手在复杂动态任务中的可行性。

Abstract: Language-guided long-horizon mobile manipulation has long been a grand
challenge in embodied semantic reasoning, generalizable manipulation, and
adaptive locomotion. Three fundamental limitations hinder progress: First,
although large language models have improved spatial reasoning and task
planning through semantic priors, existing implementations remain confined to
tabletop scenarios, failing to address the constrained perception and limited
actuation ranges of mobile platforms. Second, current manipulation strategies
exhibit insufficient generalization when confronted with the diverse object
configurations encountered in open-world environments. Third, while crucial for
practical deployment, the dual requirement of maintaining high platform
maneuverability alongside precise end-effector control in unstructured settings
remains understudied.
  In this work, we present ODYSSEY, a unified mobile manipulation framework for
agile quadruped robots equipped with manipulators, which seamlessly integrates
high-level task planning with low-level whole-body control. To address the
challenge of egocentric perception in language-conditioned tasks, we introduce
a hierarchical planner powered by a vision-language model, enabling
long-horizon instruction decomposition and precise action execution. At the
control level, our novel whole-body policy achieves robust coordination across
challenging terrains. We further present the first benchmark for long-horizon
mobile manipulation, evaluating diverse indoor and outdoor scenarios. Through
successful sim-to-real transfer, we demonstrate the system's generalization and
robustness in real-world deployments, underscoring the practicality of legged
manipulators in unstructured environments. Our work advances the feasibility of
generalized robotic assistants capable of complex, dynamic tasks. Our project
page: https://kaijwang.github.io/odyssey.github.io/

</details>


### [376] [BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion](https://arxiv.org/abs/2508.08241)
*Takara E. Truong,Qiayuan Liao,Xiaoyu Huang,Guy Tevet,C. Karen Liu,Koushil Sreenath*

Main category: cs.RO

TL;DR: BeyondMimic是一个从人类动作中学习的框架，通过扩散策略实现自然化的人形机器人控制，解决了高质量运动跟踪和动作蒸馏的问题。


<details>
  <summary>Details</summary>
Motivation: 从人类动作中学习技能是实现通用人形机器人控制的关键，但目前缺乏高质量的运动跟踪框架和有效的动作蒸馏方法。

Method: 提出BeyondMimic框架，结合运动跟踪管道和扩散策略，实现动态运动跟踪和任务特定控制。

Result: 在硬件上实现了多样任务，如导航、遥操作和避障，展示了高质量的运动跟踪和灵活的动作合成能力。

Conclusion: BeyondMimic填补了运动跟踪和动作合成的空白，为人形机器人控制提供了新方向。

Abstract: Learning skills from human motions offers a promising path toward
generalizable policies for whole-body humanoid control, yet two key
cornerstones are missing: (1) a high-quality motion tracking framework that
faithfully transforms large-scale kinematic references into robust and
extremely dynamic motions on real hardware, and (2) a distillation approach
that can effectively learn these motion primitives and compose them to solve
downstream tasks. We address these gaps with BeyondMimic, the first real-world
framework to learn from human motions for versatile and naturalistic humanoid
control via guided diffusion. Our framework provides a motion tracking pipeline
capable of challenging skills such as jumping spins, sprinting, and cartwheels
with state-of-the-art motion quality. Moving beyond mimicking existing motions
and synthesize novel ones, we further introduce a unified diffusion policy that
enables zero-shot task-specific control at test time using simple cost
functions. Deployed on hardware, BeyondMimic performs diverse tasks at test
time, including waypoint navigation, joystick teleoperation, and obstacle
avoidance, bridging sim-to-real motion tracking and flexible synthesis of human
motion primitives for whole-body control. https://beyondmimic.github.io/.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [377] [Transfer Learning with EfficientNet for Accurate Leukemia Cell Classification](https://arxiv.org/abs/2508.06535)
*Faisal Ahmed*

Main category: eess.IV

TL;DR: 该研究通过迁移学习和数据增强技术，使用预训练的卷积神经网络（如EfficientNet-B3）对急性淋巴细胞白血病（ALL）进行分类，取得了优于现有方法的结果。


<details>
  <summary>Details</summary>
Motivation: 急性淋巴细胞白血病的准确分类对早期诊断和治疗至关重要，但现有方法在性能上仍有提升空间。

Method: 研究采用迁移学习，结合数据增强技术平衡数据集，并评估了多种预训练CNN模型（如ResNet50、ResNet101和EfficientNet变体）。

Result: EfficientNet-B3表现最佳，F1分数为94.30%，准确率为92.02%，AUC为94.79%，优于C-NMC挑战中的现有方法。

Conclusion: 结合数据增强和迁移学习（尤其是EfficientNet-B3）可开发出准确且稳健的血液恶性肿瘤诊断工具。

Abstract: Accurate classification of Acute Lymphoblastic Leukemia (ALL) from peripheral
blood smear images is essential for early diagnosis and effective treatment
planning. This study investigates the use of transfer learning with pretrained
convolutional neural networks (CNNs) to improve diagnostic performance. To
address the class imbalance in the dataset of 3,631 Hematologic and 7,644 ALL
images, we applied extensive data augmentation techniques to create a balanced
training set of 10,000 images per class. We evaluated several models, including
ResNet50, ResNet101, and EfficientNet variants B0, B1, and B3. EfficientNet-B3
achieved the best results, with an F1-score of 94.30%, accuracy of 92.02%,
andAUCof94.79%,outperformingpreviouslyreported methods in the C-NMCChallenge.
Thesefindings demonstrate the effectiveness of combining data augmentation with
advanced transfer learning models, particularly EfficientNet-B3, in developing
accurate and robust diagnostic tools for hematologic malignancy detection.

</details>


### [378] [LWT-ARTERY-LABEL: A Lightweight Framework for Automated Coronary Artery Identification](https://arxiv.org/abs/2508.06874)
*Shisheng Zhang,Ramtin Gharleghi,Sonit Singh,Daniel Moses,Dona Adikari,Arcot Sowmya,Susann Beier*

Main category: eess.IV

TL;DR: 提出一种结合解剖学知识和规则拓扑约束的轻量级方法，用于冠状动脉自动标记，解决了传统方法和深度学习方法的不足。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉疾病是全球主要死因，CTCA是其关键诊断工具，但手动分析耗时且劳动密集。现有自动标记方法未能充分利用数据驱动或临床知识。

Method: 提出一种轻量级方法，整合解剖学知识和规则拓扑约束，用于冠状动脉自动标记。

Result: 在基准数据集上达到最先进性能。

Conclusion: 该方法为冠状动脉自动标记提供了高效且实用的解决方案。

Abstract: Coronary artery disease (CAD) remains the leading cause of death globally,
with computed tomography coronary angiography (CTCA) serving as a key
diagnostic tool. However, coronary arterial analysis using CTCA, such as
identifying artery-specific features from computational modelling, is
labour-intensive and time-consuming. Automated anatomical labelling of coronary
arteries offers a potential solution, yet the inherent anatomical variability
of coronary trees presents a significant challenge. Traditional knowledge-based
labelling methods fall short in leveraging data-driven insights, while recent
deep-learning approaches often demand substantial computational resources and
overlook critical clinical knowledge. To address these limitations, we propose
a lightweight method that integrates anatomical knowledge with rule-based
topology constraints for effective coronary artery labelling. Our approach
achieves state-of-the-art performance on benchmark datasets, providing a
promising alternative for automated coronary artery labelling.

</details>


### [379] [Fusion-Based Brain Tumor Classification Using Deep Learning and Explainable AI, and Rule-Based Reasoning](https://arxiv.org/abs/2508.06891)
*Melika Filvantorkaman,Mohsen Piri,Maral Filvan Torkaman,Ashkan Zabihi,Hamidreza Moradi*

Main category: eess.IV

TL;DR: 该研究提出了一种基于MobileNetV2和DenseNet121的集成深度学习框架，用于MRI脑肿瘤分类，结合可解释AI模块，取得了高准确率和临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 提高脑肿瘤MRI分类的准确性和可解释性，以支持临床诊断和治疗规划。

Method: 使用MobileNetV2和DenseNet121的集成框架，结合软投票策略和Grad-CAM++可视化，并引入临床决策规则。

Result: 集成模型准确率达91.7%，Grad-CAM++可视化与专家标注区域高度一致，临床评估得分高。

Conclusion: 该框架为脑肿瘤分类提供了准确、可解释且临床相关的解决方案。

Abstract: Accurate and interpretable classification of brain tumors from magnetic
resonance imaging (MRI) is critical for effective diagnosis and treatment
planning. This study presents an ensemble-based deep learning framework that
combines MobileNetV2 and DenseNet121 convolutional neural networks (CNNs) using
a soft voting strategy to classify three common brain tumor types: glioma,
meningioma, and pituitary adenoma. The models were trained and evaluated on the
Figshare dataset using a stratified 5-fold cross-validation protocol. To
enhance transparency and clinical trust, the framework integrates an
Explainable AI (XAI) module employing Grad-CAM++ for class-specific saliency
visualization, alongside a symbolic Clinical Decision Rule Overlay (CDRO) that
maps predictions to established radiological heuristics. The ensemble
classifier achieved superior performance compared to individual CNNs, with an
accuracy of 91.7%, precision of 91.9%, recall of 91.7%, and F1-score of 91.6%.
Grad-CAM++ visualizations revealed strong spatial alignment between model
attention and expert-annotated tumor regions, supported by Dice coefficients up
to 0.88 and IoU scores up to 0.78. Clinical rule activation further validated
model predictions in cases with distinct morphological features. A
human-centered interpretability assessment involving five board-certified
radiologists yielded high Likert-scale scores for both explanation usefulness
(mean = 4.4) and heatmap-region correspondence (mean = 4.0), reinforcing the
framework's clinical relevance. Overall, the proposed approach offers a robust,
interpretable, and generalizable solution for automated brain tumor
classification, advancing the integration of deep learning into clinical
neurodiagnostics.

</details>


### [380] [Spatio-Temporal Conditional Diffusion Models for Forecasting Future Multiple Sclerosis Lesion Masks Conditioned on Treatments](https://arxiv.org/abs/2508.07006)
*Gian Mario Favero,Ge Ya Luo,Nima Fathi,Justin Szeto,Douglas L. Arnold,Brennan Nichyporuk,Chris Pal,Tal Arbel*

Main category: eess.IV

TL;DR: 提出首个治疗感知的时空扩散模型，用于预测多发性硬化症（MS）病灶的演变，结合多模态数据和治疗效果，生成未来病灶掩模。


<details>
  <summary>Details</summary>
Motivation: 个性化医疗在MS等异质性进展疾病中潜力巨大，但缺乏预测病灶演变的工具。

Method: 采用基于体素空间的方法，整合MRI和治疗信息，预测未来新发或扩大的T2病灶掩模。

Result: 在2131例患者的多中心数据集上验证，模型能准确预测六种治疗下的病灶演变，并展示临床应用的潜力。

Conclusion: 因果图像生成模型有望成为MS数据驱动预后的强大工具。

Abstract: Image-based personalized medicine has the potential to transform healthcare,
particularly for diseases that exhibit heterogeneous progression such as
Multiple Sclerosis (MS). In this work, we introduce the first treatment-aware
spatio-temporal diffusion model that is able to generate future masks
demonstrating lesion evolution in MS. Our voxel-space approach incorporates
multi-modal patient data, including MRI and treatment information, to forecast
new and enlarging T2 (NET2) lesion masks at a future time point. Extensive
experiments on a multi-centre dataset of 2131 patient 3D MRIs from randomized
clinical trials for relapsing-remitting MS demonstrate that our generative
model is able to accurately predict NET2 lesion masks for patients across six
different treatments. Moreover, we demonstrate our model has the potential for
real-world clinical applications through downstream tasks such as future lesion
count and location estimation, binary lesion activity classification, and
generating counterfactual future NET2 masks for several treatments with
different efficacies. This work highlights the potential of causal, image-based
generative models as powerful tools for advancing data-driven prognostics in
MS.

</details>


### [381] [Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities](https://arxiv.org/abs/2508.07031)
*Anindya Bijoy Das,Shahnewaz Karim Sakib,Shibbir Ahmed*

Main category: eess.IV

TL;DR: 研究分析了大型语言模型（LLMs）在医学影像任务中的幻觉问题，包括图像到文本和文本到图像两种方向，揭示了常见错误模式及其对临床可靠性的影响。


<details>
  <summary>Details</summary>
Motivation: LLMs在医学影像任务中常产生幻觉（自信但错误的输出），可能误导临床决策，因此需要系统性研究其错误模式和原因。

Method: 研究通过专家标准评估LLMs在图像到文本（生成报告）和文本到图像（生成影像）任务中的幻觉，分析错误类型如事实不一致和解剖学不准确。

Result: 发现两种任务中均存在常见幻觉模式，模型架构和训练数据是导致失败的主要因素。

Conclusion: 研究为提升LLM驱动的医学影像系统的安全性和可信度提供了见解。

Abstract: Large Language Models (LLMs) are increasingly applied to medical imaging
tasks, including image interpretation and synthetic image generation. However,
these models often produce hallucinations, which are confident but incorrect
outputs that can mislead clinical decisions. This study examines hallucinations
in two directions: image to text, where LLMs generate reports from X-ray, CT,
or MRI scans, and text to image, where models create medical images from
clinical prompts. We analyze errors such as factual inconsistencies and
anatomical inaccuracies, evaluating outputs using expert informed criteria
across imaging modalities. Our findings reveal common patterns of hallucination
in both interpretive and generative tasks, with implications for clinical
reliability. We also discuss factors contributing to these failures, including
model architecture and training data. By systematically studying both image
understanding and generation, this work provides insights into improving the
safety and trustworthiness of LLM driven medical imaging systems.

</details>


### [382] [3DGS-VBench: A Comprehensive Video Quality Evaluation Benchmark for 3DGS Compression](https://arxiv.org/abs/2508.07038)
*Yuke Xing,William Gordon,Qi Yang,Kaifa Yang,Jiarui Wang,Yiling Xu*

Main category: eess.IV

TL;DR: 3DGS-VBench是一个用于评估3D高斯泼溅（3DGS）压缩算法视觉质量的大规模数据集和基准测试，包含660个压缩模型和视频序列，并提供了MOS评分和15种质量评估指标的对比。


<details>
  <summary>Details</summary>
Motivation: 3DGS的高存储需求限制了实际应用，现有压缩方法缺乏系统性的视觉质量评估研究。

Method: 建立了3DGS-VBench数据集，包含11个场景的660个压缩模型和视频序列，由50名参与者标注MOS评分，并验证了数据集的可靠性。

Result: 评估了6种3DGS压缩算法的存储效率和视觉质量，对比了15种质量评估指标。

Conclusion: 该工作为3DGS压缩和质量评估研究提供了专用数据集和基准测试，推动了相关领域的发展。

Abstract: 3D Gaussian Splatting (3DGS) enables real-time novel view synthesis with high
visual fidelity, but its substantial storage requirements hinder practical
deployment, prompting state-of-the-art (SOTA) 3DGS methods to incorporate
compression modules. However, these 3DGS generative compression techniques
introduce unique distortions lacking systematic quality assessment research. To
this end, we establish 3DGS-VBench, a large-scale Video Quality Assessment
(VQA) Dataset and Benchmark with 660 compressed 3DGS models and video sequences
generated from 11 scenes across 6 SOTA 3DGS compression algorithms with
systematically designed parameter levels. With annotations from 50
participants, we obtained MOS scores with outlier removal and validated dataset
reliability. We benchmark 6 3DGS compression algorithms on storage efficiency
and visual quality, and evaluate 15 quality assessment metrics across multiple
paradigms. Our work enables specialized VQA model training for 3DGS, serving as
a catalyst for compression and quality assessment research. The dataset is
available at https://github.com/YukeXing/3DGS-VBench.

</details>


### [383] [SAGCNet: Spatial-Aware Graph Completion Network for Missing Slice Imputation in Population CMR Imaging](https://arxiv.org/abs/2508.07041)
*Junkai Liu,Nay Aung,Theodoros N. Arvanitis,Stefan K. Piechnik,Joao A C Lima,Steffen E. Petersen,Le Zhang*

Main category: eess.IV

TL;DR: SAGCNet提出了一种新的MRI缺失切片合成方法，通过图结构和空间适配器解决3D MRI数据中的局部和全局依赖问题。


<details>
  <summary>Details</summary>
Motivation: MRI缺失或不可用切片影响诊断准确性，现有方法难以建模3D数据的局部和全局依赖关系。

Method: 提出SAGCNet，包含体积切片图完成模块和体积空间适配器组件，以捕捉3D空间上下文。

Result: 在心脏MRI数据集上，SAGCNet表现优于现有方法，且在切片数据有限时仍保持高性能。

Conclusion: SAGCNet有效解决了MRI缺失切片合成问题，为临床实践提供了更可靠的辅助工具。

Abstract: Magnetic resonance imaging (MRI) provides detailed soft-tissue
characteristics that assist in disease diagnosis and screening. However, the
accuracy of clinical practice is often hindered by missing or unusable slices
due to various factors. Volumetric MRI synthesis methods have been developed to
address this issue by imputing missing slices from available ones. The inherent
3D nature of volumetric MRI data, such as cardiac magnetic resonance (CMR),
poses significant challenges for missing slice imputation approaches, including
(1) the difficulty of modeling local inter-slice correlations and dependencies
of volumetric slices, and (2) the limited exploration of crucial 3D spatial
information and global context. In this study, to mitigate these issues, we
present Spatial-Aware Graph Completion Network (SAGCNet) to overcome the
dependency on complete volumetric data, featuring two main innovations: (1) a
volumetric slice graph completion module that incorporates the inter-slice
relationships into a graph structure, and (2) a volumetric spatial adapter
component that enables our model to effectively capture and utilize various
forms of 3D spatial context. Extensive experiments on cardiac MRI datasets
demonstrate that SAGCNet is capable of synthesizing absent CMR slices,
outperforming competitive state-of-the-art MRI synthesis methods both
quantitatively and qualitatively. Notably, our model maintains superior
performance even with limited slice data.

</details>


### [384] [Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications](https://arxiv.org/abs/2508.07165)
*Zelin Qiu,Xi Wang,Zhuoyao Xie,Juan Zhou,Yu Wang,Lingjie Yang,Xinrui Jiang,Juyoung Bae,Moo Hyun Son,Qiang Ye,Dexuan Chen,Rui Zhang,Tao Li,Neeraj Ramesh Mahboobani,Varut Vardhanabhuti,Xiaohui Duan,Yinghua Zhao,Hao Chen*

Main category: eess.IV

TL;DR: PRISM是一种基于大规模多序列MRI预训练的基础模型，通过解耦解剖不变特征和序列特异性变化，显著提升了深度学习模型在多样化MRI协议下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多序列MRI的异质性限制了深度学习模型的泛化能力，影响了临床实用性。PRISM旨在解决这一问题。

Method: 收集64个数据集（34个用于预训练），提出新预训练范式，解耦解剖不变特征与序列特异性变化。

Result: PRISM在44个下游任务中表现优异，39项排名第一，显著优于非预训练模型和其他基础模型。

Conclusion: PRISM为多序列MRI分析提供了可扩展框架，增强了AI在放射学中的临床适用性。

Abstract: Multi-sequence Magnetic Resonance Imaging (MRI) offers remarkable
versatility, enabling the distinct visualization of different tissue types.
Nevertheless, the inherent heterogeneity among MRI sequences poses significant
challenges to the generalization capability of deep learning models. These
challenges undermine model performance when faced with varying acquisition
parameters, thereby severely restricting their clinical utility. In this study,
we present PRISM, a foundation model PRe-trained with large-scale
multI-Sequence MRI. We collected a total of 64 datasets from both public and
private sources, encompassing a wide range of whole-body anatomical structures,
with scans spanning diverse MRI sequences. Among them, 336,476 volumetric MRI
scans from 34 datasets (8 public and 26 private) were curated to construct the
largest multi-organ multi-sequence MRI pretraining corpus to date. We propose a
novel pretraining paradigm that disentangles anatomically invariant features
from sequence-specific variations in MRI, while preserving high-level semantic
representations. We established a benchmark comprising 44 downstream tasks,
including disease diagnosis, image segmentation, registration, progression
prediction, and report generation. These tasks were evaluated on 32 public
datasets and 5 private cohorts. PRISM consistently outperformed both
non-pretrained models and existing foundation models, achieving first-rank
results in 39 out of 44 downstream benchmarks with statistical significance
improvements. These results underscore its ability to learn robust and
generalizable representations across unseen data acquired under diverse MRI
protocols. PRISM provides a scalable framework for multi-sequence MRI analysis,
thereby enhancing the translational potential of AI in radiology. It delivers
consistent performance across diverse imaging protocols, reinforcing its
clinical applicability.

</details>


### [385] [HaDM-ST: Histology-Assisted Differential Modeling for Spatial Transcriptomics Generation](https://arxiv.org/abs/2508.07225)
*Xuepeng Liu,Zheng Jiang,Pinan Zhu,Hanyu Liu,Chao Li*

Main category: eess.IV

TL;DR: HaDM-ST提出了一种基于H&E图像和低分辨率空间转录组学的高分辨率ST生成框架，解决了特征提取、多模态对齐和基因特异性建模的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前空间转录组学(ST)的分辨率受限于平台，而现有方法在利用H&E图像时面临特征提取、对齐和基因特异性建模的三大挑战。

Method: HaDM-ST包括语义蒸馏网络提取H&E特征、空间对齐模块确保像素级对应，以及通道感知对抗学习器进行基因级建模。

Result: 在200个基因的实验中，HaDM-ST在空间保真度和基因一致性上优于现有方法。

Conclusion: HaDM-ST为高分辨率ST生成提供了有效解决方案，显著提升了预测质量。

Abstract: Spatial transcriptomics (ST) reveals spatial heterogeneity of gene
expression, yet its resolution is limited by current platforms. Recent methods
enhance resolution via H&E-stained histology, but three major challenges
persist: (1) isolating expression-relevant features from visually complex H&E
images; (2) achieving spatially precise multimodal alignment in diffusion-based
frameworks; and (3) modeling gene-specific variation across expression
channels. We propose HaDM-ST (Histology-assisted Differential Modeling for ST
Generation), a high-resolution ST generation framework conditioned on H&E
images and low-resolution ST. HaDM-ST includes: (i) a semantic distillation
network to extract predictive cues from H&E; (ii) a spatial alignment module
enforcing pixel-wise correspondence with low-resolution ST; and (iii) a
channel-aware adversarial learner for fine-grained gene-level modeling.
Experiments on 200 genes across diverse tissues and species show HaDM-ST
consistently outperforms prior methods, enhancing spatial fidelity and
gene-level coherence in high-resolution ST predictions.

</details>


### [386] [DiffVC-OSD: One-Step Diffusion-based Perceptual Neural Video Compression Framework](https://arxiv.org/abs/2508.07682)
*Wenzhuo Ma,Zhenzhong Chen*

Main category: eess.IV

TL;DR: DiffVC-OSD是一种基于一步扩散的感知神经视频压缩框架，通过单步扩散提升感知质量，显著提高解码速度并降低比特率。


<details>
  <summary>Details</summary>
Motivation: 传统多步扩散方法效率较低，DiffVC-OSD旨在通过一步扩散模型提升视频压缩的感知质量和效率。

Method: 提出一步扩散模型，结合时间上下文适配器和端到端微调策略，优化潜在表示和去噪过程。

Result: DiffVC-OSD在感知压缩性能上达到最优，解码速度提升20倍，比特率降低86.92%。

Conclusion: DiffVC-OSD通过一步扩散显著提升了视频压缩的效率和质量，为神经视频压缩提供了新方向。

Abstract: In this work, we first propose DiffVC-OSD, a One-Step Diffusion-based
Perceptual Neural Video Compression framework. Unlike conventional multi-step
diffusion-based methods, DiffVC-OSD feeds the reconstructed latent
representation directly into a One-Step Diffusion Model, enhancing perceptual
quality through a single diffusion step guided by both temporal context and the
latent itself. To better leverage temporal dependencies, we design a Temporal
Context Adapter that encodes conditional inputs into multi-level features,
offering more fine-grained guidance for the Denoising Unet. Additionally, we
employ an End-to-End Finetuning strategy to improve overall compression
performance. Extensive experiments demonstrate that DiffVC-OSD achieves
state-of-the-art perceptual compression performance, offers about 20$\times$
faster decoding and a 86.92\% bitrate reduction compared to the corresponding
multi-step diffusion-based variant.

</details>


### [387] [Anatomy-Aware Low-Dose CT Denoising via Pretrained Vision Models and Semantic-Guided Contrastive Learning](https://arxiv.org/abs/2508.07788)
*Runze Wang,Zeli Chen,Zhiyun Song,Wei Fang,Jiajin Zhang,Danyang Tu,Yuxing Tang,Minfeng Xu,Xianghua Ye,Le Lu,Dakai Jin*

Main category: eess.IV

TL;DR: ALDEN是一种基于深度学习的低剂量CT去噪方法，通过结合预训练视觉模型的语义特征与对抗和对比学习，显著提升了去噪效果并保留了解剖结构。


<details>
  <summary>Details</summary>
Motivation: 现有低剂量CT去噪方法常忽略解剖语义，导致去噪效果不佳。ALDEN旨在通过引入解剖感知机制解决这一问题。

Method: ALDEN结合了对抗学习和对比学习，利用解剖感知判别器和语义引导的对比学习模块，动态融合层次语义特征并保持解剖一致性。

Result: 在多个数据集上，ALDEN实现了最先进的性能，显著减少了过平滑问题，并在下游多器官分割任务中验证了其解剖感知能力。

Conclusion: ALDEN通过解剖感知机制显著提升了低剂量CT去噪效果，为医学影像分析提供了更可靠的解决方案。

Abstract: To reduce radiation exposure and improve the diagnostic efficacy of low-dose
computed tomography (LDCT), numerous deep learning-based denoising methods have
been developed to mitigate noise and artifacts. However, most of these
approaches ignore the anatomical semantics of human tissues, which may
potentially result in suboptimal denoising outcomes. To address this problem,
we propose ALDEN, an anatomy-aware LDCT denoising method that integrates
semantic features of pretrained vision models (PVMs) with adversarial and
contrastive learning. Specifically, we introduce an anatomy-aware discriminator
that dynamically fuses hierarchical semantic features from reference
normal-dose CT (NDCT) via cross-attention mechanisms, enabling tissue-specific
realism evaluation in the discriminator. In addition, we propose a
semantic-guided contrastive learning module that enforces anatomical
consistency by contrasting PVM-derived features from LDCT, denoised CT and
NDCT, preserving tissue-specific patterns through positive pairs and
suppressing artifacts via dual negative pairs. Extensive experiments conducted
on two LDCT denoising datasets reveal that ALDEN achieves the state-of-the-art
performance, offering superior anatomy preservation and substantially reducing
over-smoothing issue of previous work. Further validation on a downstream
multi-organ segmentation task (encompassing 117 anatomical structures) affirms
the model's ability to maintain anatomical awareness.

</details>


### [388] [Towards Human-AI Collaboration System for the Detection of Invasive Ductal Carcinoma in Histopathology Images](https://arxiv.org/abs/2508.07875)
*Shuo Han,Ahmed Karam Eldaly,Solomon Sunday Oyelere*

Main category: eess.IV

TL;DR: 提出了一种人机协作的深度学习系统，用于提高浸润性导管癌（IDC）的检测精度，结合AI初步诊断和专家反馈，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 早期准确诊断IDC对提高患者生存率至关重要，结合AI与医学专家知识可提升诊断效率和精度。

Method: 采用EfficientNetV2S模型进行初步诊断，通过人机反馈循环（HITL）修正误分类图像并优化模型。

Result: 模型初始准确率达93.65%，结合HITL系统后进一步提升了性能。

Conclusion: 人机协作方法为AI辅助医疗诊断提供了高效、高精度的新方向。

Abstract: Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer,
and early, accurate diagnosis is critical to improving patient survival rates
by guiding treatment decisions. Combining medical expertise with artificial
intelligence (AI) holds significant promise for enhancing the precision and
efficiency of IDC detection. In this work, we propose a human-in-the-loop
(HITL) deep learning system designed to detect IDC in histopathology images.
The system begins with an initial diagnosis provided by a high-performance
EfficientNetV2S model, offering feedback from AI to the human expert. Medical
professionals then review the AI-generated results, correct any misclassified
images, and integrate the revised labels into the training dataset, forming a
feedback loop from the human back to the AI. This iterative process refines the
model's performance over time. The EfficientNetV2S model itself achieves
state-of-the-art performance compared to existing methods in the literature,
with an overall accuracy of 93.65\%. Incorporating the human-in-the-loop system
further improves the model's accuracy using four experimental groups with
misclassified images. These results demonstrate the potential of this
collaborative approach to enhance AI performance in diagnostic systems. This
work contributes to advancing automated, efficient, and highly accurate methods
for IDC detection through human-AI collaboration, offering a promising
direction for future AI-assisted medical diagnostics.

</details>


### [389] [Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models](https://arxiv.org/abs/2508.07903)
*Johanna P. Müller,Anika Knupfer,Pedro Blöss,Edoardo Berardi Vittur,Bernhard Kainz,Jana Hutter*

Main category: eess.IV

TL;DR: 提出了一种新的扩散模型框架，用于生成高保真度的女性盆腔MRI图像，解决了现有模型在解剖学精确性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成女性盆腔图像时解剖学精确性不足，限制了其在妇科影像中的应用，尤其是在数据稀缺和隐私问题严重的领域。

Method: 结合了无条件与条件化的DDPMs和LDMs（2D和3D），生成解剖学一致的高保真合成图像。

Result: 生成的图像在感知和分布指标上表现优异，显著提升了诊断模型的准确性，并通过专家盲评验证了临床真实性。

Conclusion: 该框架为妇科影像提供了高质量合成数据，支持可重复研究，并推动了妇科AI的公平发展。

Abstract: Despite significant progress in generative modelling, existing diffusion
models often struggle to produce anatomically precise female pelvic images,
limiting their application in gynaecological imaging, where data scarcity and
patient privacy concerns are critical. To overcome these barriers, we introduce
a novel diffusion-based framework for uterine MRI synthesis, integrating both
unconditional and conditioned Denoising Diffusion Probabilistic Models (DDPMs)
and Latent Diffusion Models (LDMs) in 2D and 3D. Our approach generates
anatomically coherent, high fidelity synthetic images that closely mimic real
scans and provide valuable resources for training robust diagnostic models. We
evaluate generative quality using advanced perceptual and distributional
metrics, benchmarking against standard reconstruction methods, and demonstrate
substantial gains in diagnostic accuracy on a key classification task. A
blinded expert evaluation further validates the clinical realism of our
synthetic images. We release our models with privacy safeguards and a
comprehensive synthetic uterine MRI dataset to support reproducible research
and advance equitable AI in gynaecology.

</details>


### [390] [A Physics-Driven Neural Network with Parameter Embedding for Generating Quantitative MR Maps from Weighted Images](https://arxiv.org/abs/2508.08123)
*Lingjing Chen,Chengxiu Zhang,Yinqiao Yi,Yida Wang,Yang Song,Xu Yan,Shengfang Xu,Dalin Zhu,Mengqiu Cao,Yan Zhou,Chenglong Wang,Guang Yang*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的MRI定量图像合成方法，通过嵌入MRI序列参数（TR、TE、TI）提升准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统MRI定量图像合成方法在准确性和泛化性上存在不足，尤其是在面对未见过的病理区域时表现不佳。

Method: 采用物理驱动的神经网络，嵌入MRI序列参数，输入T1、T2和T2-FLAIR图像，合成T1、T2和质子密度定量图。

Result: 在内部和外部测试数据集上表现优异，PSNR超过34 dB，SSIM高于0.92，优于传统深度学习模型。

Conclusion: 该方法通过参数嵌入显著提升了定量MRI合成的性能和可靠性，具有加速qMRI和提升临床实用性的潜力。

Abstract: We propose a deep learning-based approach that integrates MRI sequence
parameters to improve the accuracy and generalizability of quantitative image
synthesis from clinical weighted MRI. Our physics-driven neural network embeds
MRI sequence parameters -- repetition time (TR), echo time (TE), and inversion
time (TI) -- directly into the model via parameter embedding, enabling the
network to learn the underlying physical principles of MRI signal formation.
The model takes conventional T1-weighted, T2-weighted, and T2-FLAIR images as
input and synthesizes T1, T2, and proton density (PD) quantitative maps.
Trained on healthy brain MR images, it was evaluated on both internal and
external test datasets. The proposed method achieved high performance with PSNR
values exceeding 34 dB and SSIM values above 0.92 for all synthesized parameter
maps. It outperformed conventional deep learning models in accuracy and
robustness, including data with previously unseen brain structures and lesions.
Notably, our model accurately synthesized quantitative maps for these unseen
pathological regions, highlighting its superior generalization capability.
Incorporating MRI sequence parameters via parameter embedding allows the neural
network to better learn the physical characteristics of MR signals,
significantly enhancing the performance and reliability of quantitative MRI
synthesis. This method shows great potential for accelerating qMRI and
improving its clinical utility.

</details>


### [391] [RedDino: A foundation model for red blood cell analysis](https://arxiv.org/abs/2508.08180)
*Luca Zedda,Andrea Loddo,Cecilia Di Ruberto,Carsten Marr*

Main category: eess.IV

TL;DR: RedDino是一种自监督基础模型，专为红细胞图像分析设计，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 红细胞形态分析对血液病诊断至关重要，但目前缺乏全面的AI解决方案。

Method: RedDino基于DINOv2框架，使用125万张红细胞图像训练，并通过线性探测和最近邻分类评估性能。

Result: RedDino在红细胞形状分类上表现优异，具有较强的特征表示和泛化能力。

Conclusion: RedDino通过捕捉细微形态特征，推动了可靠诊断工具的发展，代码和模型已开源。

Abstract: Red blood cells (RBCs) are essential to human health, and their precise
morphological analysis is important for diagnosing hematological disorders.
Despite the promise of foundation models in medical diagnostics, comprehensive
AI solutions for RBC analysis remain scarce. We present RedDino, a
self-supervised foundation model designed for RBC image analysis. RedDino uses
an RBC-specific adaptation of the DINOv2 self-supervised learning framework and
is trained on a curated dataset of 1.25 million RBC images from diverse
acquisition modalities and sources. Extensive evaluations show that RedDino
outperforms existing state-of-the-art models on RBC shape classification.
Through assessments including linear probing and nearest neighbor
classification, we confirm its strong feature representations and
generalization ability. Our main contributions are: (1) a foundation model
tailored for RBC analysis, (2) ablation studies exploring DINOv2 configurations
for RBC modeling, and (3) a detailed evaluation of generalization performance.
RedDino addresses key challenges in computational hematology by capturing
nuanced morphological features, advancing the development of reliable
diagnostic tools. The source code and pretrained models for RedDino are
available at https://github.com/Snarci/RedDino, and the pretrained models can
be downloaded from our Hugging Face collection at
https://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc

</details>
